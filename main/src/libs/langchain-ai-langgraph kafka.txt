Directory structure:
└── kafka/
    ├── __init__.py
    ├── default_async.py
    ├── default_sync.py
    ├── executor.py
    ├── orchestrator.py
    ├── py.typed
    ├── retry.py
    ├── serde.py
    └── types.py

================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/__init__.py
================================================



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/default_async.py
================================================
import aiokafka


class DefaultAsyncConsumer(aiokafka.AIOKafkaConsumer):
    pass


class DefaultAsyncProducer(aiokafka.AIOKafkaProducer):
    pass



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/default_sync.py
================================================
import concurrent.futures
from typing import Optional, Sequence

from kafka import KafkaConsumer, KafkaProducer
from langgraph.scheduler.kafka.types import ConsumerRecord, TopicPartition


class DefaultConsumer(KafkaConsumer):
    def getmany(
        self, timeout_ms: int, max_records: int
    ) -> dict[TopicPartition, Sequence[ConsumerRecord]]:
        return self.poll(timeout_ms=timeout_ms, max_records=max_records)

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()


class DefaultProducer(KafkaProducer):
    def send(
        self,
        topic: str,
        *,
        key: Optional[bytes] = None,
        value: Optional[bytes] = None,
    ) -> concurrent.futures.Future:
        fut = concurrent.futures.Future()
        kfut = super().send(topic, key=key, value=value)
        kfut.add_callback(fut.set_result)
        kfut.add_errback(fut.set_exception)
        return fut

    def __enter__(self):
        return self

    def __exit__(self, *args):
        self.close()



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/executor.py
================================================
import asyncio
import binascii
import concurrent.futures
import weakref
from collections.abc import Sequence
from contextlib import (
    AbstractAsyncContextManager,
    AbstractContextManager,
    AsyncExitStack,
    ExitStack,
)
from functools import partial
from typing import Any, Optional
from uuid import UUID

import orjson
from langchain_core.runnables import RunnableConfig
from typing_extensions import Self

import langgraph.scheduler.kafka.serde as serde
from langgraph.constants import CONFIG_KEY_DELEGATE, ERROR
from langgraph.errors import CheckpointNotLatest, GraphDelegate, TaskNotFound
from langgraph.pregel import Pregel
from langgraph.pregel.algo import checkpoint_null_version, prepare_single_task
from langgraph.pregel.executor import (
    AsyncBackgroundExecutor,
    BackgroundExecutor,
    Submit,
)
from langgraph.pregel.manager import AsyncChannelsManager, ChannelsManager
from langgraph.pregel.runner import PregelRunner
from langgraph.scheduler.kafka.retry import aretry, retry
from langgraph.scheduler.kafka.types import (
    AsyncConsumer,
    AsyncProducer,
    Consumer,
    ErrorMessage,
    MessageToExecutor,
    MessageToOrchestrator,
    Producer,
    Sendable,
    Topics,
)
from langgraph.types import LoopProtocol, PregelExecutableTask, RetryPolicy
from langgraph.utils.config import patch_configurable, recast_checkpoint_ns


class AsyncKafkaExecutor(AbstractAsyncContextManager):
    consumer: AsyncConsumer

    producer: AsyncProducer

    def __init__(
        self,
        graph: Pregel,
        topics: Topics,
        *,
        batch_max_n: int = 10,
        batch_max_ms: int = 1000,
        retry_policy: Optional[RetryPolicy] = None,
        consumer: Optional[AsyncConsumer] = None,
        producer: Optional[AsyncProducer] = None,
        **kwargs: Any,
    ) -> None:
        self.graph = graph
        self.topics = topics
        self.stack = AsyncExitStack()
        self.kwargs = kwargs
        self.consumer = consumer
        self.producer = producer
        self.batch_max_n = batch_max_n
        self.batch_max_ms = batch_max_ms
        self.retry_policy = retry_policy

    async def __aenter__(self) -> Self:
        loop = asyncio.get_running_loop()
        self.subgraphs = {
            k: v async for k, v in self.graph.aget_subgraphs(recurse=True)
        }
        if self.consumer is None:
            from langgraph.scheduler.kafka.default_async import DefaultAsyncConsumer

            self.consumer = await self.stack.enter_async_context(
                DefaultAsyncConsumer(
                    self.topics.executor,
                    auto_offset_reset="earliest",
                    group_id="executor",
                    enable_auto_commit=False,
                    loop=loop,
                    **self.kwargs,
                )
            )
        if self.producer is None:
            from langgraph.scheduler.kafka.default_async import DefaultAsyncProducer

            self.producer = await self.stack.enter_async_context(
                DefaultAsyncProducer(
                    loop=loop,
                    **self.kwargs,
                )
            )
        return self

    async def __aexit__(self, *args: Any) -> None:
        return await self.stack.__aexit__(*args)

    def __aiter__(self) -> Self:
        return self

    async def __anext__(self) -> Sequence[MessageToExecutor]:
        # wait for next batch
        recs = await self.consumer.getmany(
            timeout_ms=self.batch_max_ms, max_records=self.batch_max_n
        )
        msgs: list[MessageToExecutor] = [
            serde.loads(msg.value) for msgs in recs.values() for msg in msgs
        ]
        # process batch
        await asyncio.gather(*(self.each(msg) for msg in msgs))
        # commit offsets
        await self.consumer.commit()
        # return message
        return msgs

    async def each(self, msg: MessageToExecutor) -> None:
        try:
            await aretry(self.retry_policy, self.attempt, msg)
        except CheckpointNotLatest:
            pass
        except GraphDelegate as exc:
            for arg in exc.args:
                fut = await self.producer.send(
                    self.topics.orchestrator,
                    value=serde.dumps(
                        MessageToOrchestrator(
                            config=arg["config"],
                            input=orjson.Fragment(
                                self.graph.checkpointer.serde.dumps(arg["input"])
                            ),
                            finally_send=[
                                Sendable(topic=self.topics.executor, value=msg)
                            ],
                        )
                    ),
                    # use thread_id, checkpoint_ns as partition key
                    key=serde.dumps(
                        (
                            arg["config"]["configurable"]["thread_id"],
                            arg["config"]["configurable"].get("checkpoint_ns"),
                        )
                    ),
                )
                await fut
        except Exception as exc:
            fut = await self.producer.send(
                self.topics.error,
                value=serde.dumps(
                    ErrorMessage(
                        topic=self.topics.executor,
                        msg=msg,
                        error=repr(exc),
                    )
                ),
            )
            await fut

    async def attempt(self, msg: MessageToExecutor) -> None:
        # find graph
        if checkpoint_ns := msg["config"]["configurable"].get("checkpoint_ns"):
            # remove task_ids from checkpoint_ns
            recast = recast_checkpoint_ns(checkpoint_ns)
            # find the subgraph with the matching name
            if recast in self.subgraphs:
                graph = self.subgraphs[recast]
            else:
                raise ValueError(f"Subgraph {recast} not found")
        else:
            graph = self.graph
        # process message
        saved = await self.graph.checkpointer.aget_tuple(
            patch_configurable(msg["config"], {"checkpoint_id": None})
        )
        if saved is None:
            raise RuntimeError("Checkpoint not found")
        if saved.checkpoint["id"] != msg["config"]["configurable"]["checkpoint_id"]:
            raise CheckpointNotLatest()
        async with (
            AsyncChannelsManager(
                graph.channels,
                saved.checkpoint,
                LoopProtocol(
                    config=msg["config"],
                    store=self.graph.store,
                    step=saved.metadata["step"] + 1,
                    stop=saved.metadata["step"] + 2,
                ),
            ) as (channels, managed),
            AsyncBackgroundExecutor(msg["config"]) as submit,
        ):
            if task := await asyncio.to_thread(
                prepare_single_task,
                msg["task"]["path"],
                msg["task"]["id"],
                checkpoint=saved.checkpoint,
                pending_writes=saved.pending_writes or [],
                processes=graph.nodes,
                channels=channels,
                managed=managed,
                config=patch_configurable(msg["config"], {CONFIG_KEY_DELEGATE: True}),
                step=saved.metadata["step"] + 1,
                for_execution=True,
                checkpointer=self.graph.checkpointer,
                store=self.graph.store,
                checkpoint_id_bytes=binascii.unhexlify(
                    saved.checkpoint["id"].replace("-", "")
                ),
                checkpoint_null_version=checkpoint_null_version(saved.checkpoint),
            ):
                # execute task, saving writes
                put_writes = partial(self._put_writes, submit, msg["config"])
                runner = PregelRunner(
                    submit=weakref.ref(submit),
                    put_writes=weakref.ref(put_writes),
                    schedule_task=weakref.WeakMethod(self._schedule_task),
                )
                async for _ in runner.atick([task], reraise=False):
                    pass
            else:
                # task was not found
                await self.graph.checkpointer.aput_writes(
                    msg["config"], [(ERROR, TaskNotFound())], str(UUID(int=0))
                )
        # notify orchestrator
        fut = await self.producer.send(
            self.topics.orchestrator,
            value=serde.dumps(
                MessageToOrchestrator(
                    input=None,
                    config=msg["config"],
                    finally_send=msg.get("finally_send"),
                )
            ),
            # use thread_id, checkpoint_ns as partition key
            key=serde.dumps(
                (
                    msg["config"]["configurable"]["thread_id"],
                    msg["config"]["configurable"].get("checkpoint_ns"),
                )
            ),
        )
        await fut

    def _schedule_task(
        self,
        task: PregelExecutableTask,
        idx: int,
    ) -> None:
        # will be scheduled by orchestrator when executor finishes
        pass

    def _put_writes(
        self,
        submit: Submit,
        config: RunnableConfig,
        task_id: str,
        writes: list[tuple[str, Any]],
    ) -> None:
        return submit(self.graph.checkpointer.aput_writes, config, writes, task_id)


class KafkaExecutor(AbstractContextManager):
    consumer: Consumer

    producer: Producer

    def __init__(
        self,
        graph: Pregel,
        topics: Topics,
        *,
        batch_max_n: int = 10,
        batch_max_ms: int = 1000,
        retry_policy: Optional[RetryPolicy] = None,
        consumer: Optional[Consumer] = None,
        producer: Optional[Producer] = None,
        **kwargs: Any,
    ) -> None:
        self.graph = graph
        self.topics = topics
        self.stack = ExitStack()
        self.kwargs = kwargs
        self.consumer = consumer
        self.producer = producer
        self.batch_max_n = batch_max_n
        self.batch_max_ms = batch_max_ms
        self.retry_policy = retry_policy

    def __enter__(self) -> Self:
        self.subgraphs = dict(self.graph.get_subgraphs(recurse=True))
        self.submit = self.stack.enter_context(BackgroundExecutor({}))
        if self.consumer is None:
            from langgraph.scheduler.kafka.default_sync import DefaultConsumer

            self.consumer = self.stack.enter_context(
                DefaultConsumer(
                    self.topics.executor,
                    auto_offset_reset="earliest",
                    group_id="executor",
                    enable_auto_commit=False,
                    **self.kwargs,
                )
            )
        if self.producer is None:
            from langgraph.scheduler.kafka.default_sync import DefaultProducer

            self.producer = self.stack.enter_context(
                DefaultProducer(
                    **self.kwargs,
                )
            )
        return self

    def __exit__(self, *args: Any) -> None:
        return self.stack.__exit__(*args)

    def __iter__(self) -> Self:
        return self

    def __next__(self) -> Sequence[MessageToExecutor]:
        # wait for next batch
        recs = self.consumer.getmany(
            timeout_ms=self.batch_max_ms, max_records=self.batch_max_n
        )
        msgs: list[MessageToExecutor] = [
            serde.loads(msg.value) for msgs in recs.values() for msg in msgs
        ]
        # process batch
        concurrent.futures.wait(self.submit(self.each, msg) for msg in msgs)
        # commit offsets
        self.consumer.commit()
        # return message
        return msgs

    def each(self, msg: MessageToExecutor) -> None:
        try:
            retry(self.retry_policy, self.attempt, msg)
        except CheckpointNotLatest:
            pass
        except GraphDelegate as exc:
            for arg in exc.args:
                fut = self.producer.send(
                    self.topics.orchestrator,
                    value=serde.dumps(
                        MessageToOrchestrator(
                            config=arg["config"],
                            input=orjson.Fragment(
                                self.graph.checkpointer.serde.dumps(arg["input"])
                            ),
                            finally_send=[
                                Sendable(topic=self.topics.executor, value=msg)
                            ],
                        )
                    ),
                    # use thread_id, checkpoint_ns as partition key
                    key=serde.dumps(
                        (
                            arg["config"]["configurable"]["thread_id"],
                            arg["config"]["configurable"].get("checkpoint_ns"),
                        )
                    ),
                )
                fut.result()
        except Exception as exc:
            fut = self.producer.send(
                self.topics.error,
                value=serde.dumps(
                    ErrorMessage(
                        topic=self.topics.executor,
                        msg=msg,
                        error=repr(exc),
                    )
                ),
            )
            fut.result()

    def attempt(self, msg: MessageToExecutor) -> None:
        # find graph
        if checkpoint_ns := msg["config"]["configurable"].get("checkpoint_ns"):
            # remove task_ids from checkpoint_ns
            recast = recast_checkpoint_ns(checkpoint_ns)
            # find the subgraph with the matching name
            if recast in self.subgraphs:
                graph = self.subgraphs[recast]
            else:
                raise ValueError(f"Subgraph {recast} not found")
        else:
            graph = self.graph
        # process message
        saved = self.graph.checkpointer.get_tuple(
            patch_configurable(msg["config"], {"checkpoint_id": None})
        )
        if saved is None:
            raise RuntimeError("Checkpoint not found")
        if saved.checkpoint["id"] != msg["config"]["configurable"]["checkpoint_id"]:
            raise CheckpointNotLatest()
        with (
            ChannelsManager(
                graph.channels,
                saved.checkpoint,
                LoopProtocol(
                    config=msg["config"],
                    store=self.graph.store,
                    step=saved.metadata["step"] + 1,
                    stop=saved.metadata["step"] + 2,
                ),
            ) as (channels, managed),
            BackgroundExecutor({}) as submit,
        ):
            if task := prepare_single_task(
                msg["task"]["path"],
                msg["task"]["id"],
                checkpoint=saved.checkpoint,
                pending_writes=saved.pending_writes or [],
                processes=graph.nodes,
                channels=channels,
                managed=managed,
                config=patch_configurable(msg["config"], {CONFIG_KEY_DELEGATE: True}),
                step=saved.metadata["step"] + 1,
                for_execution=True,
                checkpointer=self.graph.checkpointer,
                checkpoint_id_bytes=binascii.unhexlify(
                    saved.checkpoint["id"].replace("-", "")
                ),
                checkpoint_null_version=checkpoint_null_version(saved.checkpoint),
            ):
                # execute task, saving writes
                put_writes = partial(self._put_writes, submit, msg["config"])
                runner = PregelRunner(
                    submit=weakref.ref(submit),
                    put_writes=weakref.ref(put_writes),
                    schedule_task=weakref.WeakMethod(self._schedule_task),
                )
                for _ in runner.tick([task], reraise=False):
                    pass
            else:
                # task was not found
                self.graph.checkpointer.put_writes(
                    msg["config"], [(ERROR, TaskNotFound())], str(UUID(int=0))
                )
        # notify orchestrator
        fut = self.producer.send(
            self.topics.orchestrator,
            value=serde.dumps(
                MessageToOrchestrator(
                    input=None,
                    config=msg["config"],
                    finally_send=msg.get("finally_send"),
                )
            ),
            # use thread_id, checkpoint_ns as partition key
            key=serde.dumps(
                (
                    msg["config"]["configurable"]["thread_id"],
                    msg["config"]["configurable"].get("checkpoint_ns"),
                )
            ),
        )
        fut.result()

    def _schedule_task(
        self,
        task: PregelExecutableTask,
        idx: int,
    ) -> None:
        # will be scheduled by orchestrator when executor finishes
        pass

    def _put_writes(
        self,
        submit: Submit,
        config: RunnableConfig,
        task_id: str,
        writes: list[tuple[str, Any]],
    ) -> None:
        return submit(self.graph.checkpointer.put_writes, config, writes, task_id)



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/orchestrator.py
================================================
import asyncio
import concurrent.futures
from contextlib import (
    AbstractAsyncContextManager,
    AbstractContextManager,
    AsyncExitStack,
    ExitStack,
)
from typing import Any, Optional

from langchain_core.runnables import ensure_config
from typing_extensions import Self

import langgraph.scheduler.kafka.serde as serde
from langgraph.constants import (
    CONFIG_KEY_DEDUPE_TASKS,
    CONFIG_KEY_ENSURE_LATEST,
    INTERRUPT,
    SCHEDULED,
)
from langgraph.errors import CheckpointNotLatest, GraphInterrupt
from langgraph.pregel import Pregel
from langgraph.pregel.executor import BackgroundExecutor, Submit
from langgraph.pregel.loop import AsyncPregelLoop, SyncPregelLoop
from langgraph.scheduler.kafka.retry import aretry, retry
from langgraph.scheduler.kafka.types import (
    AsyncConsumer,
    AsyncProducer,
    Consumer,
    ErrorMessage,
    ExecutorTask,
    MessageToExecutor,
    MessageToOrchestrator,
    Producer,
    Topics,
)
from langgraph.types import RetryPolicy
from langgraph.utils.config import patch_configurable, recast_checkpoint_ns


class AsyncKafkaOrchestrator(AbstractAsyncContextManager):
    consumer: AsyncConsumer

    producer: AsyncProducer

    def __init__(
        self,
        graph: Pregel,
        topics: Topics,
        batch_max_n: int = 10,
        batch_max_ms: int = 1000,
        retry_policy: Optional[RetryPolicy] = None,
        consumer: Optional[AsyncConsumer] = None,
        producer: Optional[AsyncProducer] = None,
        **kwargs: Any,
    ) -> None:
        self.graph = graph
        self.topics = topics
        self.stack = AsyncExitStack()
        self.kwargs = kwargs
        self.consumer = consumer
        self.producer = producer
        self.batch_max_n = batch_max_n
        self.batch_max_ms = batch_max_ms
        self.retry_policy = retry_policy

    async def __aenter__(self) -> Self:
        loop = asyncio.get_running_loop()
        self.subgraphs = {
            k: v async for k, v in self.graph.aget_subgraphs(recurse=True)
        }
        if self.consumer is None:
            from langgraph.scheduler.kafka.default_async import DefaultAsyncConsumer

            self.consumer = await self.stack.enter_async_context(
                DefaultAsyncConsumer(
                    self.topics.orchestrator,
                    auto_offset_reset="earliest",
                    group_id="orchestrator",
                    enable_auto_commit=False,
                    loop=loop,
                    **self.kwargs,
                )
            )
        if self.producer is None:
            from langgraph.scheduler.kafka.default_async import DefaultAsyncProducer

            self.producer = await self.stack.enter_async_context(
                DefaultAsyncProducer(
                    loop=loop,
                    **self.kwargs,
                )
            )
        return self

    async def __aexit__(self, *args: Any) -> None:
        return await self.stack.__aexit__(*args)

    def __aiter__(self) -> Self:
        return self

    async def __anext__(self) -> list[MessageToOrchestrator]:
        # wait for next batch
        recs = await self.consumer.getmany(
            timeout_ms=self.batch_max_ms, max_records=self.batch_max_n
        )
        # dedupe messages, eg. if multiple nodes finish around same time
        uniq = set(msg.value for msgs in recs.values() for msg in msgs)
        msgs: list[MessageToOrchestrator] = [serde.loads(msg) for msg in uniq]
        # process batch
        await asyncio.gather(*(self.each(msg) for msg in msgs))
        # commit offsets
        await self.consumer.commit()
        # return message
        return msgs

    async def each(self, msg: MessageToOrchestrator) -> None:
        try:
            await aretry(self.retry_policy, self.attempt, msg)
        except CheckpointNotLatest:
            pass
        except GraphInterrupt:
            pass
        except Exception as exc:
            fut = await self.producer.send(
                self.topics.error,
                value=serde.dumps(
                    ErrorMessage(
                        topic=self.topics.orchestrator,
                        msg=msg,
                        error=repr(exc),
                    )
                ),
            )
            await fut

    async def attempt(self, msg: MessageToOrchestrator) -> None:
        # find graph
        if checkpoint_ns := msg["config"]["configurable"].get("checkpoint_ns"):
            # remove task_ids from checkpoint_ns
            recast = recast_checkpoint_ns(checkpoint_ns)
            # find the subgraph with the matching name
            if recast in self.subgraphs:
                graph = self.subgraphs[recast]
            else:
                raise ValueError(f"Subgraph {recast} not found")
        else:
            graph = self.graph
        # process message
        async with AsyncPregelLoop(
            msg["input"],
            config=ensure_config(msg["config"]),
            stream=None,
            store=self.graph.store,
            checkpointer=self.graph.checkpointer,
            nodes=graph.nodes,
            specs=graph.channels,
            output_keys=graph.output_channels,
            stream_keys=graph.stream_channels,
            interrupt_after=graph.interrupt_after_nodes,
            interrupt_before=graph.interrupt_before_nodes,
        ) as loop:
            if loop.tick(input_keys=graph.input_channels):
                # wait for checkpoint to be saved
                if hasattr(loop, "_put_checkpoint_fut"):
                    await loop._put_checkpoint_fut
                # schedule any new tasks
                if new_tasks := [
                    t for t in loop.tasks.values() if not t.scheduled and not t.writes
                ]:
                    config = patch_configurable(
                        loop.config,
                        {
                            **loop.checkpoint_config["configurable"],
                            CONFIG_KEY_DEDUPE_TASKS: True,
                            CONFIG_KEY_ENSURE_LATEST: True,
                        },
                    )
                    # send messages to executor
                    futures = await asyncio.gather(
                        *(
                            self.producer.send(
                                self.topics.executor,
                                value=serde.dumps(
                                    MessageToExecutor(
                                        config=config,
                                        task=ExecutorTask(id=task.id, path=task.path),
                                        finally_send=msg.get("finally_send"),
                                    )
                                ),
                            )
                            for task in new_tasks
                        )
                    )
                    # wait for messages to be sent
                    await asyncio.gather(*futures)
                    # mark as scheduled
                    for task in new_tasks:
                        loop.put_writes(
                            task.id,
                            [
                                (
                                    SCHEDULED,
                                    max(
                                        loop.checkpoint["versions_seen"]
                                        .get(INTERRUPT, {})
                                        .values(),
                                        default=None,
                                    ),
                                )
                            ],
                        )
            elif loop.status == "done" and msg.get("finally_send"):
                # send any finally_send messages
                futs = await asyncio.gather(
                    *(
                        self.producer.send(
                            m["topic"],
                            value=serde.dumps(m["value"]) if m.get("value") else None,
                            key=serde.dumps(m["key"]) if m.get("key") else None,
                        )
                        for m in msg["finally_send"]
                    )
                )
                # wait for messages to be sent
                await asyncio.gather(*futs)


class KafkaOrchestrator(AbstractContextManager):
    consumer: Consumer

    producer: Producer

    submit: Submit

    def __init__(
        self,
        graph: Pregel,
        topics: Topics,
        batch_max_n: int = 10,
        batch_max_ms: int = 1000,
        retry_policy: Optional[RetryPolicy] = None,
        consumer: Optional[Consumer] = None,
        producer: Optional[Producer] = None,
        **kwargs: Any,
    ) -> None:
        self.graph = graph
        self.topics = topics
        self.stack = ExitStack()
        self.kwargs = kwargs
        self.consumer = consumer
        self.producer = producer
        self.batch_max_n = batch_max_n
        self.batch_max_ms = batch_max_ms
        self.retry_policy = retry_policy

    def __enter__(self) -> Self:
        self.subgraphs = dict(self.graph.get_subgraphs(recurse=True))
        self.submit = self.stack.enter_context(BackgroundExecutor({}))
        if self.consumer is None:
            from langgraph.scheduler.kafka.default_sync import DefaultConsumer

            self.consumer = self.stack.enter_context(
                DefaultConsumer(
                    self.topics.orchestrator,
                    auto_offset_reset="earliest",
                    group_id="orchestrator",
                    enable_auto_commit=False,
                    **self.kwargs,
                )
            )
        if self.producer is None:
            from langgraph.scheduler.kafka.default_sync import DefaultProducer

            self.producer = self.stack.enter_context(
                DefaultProducer(
                    **self.kwargs,
                )
            )
        return self

    def __exit__(self, *args: Any) -> None:
        return self.stack.__exit__(*args)

    def __iter__(self) -> Self:
        return self

    def __next__(self) -> list[MessageToOrchestrator]:
        # wait for next batch
        recs = self.consumer.getmany(
            timeout_ms=self.batch_max_ms, max_records=self.batch_max_n
        )
        # dedupe messages, eg. if multiple nodes finish around same time
        uniq = set(msg.value for msgs in recs.values() for msg in msgs)
        msgs: list[MessageToOrchestrator] = [serde.loads(msg) for msg in uniq]
        # process batch
        concurrent.futures.wait(self.submit(self.each, msg) for msg in msgs)
        # commit offsets
        self.consumer.commit()
        # return message
        return msgs

    def each(self, msg: MessageToOrchestrator) -> None:
        try:
            retry(self.retry_policy, self.attempt, msg)
        except CheckpointNotLatest:
            pass
        except GraphInterrupt:
            pass
        except Exception as exc:
            fut = self.producer.send(
                self.topics.error,
                value=serde.dumps(
                    ErrorMessage(
                        topic=self.topics.orchestrator,
                        msg=msg,
                        error=repr(exc),
                    )
                ),
            )
            fut.result()

    def attempt(self, msg: MessageToOrchestrator) -> None:
        # find graph
        if checkpoint_ns := msg["config"]["configurable"].get("checkpoint_ns"):
            # remove task_ids from checkpoint_ns
            recast = recast_checkpoint_ns(checkpoint_ns)
            # find the subgraph with the matching name
            if recast in self.subgraphs:
                graph = self.subgraphs[recast]
            else:
                raise ValueError(f"Subgraph {recast} not found")
        else:
            graph = self.graph
        # process message
        with SyncPregelLoop(
            msg["input"],
            config=ensure_config(msg["config"]),
            stream=None,
            store=self.graph.store,
            checkpointer=self.graph.checkpointer,
            nodes=graph.nodes,
            specs=graph.channels,
            output_keys=graph.output_channels,
            stream_keys=graph.stream_channels,
            interrupt_after=graph.interrupt_after_nodes,
            interrupt_before=graph.interrupt_before_nodes,
        ) as loop:
            if loop.tick(input_keys=graph.input_channels):
                # wait for checkpoint to be saved
                if hasattr(loop, "_put_checkpoint_fut"):
                    loop._put_checkpoint_fut.result()
                # schedule any new tasks
                if new_tasks := [
                    t for t in loop.tasks.values() if not t.scheduled and not t.writes
                ]:
                    config = patch_configurable(
                        loop.config,
                        {
                            **loop.checkpoint_config["configurable"],
                            CONFIG_KEY_DEDUPE_TASKS: True,
                            CONFIG_KEY_ENSURE_LATEST: True,
                        },
                    )
                    # send messages to executor
                    futures = [
                        self.producer.send(
                            self.topics.executor,
                            value=serde.dumps(
                                MessageToExecutor(
                                    config=config,
                                    task=ExecutorTask(id=task.id, path=task.path),
                                    finally_send=msg.get("finally_send"),
                                )
                            ),
                        )
                        for task in new_tasks
                    ]
                    # wait for messages to be sent
                    concurrent.futures.wait(futures)
                    # mark as scheduled
                    for task in new_tasks:
                        loop.put_writes(
                            task.id,
                            [
                                (
                                    SCHEDULED,
                                    max(
                                        loop.checkpoint["versions_seen"]
                                        .get(INTERRUPT, {})
                                        .values(),
                                        default=None,
                                    ),
                                )
                            ],
                        )
            elif loop.status == "done" and msg.get("finally_send"):
                # schedule any finally_send msgs
                futs = [
                    self.producer.send(
                        m["topic"],
                        value=serde.dumps(m["value"]) if m.get("value") else None,
                        key=serde.dumps(m["key"]) if m.get("key") else None,
                    )
                    for m in msg["finally_send"]
                ]
                # wait for messages to be sent
                concurrent.futures.wait(futs)



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/py.typed
================================================



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/retry.py
================================================
import asyncio
import logging
import random
import time
from typing import Awaitable, Callable, Optional

from typing_extensions import ParamSpec

from langgraph.types import RetryPolicy

logger = logging.getLogger(__name__)
P = ParamSpec("P")


def retry(
    retry_policy: Optional[RetryPolicy],
    func: Callable[P, None],
    *args: P.args,
    **kwargs: P.kwargs,
) -> None:
    """Run a task asynchronously with retries."""
    interval = retry_policy.initial_interval if retry_policy else 0
    attempts = 0
    while True:
        try:
            func(*args, **kwargs)
            # if successful, end
            break
        except Exception as exc:
            if retry_policy is None:
                raise
            # increment attempts
            attempts += 1
            # check if we should retry
            if callable(retry_policy.retry_on):
                if not retry_policy.retry_on(exc):
                    raise
            elif not isinstance(exc, retry_policy.retry_on):
                raise
            # check if we should give up
            if attempts >= retry_policy.max_attempts:
                raise
            # sleep before retrying
            interval = min(
                retry_policy.max_interval,
                interval * retry_policy.backoff_factor,
            )
            time.sleep(
                interval + random.uniform(0, 1) if retry_policy.jitter else interval
            )
            # log the retry
            logger.info(
                f"Retrying function {func} with {args} after {interval:.2f} seconds (attempt {attempts}) after {exc.__class__.__name__} {exc}",
                exc_info=exc,
            )


async def aretry(
    retry_policy: Optional[RetryPolicy],
    func: Callable[P, Awaitable[None]],
    *args: P.args,
    **kwargs: P.kwargs,
) -> None:
    """Run a task asynchronously with retries."""
    interval = retry_policy.initial_interval if retry_policy else 0
    attempts = 0
    while True:
        try:
            await func(*args, **kwargs)
            # if successful, end
            break
        except Exception as exc:
            if retry_policy is None:
                raise
            # increment attempts
            attempts += 1
            # check if we should retry
            if callable(retry_policy.retry_on):
                if not retry_policy.retry_on(exc):
                    raise
            elif not isinstance(exc, retry_policy.retry_on):
                raise
            # check if we should give up
            if attempts >= retry_policy.max_attempts:
                raise
            # sleep before retrying
            interval = min(
                retry_policy.max_interval,
                interval * retry_policy.backoff_factor,
            )
            await asyncio.sleep(
                interval + random.uniform(0, 1) if retry_policy.jitter else interval
            )
            # log the retry
            logger.info(
                f"Retrying function {func} with {args} after {interval:.2f} seconds (attempt {attempts}) after {exc.__class__.__name__} {exc}",
                exc_info=exc,
            )



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/serde.py
================================================
from typing import Any

import orjson

from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

SERIALIZER = JsonPlusSerializer()


def loads(v: bytes) -> Any:
    return SERIALIZER.loads(v)


def dumps(v: Any) -> bytes:
    return orjson.dumps(v, default=_default)


def _default(v: Any) -> Any:
    # things we don't know how to serialize (eg. functions) ignore
    return None



================================================
FILE: libs/scheduler-kafka/langgraph/scheduler/kafka/types.py
================================================
import asyncio
import concurrent.futures
from typing import Any, NamedTuple, Optional, Protocol, Sequence, TypedDict, Union

from langchain_core.runnables import RunnableConfig


class Topics(NamedTuple):
    orchestrator: str
    executor: str
    error: str


class Sendable(TypedDict):
    topic: str
    value: Optional[Any]
    key: Optional[Any]


class MessageToOrchestrator(TypedDict):
    input: Optional[dict[str, Any]]
    config: RunnableConfig
    finally_send: Optional[Sequence[Sendable]]


class ExecutorTask(TypedDict):
    id: Optional[str]
    path: tuple[Union[str, int], ...]


class MessageToExecutor(TypedDict):
    config: RunnableConfig
    task: ExecutorTask
    finally_send: Optional[Sequence[Sendable]]


class ErrorMessage(TypedDict):
    topic: str
    error: str
    msg: Union[MessageToExecutor, MessageToOrchestrator]


class TopicPartition(Protocol):
    topic: str
    partition: int


class ConsumerRecord(Protocol):
    topic: str
    "The topic this record is received from"
    partition: int
    "The partition from which this record is received"
    offset: int
    "The position of this record in the corresponding Kafka partition."
    timestamp: int
    "The timestamp of this record"
    timestamp_type: int
    "The timestamp type of this record"
    key: Optional[bytes]
    "The key (or `None` if no key is specified)"
    value: Optional[bytes]
    "The value"


class Consumer(Protocol):
    def getmany(
        self, timeout_ms: int, max_records: int
    ) -> dict[TopicPartition, Sequence[ConsumerRecord]]: ...

    def commit(self) -> None: ...


class AsyncConsumer(Protocol):
    async def getmany(
        self, timeout_ms: int, max_records: int
    ) -> dict[TopicPartition, Sequence[ConsumerRecord]]: ...

    async def commit(self) -> None: ...


class Producer(Protocol):
    def send(
        self,
        topic: str,
        *,
        key: Optional[bytes] = None,
        value: Optional[bytes] = None,
    ) -> concurrent.futures.Future: ...


class AsyncProducer(Protocol):
    async def send(
        self,
        topic: str,
        *,
        key: Optional[bytes] = None,
        value: Optional[bytes] = None,
    ) -> asyncio.Future: ...


