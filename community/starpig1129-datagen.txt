Directory structure:
└── starpig1129-datagen/
    ├── README.md
    ├── create_agent.py
    ├── LICENSE
    ├── load_cfg.py
    ├── logger.py
    ├── main.ipynb
    ├── main.py
    ├── requirements.txt
    ├── .env Example
    ├── agent/
    │   ├── code_agent.py
    │   ├── hypothesis_agent.py
    │   ├── note_agent.py
    │   ├── process_agent.py
    │   ├── quality_review_agent.py
    │   ├── refiner_agent.py
    │   ├── report_agent.py
    │   ├── search_agent.py
    │   └── visualization_agent.py
    ├── core/
    │   ├── language_models.py
    │   ├── node.py
    │   ├── router.py
    │   ├── state.py
    │   └── workflow.py
    └── tools/
        ├── __init__.py
        ├── basetool.py
        ├── FileEdit.py
        └── internet.py

================================================
FILE: README.md
================================================
# DATAGEN (Previously AI-Data-Analysis-MultiAgent)

![DATAGEN Banner](DATAGEN.jpg "DATAGEN Banner")

## About DATAGEN
DATAGEN is a powerful brand name that represents our vision of leveraging artificial intelligence technology for data generation and analysis. The name combines "DATA" and "GEN"(generation), perfectly embodying the core functionality of this project - automated data analysis and research through a multi-agent system.

Visit us at [DATAGEN Digital](https://datagen.digital/)(website under development) to learn more about our vision and services.

![System Architecture](Architecture.png)
## Overview

DATAGEN is an advanced AI-powered data analysis and research platform that utilizes multiple specialized agents to streamline tasks such as data analysis, visualization, and report generation. Our platform leverages cutting-edge technologies including LangChain, OpenAI's GPT models, and LangGraph to handle complex research processes, integrating diverse AI architectures for optimal performance.

## Key Features

### Intelligent Analysis Core
- **Advanced Hypothesis Engine**
  - AI-driven hypothesis generation and validation
  - Automated research direction optimization
  - Real-time hypothesis refinement
- **Enterprise Data Processing**
  - Robust data cleaning and transformation
  - Scalable analysis pipelines
  - Automated quality assurance
- **Dynamic Visualization Suite**
  - Interactive data visualization
  - Custom report generation
  - Automated insight extraction

### Advanced Technical Architecture
- **Multi-Agent Intelligence** 
  - Specialized agents for diverse tasks
  - Intelligent task distribution
  - Real-time coordination and optimization
- **Smart Memory Management**
  - State-of-the-art Note Taker agent
  - Efficient context retention system
  - Seamless workflow integration
- **Adaptive Processing Pipeline**
  - Dynamic workflow adjustment
  - Automated resource optimization
  - Real-time performance monitoring

## Why DATAGEN Stands Out

DATAGEN revolutionizes data analysis through its innovative multi-agent architecture and intelligent automation capabilities:

1. **Advanced Multi-Agent System**
   - Specialized agents working in harmony
   - Intelligent task distribution and coordination
   - Real-time adaptation to complex analysis requirements

2. **Smart Context Management**
   - Pioneering Note Taker agent for state tracking
   - Efficient memory utilization and context retention
   - Seamless integration across analysis phases

3. **Enterprise-Grade Performance**
   - Robust and scalable architecture
   - Consistent and reliable outcomes
   - Production-ready implementation

## System Requirements

- Python 3.10 or higher
- Jupyter Notebook environment

## Installation

1. Clone the repository:
```bash
git clone https://github.com/starpig1129/DATAGEN.git
```
2. Create and activate a Conda virtual environment:
```bash
conda create -n data_assistant python=3.10
conda activate data_assistant
```
3. Install dependencies:
```bash
pip install -r requirements.txt
```
4. Set up environment variables:
**Rename `.env Example` to `.env` and fill all the values**
```sh
# Your data storage path(required)
DATA_STORAGE_PATH =./data_storage/

# Anaconda installation path(required)
CONDA_PATH = /home/user/anaconda3

# Conda environment name(required)
CONDA_ENV = envname

# ChromeDriver executable path(required)
CHROMEDRIVER_PATH =./chromedriver-linux64/chromedriver

# Firecrawl API key (optional)
# Note: If this key is missing, query capabilities may be reduced
FIRECRAWL_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# OpenAI API key (required)
# Warning: This key is essential; the program will not run without it
OPENAI_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# LangChain API key (optional)
# Used for monitoring the processing
LANGCHAIN_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```
## Usage

### Using Jupyter Notebook

1. Start Jupyter Notebook:

2. Set YourDataName.csv in data_storage

3. Open the `main.ipynb` file.

4. Run all cells to initialize the system and create the workflow.

5. In the last cell, you can customize the research task by modifying the `userInput` variable.

6. Run the final few cells to execute the research process and view the results.

### Using Python Script

You can also run the system directly using main.py:

1. Place your data file (e.g., YourDataName.csv) in the data_storage directory

2. Run the script:
```bash
python main.py
```

3. By default, it will process 'OnlineSalesData.csv'. To analyze a different dataset, modify the user_input variable in the main() function of main.py:
```python
user_input = '''
datapath:YourDataName.csv
Use machine learning to perform data analysis and write complete graphical reports
'''
```

## Main Components

- `hypothesis_agent`: Generates research hypotheses
- `process_agent`: Supervises the entire research process
- `visualization_agent`: Creates data visualizations
- `code_agent`: Writes data analysis code
- `searcher_agent`: Conducts literature and web searches
- `report_agent`: Writes research reports
- `quality_review_agent`: Performs quality reviews
- `note_agent`: Records the research process

## Workflow

The system uses LangGraph to create a state graph that manages the entire research process. The workflow includes the following steps:

1. Hypothesis generation
2. Human choice (continue or regenerate hypothesis)
3. Processing (including data analysis, visualization, search, and report writing)
4. Quality review
5. Revision as needed

## Customization

You can customize the system behavior by modifying the agent creation and workflow definition in `main.ipynb`.

## Notes

- Ensure you have sufficient OpenAI API credits, as the system will make multiple API calls.
- The system may take some time to complete the entire research process, depending on the complexity of the task.
- **WARNING**: The agent system may modify the data being analyzed. It is highly recommended to backup your data before using this system.
## Current Issues and Solutions
1. OpenAI Internal Server Error (Error code: 500)
2. NoteTaker Efficiency Improvement
3. Overall Runtime Optimization
4. Refiner needs to be better
## Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## Strategic Partnership
[![CTL GROUP](https://img.shields.io/badge/DATAGEN-Strategic_Partner-blue)](https://datagen.digital/)

We are excited to announce our upcoming strategic partnership with CTL GROUP, an innovative AI-Powered Crypto Intelligence Platform currently in development. This collaboration will bring together advanced AI research capabilities with crypto market intelligence:

### Upcoming Partnership Features
- **AI Crypto Research Integration**
  - Automated market research and analysis system
  - Advanced whale tracking capabilities
  - Real-time sentiment analysis tools
  - Comprehensive trading insights and strategies

- **Platform Features** (Coming Soon)
  - State-of-the-art AI-powered crypto insights
  - Smart trading strategy development
  - Advanced whale & on-chain activity monitoring
  - Interactive community engagement tools

- **Token Integration Benefits** (Coming Soon)
  - Dynamic staking rewards system
  - Premium tools and features access
  - Innovative passive income opportunities
  - Exclusive platform privileges

The platform is currently under development. Follow our progress on [GitHub](https://github.com/ctlgroupdev).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=starpig1129/DATAGEN&type=Date)](https://star-history.com/#starpig1129/DATAGEN&Date)

## Other Projects
Here are some of my other notable projects:
### ShareLMAPI
ShareLMAPI is a local language model sharing API that uses FastAPI to provide interfaces, allowing different programs or device to share the same local model, thereby reducing resource consumption. It supports streaming generation and various model configuration methods.
- GitHub: [ShareLMAPI](https://github.com/starpig1129/ShareLMAPI)
### PigPig: Advanced Multi-modal LLM Discord Bot: 
A powerful Discord bot based on multi-modal Large Language Models (LLM), designed to interact with users through natural language. 
It combines advanced AI capabilities with practical features, offering a rich experience for Discord communities.
- GitHub: [ai-discord-bot-PigPig](https://github.com/starpig1129/ai-discord-bot-PigPig)



================================================
FILE: create_agent.py
================================================
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
from langchain_openai import ChatOpenAI
from typing import List
from langchain.tools import tool
import os
from logger import setup_logger

# Set up logger
logger = setup_logger()

@tool
def list_directory_contents(directory: str = './data_storage/') -> str:
    """
    List the contents of the specified directory.
    
    Args:
        directory (str): The path to the directory to list. Defaults to the data storage directory.
    
    Returns:
        str: A string representation of the directory contents.
    """
    try:
        logger.info(f"Listing contents of directory: {directory}")
        contents = os.listdir(directory)
        logger.debug(f"Directory contents: {contents}")
        return f"Directory contents :\n" + "\n".join(contents)
    except Exception as e:
        logger.error(f"Error listing directory contents: {str(e)}")
        return f"Error listing directory contents: {str(e)}"

def create_agent(
    llm: ChatOpenAI,
    tools: list[tool],
    system_message: str,
    team_members: list[str],
    working_directory: str = './data_storage/'
) -> AgentExecutor:
    """
    Create an agent with the given language model, tools, system message, and team members.
    
    Parameters:
        llm (ChatOpenAI): The language model to use for the agent.
        tools (list[tool]): A list of tools the agent can use.
        system_message (str): A message defining the agent's role and tasks.
        team_members (list[str]): A list of team member roles for collaboration.
        working_directory (str): The directory where the agent's data will be stored.
        
    Returns:
        AgentExecutor: An executor that manages the agent's task execution.
    """
    
    logger.info("Creating agent")

    # Ensure the ListDirectoryContents tool is available
    if list_directory_contents not in tools:
        tools.append(list_directory_contents)

    # Prepare the tool names and team members for the system prompt
    tool_names = ", ".join([tool.name for tool in tools])
    team_members_str = ", ".join(team_members)

    # List the initial contents of the working directory
    initial_directory_contents = list_directory_contents(working_directory)

    # Create the system prompt for the agent
    system_prompt = (
        "You are a specialized AI assistant in a data analysis team. "
        "Your role is to complete specific tasks in the research process. "
        "Use the provided tools to make progress on your task. "
        "If you can't fully complete a task, explain what you've done and what's needed next. "
        "Always aim for accurate and clear outputs. "
        f"You have access to the following tools: {tool_names}. "
        f"Your specific role: {system_message}\n"
        "Work autonomously according to your specialty, using the tools available to you. "
        "Do not ask for clarification. "
        "Your other team members (and other teams) will collaborate with you based on their specialties. "
        f"You are chosen for a reason! You are one of the following team members: {team_members_str}.\n"
        f"The initial contents of your working directory are:\n{initial_directory_contents}\n"
        "Use the ListDirectoryContents tool to check for updates in the directory contents when needed."
    )

    # Define the prompt structure with placeholders for dynamic content
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
        ("ai", "hypothesis: {hypothesis}"),
        ("ai", "process: {process}"),
        ("ai", "process_decision: {process_decision}"),
        ("ai", "visualization_state: {visualization_state}"),
        ("ai", "searcher_state: {searcher_state}"),
        ("ai", "code_state: {code_state}"),
        ("ai", "report_section: {report_section}"),
        ("ai", "quality_review: {quality_review}"),
        ("ai", "needs_revision: {needs_revision}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])

    # Create the agent using the defined prompt and tools
    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
    
    logger.info("Agent created successfully")
    
    # Return an executor to manage the agent's task execution
    return AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=False)


def create_supervisor(llm: ChatOpenAI, system_prompt: str, members: list[str]) -> AgentExecutor:
    # Log the start of supervisor creation
    logger.info("Creating supervisor")
    
    # Define options for routing, including FINISH and team members
    options = ["FINISH"] + members
    
    # Define the function for routing and task assignment
    function_def = {
        "name": "route",
        "description": "Select the next role and assign a task.",
        "parameters": {
            "title": "routeSchema",
            "type": "object",
            "properties": {
                "next": {
                    "title": "Next",
                    "anyOf": [
                        {"enum": options},
                    ],
                },
                "task": {
                    "title": "Task",
                    "type": "string",
                    "description": "The task to be performed by the selected agent"
                }
            },
            "required": ["next", "task"],
        },
    }
    
    # Create the prompt template
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="messages"),
            (
                "system",
                "Given the conversation above, who should act next? "
                "Or should we FINISH? Select one of: {options}. "
                "Additionally, specify the task that the selected role should perform."
            ),
        ]
    ).partial(options=str(options), team_members=", ".join(members))
    
    # Log successful creation of supervisor
    logger.info("Supervisor created successfully")
    
    # Return the chained operations
    return (
        prompt
        | llm.bind_functions(functions=[function_def], function_call="route")
        | JsonOutputFunctionsParser()
    )

from core.state import NoteState
from langchain.output_parsers import PydanticOutputParser

def create_note_agent(
    llm: ChatOpenAI,
    tools: list,
    system_prompt: str,
) -> AgentExecutor:
    """
    Create a Note Agent that updates the entire state.
    """
    logger.info("Creating note agent")
    parser = PydanticOutputParser(pydantic_object=NoteState)
    output_format = parser.get_format_instructions()
    escaped_output_format = output_format.replace("{", "{{").replace("}", "}}")
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt+"\n\nPlease format your response as a JSON object with the following structure:\n"+escaped_output_format),
        MessagesPlaceholder(variable_name="messages"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    logger.debug(f"Note agent prompt: {prompt}")
    agent = create_openai_functions_agent(llm=llm, tools=tools, prompt=prompt)
    logger.info("Note agent created successfully")
    return AgentExecutor.from_agent_and_tools(
        agent=agent, 
        tools=tools, 
        verbose=False,
    )

logger.info("Agent creation module initialized")


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 starpig1129

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
FILE: load_cfg.py
================================================
import os
from dotenv import load_dotenv
# Load environment variables
load_dotenv()

# Set up API keys and environment variables
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')
FIRECRAWL_API_KEY = os.getenv('FIRECRAWL_API_KEY')
# Get working directory from environment variable
WORKING_DIRECTORY = os.getenv('WORKING_DIRECTORY', './data_storage/')
# Get Conda-related paths from environment variables
CONDA_PATH = os.getenv('CONDA_PATH', '/home/user/anaconda3')
CONDA_ENV = os.getenv('CONDA_ENV', 'base')
# Get ChromeDriver
CHROMEDRIVER_PATH = os.getenv('CHROMEDRIVER_PATH', './chromedriver/chromedriver')


================================================
FILE: logger.py
================================================
import logging
# Configure logging
def setup_logger(log_file:str='agent.log'):
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    
    # Remove any existing handlers to prevent duplicates
    if logger.hasHandlers():
        logger.handlers.clear()

    # File handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # Formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    # Add handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger



================================================
FILE: main.ipynb
================================================
# Jupyter notebook converted to Python script.

import os
from logger import setup_logger
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from load_cfg import OPENAI_API_KEY,LANGCHAIN_API_KEY,WORKING_DIRECTORY
# Set environment variables
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "Multi-Agent Data Analysis System"

# Set up logger
logger = setup_logger()

# Initialize language models
try:
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=4096)
    power_llm = ChatOpenAI(model="gpt-4o", temperature=0.5, max_tokens=4096)
    json_llm = ChatOpenAI(
        model="gpt-4o",
        model_kwargs={"response_format": {"type": "json_object"}},
        temperature=0,
        max_tokens=4096
    )
    logger.info("Language models initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing language models: {str(e)}")
    raise

# Ensure working directory exists
if not os.path.exists(WORKING_DIRECTORY):
    os.makedirs(WORKING_DIRECTORY)
    logger.info(f"Created working directory: {WORKING_DIRECTORY}")

logger.info("Initialization complete.")

from core.state import State
from core.node import agent_node,human_choice_node,note_agent_node,human_review_node,refiner_node
from create_agent import create_agent,create_supervisor
from core.router import QualityReview_router,hypothesis_router,process_router

# Create state graph for the workflow
workflow = StateGraph(State)

members = ["Hypothesis","Process","Visualization", "Search", "Coder", "Report", "QualityReview","Refiner"]

from tools.internet import google_search,scrape_webpages_with_fallback
from tools.basetool import execute_code,execute_command
from tools.FileEdit import create_document,read_document,edit_document,collect_data
from langchain.agents import load_tools
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
hypothesis_agent = create_agent(
llm, 
[collect_data,wikipedia,google_search,scrape_webpages_with_fallback]+load_tools(["arxiv"],),
'''
As an esteemed expert in data analysis, your task is to formulate a set of research hypotheses and outline the steps to be taken based on the information table provided. Utilize statistics, machine learning, deep learning, and artificial intelligence in developing these hypotheses. Your hypotheses should be precise, achievable, professional, and innovative. To ensure the feasibility and uniqueness of your hypotheses, thoroughly investigate relevant information. For each hypothesis, include ample references to support your claims.

Upon analyzing the information table, you are required to:

1. Formulate research hypotheses that leverage statistics, machine learning, deep learning, and AI techniques.
2. Outline the steps involved in testing these hypotheses.
3. Verify the feasibility and uniqueness of each hypothesis through a comprehensive literature review.

At the conclusion of your analysis, present the complete research hypotheses, elaborate on their uniqueness and feasibility, and provide relevant references to support your assertions. Please answer in structured way to enhance readability.
Just answer a research hypothesis.
''',
members,WORKING_DIRECTORY)

process_agent = create_supervisor(
    power_llm,
    """
    You are a research supervisor responsible for overseeing and coordinating a comprehensive data analysis project, resulting in a complete and cohesive research report. Your primary tasks include:

    1. Validating and refining the research hypothesis to ensure it is clear, specific, and testable.
    2. Orchestrating a thorough data analysis process, with all code well-documented and reproducible.
    3. Compiling and refining a research report that includes:
        - Introduction
        - Hypothesis
        - Methodology
        - Results, accompanied by relevant visualizations
        - Discussion
        - Conclusion
        - References

    **Step-by-Step Process:**
    1. **Planning:** Define clear objectives and expected outcomes for each phase of the project.
    2. **Task Assignment:** Assign specific tasks to the appropriate agents ("Visualization," "Search," "Coder," "Report").
    3. **Review and Integration:** Critically review and integrate outputs from each agent, ensuring consistency, quality, and relevance.
    4. **Feedback:** Provide feedback and further instructions as needed to refine outputs.
    5. **Final Compilation:** Ensure all components are logically connected and meet high academic standards.

    **Agent Guidelines:**
    - **Visualization Agent:** Develop and explain data visualizations that effectively communicate key findings.
    - **Search Agent:** Collect and summarize relevant information, and compile a comprehensive list of references.
    - **Coder Agent:** Write and document efficient Python code for data analysis, ensuring that the code is clean and reproducible.
    - **Report Agent:** Draft, refine, and finalize the research report, integrating inputs from all agents and ensuring the narrative is clear and cohesive.

    **Workflow:**
    1. Plan the overall analysis and reporting process.
    2. Assign tasks to the appropriate agents and oversee their progress.
    3. Continuously review and integrate the outputs from each agent, ensuring that each contributes effectively to the final report.
    4. Adjust the analysis and reporting process based on emerging results and insights.
    5. Compile the final report, ensuring all sections are complete and well-integrated.

    **Completion Criteria:**
    Respond with "FINISH" only when:
    1. The hypothesis has been thoroughly tested and validated.
    2. The data analysis is complete, with all code documented and reproducible.
    3. All required visualizations have been created, properly labeled, and explained.
    4. The research report is comprehensive, logically structured, and includes all necessary sections.
    5. The reference list is complete and accurately cited.
    6. All components are cohesively integrated into a polished final report.

    Ensure that the final report delivers a clear, insightful analysis, addressing all aspects of the hypothesis and meeting the highest academic standards.
    """,
    ["Visualization", "Search", "Coder", "Report"],
)

visualization_agent = create_agent(
    llm, 
    [read_document, execute_code, execute_command],
    """
    You are a data visualization expert tasked with creating insightful visual representations of data. Your primary responsibilities include:
    
    1. Designing appropriate visualizations that clearly communicate data trends and patterns.
    2. Selecting the most suitable chart types (e.g., bar charts, scatter plots, heatmaps) for different data types and analytical purposes.
    3. Providing executable Python code (using libraries such as matplotlib, seaborn, or plotly) that generates these visualizations.
    4. Including well-defined titles, axis labels, legends, and saving the visualizations as files.
    5. Offering brief but clear interpretations of the visual findings.

    **File Saving Guidelines:**
    - Save all visualizations as files with descriptive and meaningful filenames.
    - Ensure filenames are structured to easily identify the content (e.g., 'sales_trends_2024.png' for a sales trend chart).
    - Confirm that the saved files are organized in the working directory, making them easy for other agents to locate and use.

    **Constraints:**
    - Focus solely on visualization tasks; do not perform data analysis or preprocessing.
    - Ensure all visual elements are suitable for the target audience, with attention to color schemes and design principles.
    - Avoid over-complicating visualizations; aim for clarity and simplicity.
    """,
    members,WORKING_DIRECTORY
    )

code_agent = create_agent(
    power_llm,
    [read_document,execute_code, execute_command],
    """
    You are an expert Python programmer specializing in data processing and analysis. Your main responsibilities include:

    1. Writing clean, efficient Python code for data manipulation, cleaning, and transformation.
    2. Implementing statistical methods and machine learning algorithms as needed.
    3. Debugging and optimizing existing code for performance improvements.
    4. Adhering to PEP 8 standards and ensuring code readability with meaningful variable and function names.

    Constraints:
    - Focus solely on data processing tasks; do not generate visualizations or write non-Python code.
    - Provide only valid, executable Python code, including necessary comments for complex logic.
    - Avoid unnecessary complexity; prioritize readability and efficiency.
    """,
    members,WORKING_DIRECTORY
)

searcher_agent= create_agent(
    llm,
    [create_document,read_document, collect_data,wikipedia,google_search,scrape_webpages_with_fallback]+load_tools(["arxiv"],),
    """
    You are a skilled research assistant responsible for gathering and summarizing relevant information. Your main tasks include:

    1. Conducting thorough literature reviews using academic databases and reputable online sources.
    2. Summarizing key findings in a clear, concise manner.
    3. Providing citations for all sources, prioritizing peer-reviewed and academically reputable materials.

    Constraints:
    - Focus exclusively on information retrieval and summarization; do not engage in data analysis or processing.
    - Present information in an organized format, with clear attributions to sources.
    - Evaluate the credibility of sources and prioritize high-quality, reliable information.
    """,
    members,WORKING_DIRECTORY
    )

report_agent = create_agent(
    power_llm, 
    [create_document, read_document, edit_document], 
    """
    You are an experienced scientific writer tasked with drafting comprehensive research reports. Your primary duties include:

    1. Clearly stating the research hypothesis and objectives in the introduction.
    2. Detailing the methodology used, including data collection and analysis techniques.
    3. Structuring the report into coherent sections (e.g., Introduction, Methodology, Results, Discussion, Conclusion).
    4. Synthesizing information from various sources into a unified narrative.
    5. Integrating relevant data visualizations and ensuring they are appropriately referenced and explained.

    Constraints:
    - Focus solely on report writing; do not perform data analysis or create visualizations.
    - Maintain an objective, academic tone throughout the report.
    - Cite all sources using APA style and ensure that all findings are supported by evidence.
    """,
    members,WORKING_DIRECTORY
)

quality_review_agent=create_agent(
    llm, 
    [create_document,read_document,edit_document], 
    '''
    You are a meticulous quality control expert responsible for reviewing and ensuring the high standard of all research outputs. Your tasks include:

    1. Critically evaluating the content, methodology, and conclusions of research reports.
    2. Checking for consistency, accuracy, and clarity in all documents.
    3. Identifying areas that need improvement or further elaboration.
    4. Ensuring adherence to scientific writing standards and ethical guidelines.

    After your review, if revisions are needed, respond with 'REVISION' as a prefix, set needs_revision=True, and provide specific feedback on parts that need improvement. If no revisions are necessary, respond with 'CONTINUE' as a prefix and set needs_revision=False.
    ''',
    members,WORKING_DIRECTORY
    )
                        

from create_agent import create_note_agent
note_agent=create_note_agent(
    json_llm, 
    [read_document], 
    '''
    You are a meticulous research process note-taker. Your main responsibility is to observe, summarize, and document the actions and findings of the research team. Your tasks include:

    1. Observing and recording key activities, decisions, and discussions among team members.
    2. Summarizing complex information into clear, concise, and accurate notes.
    3. Organizing notes in a structured format that ensures easy retrieval and reference.
    4. Highlighting significant insights, breakthroughs, challenges, or any deviations from the research plan.
    5. Responding only in JSON format to ensure structured documentation.

    Your output should be well-organized and easy to integrate with other project documentation.
    ''',
    )

refiner_agent = create_agent(
    power_llm,  
    [read_document, edit_document,create_document,collect_data,wikipedia,google_search,scrape_webpages_with_fallback]+load_tools(["arxiv"],),
    '''
    You are an expert AI report refiner tasked with optimizing and enhancing research reports. Your responsibilities include:

    1. Thoroughly reviewing the entire research report, focusing on content, structure, and readability.
    2. Identifying and emphasizing key findings, insights, and conclusions.
    3. Restructuring the report to improve clarity, coherence, and logical flow.
    4. Ensuring that all sections are well-integrated and support the primary research hypothesis.
    5. Condensing redundant or repetitive content while preserving essential details.
    6. Enhancing the overall readability, ensuring the report is engaging and impactful.

    Refinement Guidelines:
    - Maintain the scientific accuracy and integrity of the original content.
    - Ensure all critical points from the original report are preserved and clearly articulated.
    - Improve the logical progression of ideas and arguments.
    - Highlight the most significant results and their implications for the research hypothesis.
    - Ensure that the refined report aligns with the initial research objectives and hypothesis.

    After refining the report, submit it for final human review, ensuring it is ready for publication or presentation.
    ''',
    members,  
    WORKING_DIRECTORY
)

workflow.add_node("Hypothesis", lambda state: agent_node(state, hypothesis_agent, "hypothesis_agent"))
workflow.add_node("Process", lambda state: agent_node(state, process_agent, "process_agent"))
workflow.add_node("Visualization", lambda state: agent_node(state, visualization_agent, "visualization_agent"))
workflow.add_node("Search", lambda state: agent_node(state, searcher_agent, "searcher_agent"))
workflow.add_node("Coder", lambda state: agent_node(state, code_agent, "code_agent"))
workflow.add_node("Report", lambda state: agent_node(state, report_agent, "report_agent"))
workflow.add_node("QualityReview", lambda state: agent_node(state, quality_review_agent, "quality_review_agent"))
workflow.add_node("NoteTaker", lambda state: note_agent_node(state, note_agent, "note_agent"))
workflow.add_node("HumanChoice", human_choice_node)
workflow.add_node("HumanReview", human_review_node)
workflow.add_node("Refiner", lambda state: refiner_node(state, refiner_agent, "refiner_agent"))

from langgraph.graph import END, START

workflow.add_edge("Hypothesis", "HumanChoice")
workflow.add_conditional_edges(
    "HumanChoice",
    hypothesis_router,
    {
        "Hypothesis": "Hypothesis",
        "Process": "Process"
    }
)

workflow.add_conditional_edges(
    "Process",
    process_router,
    {
        "Coder": "Coder",
        "Search": "Search",
        "Visualization": "Visualization",
        "Report": "Report",
        "Process": "Process",
        "Refiner": "Refiner",
    }
)

for member in ["Visualization",'Search','Coder','Report']:
    workflow.add_edge(member, "QualityReview")

workflow.add_conditional_edges(
    "QualityReview",
    QualityReview_router,
    {
        'Visualization': "Visualization",
        'Search': "Search",
        'Coder': "Coder",
        'Report': "Report",
        'NoteTaker': "NoteTaker",
    }
)
workflow.add_edge("NoteTaker", "Process")

workflow.add_edge("Refiner", "HumanReview")

# Add an edge from HumanReview to Process
workflow.add_conditional_edges(
    "HumanReview",
    lambda state: "Process" if state and state.get("needs_revision", False) else "END",
    {
        "Process": "Process",
        "END": END
    }
)

from langgraph.checkpoint.memory import MemorySaver
workflow.add_edge(START, "Hypothesis")
memory = MemorySaver()
graph = workflow.compile()

from IPython.display import Image, display

#display(Image(graph.get_graph().draw_mermaid_png()))

#OnlineSalesData.csv is my data set for demo
userInput = '''
datapath:OnlineSalesData.csv
Use machine learning to perform data analysis and write complete graphical reports
'''
#Here you can describe how you want your data to be processed

from langchain_core.messages import HumanMessage
events = graph.stream(
    {
        "messages": [
            HumanMessage(
                content=userInput
            ),
        ],
        "hypothesis": "",
        "process_decision":"",
        "process": "",
        "visualization_state": "",
        "searcher_state": "",
        "code_state": "",
        "report_section": "",
        "quality_review": "",
        "needs_revision": False,
        "last_sender": "",
    },
    {"configurable": {"thread_id": "1"}, "recursion_limit": 3000},
    stream_mode="values",
    debug=False
)

def print_stream(stream):
    for s in stream:
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message,end='',flush=True)
        else:
            message.pretty_print()

print_stream(events)



================================================
FILE: main.py
================================================
#!/usr/bin/env python3

import os
from typing import Dict, Any
from logger import setup_logger
from langchain_core.messages import HumanMessage

from load_cfg import OPENAI_API_KEY, LANGCHAIN_API_KEY, WORKING_DIRECTORY
from core.workflow import WorkflowManager
from core.language_models import LanguageModelManager

class MultiAgentSystem:
    def __init__(self):
        self.logger = setup_logger()
        self.setup_environment()
        self.lm_manager = LanguageModelManager()
        self.workflow_manager = WorkflowManager(
            language_models=self.lm_manager.get_models(),
            working_directory=WORKING_DIRECTORY
        )

    def setup_environment(self):
        """Initialize environment variables"""
        os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
        os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = "Multi-Agent Data Analysis System"

        if not os.path.exists(WORKING_DIRECTORY):
            os.makedirs(WORKING_DIRECTORY)
            self.logger.info(f"Created working directory: {WORKING_DIRECTORY}")

    def run(self, user_input: str) -> None:
        """Run the multi-agent system with user input"""
        graph = self.workflow_manager.get_graph()
        events = graph.stream(
            {
                "messages": [HumanMessage(content=user_input)],
                "hypothesis": "",
                "process_decision": "",
                "process": "",
                "visualization_state": "",
                "searcher_state": "",
                "code_state": "",
                "report_section": "",
                "quality_review": "",
                "needs_revision": False,
                "last_sender": "",
            },
            {"configurable": {"thread_id": "1"}, "recursion_limit": 3000},
            stream_mode="values",
            debug=False
        )
        
        for event in events:
            message = event["messages"][-1]
            if isinstance(message, tuple):
                print(message, end='', flush=True)
            else:
                message.pretty_print()

def main():
    """Main entry point"""
    system = MultiAgentSystem()
    
    # Example usage
    user_input = '''
    datapath:OnlineSalesData.csv
    Use machine learning to perform data analysis and write complete graphical reports
    '''
    system.run(user_input)

if __name__ == "__main__":
    main()



================================================
FILE: requirements.txt
================================================
arxiv==2.1.3
beautifulsoup4==4.13.3
langchain==0.3.12
langchain-community==0.3.12
langchain-openai==0.2.13
langgraph==0.2.73
pandas==2.2.2
python-dotenv==1.0.1
selenium==4.27.1
wikipedia==1.4.0
firecrawl-py==0.0.20
openai==1.55.3



================================================
FILE: .env Example
================================================
# Your data storage path(required)
WORKING_DIRECTORY =./data_storage/

# Anaconda installation path(required)
CONDA_PATH = /home/user/anaconda3

# Conda environment name(required)
CONDA_ENV = data_assistant

# ChromeDriver executable path(required)
CHROMEDRIVER_PATH =./chromedriver-linux64/chromedriver

# Firecrawl API key (optional)
# Note: If this key is missing, query capabilities may be reduced
FIRECRAWL_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# OpenAI API key (required)
# Warning: This key is essential; the program will not run without it
OPENAI_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# LangChain API key (optional)
# Used for monitoring the processing
LANGCHAIN_API_KEY = XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX


================================================
FILE: agent/code_agent.py
================================================
from create_agent import create_agent
from tools.basetool import execute_code, execute_command
from tools.FileEdit import read_document

def create_code_agent(power_llm, members, working_directory):
    """Create the code agent"""
    tools = [read_document, execute_code, execute_command]
    system_prompt = """
    You are an expert Python programmer specializing in data processing and analysis. Your main responsibilities include:

    1. Writing clean, efficient Python code for data manipulation, cleaning, and transformation.
    2. Implementing statistical methods and machine learning algorithms as needed.
    3. Debugging and optimizing existing code for performance improvements.
    4. Adhering to PEP 8 standards and ensuring code readability with meaningful variable and function names.

    Constraints:
    - Focus solely on data processing tasks; do not generate visualizations or write non-Python code.
    - Provide only valid, executable Python code, including necessary comments for complex logic.
    - Avoid unnecessary complexity; prioritize readability and efficiency.
    """
    return create_agent(
        power_llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/hypothesis_agent.py
================================================
from create_agent import create_agent
from tools.FileEdit import collect_data
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from tools.internet import google_search, scrape_webpages_with_fallback
from langchain.agents import load_tools

def create_hypothesis_agent(llm, members, working_directory):
    """Create the hypothesis agent"""
    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
    base_tools = [
        collect_data, 
        wikipedia, 
        google_search, 
        scrape_webpages_with_fallback
    ] + load_tools(["arxiv"],)
    
    system_prompt = '''
    As an esteemed expert in data analysis, your task is to formulate a set of research hypotheses and outline the steps to be taken based on the information table provided. Utilize statistics, machine learning, deep learning, and artificial intelligence in developing these hypotheses. Your hypotheses should be precise, achievable, professional, and innovative. To ensure the feasibility and uniqueness of your hypotheses, thoroughly investigate relevant information. For each hypothesis, include ample references to support your claims.

    Upon analyzing the information table, you are required to:

    1. Formulate research hypotheses that leverage statistics, machine learning, deep learning, and AI techniques.
    2. Outline the steps involved in testing these hypotheses.
    3. Verify the feasibility and uniqueness of each hypothesis through a comprehensive literature review.

    At the conclusion of your analysis, present the complete research hypotheses, elaborate on their uniqueness and feasibility, and provide relevant references to support your assertions. Please answer in structured way to enhance readability.
    Just answer a research hypothesis.
    '''

    return create_agent(
        llm, 
        base_tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/note_agent.py
================================================
from create_agent import create_note_agent as base_create_note_agent
from tools.FileEdit import read_document

def create_note_agent(json_llm):
    """Create the note agent"""
    tools = [read_document]
    system_prompt = '''
    You are a meticulous research process note-taker. Your main responsibility is to observe, summarize, and document the actions and findings of the research team. Your tasks include:

    1. Observing and recording key activities, decisions, and discussions among team members.
    2. Summarizing complex information into clear, concise, and accurate notes.
    3. Organizing notes in a structured format that ensures easy retrieval and reference.
    4. Highlighting significant insights, breakthroughs, challenges, or any deviations from the research plan.
    5. Responding only in JSON format to ensure structured documentation.

    Your output should be well-organized and easy to integrate with other project documentation.
    '''
    return base_create_note_agent(
        json_llm,
        tools,
        system_prompt    
        )



================================================
FILE: agent/process_agent.py
================================================
from create_agent import create_supervisor

def create_process_agent(power_llm):
    """Create the process/supervisor agent"""
    system_prompt = """
    You are a research supervisor responsible for overseeing and coordinating a comprehensive data analysis project, resulting in a complete and cohesive research report. Your primary tasks include:

    1. Validating and refining the research hypothesis to ensure it is clear, specific, and testable.
    2. Orchestrating a thorough data analysis process, with all code well-documented and reproducible.
    3. Compiling and refining a research report that includes:
        - Introduction
        - Hypothesis
        - Methodology
        - Results, accompanied by relevant visualizations
        - Discussion
        - Conclusion
        - References

    **Step-by-Step Process:**
    1. **Planning:** Define clear objectives and expected outcomes for each phase of the project.
    2. **Task Assignment:** Assign specific tasks to the appropriate agents ("Visualization," "Search," "Coder," "Report").
    3. **Review and Integration:** Critically review and integrate outputs from each agent, ensuring consistency, quality, and relevance.
    4. **Feedback:** Provide feedback and further instructions as needed to refine outputs.
    5. **Final Compilation:** Ensure all components are logically connected and meet high academic standards.

    **Agent Guidelines:**
    - **Visualization Agent:** Develop and explain data visualizations that effectively communicate key findings.
    - **Search Agent:** Collect and summarize relevant information, and compile a comprehensive list of references.
    - **Coder Agent:** Write and document efficient Python code for data analysis, ensuring that the code is clean and reproducible.
    - **Report Agent:** Draft, refine, and finalize the research report, integrating inputs from all agents and ensuring the narrative is clear and cohesive.

    **Workflow:**
    1. Plan the overall analysis and reporting process.
    2. Assign tasks to the appropriate agents and oversee their progress.
    3. Continuously review and integrate the outputs from each agent, ensuring that each contributes effectively to the final report.
    4. Adjust the analysis and reporting process based on emerging results and insights.
    5. Compile the final report, ensuring all sections are complete and well-integrated.

    **Completion Criteria:**
    Respond with "FINISH" only when:
    1. The hypothesis has been thoroughly tested and validated.
    2. The data analysis is complete, with all code documented and reproducible.
    3. All required visualizations have been created, properly labeled, and explained.
    4. The research report is comprehensive, logically structured, and includes all necessary sections.
    5. The reference list is complete and accurately cited.
    6. All components are cohesively integrated into a polished final report.

    Ensure that the final report delivers a clear, insightful analysis, addressing all aspects of the hypothesis and meeting the highest academic standards.
    """
    
    member = ["Visualization", "Search", "Coder", "Report"]
    return create_supervisor(
        power_llm,
        system_prompt,
        member
    )



================================================
FILE: agent/quality_review_agent.py
================================================
from create_agent import create_agent
from tools.FileEdit import create_document, read_document, edit_document

def create_quality_review_agent(llm, members, working_directory):
    """Create the quality review agent"""
    tools = [create_document, read_document, edit_document]
    system_prompt = '''
    You are a meticulous quality control expert responsible for reviewing and ensuring the high standard of all research outputs. Your tasks include:

    1. Critically evaluating the content, methodology, and conclusions of research reports.
    2. Checking for consistency, accuracy, and clarity in all documents.
    3. Identifying areas that need improvement or further elaboration.
    4. Ensuring adherence to scientific writing standards and ethical guidelines.

    After your review, if revisions are needed, respond with 'REVISION' as a prefix, set needs_revision=True, and provide specific feedback on parts that need improvement. If no revisions are necessary, respond with 'CONTINUE' as a prefix and set needs_revision=False.
    '''
    return create_agent(
        llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/refiner_agent.py
================================================
from create_agent import create_agent
from tools.FileEdit import create_document, read_document, edit_document
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from tools.internet import google_search, scrape_webpages_with_fallback
from langchain.agents import load_tools

def create_refiner_agent(power_llm, members, working_directory):
    """Create the refiner agent"""
    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
    tools = [
        create_document, 
        read_document, 
        edit_document,
        wikipedia, 
        google_search, 
        scrape_webpages_with_fallback
    ] + load_tools(["arxiv"],)
    
    system_prompt = '''
    You are an expert AI report refiner tasked with optimizing and enhancing research reports. Your responsibilities include:

    1. Thoroughly reviewing the entire research report, focusing on content, structure, and readability.
    2. Identifying and emphasizing key findings, insights, and conclusions.
    3. Restructuring the report to improve clarity, coherence, and logical flow.
    4. Ensuring that all sections are well-integrated and support the primary research hypothesis.
    5. Condensing redundant or repetitive content while preserving essential details.
    6. Enhancing the overall readability, ensuring the report is engaging and impactful.

    Refinement Guidelines:
    - Maintain the scientific accuracy and integrity of the original content.
    - Ensure all critical points from the original report are preserved and clearly articulated.
    - Improve the logical progression of ideas and arguments.
    - Highlight the most significant results and their implications for the research hypothesis.
    - Ensure that the refined report aligns with the initial research objectives and hypothesis.

    After refining the report, submit it for final human review, ensuring it is ready for publication or presentation.
    '''
    return create_agent(
        power_llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/report_agent.py
================================================
from create_agent import create_agent
from tools.FileEdit import create_document, read_document, edit_document

def create_report_agent(power_llm, members, working_directory):
    """Create the report agent"""
    tools = [create_document, read_document, edit_document]
    
    system_prompt = """
    You are an experienced scientific writer tasked with drafting comprehensive research reports. Your primary duties include:

    1. Clearly stating the research hypothesis and objectives in the introduction.
    2. Detailing the methodology used, including data collection and analysis techniques.
    3. Structuring the report into coherent sections (e.g., Introduction, Methodology, Results, Discussion, Conclusion).
    4. Synthesizing information from various sources into a unified narrative.
    5. Integrating relevant data visualizations and ensuring they are appropriately referenced and explained.

    Constraints:
    - Focus solely on report writing; do not perform data analysis or create visualizations.
    - Maintain an objective, academic tone throughout the report.
    - Cite all sources using APA style and ensure that all findings are supported by evidence.
    """
    return create_agent(
        power_llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/search_agent.py
================================================
from create_agent import create_agent
from tools.FileEdit import create_document, read_document, collect_data
from langchain_community.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper
from tools.internet import google_search, scrape_webpages_with_fallback
from langchain.agents import load_tools

def create_search_agent(llm, members, working_directory):
    """Create the search agent"""
    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
    tools = [
        create_document, 
        read_document, 
        collect_data, 
        wikipedia, 
        google_search, 
        scrape_webpages_with_fallback
    ] + load_tools(["arxiv"],)
    
    system_prompt = """
    You are a skilled research assistant responsible for gathering and summarizing relevant information. Your main tasks include:

    1. Conducting thorough literature reviews using academic databases and reputable online sources.
    2. Summarizing key findings in a clear, concise manner.
    3. Providing citations for all sources, prioritizing peer-reviewed and academically reputable materials.

    Constraints:
    - Focus exclusively on information retrieval and summarization; do not engage in data analysis or processing.
    - Present information in an organized format, with clear attributions to sources.
    - Evaluate the credibility of sources and prioritize high-quality, reliable information.
    """
    return create_agent(
        llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: agent/visualization_agent.py
================================================
from create_agent import create_agent
from tools.basetool import execute_code, execute_command
from tools.FileEdit import read_document

def create_visualization_agent(llm, members, working_directory):
    """Create the visualization agent"""
    tools = [read_document, execute_code, execute_command]
    
    system_prompt = """
    You are a data visualization expert tasked with creating insightful visual representations of data. Your primary responsibilities include:
    
    1. Designing appropriate visualizations that clearly communicate data trends and patterns.
    2. Selecting the most suitable chart types (e.g., bar charts, scatter plots, heatmaps) for different data types and analytical purposes.
    3. Providing executable Python code (using libraries such as matplotlib, seaborn, or plotly) that generates these visualizations.
    4. Including well-defined titles, axis labels, legends, and saving the visualizations as files.
    5. Offering brief but clear interpretations of the visual findings.

    **File Saving Guidelines:**
    - Save all visualizations as files with descriptive and meaningful filenames.
    - Ensure filenames are structured to easily identify the content (e.g., 'sales_trends_2024.png' for a sales trend chart).
    - Confirm that the saved files are organized in the working directory, making them easy for other agents to locate and use.

    **Constraints:**
    - Focus solely on visualization tasks; do not perform data analysis or preprocessing.
    - Ensure all visual elements are suitable for the target audience, with attention to color schemes and design principles.
    - Avoid over-complicating visualizations; aim for clarity and simplicity.
    """
    return create_agent(
        llm,
        tools,
        system_prompt,
        members,
        working_directory
    )



================================================
FILE: core/language_models.py
================================================
from langchain_openai import ChatOpenAI
from logger import setup_logger

class LanguageModelManager:
    def __init__(self):
        """Initialize the language model manager"""
        self.logger = setup_logger()
        self.llm = None
        self.power_llm = None
        self.json_llm = None
        self.initialize_llms()

    def initialize_llms(self):
        """Initialize language models"""
        try:
            self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=4096)
            self.power_llm = ChatOpenAI(model="gpt-4o", temperature=0.5, max_tokens=4096)
            self.json_llm = ChatOpenAI(
                model="gpt-4o",
                model_kwargs={"response_format": {"type": "json_object"}},
                temperature=0,
                max_tokens=4096
            )
            self.logger.info("Language models initialized successfully.")
        except Exception as e:
            self.logger.error(f"Error initializing language models: {str(e)}")
            raise

    def get_models(self):
        """Return all initialized language models"""
        return {
            "llm": self.llm,
            "power_llm": self.power_llm,
            "json_llm": self.json_llm
        }



================================================
FILE: core/node.py
================================================
from typing import Any
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage,ToolMessage
from openai import InternalServerError
from core.state import State
import logging
import json
import re
import os
from pathlib import Path
from langchain.agents import AgentExecutor
# Set up logger
logger = logging.getLogger(__name__)

def agent_node(state: State, agent: AgentExecutor, name: str) -> State:
    """
    Process an agent's action and update the state accordingly.
    """
    logger.info(f"Processing agent: {name}")
    try:
        result = agent.invoke(state)
        logger.debug(f"Agent {name} result: {result}")
        
        output = result["output"] if isinstance(result, dict) and "output" in result else str(result)
        
        ai_message = AIMessage(content=output, name=name)
        state["messages"].append(ai_message)
        state["sender"] = name
        
        if name == "hypothesis_agent" and not state["hypothesis"]:
            state["hypothesis"] = ai_message
            logger.info("Hypothesis updated")
        elif name == "process_agent":
            state["process_decision"] = ai_message
            logger.info("Process decision updated")
        elif name == "visualization_agent":
            state["visualization_state"] = ai_message
            logger.info("Visualization state updated")
        elif name == "searcher_agent":
            state["searcher_state"] = ai_message
            logger.info("Searcher state updated")
        elif name == "report_agent":
            state["report_section"] = ai_message
            logger.info("Report section updated")
        elif name == "quality_review_agent":
            state["quality_review"] = ai_message
            state["needs_revision"] = "revision needed" in output.lower()
            logger.info(f"Quality review updated. Needs revision: {state['needs_revision']}")
        
        logger.info(f"Agent {name} processing completed")
        return state
    except Exception as e:
        logger.error(f"Error occurred while processing agent {name}: {str(e)}", exc_info=True)
        error_message = AIMessage(content=f"Error: {str(e)}", name=name)
        return {"messages": [error_message]}

def human_choice_node(state: State) -> State:
    """
    Handle human input to choose the next step in the process.
    If regenerating hypothesis, prompt for specific areas to modify.
    """
    logger.info("Prompting for human choice")
    print("Please choose the next step:")
    print("1. Regenerate hypothesis")
    print("2. Continue the research process")
    
    while True:
        choice = input("Please enter your choice (1 or 2): ")
        if choice in ["1", "2"]:
            break
        logger.warning(f"Invalid input received: {choice}")
        print("Invalid input, please try again.")
    
    if choice == "1":
        modification_areas = input("Please specify which parts of the hypothesis you want to modify: ")
        content = f"Regenerate hypothesis. Areas to modify: {modification_areas}"
        state["hypothesis"] = ""
        state["modification_areas"] = modification_areas
        logger.info("Hypothesis cleared for regeneration")
        logger.info(f"Areas to modify: {modification_areas}")
    else:
        content = "Continue the research process"
        state["process"] = "Continue the research process"
        logger.info("Continuing research process")
    
    human_message = HumanMessage(content=content)
    
    state["messages"].append(human_message)
    state["sender"] = 'human'
    
    logger.info("Human choice processed")
    return state

def create_message(message: dict[str], name: str) -> BaseMessage:
    """
    Create a BaseMessage object based on the message type.
    """
    content = message.get("content", "")
    message_type = message.get("type", "").lower()
    
    logger.debug(f"Creating message of type {message_type} for {name}")
    return HumanMessage(content=content) if message_type == "human" else AIMessage(content=content, name=name)

def note_agent_node(state: State, agent: AgentExecutor, name: str) -> State:
    """
    Process the note agent's action and update the entire state.
    """
    logger.info(f"Processing note agent: {name}")
    try:
        current_messages = state.get("messages", [])
        
        head_messages, tail_messages = [], []
        
        if len(current_messages) > 6:
            head_messages = current_messages[:2] 
            tail_messages = current_messages[-2:]
            state = {**state, "messages": current_messages[2:-2]}
            logger.debug("Trimmed messages for processing")
        
        result = agent.invoke(state)
        logger.debug(f"Note agent {name} result: {result}")
        output = result["output"] if isinstance(result, dict) and "output" in result else str(result)

        cleaned_output = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', output)
        parsed_output = json.loads(cleaned_output)
        logger.debug(f"Parsed output: {parsed_output}")

        new_messages = [create_message(msg, name) for msg in parsed_output.get("messages", [])]
        
        messages = new_messages if new_messages else current_messages
        
        combined_messages = head_messages + messages + tail_messages
        
        updated_state: State = {
            "messages": combined_messages,
            "hypothesis": str(parsed_output.get("hypothesis", state.get("hypothesis", ""))),
            "process": str(parsed_output.get("process", state.get("process", ""))),
            "process_decision": str(parsed_output.get("process_decision", state.get("process_decision", ""))),
            "visualization_state": str(parsed_output.get("visualization_state", state.get("visualization_state", ""))),
            "searcher_state": str(parsed_output.get("searcher_state", state.get("searcher_state", ""))),
            "code_state": str(parsed_output.get("code_state", state.get("code_state", ""))),
            "report_section": str(parsed_output.get("report_section", state.get("report_section", ""))),
            "quality_review": str(parsed_output.get("quality_review", state.get("quality_review", ""))),
            "needs_revision": bool(parsed_output.get("needs_revision", state.get("needs_revision", False))),
            "sender": 'note_agent'
        }
        
        logger.info("Updated state successfully")
        return updated_state

    except json.JSONDecodeError as e:
        logger.error(f"JSON decode error: {e}", exc_info=True)
        return _create_error_state(state, AIMessage(content=f"Error parsing output: {output}", name=name), name, "JSON decode error")

    except InternalServerError as e:
        logger.error(f"OpenAI Internal Server Error: {e}", exc_info=True)
        return _create_error_state(state, AIMessage(content=f"OpenAI Error: {str(e)}", name=name), name, "OpenAI error")

    except Exception as e:
        logger.error(f"Unexpected error in note_agent_node: {e}", exc_info=True)
        return _create_error_state(state, AIMessage(content=f"Unexpected error: {str(e)}", name=name), name, "Unexpected error")

def _create_error_state(state: State, error_message: AIMessage, name: str, error_type: str) -> State:
    """
    Create an error state when an exception occurs.
    """
    logger.info(f"Creating error state for {name}: {error_type}")
    error_state:State = {
            "messages": state.get("messages", []) + [error_message],
            "hypothesis": str(state.get("hypothesis", "")),
            "process": str(state.get("process", "")),
            "process_decision": str(state.get("process_decision", "")),
            "visualization_state": str(state.get("visualization_state", "")),
            "searcher_state": str(state.get("searcher_state", "")),
            "code_state": str(state.get("code_state", "")),
            "report_section": str(state.get("report_section", "")),
            "quality_review": str(state.get("quality_review", "")),
            "needs_revision": bool(state.get("needs_revision", False)),
            "sender": 'note_agent'
        }
    return error_state

def human_review_node(state: State) -> State:
    """
    Display current state to the user and update the state based on user input.
    Includes error handling for robustness.
    """
    try:
        print("Current research progress:")
        print(state)
        print("\nDo you need additional analysis or modifications?")
        
        while True:
            user_input = input("Enter 'yes' to continue analysis, or 'no' to end the research: ").lower()
            if user_input in ['yes', 'no']:
                break
            print("Invalid input. Please enter 'yes' or 'no'.")
        
        if user_input == 'yes':
            while True:
                additional_request = input("Please enter your additional analysis request: ").strip()
                if additional_request:
                    state["messages"].append(HumanMessage(content=additional_request))
                    state["needs_revision"] = True
                    break
                print("Request cannot be empty. Please try again.")
        else:
            state["needs_revision"] = False
        
        state["sender"] = "human"
        logger.info("Human review completed successfully.")
        return state
    
    except KeyboardInterrupt:
        logger.warning("Human review interrupted by user.")
        return None
    
    except Exception as e:
        logger.error(f"An error occurred during human review: {str(e)}", exc_info=True)
        return None
    
def refiner_node(state: State, agent: AgentExecutor, name: str) -> State:
    """
    Read MD file contents and PNG file names from the specified storage path,
    add them as report materials to a new message,
    then process with the agent and update the original state.
    If token limit is exceeded, use only MD file names instead of full content.
    """
    try:
        # Get storage path
        storage_path = Path(os.getenv('STORAGE_PATH', './data_storage/'))
        
        # Collect materials
        materials = []
        md_files = list(storage_path.glob("*.md"))
        png_files = list(storage_path.glob("*.png"))
        
        # Process MD files
        for md_file in md_files:
            with open(md_file, "r", encoding="utf-8") as f:
                materials.append(f"MD file '{md_file.name}':\n{f.read()}")
        
        # Process PNG files
        materials.extend(f"PNG file: '{png_file.name}'" for png_file in png_files)
        
        # Combine materials
        combined_materials = "\n\n".join(materials)
        report_content = f"Report materials:\n{combined_materials}"
        
        # Create refiner state
        refiner_state = state.copy()
        refiner_state["messages"] = [BaseMessage(content=report_content)]
        
        try:
            # Attempt to invoke agent with full content
            result = agent.invoke(refiner_state)
        except Exception as token_error:
            # If token limit is exceeded, retry with only MD file names
            logger.warning("Token limit exceeded. Retrying with MD file names only.")
            md_file_names = [f"MD file: '{md_file.name}'" for md_file in md_files]
            png_file_names = [f"PNG file: '{png_file.name}'" for png_file in png_files]
            
            simplified_materials = "\n".join(md_file_names + png_file_names)
            simplified_report_content = f"Report materials (file names only):\n{simplified_materials}"
            
            refiner_state["messages"] = [BaseMessage(content=simplified_report_content)]
            result = agent.invoke(refiner_state)
        
        # Update original state
        state["messages"].append(AIMessage(content=result))
        state["sender"] = name
        
        logger.info("Refiner node processing completed")
        return state
    except Exception as e:
        logger.error(f"Error occurred while processing refiner node: {str(e)}", exc_info=True)
        state["messages"].append(AIMessage(content=f"Error: {str(e)}", name=name))
        return state
    
logger.info("Agent processing module initialized")


================================================
FILE: core/router.py
================================================
from core.state import State
from typing import Literal, Union, Dict, List, Optional
from langchain_core.messages import AIMessage
import logging
import json

# Set up logger
logger = logging.getLogger(__name__)

# Define types for node routing
NodeType = Literal['Visualization', 'Search', 'Coder', 'Report', 'Process', 'NoteTaker', 'Hypothesis', 'QualityReview']
ProcessNodeType = Literal['Coder', 'Search', 'Visualization', 'Report', 'Process', 'Refiner']

def hypothesis_router(state: State) -> NodeType:
    """
    Route based on the presence of a hypothesis in the state.

    Args:
        state (State): The current state of the system.

    Returns:
        NodeType: 'Hypothesis' if no hypothesis exists, otherwise 'Process'.
    """
    logger.info("Entering hypothesis_router")
    hypothesis: Union[AIMessage, str, None] = state.get("hypothesis")
    
    try:
        if isinstance(hypothesis, AIMessage):
            hypothesis_content = hypothesis.content
            logger.debug("Hypothesis is an AIMessage")
        elif isinstance(hypothesis, str):
            hypothesis_content = hypothesis
            logger.debug("Hypothesis is a string")
        else:
            hypothesis_content = ""
            logger.warning(f"Unexpected hypothesis type: {type(hypothesis)}")
            
        if not isinstance(hypothesis_content, str):
            hypothesis_content = str(hypothesis_content)
            logger.warning("Converting hypothesis content to string")
    except Exception as e:
        logger.error(f"Error processing hypothesis: {e}")
        hypothesis_content = ""
    
    result = "Hypothesis" if not hypothesis_content.strip() else "Process"
    logger.info(f"hypothesis_router decision: {result}")
    return result

def QualityReview_router(state: State) -> NodeType:
    """
    Route based on the quality review outcome and process decision.

    Args:
    state (State): The current state of the system.

    Returns:
    NodeType: The next node to route to based on the quality review and process decision.
    """
    logger.info("Entering QualityReview_router")
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    # Check if revision is needed
    if (last_message and 'REVISION' in str(last_message.content)) or state.get("needs_revision", False):
        previous_node = state.get("last_sender", "")
        revision_routes = {
            "Visualization": "Visualization",
            "Search": "Search",
            "Coder": "Coder",
            "Report": "Report"
        }
        result = revision_routes.get(previous_node, "NoteTaker")
        logger.info(f"Revision needed. Routing to: {result}")
        return result
    
    else:
        return "NoteTaker"
    

def process_router(state: State) -> ProcessNodeType:
    """
    Route based on the process decision in the state.

    Args:
        state (State): The current state of the system.

    Returns:
        ProcessNodeType: The next process node to route to based on the process decision.
    """
    logger.info("Entering process_router")
    process_decision: Union[AIMessage, Dict, str, None] = state.get("process_decision", "")
    
    decision_str: str = ""
    
    try:
        if isinstance(process_decision, AIMessage):
            logger.debug("Process decision is an AIMessage")
            try:
                decision_dict = json.loads(process_decision.content.replace("'", '"'))
                decision_str = str(decision_dict.get('next', ''))
            except json.JSONDecodeError as e:
                logger.warning(f"JSON parse error: {e}. Using content directly.")
                decision_str = process_decision.content
        elif isinstance(process_decision, dict):
            decision_str = str(process_decision.get('next', ''))
        else:
            decision_str = str(process_decision)
    except Exception as e:
        logger.error(f"Error processing decision: {e}")
        decision_str = ""
    
    # Define valid decisions
    valid_decisions = {"Coder", "Search", "Visualization", "Report"}
    
    if decision_str in valid_decisions:
        logger.info(f"Valid process decision: {decision_str}")
        return decision_str
    
    if decision_str == "FINISH":
        logger.info("Process decision is FINISH. Ending process.")
        return "Refiner"
    
    # If decision_str is empty or not a valid decision, return "Process"
    if not decision_str or decision_str not in valid_decisions:
        logger.warning(f"Invalid or empty process decision: {decision_str}. Defaulting to 'Process'.")
        return "Process"
    
    # Default to "Process"
    logger.info("Defaulting to 'Process'")
    return "Process"

logger.info("Router module initialized")



================================================
FILE: core/state.py
================================================
from langchain_core.messages import BaseMessage
from typing import Sequence, TypedDict
from pydantic import BaseModel, Field

class State(TypedDict):
    """TypedDict for the entire state structure."""
    # The sequence of messages exchanged in the conversation
    messages: Sequence[BaseMessage]

    # The complete content of the research hypothesis
    hypothesis: str = ""
    
    # The complete content of the research process
    process: str = ""
    
    # next process
    process_decision: str = ""
    
    # The current state of data visualization planning and execution
    visualization_state: str = ""
    
    # The current state of the search process, including queries and results
    searcher_state: str = ""
    
    # The current state of Coder development, including scripts and outputs
    code_state: str = ""
    
    # The content of the report sections being written
    report_section: str = ""
    
    # The feedback and comments from the quality review process
    quality_review: str = ""
    
    # A boolean flag indicating if the current output requires revision
    needs_revision: bool = False
    
    # The identifier of the agent who sent the last message
    sender: str = ""

class NoteState(BaseModel):
    """Pydantic model for the entire state structure."""
    messages: Sequence[BaseMessage] = Field(default_factory=list, description="List of message dictionaries")
    hypothesis: str = Field(default="", description="Current research hypothesis")
    process: str = Field(default="", description="Current research process")
    process_decision: str = Field(default="", description="Decision about the next process step")
    visualization_state: str = Field(default="", description="Current state of data visualization")
    searcher_state: str = Field(default="", description="Current state of the search process")
    code_state: str = Field(default="", description="Current state of code development")
    report_section: str = Field(default="", description="Content of the report sections")
    quality_review: str = Field(default="", description="Feedback from quality review")
    needs_revision: bool = Field(default=False, description="Flag indicating if revision is needed")
    sender: str = Field(default="", description="Identifier of the last message sender")

    class Config:
        arbitrary_types_allowed = True  # Allow BaseMessage type without explicit validator



================================================
FILE: core/workflow.py
================================================
from typing import Dict, Any
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import MemorySaver
from core.state import State
from core.node import agent_node, human_choice_node, note_agent_node, human_review_node, refiner_node
from core.router import QualityReview_router, hypothesis_router, process_router
from agent.hypothesis_agent import create_hypothesis_agent
from agent.process_agent import create_process_agent
from agent.visualization_agent import create_visualization_agent
from agent.code_agent import create_code_agent
from agent.search_agent import create_search_agent
from agent.report_agent import create_report_agent
from agent.quality_review_agent import create_quality_review_agent
from agent.note_agent import create_note_agent
from agent.refiner_agent import create_refiner_agent

class WorkflowManager:
    def __init__(self, language_models, working_directory):
        """
        Initialize the workflow manager with language models and working directory.
        
        Args:
            language_models (dict): Dictionary containing language model instances
            working_directory (str): Path to the working directory
        """
        self.language_models = language_models
        self.working_directory = working_directory
        self.workflow = None
        self.memory = None
        self.graph = None
        self.members = ["Hypothesis", "Process", "Visualization", "Search", "Coder", "Report", "QualityReview", "Refiner"]
        self.agents = self.create_agents()
        self.setup_workflow()

    def create_agents(self):
        """Create all system agents"""
        # Get language models
        llm = self.language_models["llm"]
        power_llm = self.language_models["power_llm"]
        json_llm = self.language_models["json_llm"]

        # Create agents dictionary
        agents = {}

        # Create each agent using their respective creation functions
        agents["hypothesis_agent"] = create_hypothesis_agent(
            llm, 
            self.members,
            self.working_directory
        )

        agents["process_agent"] = create_process_agent(power_llm)

        agents["visualization_agent"] = create_visualization_agent(
            llm,
            self.members,
            self.working_directory
        )

        agents["code_agent"] = create_code_agent(
            power_llm,
            self.members,
            self.working_directory
        )

        agents["searcher_agent"] = create_search_agent(
            llm,
            self.members,
            self.working_directory
        )

        agents["report_agent"] = create_report_agent(
            power_llm,
            self.members,
            self.working_directory
        )

        agents["quality_review_agent"] = create_quality_review_agent(
            llm,
            self.members,
            self.working_directory
        )

        agents["note_agent"] = create_note_agent(json_llm)

        agents["refiner_agent"] = create_refiner_agent(
            power_llm,
            self.members,
            self.working_directory
        )

        return agents

    def setup_workflow(self):
        """Set up the workflow graph"""
        self.workflow = StateGraph(State)
        
        # Add nodes
        self.workflow.add_node("Hypothesis", lambda state: agent_node(state, self.agents["hypothesis_agent"], "hypothesis_agent"))
        self.workflow.add_node("Process", lambda state: agent_node(state, self.agents["process_agent"], "process_agent"))
        self.workflow.add_node("Visualization", lambda state: agent_node(state, self.agents["visualization_agent"], "visualization_agent"))
        self.workflow.add_node("Search", lambda state: agent_node(state, self.agents["searcher_agent"], "searcher_agent"))
        self.workflow.add_node("Coder", lambda state: agent_node(state, self.agents["code_agent"], "code_agent"))
        self.workflow.add_node("Report", lambda state: agent_node(state, self.agents["report_agent"], "report_agent"))
        self.workflow.add_node("QualityReview", lambda state: agent_node(state, self.agents["quality_review_agent"], "quality_review_agent"))
        self.workflow.add_node("NoteTaker", lambda state: note_agent_node(state, self.agents["note_agent"], "note_agent"))
        self.workflow.add_node("HumanChoice", human_choice_node)
        self.workflow.add_node("HumanReview", human_review_node)
        self.workflow.add_node("Refiner", lambda state: refiner_node(state, self.agents["refiner_agent"], "refiner_agent"))

        # Add edges
        self.workflow.add_edge(START, "Hypothesis")
        self.workflow.add_edge("Hypothesis", "HumanChoice")
        
        self.workflow.add_conditional_edges(
            "HumanChoice",
            hypothesis_router,
            {
                "Hypothesis": "Hypothesis",
                "Process": "Process"
            }
        )

        self.workflow.add_conditional_edges(
            "Process",
            process_router,
            {
                "Coder": "Coder",
                "Search": "Search",
                "Visualization": "Visualization",
                "Report": "Report",
                "Process": "Process",
                "Refiner": "Refiner",
            }
        )

        for member in ["Visualization", 'Search', 'Coder', 'Report']:
            self.workflow.add_edge(member, "QualityReview")

        self.workflow.add_conditional_edges(
            "QualityReview",
            QualityReview_router,
            {
                'Visualization': "Visualization",
                'Search': "Search",
                'Coder': "Coder",
                'Report': "Report",
                'NoteTaker': "NoteTaker",
            }
        )

        self.workflow.add_edge("NoteTaker", "Process")
        self.workflow.add_edge("Refiner", "HumanReview")
        
        self.workflow.add_conditional_edges(
            "HumanReview",
            lambda state: "Process" if state and state.get("needs_revision", False) else "END",
            {
                "Process": "Process",
                "END": END
            }
        )

        # Compile workflow
        self.memory = MemorySaver()
        self.graph = self.workflow.compile()

    def get_graph(self):
        """Return the compiled workflow graph"""
        return self.graph



================================================
FILE: tools/__init__.py
================================================



================================================
FILE: tools/basetool.py
================================================
import os
import logging
import platform
from typing import Annotated
import subprocess
from langchain_core.tools import tool
from logger import setup_logger
from load_cfg import WORKING_DIRECTORY,CONDA_PATH,CONDA_ENV

# Initialize logger
logger = setup_logger()

# Ensure the storage directory exists
if not os.path.exists(WORKING_DIRECTORY):
    os.makedirs(WORKING_DIRECTORY)
    logger.info(f"Created storage directory: {WORKING_DIRECTORY}")

def get_platform_specific_command(command: str) -> tuple:
    """
    Get platform-specific command execution details.
    Returns a tuple of (shell_command, shell_type, executable)
    """
    system = platform.system().lower()
    if system == "windows":
        # Windows-specific command
        conda_commands = [
            f"call {os.path.join(CONDA_PATH, 'Scripts', 'activate.bat')}",
            f"conda activate {CONDA_ENV}",
            command
        ]
        return (" && ".join(conda_commands), True, None)
    else:
        # Unix-like systems (Linux, macOS)
        conda_commands = [
            f"source {os.path.join(CONDA_PATH, 'etc/profile.d/conda.sh')}",
            f"conda activate {CONDA_ENV}",
            command
        ]
        return (" && ".join(conda_commands), True, "/bin/bash")

@tool
def execute_code(
    input_code: Annotated[str, "The Python code to execute."],
    codefile_name: Annotated[str, "The Python code file name or full path."] = 'code.py'
):
    """
    Execute Python code in a specified conda environment and return the result.

    This function takes Python code as input, writes it to a file, executes it in the specified
    conda environment, and returns the output or any errors encountered during execution.

    Args:
    input_code (str): The Python code to be executed.
    codefile_name (str): The name of the file to save the code in, or the full path.

    Returns:
    dict: A dictionary containing the execution result, output, and file path.
    """
    try:
        # Ensure WORKING_DIRECTORY exists
        os.makedirs(WORKING_DIRECTORY, exist_ok=True)
        
        # Handle codefile_name, ensuring it's a valid path
        if os.path.isabs(codefile_name):
            code_file_path = codefile_name
        else:
            if WORKING_DIRECTORY not in codefile_name:
                code_file_path = os.path.join(WORKING_DIRECTORY, codefile_name)
            else:
                code_file_path = codefile_name

        # Normalize the path for the current platform
        code_file_path = os.path.normpath(code_file_path)

        logger.info(f"Code will be written to file: {code_file_path}")
        
        # Write the code to the file with UTF-8 encoding
        with open(code_file_path, 'w', encoding='utf-8') as code_file:
            code_file.write(input_code)
        
        logger.info(f"Code has been written to file: {code_file_path}")
        
        # Get platform-specific command
        python_cmd = f"python {codefile_name}"
        full_command, shell, executable = get_platform_specific_command(python_cmd)
        
        logger.info(f"Executing command: {full_command}")
        
        # Execute the code
        result = subprocess.run(
            full_command,
            shell=shell,
            capture_output=True,
            text=True,
            executable=executable,
            cwd=WORKING_DIRECTORY
        )
        
        # Capture standard output and error output
        output = result.stdout
        error_output = result.stderr
        
        if result.returncode == 0:
            logger.info("Code executed successfully")
            return {
                "result": "Code executed successfully",
                "output": output + "\n\nIf you have completed all tasks, respond with FINAL ANSWER.",
                "file_path": code_file_path
            }
        else:
            logger.error(f"Code execution failed: {error_output}")
            return {
                "result": "Failed to execute",
                "error": error_output,
                "file_path": code_file_path
            }
    except Exception as e:
        logger.exception("An error occurred while executing code")
        return {
            "result": "Error occurred",
            "error": str(e),
            "file_path": code_file_path if 'code_file_path' in locals() else "Unknown"
        }

@tool
def execute_command(
    command: Annotated[str, "Command to be executed."]
) -> Annotated[str, "Output of the command."]:
    """
    Execute a command in a specified Conda environment and return its output.

    This function activates a Conda environment, executes the given command,
    and returns the output or any errors encountered during execution.
    Please use pip to install the package.

    Args:
    command (str): The command to be executed in the Conda environment.

    Returns:
    str: The output of the command or an error message.
    """
    try:
        # Get platform-specific command
        full_command, shell, executable = get_platform_specific_command(command)
        
        logger.info(f"Executing command: {command}")
        
        # Execute the command and capture the output
        result = subprocess.run(
            full_command,
            shell=shell,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            executable=executable,
            cwd=WORKING_DIRECTORY
        )
        logger.info("Command executed successfully")
        return result.stdout
    except subprocess.CalledProcessError as e:
        logger.error(f"Error executing command: {e.stderr}")
        return f"Error: {e.stderr}"

logger.info("Module initialized successfully")



================================================
FILE: tools/FileEdit.py
================================================
import os
from langchain_core.tools import tool
import pandas as pd
from typing import Dict, Optional, Annotated, List
from logger import setup_logger
from load_cfg import WORKING_DIRECTORY

# Set up logger
logger = setup_logger()

# Ensure the working directory exists
if not os.path.exists(WORKING_DIRECTORY):
    os.makedirs(WORKING_DIRECTORY)
    logger.info(f"Created working directory: {WORKING_DIRECTORY}")

def normalize_path(file_path: str) -> str:
    """
    Normalize file path for cross-platform compatibility.
    
    Args:
    file_path (str): The file path to normalize
    
    Returns:
    str: Normalized file path
    """
    if WORKING_DIRECTORY not in file_path:
        file_path = os.path.join(WORKING_DIRECTORY, file_path)
    return os.path.normpath(file_path)

@tool
def collect_data(data_path: Annotated[str, "Path to the CSV file"] = './data.csv'):
    """
    Collect data from a CSV file.

    This function attempts to read a CSV file using different encodings.

    Returns:
    pandas.DataFrame: The data read from the CSV file.

    Raises:
    ValueError: If unable to read the file with any of the provided encodings.
    """
    data_path = normalize_path(data_path)
    logger.info(f"Attempting to read CSV file: {data_path}")
    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
    for encoding in encodings:
        try:
            data = pd.read_csv(data_path, encoding=encoding)
            logger.info(f"Successfully read CSV file with encoding: {encoding}")
            return data
        except Exception as e:
            logger.warning(f"Error with encoding {encoding}: {e}")
    logger.error("Unable to read file with provided encodings")
    raise ValueError("Unable to read file with provided encodings")

@tool
def create_document(
    points: Annotated[List[str], "List of points to be included in the document"],
    file_name: Annotated[str, "Name of the file to save the document"]
) -> str:
    """
    Create and save a text document in Markdown format.

    This function takes a list of points and writes them as numbered items in a Markdown file.
    
    Returns:
    str: A message indicating where the outline was saved or an error message.
    """
    try:
        file_path = normalize_path(file_name)
        logger.info(f"Creating document: {file_path}")
        with open(file_path, "w", encoding='utf-8') as file:
            for i, point in enumerate(points):
                file.write(f"{i + 1}. {point}\n")
        logger.info(f"Document created successfully: {file_path}")
        return f"Outline saved to {file_path}"
    except Exception as e:
        logger.error(f"Error while saving outline: {str(e)}")
        return f"Error while saving outline: {str(e)}"

@tool
def read_document(
    file_name: Annotated[str, "Name of the file to read"],
    start: Annotated[Optional[int], "Starting line number to read from"] = None,
    end: Annotated[Optional[int], "Ending line number to read to"] = None
) -> str:
    """
    Read the specified document.

    This function reads a document from the specified file and returns its content.
    Optionally, it can return a specific range of lines.

    Returns:
    str: The content of the document or an error message.
    """
    try:
        file_path = normalize_path(file_name)
        logger.info(f"Reading document: {file_path}")
        with open(file_path, "r", encoding='utf-8') as file:
            lines = file.readlines()
        if start is None:
            start = 0
        content = "\n".join(lines[start:end])
        logger.info(f"Document read successfully: {file_path}")
        return content
    except FileNotFoundError:
        logger.error(f"File not found: {file_name}")
        return f"Error: The file {file_name} was not found."
    except Exception as e:
        logger.error(f"Error while reading document: {str(e)}")
        return f"Error while reading document: {str(e)}"

@tool
def write_document(
    content: Annotated[str, "Content to be written to the document"],
    file_name: Annotated[str, "Name of the file to save the document"]
) -> str:
    """
    Create and save a Markdown document.

    This function takes a string of content and writes it to a file.
    """
    try:
        file_path = normalize_path(file_name)
        logger.info(f"Writing document: {file_path}")
        with open(file_path, "w", encoding='utf-8') as file:
            file.write(content)
        logger.info(f"Document written successfully: {file_path}")
        return f"Document saved to {file_path}"
    except Exception as e:
        logger.error(f"Error while saving document: {str(e)}")
        return f"Error while saving document: {str(e)}"

@tool
def edit_document(
    file_name: Annotated[str, "Name of the file to edit"],
    inserts: Annotated[Dict[int, str], "Dictionary of line numbers and text to insert"]
) -> str:
    """
    Edit a document by inserting text at specific line numbers.

    This function reads an existing document, inserts new text at specified line numbers,
    and saves the modified document.

    Args:
        file_name (str): Name of the file to edit.
        inserts (Dict[int, str]): Dictionary where keys are line numbers and values are text to insert.

    Returns:
        str: A message indicating the result of the operation.

    Example:
        file_name = "example.txt"
        inserts = {
            1: "This is the first line to insert.",
            3: "This is the third line to insert."
        }
        result = edit_document(file_name=file_name, inserts=inserts)
        print(result)
        # Output: "Document edited and saved to /path/to/example.txt"
    """
    try:
        file_path = normalize_path(file_name)
        logger.info(f"Editing document: {file_path}")
        with open(file_path, "r", encoding='utf-8') as file:
            lines = file.readlines()

        sorted_inserts = sorted(inserts.items())

        for line_number, text in sorted_inserts:
            if 1 <= line_number <= len(lines) + 1:
                lines.insert(line_number - 1, text + "\n")
            else:
                logger.error(f"Line number out of range: {line_number}")
                return f"Error: Line number {line_number} is out of range."

        with open(file_path, "w", encoding='utf-8') as file:
            file.writelines(lines)

        logger.info(f"Document edited successfully: {file_path}")
        return f"Document edited and saved to {file_path}"
    except FileNotFoundError:
        logger.error(f"File not found: {file_name}")
        return f"Error: The file {file_name} was not found."
    except Exception as e:
        logger.error(f"Error while editing document: {str(e)}")
        return f"Error while editing document: {str(e)}"

logger.info("Document management tools initialized")



================================================
FILE: tools/internet.py
================================================
import os
from langchain_core.tools import tool
from langchain_community.document_loaders import WebBaseLoader, FireCrawlLoader
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from typing import Annotated, List
from bs4 import BeautifulSoup
from logger import setup_logger
from load_cfg import FIRECRAWL_API_KEY,CHROMEDRIVER_PATH
# Set up logger
logger = setup_logger()

@tool
def google_search(query: Annotated[str, "The search query to use"]) -> str:
    """
    Perform a Google search based on the given query and return the top 5 results.

    This function uses Selenium to perform a headless Google search and BeautifulSoup to parse the results.

    Args:
    query (str): The search query to use.

    Returns:
    str: A string containing the titles, snippets, and links of the top 5 search results.

    Raises:
    Exception: If there's an error during the search process.
    """
    try:
        logger.info(f"Performing Google search for query: {query}")
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        service = Service(CHROMEDRIVER_PATH)

        with webdriver.Chrome(options=chrome_options, service=service) as driver:
            url = f"https://www.google.com/search?q={query}"
            logger.debug(f"Accessing URL: {url}")
            driver.get(url)
            html = driver.page_source

        soup = BeautifulSoup(html, 'html.parser')
        search_results = soup.select('.g') 
        search = ""
        for result in search_results[:5]:
            title_element = result.select_one('h3')
            title = title_element.text if title_element else 'No Title'
            snippet_element = result.select_one('.VwiC3b')
            snippet = snippet_element.text if snippet_element else 'No Snippet'
            link_element = result.select_one('a')
            link = link_element['href'] if link_element else 'No Link'
            search += f"{title}\n{snippet}\n{link}\n\n"

        logger.info("Google search completed successfully")
        return search
    except Exception as e:
        logger.error(f"Error during Google search: {str(e)}")
        return f'Error: {e}'
@tool
def scrape_webpages(urls: Annotated[List[str], "List of URLs to scrape"]) -> str:
    """
    Scrape the provided web pages for detailed information using WebBaseLoader.

    This function uses the WebBaseLoader to load and scrape the content of the provided URLs.

    Args:
    urls (List[str]): A list of URLs to scrape.

    Returns:
    str: A string containing the concatenated content of all scraped web pages.

    Raises:
    Exception: If there's an error during the scraping process.
    """
    try:
        logger.info(f"Scraping webpages: {urls}")
        loader = WebBaseLoader(urls)
        docs = loader.load()
        content = "\n\n".join([f'\n{doc.page_content}\n' for doc in docs])
        logger.info("Webpage scraping completed successfully")
        return content
    except Exception as e:
        logger.error(f"Error during webpage scraping: {str(e)}")
        raise  # Re-raise the exception to be caught by the calling function
@tool
def FireCrawl_scrape_webpages(urls: Annotated[List[str], "List of URLs to scrape"]) -> str:
    """
    Scrape the provided web pages for detailed information using FireCrawlLoader.

    This function uses the FireCrawlLoader to load and scrape the content of the provided URLs.

    Args:
    urls (List[str]): A list of URLs to scrape.

    Returns:
    Any: The result of the FireCrawlLoader's load operation.

    Raises:
    Exception: If there's an error during the scraping process or if the API key is not set.
    """
    if not FIRECRAWL_API_KEY:
        raise ValueError("FireCrawl API key is not set")

    try:
        logger.info(f"Scraping webpages using FireCrawl: {urls}")
        loader = FireCrawlLoader(
            api_key=FIRECRAWL_API_KEY,
            url=urls,
            mode="scrape"
        )
        result = loader.load()
        logger.info("FireCrawl scraping completed successfully")
        return result
    except Exception as e:
        logger.error(f"Error during FireCrawl scraping: {str(e)}")
        raise  # Re-raise the exception to be caught by the calling function
@tool
def scrape_webpages_with_fallback(urls: Annotated[List[str], "List of URLs to scrape"]) -> str:
    """
    Attempt to scrape webpages using FireCrawl, falling back to WebBaseLoader if unsuccessful.

    Args:
    urls (List[str]): A list of URLs to scrape.

    Returns:
    str: The scraped content from either FireCrawl or WebBaseLoader.
    """
    try:
        return FireCrawl_scrape_webpages(urls)
    except Exception as e:
        logger.warning(f"FireCrawl scraping failed: {str(e)}. Falling back to WebBaseLoader.")
        try:
            return scrape_webpages(urls)
        except Exception as e:
            logger.error(f"Both scraping methods failed. Error: {str(e)}")
            return f"Error: Unable to scrape webpages using both methods. {str(e)}"

logger.info("Web scraping tools initialized")

