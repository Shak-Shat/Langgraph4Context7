Directory structure:
└── aianytime-perplexity-lite/
    ├── README.md
    ├── app.py
    ├── index.html
    ├── lang_graph.ipynb
    ├── LICENSE
    └── requirements.txt

================================================
FILE: README.md
================================================
# Perplexity-Lite
Perplexity Lite using Langgraph, Tavily, and GPT-4.



================================================
FILE: app.py
================================================
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain_openai.chat_models import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
import os
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish
from langgraph.graph import END, Graph

from fastapi import FastAPI, HTTPException, Request, Form
from fastapi.responses import JSONResponse, RedirectResponse
from fastapi.encoders import jsonable_encoder
import uvicorn
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import json
import base64

load_dotenv()
os.environ['OPENAI_API_KEY'] == os.getenv('OPENAI_API_KEY')
os.environ['TAVILY_API_KEY'] == os.getenv('TAVILY_API_KEY')


tools = [TavilySearchResults(max_results=1)]
prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-4-1106-preview")

agent_runnable = create_openai_functions_agent(llm, tools, prompt)


agent = RunnablePassthrough.assign(
    agent_outcome=agent_runnable
)


def execute_tools(data):
    agent_action = data.pop('agent_outcome')
    tool_to_use = {t.name: t for t in tools}[agent_action.tool]
    observation = tool_to_use.invoke(agent_action.tool_input)
    data['intermediate_steps'].append((agent_action, observation))
    return data


def should_continue(data):
    if isinstance(data['agent_outcome'], AgentFinish):
        return "exit"
    else:
        return "continue"


workflow = Graph()

workflow.add_node("agent", agent)
workflow.add_node("tools", execute_tools)

workflow.set_entry_point("agent")

workflow.add_conditional_edges(
    "agent",  # start node
    should_continue,
    {
        "continue": "tools",
        "exit": END
    }
)

workflow.add_edge('tools', 'agent')

chain = workflow.compile()


def make_serializable(obj):
    if isinstance(obj, dict):
        return {key: make_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [make_serializable(element) for element in obj]
    elif isinstance(obj, set):
        return list(obj)
    elif hasattr(obj, "__dict__"):
        # Convert custom objects to strings
        return str(obj)
    else:
        return obj


# chain.invoke({"input": "Tell me 5 startups in the field of Quantum Computing", "intermediate_steps": []})

app = FastAPI()

app.mount("/static", StaticFiles(directory="static"), name="static")

templates = Jinja2Templates(directory="templates")


@app.get("/")
async def index(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# Endpoint integrating your workflow


@app.post("/process_query")
async def process_query(query: str = Form(...)):
    intermediate_steps = []
    raw_data = chain.invoke(
        {"input": query, "intermediate_steps": intermediate_steps})
    encoded_raw_data = base64.b64encode(str(raw_data).encode()).decode()
    print(raw_data)
    # Extract the desired output
    desired_output = raw_data["agent_outcome"].return_values['output']
    print(desired_output)
    # Extract the URL
    # Since the URL is in a list within a list, we navigate accordingly
    url = raw_data["intermediate_steps"][0][1][0]["url"]
    return JSONResponse(content={"raw_data": encoded_raw_data, "desired_output": desired_output, "url": url})
    # serializable_raw_data = make_serializable(raw_data)
    # return JSONResponse(content={"raw_data": json.dumps(raw_data)})
    # #return JSONResponse(content=jsonable_encoder(result))



================================================
FILE: index.html
================================================
<html>
<head>
    <title>Where knowledge begins</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300;400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css"></link>
    <style>
        body {
            background-color: #121212;
            color: #fff;
            font-family: 'Open Sans', sans-serif;
        }
        .container {
            max-width: 800px;
            margin-top: 100px;
        }
        .search-box {
            display: flex;
            align-items: center;
            background-color: #252525;
            border-radius: 22px;
            padding: 10px 20px;
        }
        .search-box input {
            border: none;
            background-color: transparent;
            color: #fff;
            margin-right: 10px;
            font-size: 16px;
            flex-grow: 1;
        }
        .search-box input:focus {
            outline: none;
        }
        .search-box .btn {
            background-color: #007bff;
            color: #fff;
            border: none;
            border-radius: 50%;
            padding: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .search-box .btn:focus {
            outline: none;
            box-shadow: none;
        }
        .search-box .fas {
            color: #fff;
        }
        .title {
            text-align: center;
            font-size: 48px;
            font-weight: 300;
            margin-bottom: 50px;
        }
        .answer-card {
            background-color: #333;
            border-radius: 8px;
            padding: 20px;
            margin-top: 20px;
        }
        .accordion {
            --bs-accordion-btn-active-icon: url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' fill='%23FFFFFF'%3e%3cpath fill-rule='evenodd' d='M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z'/%3e%3c/svg%3e");
            --bs-accordion-btn-icon: url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16' fill='%23FFFFFF'%3e%3cpath fill-rule='evenodd' d='M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z'/%3e%3c/svg%3e");
        }
        #loader {
            display: none;
        }
        pre{
            white-space: pre-wrap;
        }

        #rawdata {
            max-height: 250px;
            overflow-y: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="title">Where knowledge begins</div>
        <div class="search-box">
            <input id="query" type="text" placeholder="Ask anything...">
            <button class="btn" onclick="showAnswer()"><i class="fas fa-chevron-right"></i></button>
        </div>
        <div id="loader" class="text-center mt-5">
            <div class="justify-content-center">
                <div class="spinner-border text-secondary" role="status" style="width: 10rem; height: 10rem;">
                    <span class="visually-hidden">Loading...</span>
                </div>
                <h4 class="text-blue mt-5">Generating...</h4>
            </div>
       </div>
        <div id="answer" class="mt-5" style="display: none;">

            <!-- The answer will be displayed here -->
            <div class="accordion" id="accordionExample">
                <div class="accordion-item">
                    <h2 class="accordion-header">
                      <button class="accordion-button bg-dark text-white" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
                        Raw Trace
                      </button>
                    </h2>
                    <div id="collapseOne" class="accordion-collapse collapse show" data-bs-parent="#accordionExample">
                      <div class="accordion-body" id="rawdata">
                        
                      </div>
                    </div>
                </div>            
            </div>
            <div class="answer-card">
                <pre id="result"></pre>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.7.1.js" integrity="sha256-eKhayi8LEQwp4NKxN+CfCh+3qOVUtJn3QNZ0TciWLP4=" crossorigin="anonymous"></script>
    <script src="//cdn.jsdelivr.net/npm/sweetalert2@11"></script>
    <script src="https://kit.fontawesome.com/111a58e221.js" crossorigin="anonymous"></script>
    <script>
        async function showAnswer() {
            var query = document.getElementById('query').value;
            try {
                if (query.length > 0)
                {
                    console.log(query);
                    const formData = new FormData();
                    formData.append('query', query);
                    document.getElementById('loader').style.display = 'block';
                    document.getElementById('answer').style.display = 'none';
                    const response = await fetch('/process_query', {
                        method: 'POST',
                        body: formData
                    });
                    const data = await response.json();
                    document.getElementById('loader').style.display = 'none';
                    document.getElementById('answer').style.display = 'block';
                    console.log("data raw_data:", data.raw_data); // Add this line to check the content
                    const decodedRawData = atob(data.raw_data);
                    console.log("decodedRawData:", decodedRawData); // Add this line to check the content
                    const rawdataElement = document.getElementById('rawdata');

                    // Clear the existing content (if any) and display the new data
                    rawdataElement.innerHTML = `<pre>${JSON.stringify(decodedRawData, null, 2)}</pre>`;

                    // const decodedRawData = JSON.parse(atob(data.raw_data));
                    // document.getElementById('rawdata').innerHTML = JSON.stringify(decodedRawData, null, 2);

                    // // document.getElementById('rawdata').innerHTML = data.raw_data;
                    document.getElementById('result').innerHTML = `<b>Result:</b> ${data.desired_output} <br><br><b>URL:</b> ${data.url}`;
                } else {
                    Swal.fire({
                        icon: 'error',
                        title: 'Please ask a question...',
                        confirmButtonColor: "#1C2F47"
                    })
                }
                
            } catch (error) {
                Swal.fire({
                        icon: 'error',
                        title: 'Error:'+error,
                        confirmButtonColor: "#1C2F47"
                })
                console.error('Error:', error);
            }
        }
    </script>
</body>
</html>


================================================
FILE: lang_graph.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install -U langchain langgraph langchain_openai langchainhub tavily-python
# Output:
#   Collecting langchain

#     Downloading langchain-0.1.0-py3-none-any.whl (797 kB)

#   [?25l     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/798.0 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91m━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m92.2/798.0 kB[0m [31m2.6 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m399.4/798.0 kB[0m [31m5.8 MB/s[0m eta [36m0:00:01[0m
[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m757.8/798.0 kB[0m [31m7.1 MB/s[0m eta [36m0:00:01[0m
[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m798.0/798.0 kB[0m [31m6.4 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langgraph

#     Downloading langgraph-0.0.10-py3-none-any.whl (27 kB)

#   Collecting langchain_openai

#     Downloading langchain_openai-0.0.2-py3-none-any.whl (28 kB)

#   Collecting langchainhub

#     Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)

#   Collecting tavily-python

#     Downloading tavily_python-0.3.0-py3-none-any.whl (5.4 kB)

#   Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)

#   Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)

#   Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)

#   Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)

#   Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)

#     Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)

#   Collecting jsonpatch<2.0,>=1.33 (from langchain)

#     Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)

#   Collecting langchain-community<0.1,>=0.0.9 (from langchain)

#     Downloading langchain_community-0.0.11-py3-none-any.whl (1.5 MB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.5/1.5 MB[0m [31m11.1 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langchain-core<0.2,>=0.1.7 (from langchain)

#     Downloading langchain_core-0.1.9-py3-none-any.whl (216 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m216.5/216.5 kB[0m [31m12.4 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain)

#     Downloading langsmith-0.0.79-py3-none-any.whl (48 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m48.4/48.4 kB[0m [31m4.3 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)

#   Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)

#   Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)

#   Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)

#   Collecting openai<2.0.0,>=1.6.1 (from langchain_openai)

#     Downloading openai-1.7.1-py3-none-any.whl (224 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m224.9/224.9 kB[0m [31m9.5 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting tiktoken<0.6.0,>=0.5.2 (from langchain_openai)

#     Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m2.0/2.0 MB[0m [31m15.7 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)

#     Downloading types_requests-2.31.0.20240106-py3-none-any.whl (14 kB)

#   INFO: pip is looking at multiple versions of tavily-python to determine which version is compatible with other requirements. This could take a while.

#   Collecting tavily-python

#     Downloading tavily_python-0.2.9-py3-none-any.whl (5.4 kB)

#     Downloading tavily_python-0.2.8-py3-none-any.whl (5.3 kB)

#     Downloading tavily_python-0.2.7-py3-none-any.whl (5.2 kB)

#     Downloading tavily_python-0.2.6-py3-none-any.whl (4.8 kB)

#     Downloading tavily_python-0.2.5-py3-none-any.whl (4.8 kB)

#     Downloading tavily_python-0.2.4-py3-none-any.whl (4.7 kB)

#     Downloading tavily_python-0.2.3-py3-none-any.whl (4.7 kB)

#   INFO: pip is looking at multiple versions of tavily-python to determine which version is compatible with other requirements. This could take a while.

#     Downloading tavily_python-0.2.2-py3-none-any.whl (4.6 kB)

#     Downloading tavily_python-0.1.9-py3-none-any.whl (3.0 kB)

#   Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)

#   Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)

#   Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)

#   Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)

#   Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)

#   Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)

#     Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.4/49.4 kB[0m [31m5.0 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)

#     Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)

#   Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)

#     Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)

#   Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (3.7.1)

#   Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)

#   Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (1.7.0)

#   Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.6.1->langchain_openai)

#     Downloading httpx-0.26.0-py3-none-any.whl (75 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m75.9/75.9 kB[0m [31m7.8 MB/s[0m eta [36m0:00:00[0m

#   [?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (1.3.0)

#   Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.6.1->langchain_openai) (4.66.1)

#   Collecting typing-extensions<5,>=4.7 (from openai<2.0.0,>=1.6.1->langchain_openai)

#     Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)

#   Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)

#   Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)

#   Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)

#   Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)

#   Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.6.0,>=0.5.2->langchain_openai) (2023.6.3)

#   Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.2.0)

#   Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain_openai)

#     Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m76.9/76.9 kB[0m [31m9.8 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.6.1->langchain_openai)

#     Downloading h11-0.14.0-py3-none-any.whl (58 kB)

#   [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m58.3/58.3 kB[0m [31m6.9 MB/s[0m eta [36m0:00:00[0m

#   [?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)

#     Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)

#   Installing collected packages: typing-extensions, types-requests, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, tavily-python, langchainhub, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langgraph, langchain_openai, langchain-community, langchain

#     Attempting uninstall: typing-extensions

#       Found existing installation: typing_extensions 4.5.0

#       Uninstalling typing_extensions-4.5.0:

#         Successfully uninstalled typing_extensions-4.5.0

#   [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.

#   llmx 0.0.15a0 requires cohere, which is not installed.

#   tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.[0m[31m

#   [0mSuccessfully installed dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.0 langchain-community-0.0.11 langchain-core-0.1.9 langchain_openai-0.0.2 langchainhub-0.1.14 langgraph-0.0.10 langsmith-0.0.79 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.7.1 tavily-python-0.1.9 tiktoken-0.5.2 types-requests-2.31.0.20240106 typing-extensions-4.9.0 typing-inspect-0.9.0


from google.colab import userdata
import os

os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")
os.environ["TAVILY_API_KEY"] = userdata.get("TAVILY_API_KEY")

from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain_openai.chat_models import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults

"""
## Agent Definition

### Generating openai_functions_agent
"""

tools = [TavilySearchResults(max_results=1)]
prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-4-1106-preview")

prompt
# Output:
#   ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])

agent_runnable = create_openai_functions_agent(llm, tools, prompt)

agent_runnable
# Output:
#   RunnableAssign(mapper={

#     agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))

#   })

#   | ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])

#   | RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b51274b5510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b51274b40d0>, model_name='gpt-4-1106-preview', openai_api_key='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', openai_proxy=''), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'title': 'TavilyInput', 'description': 'Input for the Tavily tool.', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}]})

#   | OpenAIFunctionsAgentOutputParser()

from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish

agent = RunnablePassthrough.assign(
    agent_outcome = agent_runnable
)

agent
# Output:
#   RunnableAssign(mapper={

#     agent_outcome: RunnableAssign(mapper={

#                      agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))

#                    })

#                    | ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])

#                    | RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7b51274b5510>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7b51274b40d0>, model_name='gpt-4-1106-preview', openai_api_key='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', openai_proxy=''), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'title': 'TavilyInput', 'description': 'Input for the Tavily tool.', 'type': 'object', 'properties': {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}]})

#                    | OpenAIFunctionsAgentOutputParser()

#   })

"""
## Service Function
"""

def execute_tools(data):
  agent_action = data.pop('agent_outcome')
  tool_to_use = {t.name: t for t in tools}[agent_action.tool]
  observation = tool_to_use.invoke(agent_action.tool_input)
  data['intermediate_steps'].append((agent_action, observation))
  return data

def should_continue(data):
  if isinstance(data['agent_outcome'], AgentFinish):
    return "exit"
  else:
    return "continue"

from langgraph.graph import END, Graph

workflow = Graph()

workflow.add_node("agent", agent)
workflow.add_node("tools", execute_tools)

workflow.set_entry_point("agent")

"""
## agent - continue -> tools
             |
             -exit -> END
"""

workflow.add_conditional_edges(
    "agent", #start node
    should_continue,
    {
        "continue": "tools",
        "exit": END
    }
)

workflow
# Output:
#   <langgraph.graph.Graph at 0x7b51271b77f0>

workflow.add_edge('tools', 'agent')

chain = workflow.compile()

"""
## Execution
"""

chain.invoke({"input": "Tell me 5 startups in the field of Quantum Computing", "intermediate_steps": []})
# Output:
#   {'input': 'Tell me 5 startups in the field of Quantum Computing',

#    'intermediate_steps': [(AgentActionMessageLog(tool='tavily_search_results_json', tool_input={'query': 'top 5 startups in Quantum Computing 2023'}, log="\nInvoking: `tavily_search_results_json` with `{'query': 'top 5 startups in Quantum Computing 2023'}`\n\n\n", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{"query":"top 5 startups in Quantum Computing 2023"}', 'name': 'tavily_search_results_json'}})]),

#      [{'url': 'https://quantumpace.com/quantum-computing-startups/',

#        'content': "Quantum Computing Startups — 2024  1QBit – quantum computing startups  Financial data – quantum computing startups  \u200bGrowth of the quantum computing startups industry\u200bQuantum Computing Startups - discover the most promising young companies in the QC industry! Qiskit. Founded in 2017 in Armonk, NY, Qiskit is an open-source platform aimed at making quantum computing more accessible. It offers tools for programming and simulating quantum computers, as well as access to IBM's cloud-based quantum devices."}])],

#    'agent_outcome': AgentFinish(return_values={'output': "Here are 5 startups in the field of Quantum Computing:\n\n1. **1QBit** - A startup that focuses on quantum computing solutions, particularly in the financial data sector, aiming to advance the growth of the quantum computing industry.\n\n2. **Qiskit** - Founded in 2017 in Armonk, NY, Qiskit is an open-source platform designed to make quantum computing more accessible. It provides tools for programming and simulating quantum computers and offers access to IBM's cloud-based quantum devices.\n\nUnfortunately, the source provided only listed two specific startups in quantum computing. To give you a more comprehensive list, I would need to conduct further research. Would you like me to do that?"}, log="Here are 5 startups in the field of Quantum Computing:\n\n1. **1QBit** - A startup that focuses on quantum computing solutions, particularly in the financial data sector, aiming to advance the growth of the quantum computing industry.\n\n2. **Qiskit** - Founded in 2017 in Armonk, NY, Qiskit is an open-source platform designed to make quantum computing more accessible. It provides tools for programming and simulating quantum computers and offers access to IBM's cloud-based quantum devices.\n\nUnfortunately, the source provided only listed two specific startups in quantum computing. To give you a more comprehensive list, I would need to conduct further research. Would you like me to do that?")}



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 AI Anytime

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: requirements.txt
================================================
langchain
openai
tavily-python
langgraph
langchainhub
fastapi
uvicorn
python-dotenv
jinja2

