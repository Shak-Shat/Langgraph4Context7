Directory structure:
â””â”€â”€ xyin-anl-nodeology/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CITATION.cff
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ MANIFEST.in
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ assets/
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ chainlit.md
    â”‚   â”œâ”€â”€ particle_trajectory_analysis.yaml
    â”‚   â”œâ”€â”€ trajectory_analysis.py
    â”‚   â”œâ”€â”€ writing_improvement.py
    â”‚   â”œâ”€â”€ public/
    â”‚   â”‚   â”œâ”€â”€ hide-watermark.js
    â”‚   â”‚   â”œâ”€â”€ theme.json
    â”‚   â”‚   â””â”€â”€ elements/
    â”‚   â”‚       â””â”€â”€ DataDisplay.jsx
    â”‚   â””â”€â”€ .chainlit/
    â”‚       â””â”€â”€ config.toml
    â”œâ”€â”€ nodeology/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ client.py
    â”‚   â”œâ”€â”€ interface.py
    â”‚   â”œâ”€â”€ log.py
    â”‚   â”œâ”€â”€ node.py
    â”‚   â”œâ”€â”€ state.py
    â”‚   â””â”€â”€ workflow.py
    â””â”€â”€ tests/
        â”œâ”€â”€ test_node.py
        â”œâ”€â”€ test_state.py
        â””â”€â”€ test_workflow.py

================================================
FILE: README.md
================================================
> [!IMPORTANT]
> This package is actively in development, and breaking changes may occur.

<div align="center">
  <img src="assets/logo.jpg" alt="Nodeology Logo" width="600"/>
  <h3></h3>
</div>

## ðŸ¤– Foundation AI-Enhanced Scientific Workflow

Foundation AI holds enormous potential for scientific research, especially in analyzing unstructured data, automating complex reasoning tasks, and simplifying human-computer interactions. However, integrating foundation AI models like LLMs and VLMs into scientific workflows poses challenges: handling diverse data types beyond text and images, managing model inaccuracies (hallucinations), and adapting general-purpose models to highly specialized scientific contexts.

`nodeology` addresses these challenges by combining the strengths of foundation AI with traditional scientific methods and expert oversight. Built on `langgraph`'s state machine framework, it simplifies creating robust, AI-driven workflows through an intuitive, accessible interface. Originally developed at Argonne National Lab, the framework enables researchersâ€”especially those without extensive programming experienceâ€”to quickly design and deploy full-stack AI workflows simply using prompt templates and existing functions as reusable nodes.

Key features include:

- Easy creation of AI-integrated workflows without complex syntax
- Flexible and composable node architecture for various tasks
- Seamless human-in-the-loop interactions for expert oversight
- Portable workflow templates for collaboration and reproducibility
- Quickly spin up simple chatbots for immediate AI interaction
- Built-in tracing and telemetry for workflow monitoring and optimization

## ðŸš€ Getting Started

### Install the package

To use the latest development version:

```bash
pip install git+https://github.com/xyin-anl/Nodeology.git
```

To use the latest release version:

```bash
pip install nodeology
```

### Access foundation models

Nodeology supports various cloud-based/local foundation models via [LiteLLM](https://docs.litellm.ai/docs/), see [provider list](https://docs.litellm.ai/docs/providers). Most of cloud-based models usage requires setting up API key. For example:

```bash
# For OpenAI models
export OPENAI_API_KEY='your-api-key'

# For Anthropic models
export ANTHROPIC_API_KEY='your-api-key'

# For Gemini models
export GEMINI_API_KEY='your-api-key'

# For Together AI hosted open weight models
export TOGETHER_API_KEY='your-api-key'
```

> **ðŸ’¡ Tip:** The field of foundation models is evolving rapidly with new and improved models emerging frequently. As of **February 2025**, we recommend the following models based on their strengths:
>
> - **gpt-4o**: Excellent for broad general knowledge, writing tasks, and conversational interactions
> - **o3-mini**: Good balance of math, coding, and reasoning capabilities at a lower price point
> - **anthropic/claude-3.7**: Strong performance in general knowledge, math, science, and coding with well-constrained outputs
> - **gemini/gemini-2.0-flash**: Effective for general knowledge tasks with a large context window for processing substantial information
> - **together_ai/deepseek-ai/DeepSeek-R1**: Exceptional reasoning, math, science, and coding capabilities with transparent thinking processes

**For Argonne Users:** if you are within Argonne network, you will have access to OpenAI's models through Argonne's ARGO inference service and ALCF's open weights model inference service for free. Please check this [link](https://gist.github.com/xyin-anl/0cc744a7862e153414857b15fe31b239) to see how to use them

### Langfuse Tracing (Optional)

Nodeology supports [Langfuse](https://langfuse.com/) for observability and tracing of LLM/VLM calls. To use Langfuse:

1. Set up a Langfuse account and get your API keys
2. Configure Langfuse with your keys:

```bash
# Set environment variables
export LANGFUSE_PUBLIC_KEY='your-public-key'
export LANGFUSE_SECRET_KEY='your-secret-key'
export LANGFUSE_HOST='https://cloud.langfuse.com'  # Or your self-hosted URL
```

Or configure programmatically:

```python
from nodeology.client import configure_langfuse

configure_langfuse(
    public_key='your-public-key',
    secret_key='your-secret-key',
    host='https://cloud.langfuse.com'  # Optional
)
```

### Chainlit Interface (Optional)

Nodeology supports [Chainlit](https://docs.chainlit.io/get-started/overview) for creating chat-based user interfaces. To use this feature, simply set `ui=True` when running your workflow:

```python
# Create your workflow
workflow = MyWorkflow()

# Run with UI enabled
workflow.run(ui=True)
```

This will automatically launch a Chainlit server with a chat interface for interacting with your workflow. The interface preserves your workflow's state and configuration, allowing users to interact with it through a user-friendly chat interface.

When the Chainlit server starts, you can access the interface through your web browser at `http://localhost:8000` by default.

## ðŸ§ª Illustrating Examples

### [Writing Improvement](https://github.com/xyin-anl/Nodeology/examples/writing_improvement.py)

<div align="left">
      <a href="https://www.youtube.com/watch?v=6wRJnV0OCWA">
         <img src="https://img.youtube.com/vi/6wRJnV0OCWA/0.jpg" style="width:100%;">
      </a>
</div>

### [Trajectory Analysis](https://github.com/xyin-anl/Nodeology/examples/trajectory_analysis.py)

<div align="left">
      <a href="https://www.youtube.com/watch?v=4c-TmLCWd_U">
         <img src="https://img.youtube.com/vi/4c-TmLCWd_U/0.jpg" style="width:100%;">
      </a>
</div>

## ðŸ”¬ Scientific Applications

- [PEAR: Ptychography automation framework](https://arxiv.org/abs/2410.09034)
- [AutoScriptCopilot: TEM experiment control](https://github.com/xyin-anl/AutoScriptCopilot)

## ðŸ‘¥ Contributing & Collaboration

We welcome comments, feedbacks, bugs report, code contributions and research collaborations. Please refer to CONTRIBUTING.md

If you find `nodeology` useful and may inspire your research, please use the **Cite this repository** function



================================================
FILE: CITATION.cff
================================================
cff-version: 1.2.0
message: "If you use this software, please cite it as below."
authors:
- family-names: "Xiangyu"
  given-names: "Yin"
  orcid: "https://orcid.org/0000-0003-2868-1728"
title: "Nodeology"
version: 0.0.1
date-released: 2024-11-20
url: "https://github.com/xyin-anl/Nodeology"


================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Nodeology

Thank you for your interest in contributing to Nodeology! This document provides guidelines and information about contributing to the project.

## Ways to Contribute

### Code Contributions

1. **Core Framework**

   - Bug fixes and improvements
   - Performance optimizations
   - New features
   - Test coverage

2. **Pre-built Components**
   - New node types
   - State definitions
   - Workflow templates
   - Domain-specific tools

### Documentation

- API documentation
- Usage examples
- Tutorials
- Best practices

### Research Collaborations

1. **Workflow Patterns**

   - Novel automation patterns
   - Optimization strategies
   - Human-AI interaction interfaces
   - Error handling approaches

2. **Scientific Integration**

   - Domain-specific applications
   - Instrument interfaces
   - Data processing pipelines
   - Analysis tools

3. **Evaluation Methods**

   - Benchmark development
   - Performance metrics
   - Reliability assessment
   - Comparison frameworks

4. **AI Integration**
   - Prompt optimization
   - Model evaluation
   - Hybrid processing
   - Knowledge integration

## Getting Started

1. **Development Setup**

   ```bash
   # Clone repository
   git clone https://github.com/xyin-anl/nodeology.git
   cd nodeology

   # Create virtual environment using venv or conda
   python -m venv venv
   source venv/bin/activate

   # Install dependencies
   pip install -r requirements.txt

   # Install pytest
   pip install pytest

   # Run tests
   pytest tests/
   ```

## Guidelines

1. **Code Contributions**

   - Fork repository
   - Create feature branch
   - Submit pull request
   - Use `black` formatter
   - Write clear commit messages
   - Add unit tests
   - Update/create documentation if possible
   - Include example usage if possible

2. **Documentation**

   - Clear, concise writing
   - Practical examples
   - Proper formatting
   - Complete coverage

3. **Community**
   - Be respectful
   - Provide constructive feedback
   - Help new users
   - Share knowledge



================================================
FILE: LICENSE
================================================
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.


================================================
FILE: MANIFEST.in
================================================
include LICENSE
include README.md
include requirements.txt
include CITATION.cff
include CONTRIBUTING.md
recursive-include examples *
recursive-include tests *
recursive-include nodeology *


================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta"


================================================
FILE: requirements.txt
================================================
requests
pyyaml
typing-extensions
numpy
plotly
kaleido
langgraph<=0.2.45
litellm>=1.0.0
langfuse>=2.0.0
chainlit>=2.0.0



================================================
FILE: setup.py
================================================
from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = [
        line.strip() for line in fh if line.strip() and not line.startswith("#")
    ]

setup(
    name="nodeology",
    version="0.0.2",
    author="Xiangyu Yin",
    author_email="xyin@anl.gov",
    description="Foundation AI-Enhanced Scientific Workflow",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/xyin-anl/nodeology",
    packages=find_packages(),
    include_package_data=True,
    package_data={
        "": ["*.md", "*.cff", "LICENSE"],
        "nodeology": ["examples/*", "tests/*"],
    },
    classifiers=[
        "Intended Audience :: Science/Research",
        "Programming Language :: Python :: 3",
    ],
    python_requires=">=3.10",
    install_requires=requirements,
)




================================================
FILE: examples/README.md
================================================
# Nodeology Examples

This directory contains example applications built with Nodeology, demonstrating features of the framework.

## Prerequisites

Before running the examples, ensure you have `nodeology` installed

```bash
pip install nodeology
```

## Directory Structure

- `writing_improvement.py` - Text analysis and improvement workflow
- `trajectory_analysis.py` - Particle trajectory simulation and visualization
- `public/` - Static assets for the examples (needed for `nodeology` UI elements)
- `.chainlit/` - Chainlit configuration files (needed for `nodeology` UI settings)

## Available Examples

### 1. Writing Improvement (`writing_improvement.py`)

An interactive application that helps users improve their writing through analysis and suggestions. This example demonstrates:

- State management with Nodeology
- Interactive user input handling
- Text analysis workflow
- Chainlit UI integration

To run this example:

```bash
cd examples
python writing_improvement.py
```

### 2. Particle Trajectory Analysis (`trajectory_analysis.py`)

A scientific application that simulates and visualizes particle trajectories under electromagnetic fields. This example showcases:

- Complex scientific calculations
- Interactive parameter input
- Data visualization
- State management for scientific workflows
- Advanced Chainlit UI features

To run this example:

```bash
cd examples
python trajectory_analysis.py
```

## Usage Tips

1. Each example will open in your default web browser when launched
2. Follow the interactive prompts in the Chainlit UI
3. You can modify parameters and experiment with different inputs
4. Use the chat interface to interact with the applications

## License

These examples are provided under the same license as the main Nodeology project. See the license headers in individual files for details.



================================================
FILE: examples/chainlit.md
================================================



================================================
FILE: examples/particle_trajectory_analysis.yaml
================================================
name: TrajectoryWorkflow_03_13_2025_20_06_45
state_defs:
- current_node_type: str
- previous_node_type: str
- human_input: str
- input: str
- output: str
- messages: List[dict]
- mass: float
- charge: float
- initial_velocity: ndarray
- E_field: ndarray
- B_field: ndarray
- confirm_parameters: bool
- parameters_updater_output: str
- positions: List[ndarray]
- trajectory_plot: str
- trajectory_plot_path: str
- analysis_result: dict
- continue_simulation: bool
nodes:
  display_parameters:
    type: display_parameters
    next: ask_confirm_parameters
  ask_confirm_parameters:
    type: ask_confirm_parameters
    sink: confirm_parameters
    next:
      condition: confirm_parameters
      then: calculate_trajectory
      otherwise: ask_parameters_input
  ask_parameters_input:
    type: ask_parameters_input
    sink: human_input
    next: update_parameters
  update_parameters:
    type: prompt
    template: 'Update the parameters based on the user''s input. Current parameters:
      mass: {mass} charge: {charge} initial_velocity: {initial_velocity} E_field:
      {E_field} B_field: {B_field} User input: {human_input} Please return the updated
      parameters in JSON format. {{ "mass": float, "charge": float, "initial_velocity":
      list[float], "E_field": list[float], "B_field": list[float] }}'
    sink: parameters_updater_output
    next: display_parameters
  calculate_trajectory:
    type: calculate_trajectory
    sink: positions
    next: plot_trajectory
  plot_trajectory:
    type: plot_trajectory
    sink: [trajectory_plot, trajectory_plot_path]
    next: analyze_trajectory
  analyze_trajectory:
    type: prompt
    template: 'Analyze this particle trajectory plot. Please determine: 1. The type
      of motion (linear, circular, helical, or chaotic) 2. Key physical features (radius,
      period, pitch angle if applicable) 3. Explanation of the motion 4. Anomalies
      in the motion Output in JSON format: {{ "trajectory_type": "type_name", "key_features":
      { "feature1": value, "feature2": value }, "explanation": "detailed explanation",
      "anomalies": "anomaly description" }}'
    sink: analysis_result
    image_keys: trajectory_plot_path
    next: ask_continue_simulation
  ask_continue_simulation:
    type: ask_continue_simulation
    sink: continue_simulation
    next:
      condition: continue_simulation
      then: display_parameters
      otherwise: END
entry_point: display_parameters
llm: gemini/gemini-2.0-flash
vlm: gemini/gemini-2.0-flash
exit_commands: [stop workflow, quit workflow, terminate workflow]



================================================
FILE: examples/trajectory_analysis.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2025>: Xiangyu Yin

import json
import tempfile
import numpy as np
from scipy.integrate import solve_ivp
from typing import List, Dict
from langgraph.graph import END
import chainlit as cl
from chainlit import Message, AskUserMessage, AskActionMessage, run_sync
from nodeology.state import State
from nodeology.node import Node, as_node
from nodeology.workflow import Workflow
import plotly.graph_objects as go


class TrajectoryState(State):
    """State for particle trajectory analysis workflow"""

    # Parameters
    mass: float  # Particle mass (kg)
    charge: float  # Particle charge (C)
    initial_velocity: np.ndarray  # Initial velocity vector [vx, vy, vz]
    E_field: np.ndarray  # Electric field vector [Ex, Ey, Ez]
    B_field: np.ndarray  # Magnetic field vector [Bx, By, Bz]

    # Confirm parameters
    confirm_parameters: bool

    # Parameters updater
    parameters_updater_output: str

    # Calculation results
    positions: List[np.ndarray]  # Position vectors at each time point

    # Image
    trajectory_plot: str
    trajectory_plot_path: str

    # Analysis results
    analysis_result: Dict

    # Continue simulation
    continue_simulation: bool


@as_node(sink=[])
def display_parameters(
    mass: float,
    charge: float,
    initial_velocity: np.ndarray,
    E_field: np.ndarray,
    B_field: np.ndarray,
):
    # Create a dictionary of parameters for the custom element
    parameters = {
        "Mass (kg)": mass,
        "Charge (C)": charge,
        "Initial Velocity (m/s)": initial_velocity.tolist(),
        "Electric Field (N/C)": E_field.tolist(),
        "Magnetic Field (T)": B_field.tolist(),
    }

    # Use the custom element to display parameters
    run_sync(
        Message(
            content="Below are the current simulation parameters:",
            elements=[
                cl.CustomElement(
                    name="DataDisplay",
                    props={
                        "data": parameters,
                        "title": "Particle Parameters",
                        "badge": "Configured",
                        "showScrollArea": False,
                    },
                )
            ],
        ).send()
    )
    return


@as_node(sink="confirm_parameters")
def ask_confirm_parameters():
    res = run_sync(
        AskActionMessage(
            content="Are you happy with the parameters?",
            timeout=300,
            actions=[
                cl.Action(
                    name="yes",
                    payload={"value": "yes"},
                    label="Yes",
                ),
                cl.Action(
                    name="no",
                    payload={"value": "no"},
                    label="No",
                ),
            ],
        ).send()
    )
    if res and res.get("payload").get("value") == "yes":
        return True
    else:
        return False


@as_node(sink=["human_input"])
def ask_parameters_input():
    human_input = run_sync(
        AskUserMessage(
            content="Please let me know how you want to change any of the parameters :)",
            timeout=300,
        ).send()
    )["output"]
    return human_input


parameters_updater = Node(
    node_type="parameters_updater",
    prompt_template="""Update the parameters based on the user's input.

Current parameters:
mass: {mass}
charge: {charge}
initial_velocity: {initial_velocity}
E_field: {E_field}
B_field: {B_field}

User input:
{human_input}

Please return the updated parameters in JSON format.
{{
    "mass": float,
    "charge": float,
    "initial_velocity": list[float],
    "E_field": list[float],
    "B_field": list[float]
}}
    """,
    sink="parameters_updater_output",
    sink_format="json",
)


def parameters_updater_transform(state, client, **kwargs):
    params_dict = json.loads(state["parameters_updater_output"])
    state["mass"] = params_dict["mass"]
    state["charge"] = params_dict["charge"]
    state["initial_velocity"] = np.array(params_dict["initial_velocity"])
    state["E_field"] = np.array(params_dict["E_field"])
    state["B_field"] = np.array(params_dict["B_field"])
    return state


parameters_updater.post_process = parameters_updater_transform


@as_node(sink=["positions"])
def calculate_trajectory(
    mass: float,
    charge: float,
    initial_velocity: np.ndarray,
    E_field: np.ndarray,
    B_field: np.ndarray,
) -> List[np.ndarray]:
    """Calculate particle trajectory under Lorentz force with automatic time steps"""
    B_magnitude = np.linalg.norm(B_field)
    if B_magnitude == 0 or charge == 0:
        # Handle the case where B=0 or charge=0 (no magnetic force)
        cyclotron_period = 1e-6  # Arbitrary time scale
    else:
        cyclotron_frequency = np.abs(charge) * B_magnitude / mass
        cyclotron_period = 2 * np.pi / cyclotron_frequency

    # Determine total simulation time and time steps
    num_periods = 5  # Simulate over 5 cyclotron periods
    num_points_per_period = 100  # At least 100 points per period
    total_time = num_periods * cyclotron_period
    total_points = int(num_periods * num_points_per_period)
    time_points = np.linspace(0, total_time, total_points)

    def lorentz_force(t, state):
        """Compute acceleration from Lorentz force"""
        vel = state[3:]
        force = charge * (E_field + np.cross(vel, B_field))
        acc = force / mass
        return np.concatenate([vel, acc])

    # Initial state vector [x, y, z, vx, vy, vz]
    initial_position = np.array([0.0, 0.0, 0.0])
    initial_state = np.concatenate([initial_position, initial_velocity])

    # Solve equations of motion
    solution = solve_ivp(
        lorentz_force,
        (time_points[0], time_points[-1]),
        initial_state,
        t_eval=time_points,
        method="RK45",
        rtol=1e-8,
    )

    if not solution.success:
        return [np.zeros(3) for _ in range(len(time_points))]

    return [solution.y[:3, i] for i in range(len(time_points))]


@as_node(sink=["trajectory_plot", "trajectory_plot_path"])
def plot_trajectory(positions: List[np.ndarray]) -> str:
    """Plot 3D particle trajectory and save to temp file

    Returns:
        tuple: (Plotly figure object, path to saved plot image)
    """
    positions = np.array(positions)

    # Create a Plotly 3D scatter plot
    fig = go.Figure(
        data=[
            go.Scatter3d(
                x=positions[:, 0],
                y=positions[:, 1],
                z=positions[:, 2],
                mode="lines",
                line=dict(width=4, color="green"),
            )
        ]
    )

    # Update layout
    fig.update_layout(
        scene=dict(xaxis_title="X (m)", yaxis_title="Y (m)", zaxis_title="Z (m)"),
    )

    # Save to temp file
    temp_path = tempfile.mktemp(suffix=".png")
    fig.write_image(temp_path)

    run_sync(
        Message(
            content="Below is the trajectory plot of the particle:",
            elements=[cl.Plotly(figure=fig)],
        ).send()
    )

    return fig, temp_path


trajectory_analyzer = Node(
    node_type="trajectory_analyzer",
    prompt_template="""Analyze this particle trajectory plot.

Please determine:
1. The type of motion (linear, circular, helical, or chaotic)
2. Key physical features (radius, period, pitch angle if applicable)
3. Explanation of the motion
4. Anomalies in the motion
Output in JSON format:
{{
    "trajectory_type": "type_name",
    "key_features": {
        "feature1": value,
        "feature2": value
    },
    "explanation": "detailed explanation",
    "anomalies": "anomaly description"
}}""",
    sink="analysis_result",
    sink_format="json",
    image_keys=["trajectory_plot_path"],
)


def display_trajectory_analyzer_result(state, client, **kwargs):
    state["analysis_result"] = json.loads(state["analysis_result"])

    # Use the custom element to display analysis results
    run_sync(
        Message(
            content="Below is the analysis of the particle trajectory:",
            elements=[
                cl.CustomElement(
                    name="DataDisplay",
                    props={
                        "data": state["analysis_result"],
                        "title": "Trajectory Analysis",
                        "badge": state["analysis_result"].get(
                            "trajectory_type", "Unknown"
                        ),
                        "maxHeight": "400px",
                    },
                )
            ],
        ).send()
    )
    return state


trajectory_analyzer.post_process = display_trajectory_analyzer_result


@as_node(sink="continue_simulation")
def ask_continue_simulation():
    res = run_sync(
        AskActionMessage(
            content="Would you like to continue the simulation?",
            timeout=300,
            actions=[
                cl.Action(
                    name="continue",
                    payload={"value": "continue"},
                    label="Continue Simulation",
                ),
                cl.Action(
                    name="finish",
                    payload={"value": "finish"},
                    label="Finish",
                ),
            ],
        ).send()
    )

    # Return the user's choice
    if res and res.get("payload").get("value") == "continue":
        return True
    else:
        return False


class TrajectoryWorkflow(Workflow):
    """Workflow for particle trajectory analysis"""

    def create_workflow(self):
        """Define the workflow graph structure"""
        # Add nodes
        self.add_node("display_parameters", display_parameters)
        self.add_node("ask_confirm_parameters", ask_confirm_parameters)
        self.add_node("ask_parameters_input", ask_parameters_input)
        self.add_node("update_parameters", parameters_updater)
        self.add_node("calculate_trajectory", calculate_trajectory)
        self.add_node("plot_trajectory", plot_trajectory)
        self.add_node("analyze_trajectory", trajectory_analyzer)
        self.add_node("ask_continue_simulation", ask_continue_simulation)

        self.add_flow("display_parameters", "ask_confirm_parameters")
        self.add_conditional_flow(
            "ask_confirm_parameters",
            "confirm_parameters",
            then="calculate_trajectory",
            otherwise="ask_parameters_input",
        )
        self.add_flow("ask_parameters_input", "update_parameters")
        self.add_flow("update_parameters", "display_parameters")
        self.add_flow("calculate_trajectory", "plot_trajectory")
        self.add_flow("plot_trajectory", "analyze_trajectory")
        self.add_flow("analyze_trajectory", "ask_continue_simulation")
        self.add_conditional_flow(
            "ask_continue_simulation",
            "continue_simulation",
            then="display_parameters",
            otherwise=END,
        )

        # Set entry point
        self.set_entry("display_parameters")

        # Compile workflow
        self.compile()


if __name__ == "__main__":
    workflow = TrajectoryWorkflow(
        state_defs=TrajectoryState,
        llm_name="gemini/gemini-2.0-flash",
        vlm_name="gemini/gemini-2.0-flash",
        debug_mode=False,
    )

    # # Export workflow to YAML file
    # workflow.to_yaml("particle_trajectory_analysis.yaml")

    # # Print workflow graph
    # workflow.graph.get_graph().draw_mermaid_png(
    #     output_file_path="particle_trajectory_analysis.png"
    # )

    initial_state = {
        "mass": 9.1093837015e-31,  # electron mass in kg
        "charge": -1.602176634e-19,  # electron charge in C
        "initial_velocity": np.array([1e6, 1e6, 1e6]),  # 1e6 m/s in each direction
        "E_field": np.array([5e6, 1e6, 5e6]),  # 1e6 N/C in y-direction
        "B_field": np.array(
            [0.0, 0.0, 50000.0]
        ),  # deliberately typo to be caught by validation
    }

    result = workflow.run(init_values=initial_state, ui=True)



================================================
FILE: examples/writing_improvement.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2025>: Xiangyu Yin

import json
from nodeology.state import State
from nodeology.node import Node, as_node
from nodeology.workflow import Workflow

import chainlit as cl
from chainlit import Message, AskActionMessage, run_sync
from langgraph.graph import END


# 1. Define your state
class TextAnalysisState(State):
    analysis: dict  # Analysis results
    text: str  # Enhanced text
    continue_improving: bool  # Whether to continue improving


# 2. Create nodes
@as_node(sink="text")
def parse_human_input(human_input: str):
    return human_input


analyze_text = Node(
    prompt_template="""Text to analyze: {text}

Analyze the above text for:
- Clarity (1-10)
- Grammar (1-10)
- Style (1-10)
- Suggestions for improvement

Output as JSON:
{{
    "clarity_score": int,
    "grammar_score": int,
    "style_score": int,
    "suggestions": str
}}
""",
    sink="analysis",
    sink_format="json",
)


def report_analysis(state, client, **kwargs):
    analysis = json.loads(state["analysis"])
    run_sync(
        Message(
            content="Below is the analysis of the text:",
            elements=[cl.CustomElement(name="DataDisplay", props={"data": analysis})],
        ).send()
    )
    return state


analyze_text.post_process = report_analysis

improve_text = Node(
    prompt_template="""Text to improve: {text}

Analysis: {analysis}

Rewrite the text incorporating the suggestions while maintaining the original meaning.
Focus on clarity, grammar, and style improvements. Return the improved text only.""",
    sink="text",
)


def report_improvement(state, client, **kwargs):
    text_md = f"{state['text']}"
    run_sync(
        Message(
            content="Below is the improved text:", elements=[cl.Text(content=text_md)]
        ).send()
    )
    return state


improve_text.post_process = report_improvement


@as_node(sink="continue_improving")
def ask_continue_improve():
    res = run_sync(
        AskActionMessage(
            content="Would you like to further improve the text?",
            timeout=300,
            actions=[
                cl.Action(
                    name="continue",
                    payload={"value": "continue"},
                    label="Continue Improving",
                ),
                cl.Action(
                    name="finish",
                    payload={"value": "finish"},
                    label="Finish",
                ),
            ],
        ).send()
    )

    # Return the user's choice
    if res and res.get("payload").get("value") == "continue":
        return True
    else:
        return False


# 3. Create workflow
class TextEnhancementWorkflow(Workflow):
    state_schema = TextAnalysisState

    def create_workflow(self):
        # Add nodes
        self.add_node("parse_human_input", parse_human_input)
        self.add_node("analyze", analyze_text)
        self.add_node("improve", improve_text)
        self.add_node("ask_continue", ask_continue_improve)

        # Connect nodes
        self.add_flow("parse_human_input", "analyze")
        self.add_flow("analyze", "improve")
        self.add_flow("improve", "ask_continue")

        # Add conditional flow based on user's choice
        self.add_conditional_flow(
            "ask_continue",
            "continue_improving",
            "analyze",
            END,
        )

        # Set entry point
        self.set_entry("parse_human_input")

        # Compile workflow
        self.compile(
            interrupt_before=["parse_human_input"],
            interrupt_before_phrases={
                "parse_human_input": "Please enter the text to analyze."
            },
        )


# 4. Run workflow
workflow = TextEnhancementWorkflow(
    llm_name="gemini/gemini-2.0-flash", save_artifacts=True
)

if __name__ == "__main__":
    result = workflow.run(ui=True)



================================================
FILE: examples/public/hide-watermark.js
================================================
function hideWatermark() {
  // Try multiple selector approaches
  const selectors = [
    "#chainlit-copilot",
    ".cl-copilot-container",
    "[data-testid='copilot-container']",
    // Add any other potential selectors
  ];

  for (const selector of selectors) {
    const elements = document.querySelectorAll(selector);
    
    elements.forEach(element => {
      // Try to access shadow DOM if it exists
      if (element.shadowRoot) {
        const watermarks = element.shadowRoot.querySelectorAll("a.watermark, .watermark, [class*='watermark']");
        watermarks.forEach(watermark => {
          watermark.style.display = "none";
          watermark.style.visibility = "hidden";
          watermark.remove(); // Try to remove it completely
        });
      }
      
      // Also check for watermarks in the regular DOM
      const directWatermarks = element.querySelectorAll("a.watermark, .watermark, [class*='watermark']");
      directWatermarks.forEach(watermark => {
        watermark.style.display = "none";
        watermark.style.visibility = "hidden";
        watermark.remove(); // Try to remove it completely
      });
    });
  }

  // Add CSS to hide watermarks globally
  const style = document.createElement('style');
  style.textContent = `
    a.watermark, .watermark, [class*='watermark'] {
      display: none !important;
      visibility: hidden !important;
      opacity: 0 !important;
      pointer-events: none !important;
    }
  `;
  document.head.appendChild(style);
}

// More aggressive approach with mutation observer for the entire document
function setupGlobalObserver() {
  const observer = new MutationObserver((mutations) => {
    let shouldCheck = false;
    
    for (const mutation of mutations) {
      if (mutation.addedNodes.length > 0) {
        shouldCheck = true;
        break;
      }
    }
    
    if (shouldCheck) {
      hideWatermark();
    }
  });
  
  observer.observe(document.body, { 
    childList: true, 
    subtree: true 
  });
}

// Run on page load
document.addEventListener("DOMContentLoaded", function() {
  // Try immediately
  hideWatermark();
  
  // Setup global observer
  setupGlobalObserver();
  
  // Try again after delays to catch late-loading elements
  setTimeout(hideWatermark, 1000);
  setTimeout(hideWatermark, 3000);
  
  // Periodically check
  setInterval(hideWatermark, 5000);
});

// Also run the script immediately in case the DOM is already loaded
if (document.readyState === "complete" || document.readyState === "interactive") {
  hideWatermark();
  setTimeout(setupGlobalObserver, 0);
}



================================================
FILE: examples/public/theme.json
================================================
{
    "custom_fonts": [],
    "variables": {
        "light": {
            "--font-sans": "'Inter', sans-serif",
            "--font-mono": "source-code-pro, Menlo, Monaco, Consolas, 'Courier New', monospace",
            "--background": "0 0% 100%",
            "--foreground": "0 0% 5%",
            "--card": "0 0% 100%",
            "--card-foreground": "0 0% 5%",
            "--popover": "0 0% 100%",
            "--popover-foreground": "0 0% 5%",
            "--primary": "150 40% 60%",
            "--primary-foreground": "0 0% 100%",
            "--secondary": "150 30% 90%",
            "--secondary-foreground": "150 30% 20%",
            "--muted": "0 0% 90%",
            "--muted-foreground": "150 15% 30%",
            "--accent": "0 0% 95%",
            "--accent-foreground": "150 30% 20%",
            "--destructive": "0 84.2% 60.2%",
            "--destructive-foreground": "210 40% 98%",
            "--border": "150 30% 75%",
            "--input": "150 30% 75%",
            "--ring": "150 40% 60%",
            "--radius": "0.75rem",
            "--sidebar-background": "0 0% 98%",
            "--sidebar-foreground": "240 5.3% 26.1%",
            "--sidebar-primary": "150 40% 60%",
            "--sidebar-primary-foreground": "0 0% 98%",
            "--sidebar-accent": "240 4.8% 95.9%",
            "--sidebar-accent-foreground": "240 5.9% 10%",
            "--sidebar-border": "220 13% 91%",
            "--sidebar-ring": "217.2 91.2% 59.8%"
        },
        "dark": {
            "--font-sans": "'Inter', sans-serif",
            "--font-mono": "source-code-pro, Menlo, Monaco, Consolas, 'Courier New', monospace",
            "--background": "0 0% 13%",
            "--foreground": "0 0% 93%",
            "--card": "0 0% 18%",
            "--card-foreground": "210 40% 98%",
            "--popover": "0 0% 18%",
            "--popover-foreground": "210 40% 98%",
            "--primary": "150 45% 50%",
            "--primary-foreground": "0 0% 100%",
            "--secondary": "150 35% 25%",
            "--secondary-foreground": "0 0% 98%",
            "--muted": "150 15% 30%",
            "--muted-foreground": "150 10% 80%",
            "--accent": "150 40% 40%",
            "--accent-foreground": "0 0% 98%",
            "--destructive": "0 62.8% 30.6%",
            "--destructive-foreground": "210 40% 98%",
            "--border": "150 30% 40%",
            "--input": "150 30% 40%",
            "--ring": "150 45% 50%",
            "--sidebar-background": "0 0% 9%",
            "--sidebar-foreground": "240 4.8% 95.9%",
            "--sidebar-primary": "150 45% 50%",
            "--sidebar-primary-foreground": "0 0% 100%",
            "--sidebar-accent": "150 25% 20%",
            "--sidebar-accent-foreground": "240 4.8% 95.9%",
            "--sidebar-border": "240 3.7% 15.9%",
            "--sidebar-ring": "217.2 91.2% 59.8%"
        }
    }
}


================================================
FILE: examples/public/elements/DataDisplay.jsx
================================================
import { Card, CardHeader, CardTitle, CardContent } from "@/components/ui/card"
import { Badge } from "@/components/ui/badge"
import { Separator } from "@/components/ui/separator"
import { ScrollArea } from "@/components/ui/scroll-area"
import { Button } from "@/components/ui/button"
import { Copy, ChevronDown, ChevronRight } from "lucide-react"
import { useState } from "react"

export default function DataDisplay() {
  // Data is passed via props
  const data = props.data || {};
  const title = props.title || "Data";
  const badge = props.badge || null;
  const maxHeight = props.maxHeight || "300px";
  const showScrollArea = props.showScrollArea !== false;
  const collapsible = props.collapsible !== false;
  const theme = props.theme || "default"; // default, compact, or expanded
  
  // State for collapsible sections
  const [expandedSections, setExpandedSections] = useState({});
  
  // Toggle section expansion
  const toggleSection = (key) => {
    setExpandedSections(prev => ({
      ...prev,
      [key]: !prev[key]
    }));
  };
  
  // Copy value to clipboard
  const copyToClipboard = (value) => {
    let textToCopy;
    
    if (Array.isArray(value)) {
      // Format arrays properly for copying
      textToCopy = JSON.stringify(value);
    } else if (typeof value === 'object' && value !== null) {
      textToCopy = JSON.stringify(value, null, 2);
    } else {
      textToCopy = String(value);
    }
    
    navigator.clipboard.writeText(textToCopy);
    // Could use sonner for toast notification here
  };

  // Helper to format values with scientific notation for very small/large numbers
  const formatValue = (value) => {
    if (value === undefined || value === null) return "N/A";
    
    if (Array.isArray(value) || (typeof value === 'object' && value.length)) {
      return formatArray(value);
    }
    
    if (typeof value === 'object' && value !== null) {
      return null; // Handled separately in the render
    }
    
    if (typeof value === 'number') {
      // Use scientific notation for very small or large numbers
      if (Math.abs(value) < 0.001 || Math.abs(value) > 10000) {
        return value.toExponential(4);
      }
      // Format with appropriate precision
      return Number.isInteger(value) ? value.toString() : value.toFixed(4).replace(/\.?0+$/, '');
    }
    
    if (typeof value === 'boolean') {
      return value ? "true" : "false";
    }
    
    return String(value);
  };
  
  // Helper function to format arrays nicely
  const formatArray = (arr) => {
    if (!arr) return "N/A";
    if (typeof arr === 'string') return arr;
    
    try {
      // Handle both array-like objects and actual arrays
      if (arr.length > 10) {
        const displayed = Array.from(arr).slice(0, 10);
        return `[${displayed.map(val => 
          typeof val === 'object' ? '{...}' : 
          typeof val === 'number' ? formatValue(val) : 
          JSON.stringify(val)
        ).join(", ")}, ... ${arr.length - 10} more]`;
      }
      
      return `[${Array.from(arr).map(val => 
        typeof val === 'object' ? '{...}' : 
        typeof val === 'number' ? formatValue(val) : 
        JSON.stringify(val)
      ).join(", ")}]`;
    } catch (e) {
      return String(arr);
    }
  };
  
  // Helper function to render nested objects
  const renderNestedObject = (obj, path = "", level = 0) => {
    if (!obj || typeof obj !== 'object') return <span>{String(obj || "N/A")}</span>;
    
    if (Array.isArray(obj)) {
      return (
        <div className="flex items-center justify-between">
          <span className="text-sm font-mono">{formatArray(obj)}</span>
          <Button 
            variant="ghost" 
            size="icon" 
            className="h-5 w-5 p-0 opacity-50 hover:opacity-100 ml-2" 
            onClick={() => copyToClipboard(obj)}
          >
            <Copy className="h-3 w-3" />
          </Button>
        </div>
      );
    }
    
    const isExpanded = expandedSections[path] !== false; // Default to expanded
    
    return (
      <div className={`space-y-1 ${level > 0 ? "pl-4" : ""}`}>
        {Object.entries(obj).map(([key, value], index) => {
          const currentPath = path ? `${path}.${key}` : key;
          const isObject = typeof value === 'object' && value !== null;
          
          return (
            <div key={currentPath} className="pt-1">
              {index > 0 && level === 0 && <Separator className="my-2" />}
              
              <div className="flex items-center">
                <div className="flex items-center gap-1 flex-grow">
                  {isObject && collapsible && (
                    <Button 
                      variant="ghost" 
                      size="icon" 
                      className="h-5 w-5 p-0" 
                      onClick={() => toggleSection(currentPath)}
                    >
                      {expandedSections[currentPath] === false ? 
                        <ChevronRight className="h-4 w-4" /> : 
                        <ChevronDown className="h-4 w-4" />
                      }
                    </Button>
                  )}
                  <div className="font-medium text-sm">{key}</div>
                </div>
              </div>
              
              <div className={`
                text-sm 
                ${level > 0 ? "border-l-2 border-gray-200 dark:border-gray-700 pl-2" : ""}
                ${isObject && expandedSections[currentPath] === false ? "hidden" : ""}
              `}>
                {isObject ? 
                  renderNestedObject(value, currentPath, level + 1) : 
                  <div className="flex items-center justify-between">
                    <div className="font-mono">{formatValue(value)}</div>
                    <Button 
                      variant="ghost" 
                      size="icon" 
                      className="h-5 w-5 p-0 opacity-50 hover:opacity-100 ml-2" 
                      onClick={() => copyToClipboard(value)}
                    >
                      <Copy className="h-3 w-3" />
                    </Button>
                  </div>
                }
              </div>
            </div>
          );
        })}
      </div>
    );
  };

  // Apply theme styles
  const getThemeStyles = () => {
    switch (theme) {
      case 'compact':
        return "text-xs";
      case 'expanded':
        return "text-base";
      default:
        return "text-sm";
    }
  };

  return (
    <Card className={`w-full ${getThemeStyles()}`}>
      <CardHeader className="pb-2">
        <div className="flex justify-between items-center">
          <CardTitle className="text-lg font-medium">
            {title}
          </CardTitle>
          <div className="flex items-center gap-2">
            {badge && (
              <Badge variant="outline">{badge}</Badge>
            )}
            <Button 
              variant="ghost" 
              size="icon" 
              className="h-6 w-6 p-0" 
              onClick={() => copyToClipboard(data)}
              title="Copy all data"
            >
              <Copy className="h-4 w-4" />
            </Button>
          </div>
        </div>
      </CardHeader>
      <CardContent>
        {Object.keys(data).length === 0 ? (
          <div className="text-center text-muted-foreground py-4">No data available</div>
        ) : (
          showScrollArea ? (
            <ScrollArea style={{ height: maxHeight }} className="pr-4">
              {renderNestedObject(data)}
            </ScrollArea>
          ) : (
            renderNestedObject(data)
          )
        )}
      </CardContent>
    </Card>
  )
} 


================================================
FILE: examples/.chainlit/config.toml
================================================
[project]
# Whether to enable telemetry (default: true). No personal data is collected.
enable_telemetry = false

# List of environment variables to be provided by each user to use the app.
user_env = []

# Duration (in seconds) during which the session is saved when the connection is lost
session_timeout = 3600

# Duration (in seconds) of the user session expiry
user_session_timeout = 1296000  # 15 days

# Enable third parties caching (e.g LangChain cache)
cache = false

# Authorized origins
allow_origins = ["*"]

[features]
# Process and display HTML in messages. This can be a security risk (see https://stackoverflow.com/questions/19603097/why-is-it-dangerous-to-render-user-generated-html-or-javascript)
unsafe_allow_html = false

# Process and display mathematical expressions. This can clash with "$" characters in messages.
latex = false

# Automatically tag threads with the current chat profile (if a chat profile is used)
auto_tag_thread = true

# Allow users to edit their own messages
edit_message = true

# Authorize users to spontaneously upload files with messages
[features.spontaneous_file_upload]
    enabled = true
    # Define accepted file types using MIME types
    # Examples:
    # 1. For specific file types:
    #    accept = ["image/jpeg", "image/png", "application/pdf"]
    # 2. For all files of certain type:
    #    accept = ["image/*", "audio/*", "video/*"]
    # 3. For specific file extensions:
    #    accept = { "application/octet-stream" = [".xyz", ".pdb"] }
    # Note: Using "*/*" is not recommended as it may cause browser warnings
    accept = ["*/*"]
    max_files = 20
    max_size_mb = 500

[features.audio]
    # Sample rate of the audio
    sample_rate = 24000

[UI]
# Name of the assistant.
name = "Assistant"

# default_theme = "light"

# layout = "wide"

# Description of the assistant. This is used for HTML tags.
# description = ""

# Chain of Thought (CoT) display mode. Can be "hidden", "tool_call" or "full".
cot = "full"

# Specify a CSS file that can be used to customize the user interface.
# The CSS file can be served from the public directory or via an external link.
# custom_css = "/public/test.css"

# Specify a Javascript file that can be used to customize the user interface.
# The Javascript file can be served from the public directory.
custom_js = "/public/hide-watermark.js"

# Specify a custom meta image url.
# custom_meta_image_url = "https://chainlit-cloud.s3.eu-west-3.amazonaws.com/logo/chainlit_banner.png"

# Specify a custom build directory for the frontend.
# This can be used to customize the frontend code.
# Be careful: If this is a relative path, it should not start with a slash.
# custom_build = "./public/build"

# Specify optional one or more custom links in the header.
# [[UI.header_links]]
#    name = "Nodeology"
#    icon_url = "https://avatars.githubusercontent.com/u/128686189?s=200&v=4"
#    url = "https://github.com/xyin-anl/Nodeology"

[meta]
generated_by = "2.2.1"



================================================
FILE: nodeology/__init__.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin



================================================
FILE: nodeology/client.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os, base64, json, getpass
from abc import ABC, abstractmethod
import litellm
from datetime import datetime


def get_client(model_name, **kwargs):
    """
    Factory function to create appropriate client based on model name.

    Handles three scenarios:
    1. Just model name (e.g., "gpt-4o") - Let LiteLLM figure out the provider
    2. Model name with provider keyword (e.g., model="gpt-4o", provider="openai")
    3. Provider/name convention (e.g., "openai/gpt-4o")

    Args:
        model_name (str): Name of the model to use
        **kwargs: Additional arguments including optional 'provider'

    Returns:
        LLM_Client or VLM_Client: Appropriate client instance for the requested model
    """
    # Handle special clients first
    if model_name == "mock":
        return Mock_LLM_Client(**kwargs)
    elif model_name == "mock_vlm":
        return Mock_VLM_Client(**kwargs)

    # Get provider from kwargs if specified (Scenario 2)
    provider = kwargs.pop("provider", None)

    # Get tracing_enabled from kwargs
    tracing_enabled = kwargs.pop("tracing_enabled", False)

    # Handle provider/model format (Scenario 3)
    if "/" in model_name and provider is None:
        provider, model_name = model_name.split("/", 1)

    # Create LiteLLM client - for Scenario 1, provider will be None
    try:
        return LiteLLM_Client(
            model_name, provider=provider, tracing_enabled=tracing_enabled, **kwargs
        )
    except Exception as e:
        raise ValueError(f"Error creating client for model {model_name}: {e}")


def configure_langfuse(public_key=None, secret_key=None, host=None, enabled=True):
    """
    Configure Langfuse for observability.

    Args:
        public_key (str, optional): Langfuse public key. Defaults to LANGFUSE_PUBLIC_KEY env var.
        secret_key (str, optional): Langfuse secret key. Defaults to LANGFUSE_SECRET_KEY env var.
        host (str, optional): Langfuse host URL. Defaults to LANGFUSE_HOST env var or https://cloud.langfuse.com.
        enabled (bool, optional): Whether to enable Langfuse tracing. Defaults to True.
    """
    if not enabled:
        litellm.success_callback = []
        litellm.failure_callback = []
        return

    # Set environment variables if provided
    if public_key:
        os.environ["LANGFUSE_PUBLIC_KEY"] = public_key
    if secret_key:
        os.environ["LANGFUSE_SECRET_KEY"] = secret_key
    if host:
        os.environ["LANGFUSE_HOST"] = host

    litellm.success_callback = ["langfuse"]
    litellm.failure_callback = ["langfuse"]


class LLM_Client(ABC):
    """Base abstract class for Language Model clients."""

    def __init__(self) -> None:
        pass

    @abstractmethod
    def __call__(self, messages, **kwargs) -> str:
        """
        Process messages and return model response.

        Args:
            messages (list): List of message dictionaries with 'role' and 'content'
            **kwargs: Additional model-specific parameters

        Returns:
            str: Model's response text
        """
        pass


class VLM_Client(LLM_Client):
    """Base abstract class for Vision Language Model clients."""

    def __init__(self) -> None:
        super().__init__()

    @abstractmethod
    def process_images(self, messages, images, **kwargs) -> list:
        """
        Process and format images for the model.

        Args:
            messages (list): List of message dictionaries
            images (list): List of image file paths
            **kwargs: Additional processing parameters

        Returns:
            list: Updated messages with processed images
        """
        pass


class Mock_LLM_Client(LLM_Client):
    def __init__(self, response=None, **kwargs) -> None:
        super().__init__()
        self.response = response
        self.model_name = "mock"

    def __call__(self, messages, **kwargs) -> str:
        response = (
            "\n".join([msg["role"] + ": " + msg["content"] for msg in messages])
            if self.response is None
            else self.response
        )
        return response


class Mock_VLM_Client(VLM_Client):
    def __init__(self, response=None, **kwargs) -> None:
        super().__init__()
        self.response = response
        self.model_name = "mock_vlm"

    def __call__(self, messages, images=None, **kwargs) -> str:
        if images is not None:
            messages = self.process_images(messages, images)
        if self.response is None:
            message_parts = []
            for msg in messages:
                content = msg["content"]
                if isinstance(content, str):
                    message_parts.append(f"{msg['role']}: {content}")
                else:  # content is already a list of text/image objects
                    parts = []
                    for item in content:
                        if item["type"] == "text":
                            parts.append(item["text"])
                        elif item["type"] == "image":
                            parts.append(f"[Image: {item['image_url']['url']}]")
                    message_parts.append(f"{msg['role']}: {' '.join(parts)}")
            return "\n".join(message_parts)
        return self.response

    def process_images(self, messages, images, **kwargs) -> list:
        # Make a copy to avoid modifying the original
        messages = messages.copy()

        # Simply append a placeholder for each image
        for img in images:
            if isinstance(messages[-1]["content"], str):
                messages[-1]["content"] = [
                    {"type": "text", "text": messages[-1]["content"]},
                    {"type": "image", "image_url": {"url": f"mock_processed_{img}"}},
                ]
            elif isinstance(messages[-1]["content"], list):
                messages[-1]["content"].append(
                    {"type": "image", "image_url": {"url": f"mock_processed_{img}"}}
                )
        return messages


class LiteLLM_Client(VLM_Client):
    """
    Unified client for all LLM/VLM providers using LiteLLM.
    Supports both text and image inputs across multiple providers.
    """

    def __init__(
        self,
        model_name,
        provider=None,
        model_options=None,
        api_key=None,
        tracing_enabled=False,
    ) -> None:
        """
        Initialize LiteLLM client.

        Args:
            model_name (str): Name of the model to use
            provider (str, optional): Provider name (openai, anthropic, etc.)
            model_options (dict): Model parameters like temperature and top_p
            api_key (str, optional): API key for the specified provider
            tracing_enabled (bool, optional): Whether to enable Langfuse tracing. Defaults to False.
        """
        super().__init__()
        self.model_options = model_options if model_options else {}
        self.tracing_enabled = tracing_enabled

        # Set API key if provided
        if api_key and provider:
            os.environ[f"{provider.upper()}_API_KEY"] = api_key

        # Construct the model name for LiteLLM based on whether provider is specified
        # If provider is None, LiteLLM will infer the provider from the model name
        self.model_name = f"{provider}/{model_name}" if provider else model_name

    def collect_langfuse_metadata(
        self,
        workflow=None,
        node=None,
        **kwargs,
    ):
        """
        Collect metadata for Langfuse tracing from workflow and node information.

        Args:
            workflow: The workflow instance (optional)
            node: The node instance (optional)
            **kwargs: Additional metadata to include

        Returns:
            dict: Metadata dictionary formatted for Langfuse
        """
        metadata = {}

        timestamp = datetime.now().strftime("%Y%m%d")
        user_id = getpass.getuser()
        session_id_str = f"{user_id}-{timestamp}"

        metadata["trace_metadata"] = {}

        # Extract workflow metadata if available
        if workflow:
            # Use workflow class name as generation name
            metadata["generation_name"] = workflow.__class__.__name__
            session_id_str += f"-{workflow.__class__.__name__}"

            # Create a generation ID based on workflow name and timestamp
            metadata["generation_id"] = f"gen-{workflow.name}-{timestamp}"

            # Add user ID if available
            if hasattr(workflow, "user_name"):
                metadata["trace_user_id"] = workflow.user_name
            else:
                metadata["trace_user_id"] = user_id

        # Extract node metadata if available
        if node:
            # Use node type as trace name
            metadata["trace_name"] = node.node_type
            session_id_str += f"-{node.node_type}"

            # Add node metadata to trace metadata
            metadata["trace_metadata"].update(
                {
                    "required_keys": node.required_keys,
                    "sink": node.sink,
                    "sink_format": node.sink_format,
                    "image_keys": node.image_keys,
                    "use_conversation": node.use_conversation,
                    "prompt_template": node.prompt_template,
                }
            )

        # Add session ID based on timestamp
        metadata["session_id"] = f"session-{session_id_str}"

        # Add any additional metadata from kwargs
        metadata["trace_metadata"].update(kwargs)

        return metadata

    def process_images(self, messages, images):
        """
        Process and format images for the model using LiteLLM's format.

        Args:
            messages (list): List of message dictionaries
            images (list): List of image file paths

        Returns:
            list: Updated messages with processed images
        """
        # Make a copy to avoid modifying the original
        messages = messages.copy()

        # Convert images to base64
        image_contents = []
        for img in images:
            with open(img, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                image_contents.append(
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"},
                    }
                )

        # Add images to the last message
        if isinstance(messages[-1]["content"], str):
            messages[-1]["content"] = [
                {"type": "text", "text": messages[-1]["content"]}
            ] + image_contents
        elif isinstance(messages[-1]["content"], list):
            messages[-1]["content"] += image_contents

        return messages

    def __call__(
        self, messages, images=None, format=None, workflow=None, node=None, **kwargs
    ) -> str:
        """
        Process messages and return model response using LiteLLM.

        Args:
            messages (list): List of message dictionaries
            images (list, optional): List of image file paths
            format (str, optional): Response format (e.g., 'json')
            workflow (optional): The workflow instance for metadata extraction
            node (optional): The node instance for metadata extraction
            **kwargs: Additional parameters including metadata for Langfuse

        Returns:
            str: Model's response text
        """
        # Process images if provided
        if images is not None:
            messages = self.process_images(messages, images)

        # Set up response format if needed
        response_format = {"type": "json_object"} if format == "json" else None

        # Extract Langfuse metadata only if tracing is enabled
        langfuse_metadata = {}
        if self.tracing_enabled:
            langfuse_metadata = self.collect_langfuse_metadata(
                workflow=workflow,
                node=node,
                **kwargs,
            )

        try:
            # Use LiteLLM's built-in retry mechanism with Langfuse metadata
            response = litellm.completion(
                model=self.model_name,
                messages=messages,
                response_format=response_format,
                num_retries=3,
                metadata=langfuse_metadata if self.tracing_enabled else {},
                **self.model_options,
            )

            content = response.choices[0].message.content

            # Validate JSON if requested
            if format == "json":
                try:
                    json.loads(content)
                except json.JSONDecodeError:
                    raise ValueError(f"Invalid JSON response from {self.model_name}")

            return content

        except Exception as e:
            raise ValueError(
                f"Failed to generate response from {self.model_name}. Error: {str(e)}"
            )



================================================
FILE: nodeology/interface.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2025>: Xiangyu Yin

import os, json, importlib, threading, traceback, contextvars
import logging
import chainlit as cl
from chainlit.cli import run_chainlit
from nodeology.state import StateEncoder, convert_serialized_objects


logger = logging.getLogger(__name__)


def run_chainlit_for_workflow(workflow, initial_state=None):
    """
    Called by workflow.run(ui=True). This function:
      1. stores workflow and initial_state in user session data
      2. starts the chainlit server
      3. returns the final state when the workflow completes
    """
    os.environ["NODEOLOGY_WORKFLOW_CLASS"] = (
        workflow.__class__.__module__ + "." + workflow.__class__.__name__
    )

    logger.info(f"Starting UI for workflow: {workflow.__class__.__name__}")

    # Save the initialization arguments to ensure they're passed when recreating the workflow
    if hasattr(workflow, "_init_kwargs"):
        logger.info(
            f"Found initialization kwargs: {list(workflow._init_kwargs.keys())}"
        )

        # We need to handle non-serializable objects in the kwargs
        serializable_kwargs = {}
        for key, value in workflow._init_kwargs.items():
            # Handle special cases
            if key == "state_defs":
                # For state_defs, we need to handle it specially
                if value is None:
                    # If None, we'll use the workflow's state_schema
                    serializable_kwargs[key] = None
                    logger.info(
                        f"Serializing {key} as None (will use workflow's state_schema)"
                    )
                elif isinstance(value, type) and hasattr(value, "__annotations__"):
                    # If it's a TypedDict or similar class with annotations, we'll use its name
                    # The workflow class will handle recreating it
                    serializable_kwargs["_state_defs_class"] = (
                        f"{value.__module__}.{value.__name__}"
                    )
                    logger.info(
                        f"Serializing state_defs as class reference: {serializable_kwargs['_state_defs_class']}"
                    )
                elif isinstance(value, list):
                    # If it's a list of state definitions, we'll try to serialize it
                    # This is complex and might not work for all cases
                    try:
                        # Convert any TypedDict classes to their module.name string
                        serialized_list = []
                        for item in value:
                            if isinstance(item, type) and hasattr(
                                item, "__annotations__"
                            ):
                                serialized_list.append(
                                    f"{item.__module__}.{item.__name__}"
                                )
                            elif isinstance(item, tuple) and len(item) == 2:
                                # Handle (name, type) tuples
                                name, type_hint = item
                                if isinstance(type_hint, type):
                                    serialized_list.append(
                                        [
                                            name,
                                            f"{type_hint.__module__}.{type_hint.__name__}",
                                        ]
                                    )
                                else:
                                    serialized_list.append([name, str(type_hint)])
                            elif isinstance(item, dict) and len(item) == 1:
                                # Handle {"name": type} dictionaries
                                name, type_hint = next(iter(item.items()))
                                if isinstance(type_hint, type):
                                    serialized_list.append(
                                        {
                                            name: f"{type_hint.__module__}.{type_hint.__name__}"
                                        }
                                    )
                                else:
                                    serialized_list.append({name: str(type_hint)})
                            else:
                                # Skip items we can't serialize
                                logger.info(
                                    f"Skipping non-serializable state_def item: {item}"
                                )

                        if serialized_list:
                            serializable_kwargs["_state_defs_list"] = serialized_list
                            logger.info(
                                f"Serializing state_defs as list: {serialized_list}"
                            )
                        else:
                            logger.info("Could not serialize any state_defs items")
                    except Exception as e:
                        logger.error(f"Error serializing state_defs list: {str(e)}")
                else:
                    logger.info(
                        f"Cannot serialize state_defs of type {type(value).__name__}"
                    )
            elif key == "checkpointer":
                # For checkpointer, just store "memory" if it's a string or an object
                if isinstance(value, str):
                    serializable_kwargs[key] = value
                else:
                    serializable_kwargs[key] = "memory"
                logger.info(f"Serializing checkpointer as: {serializable_kwargs[key]}")
            elif isinstance(value, (str, int, float, bool, type(None))):
                serializable_kwargs[key] = value
                logger.info(
                    f"Serializing {key} as primitive type: {type(value).__name__}"
                )
            elif isinstance(value, list) and all(
                isinstance(item, (str, int, float, bool, type(None))) for item in value
            ):
                serializable_kwargs[key] = value
                logger.info(f"Serializing {key} as list of primitives")
            elif isinstance(value, dict) and all(
                isinstance(k, str)
                and isinstance(v, (str, int, float, bool, type(None)))
                for k, v in value.items()
            ):
                serializable_kwargs[key] = value
                logger.info(f"Serializing {key} as dict of primitives")
            else:
                logger.info(
                    f"Skipping non-serializable {key} of type {type(value).__name__}"
                )
            # Skip other complex objects that can't be easily serialized

        # For client objects, just store their names
        if (
            "llm_name" in workflow._init_kwargs
            and hasattr(workflow, "llm_client")
            and hasattr(workflow.llm_client, "model_name")
        ):
            serializable_kwargs["llm_name"] = workflow.llm_client.model_name
            logger.info(
                f"Using llm_client.model_name: {workflow.llm_client.model_name}"
            )

        if (
            "vlm_name" in workflow._init_kwargs
            and hasattr(workflow, "vlm_client")
            and hasattr(workflow.vlm_client, "model_name")
        ):
            serializable_kwargs["vlm_name"] = workflow.vlm_client.model_name
            logger.info(
                f"Using vlm_client.model_name: {workflow.vlm_client.model_name}"
            )

        # Store the workflow's state_schema class name if available
        if hasattr(workflow, "state_schema") and hasattr(
            workflow.state_schema, "__name__"
        ):
            serializable_kwargs["_state_schema_class"] = (
                f"{workflow.state_schema.__module__}.{workflow.state_schema.__name__}"
            )
            logger.info(
                f"Storing state_schema class: {serializable_kwargs['_state_schema_class']}"
            )

        os.environ["NODEOLOGY_WORKFLOW_ARGS"] = json.dumps(
            serializable_kwargs, cls=StateEncoder
        )
        logger.info(f"Serialized kwargs: {list(serializable_kwargs.keys())}")
    else:
        logger.info("No initialization kwargs found on workflow")

    # Serialize any initial state if needed
    if initial_state:
        # Use StateEncoder to handle NumPy arrays
        os.environ["NODEOLOGY_INITIAL_STATE"] = json.dumps(
            initial_state, cls=StateEncoder
        )
        logger.info("Serialized initial state")

    # Create a shared variable to store the final state
    os.environ["NODEOLOGY_FINAL_STATE"] = "{}"

    # This file is nodeology/chainlit_interface.py, get its path:
    this_file = os.path.abspath(__file__)
    # Start with some standard arguments
    logger.info("Starting Chainlit server")
    run_chainlit(target=this_file)

    # Return the final state from the last session
    final_state = {}
    if (
        "NODEOLOGY_FINAL_STATE" in os.environ
        and os.environ["NODEOLOGY_FINAL_STATE"] != "{}"
    ):
        try:
            final_state_json = os.environ["NODEOLOGY_FINAL_STATE"]
            final_state_dict = json.loads(final_state_json)
            logger.info(
                f"Retrieved final state with keys: {list(final_state_dict.keys())}"
            )

            # Convert any serialized NumPy arrays back to arrays
            final_state = convert_serialized_objects(final_state_dict)
            logger.info("Converted any serialized objects in final state")

        except Exception as e:
            logger.error(f"Error parsing final state: {str(e)}")

    return final_state


@cl.on_chat_start
async def on_chat_start():
    """
    Called once a new user session is started in the chainlit UI.
    We will instantiate a new workflow for this session.
    """
    try:
        # Get the workflow class from environment variable
        workflow_class_path = os.environ.get("NODEOLOGY_WORKFLOW_CLASS")
        if not workflow_class_path:
            await cl.Message(content="No workflow class specified.").send()
            return

        logger.info(f"Creating workflow from class: {workflow_class_path}")

        # Import the workflow class dynamically
        module_path, class_name = workflow_class_path.rsplit(".", 1)
        module = importlib.import_module(module_path)
        WorkflowClass = getattr(module, class_name)
        logger.info(f"Successfully imported workflow class: {class_name}")

        # Get the saved initialization arguments
        workflow_args = {}
        state_defs_processed = False

        if "NODEOLOGY_WORKFLOW_ARGS" in os.environ:
            try:
                serialized_args = json.loads(os.environ["NODEOLOGY_WORKFLOW_ARGS"])
                logger.info(f"Loaded serialized args: {list(serialized_args.keys())}")

                # Handle special parameters

                # 1. Handle state_defs
                if "_state_defs_class" in serialized_args:
                    # We have a class reference for state_defs
                    state_defs_class_path = serialized_args.pop("_state_defs_class")
                    try:
                        module_path, class_name = state_defs_class_path.rsplit(".", 1)
                        module = importlib.import_module(module_path)
                        state_defs_class = getattr(module, class_name)
                        workflow_args["state_defs"] = state_defs_class
                        logger.info(
                            f"Imported state_defs class: {state_defs_class_path}"
                        )
                        state_defs_processed = True
                    except Exception as e:
                        logger.error(f"Error importing state_defs class: {str(e)}")
                elif "_state_defs_list" in serialized_args:
                    # We have a list of state definitions
                    state_defs_list = serialized_args.pop("_state_defs_list")
                    try:
                        # Process each item in the list
                        processed_list = []
                        for item in state_defs_list:
                            if isinstance(item, str):
                                # It's a class reference
                                try:
                                    module_path, class_name = item.rsplit(".", 1)
                                    module = importlib.import_module(module_path)
                                    class_obj = getattr(module, class_name)
                                    processed_list.append(class_obj)
                                except Exception as e:
                                    logger.error(
                                        f"Error importing state def class {item}: {str(e)}"
                                    )
                            elif isinstance(item, list) and len(item) == 2:
                                # It's a [name, type] tuple
                                name, type_str = item
                                if "." in type_str:
                                    # It's a class reference
                                    try:
                                        module_path, class_name = type_str.rsplit(
                                            ".", 1
                                        )
                                        module = importlib.import_module(module_path)
                                        type_obj = getattr(module, class_name)
                                        processed_list.append((name, type_obj))
                                    except Exception as e:
                                        logger.error(
                                            f"Error importing type {type_str}: {str(e)}"
                                        )
                                        # Fall back to string representation
                                        processed_list.append((name, type_str))
                                else:
                                    # It's a primitive type string
                                    processed_list.append((name, type_str))
                            elif isinstance(item, dict) and len(item) == 1:
                                # It's a {name: type} dict
                                name, type_str = next(iter(item.items()))
                                if "." in type_str:
                                    # It's a class reference
                                    try:
                                        module_path, class_name = type_str.rsplit(
                                            ".", 1
                                        )
                                        module = importlib.import_module(module_path)
                                        type_obj = getattr(module, class_name)
                                        processed_list.append({name: type_obj})
                                    except Exception as e:
                                        logger.error(
                                            f"Error importing type {type_str}: {str(e)}"
                                        )
                                        # Fall back to string representation
                                        processed_list.append({name: type_str})
                                else:
                                    # It's a primitive type string
                                    processed_list.append({name: type_str})

                        if processed_list:
                            workflow_args["state_defs"] = processed_list
                            logger.info(
                                f"Processed state_defs list with {len(processed_list)} items"
                            )
                            state_defs_processed = True
                        else:
                            logger.info("No state_defs items could be processed")
                    except Exception as e:
                        logger.error(f"Error processing state_defs list: {str(e)}")
                elif (
                    "state_defs" in serialized_args
                    and serialized_args["state_defs"] is None
                ):
                    # Explicit None value
                    workflow_args["state_defs"] = None
                    serialized_args.pop("state_defs")
                    logger.info("Using None for state_defs")
                    state_defs_processed = True

                # 2. Handle state_schema if needed
                if "_state_schema_class" in serialized_args:
                    # We have a class reference for state_schema
                    state_schema_class_path = serialized_args.pop("_state_schema_class")
                    logger.info(
                        f"Found state_schema class: {state_schema_class_path} (will be handled by workflow)"
                    )

                    # If we couldn't process state_defs, try to use the state_schema class as a fallback
                    if not state_defs_processed:
                        try:
                            module_path, class_name = state_schema_class_path.rsplit(
                                ".", 1
                            )
                            module = importlib.import_module(module_path)
                            state_schema_class = getattr(module, class_name)
                            workflow_args["state_defs"] = state_schema_class
                            logger.info(
                                f"Using state_schema class as fallback for state_defs: {state_schema_class_path}"
                            )
                            state_defs_processed = True
                        except Exception as e:
                            logger.error(
                                f"Error importing state_schema class as fallback: {str(e)}"
                            )

                # Add remaining arguments
                for key, value in serialized_args.items():
                    workflow_args[key] = value

                # Convert any serialized NumPy arrays back to arrays
                workflow_args = convert_serialized_objects(workflow_args)
                logger.info(f"Final workflow args: {list(workflow_args.keys())}")
            except Exception as e:
                logger.error(f"Error parsing workflow arguments: {str(e)}")
                traceback.print_exc()
                # Continue with empty args if there's an error

        # If we couldn't process state_defs, check if the workflow class has a state_schema attribute
        if not state_defs_processed and hasattr(WorkflowClass, "state_schema"):
            logger.info(f"Using workflow class's state_schema attribute as fallback")
            # We don't need to set state_defs explicitly, the workflow will use its state_schema

        # Create a new instance of the workflow with the saved arguments
        logger.info(
            f"Creating workflow instance with args: {list(workflow_args.keys())}"
        )
        workflow = WorkflowClass(**workflow_args)
        logger.info(f"Successfully created workflow instance: {workflow.name}")

        # Check if VLM client is available
        if hasattr(workflow, "vlm_client") and workflow.vlm_client is not None:
            logger.info(f"VLM client is available")
        else:
            logger.info("VLM client is not available")

        # Get initial state if available
        initial_state = None
        initial_state_json = os.environ.get("NODEOLOGY_INITIAL_STATE")
        if initial_state_json:
            logger.info("Found initial state in environment")
            # Parse the JSON and convert any serialized NumPy arrays back to arrays
            initial_state_dict = json.loads(initial_state_json)
            logger.info(
                f"Loaded initial state with keys: {list(initial_state_dict.keys())}"
            )

            # Convert any serialized NumPy arrays back to arrays
            initial_state = convert_serialized_objects(initial_state_dict)
            logger.info("Converted any serialized objects in initial state")

        # Initialize the workflow
        if initial_state:
            logger.info(
                f"Initializing workflow with initial state: {list(initial_state.keys())}"
            )
            workflow.initialize(initial_state)
        else:
            logger.info("Initializing workflow with default state")
            workflow.initialize()
        logger.info("Workflow initialized successfully")

        # Store in user session
        cl.user_session.set("workflow", workflow)
        logger.info("Stored workflow in user session")

        # Capture the current Chainlit context
        parent_ctx = contextvars.copy_context()
        logger.info("Captured Chainlit context")

        # Create a function to save the final state when workflow completes
        def save_final_state(state):
            try:
                # Use StateEncoder to handle NumPy arrays and other complex objects
                serialized_state = json.dumps(state, cls=StateEncoder)
                os.environ["NODEOLOGY_FINAL_STATE"] = serialized_state
                logger.info(f"Saved final state with keys: {list(state.keys())}")
            except Exception as e:
                logger.error(f"Error saving final state: {str(e)}")
                traceback.print_exc()

        # Start the workflow in a background thread from within the Chainlit context
        def run_workflow_with_polling():
            try:
                logger.info("Starting workflow execution in background thread")
                # Run the workflow inside the captured context
                final_state = parent_ctx.run(
                    lambda: workflow._run(initial_state, ui=True)
                )
                logger.info("Workflow execution completed")

                # Save the final state
                if final_state:
                    save_final_state(final_state)
                    logger.info("Final state saved")
            except Exception as e:
                logger.error(f"Error in workflow execution: {str(e)}")
                traceback.print_exc()

        # Start the workflow thread
        workflow_thread = threading.Thread(
            target=run_workflow_with_polling, daemon=True
        )
        cl.user_session.set("workflow_thread", workflow_thread)
        logger.info("Created workflow thread")
        workflow_thread.start()
        logger.info("Started workflow thread")

        await cl.Message(
            content=f"Welcome to the {workflow.__class__.__name__} via Nodeology!"
        ).send()
        logger.info("Sent welcome message")
    except Exception as e:
        logger.error(f"Error in on_chat_start: {str(e)}")
        traceback.print_exc()
        await cl.Message(content=f"Error initializing workflow: {str(e)}").send()



================================================
FILE: nodeology/log.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os, logging, logging.config
import sys

logger = logging.getLogger(__name__)


def setup_logging(log_dir, log_name, debug_mode=False, base_dir=None):
    """Configure the logging system with console and/or file handlers.

    Args:
        log_dir (str): Directory where log files will be stored
        log_name (str): Name of the log file (without extension)
        debug_mode (bool): If True, only console logging with debug level is enabled
        base_dir (str, optional): Base directory to prepend to log_dir
    """
    # Get root logger
    root_logger = logging.getLogger()

    # Remove any existing handlers first
    for handler in root_logger.handlers[:]:
        handler.close()  # Properly close handlers
        root_logger.removeHandler(handler)

    # Create handlers
    console_handler = logging.StreamHandler(sys.stdout)

    # Set the root logger level to DEBUG to capture all messages
    root_logger.setLevel(logging.DEBUG)

    # Use base_dir if provided, otherwise use log_dir directly
    full_log_dir = os.path.join(base_dir, log_dir) if base_dir else log_dir

    if not os.path.exists(full_log_dir):
        os.makedirs(full_log_dir)

    log_file_path = f"{full_log_dir}/{log_name}.log"

    if os.path.isfile(log_file_path):
        log_print_color(
            f"WARNING: {log_file_path} already exists and will be overwritten.",
            "red",
        )

    # Create file handler for both modes
    file_handler = logging.FileHandler(log_file_path, "w")

    if debug_mode:
        # Debug mode configuration
        # Console shows DEBUG and above
        console_handler.setLevel(logging.DEBUG)
        console_format = logging.Formatter(
            "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
        )
        console_handler.setFormatter(console_format)

        # File handler captures everything
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter(
            "%(asctime)s - %(levelname)s - %(name)s - %(message)s"
        )
        file_handler.setFormatter(file_format)
    else:
        # Production mode configuration
        # Console only shows PRINTLOG and WARNING+ messages
        console_handler.setLevel(logging.PRINTLOG)
        console_format = logging.Formatter("%(message)s")
        console_handler.setFormatter(console_format)

        # File handler captures everything (DEBUG and above)
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter(
            "%(asctime)s - %(message)s", datefmt="%Y%m%d-%H:%M:%S"
        )
        file_handler.setFormatter(file_format)

    # Add handlers to root logger
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    # Configure third-party loggers to be less verbose
    # This prevents them from cluttering the console
    for logger_name in logging.root.manager.loggerDict:
        if logger_name != __name__ and not logger_name.startswith("nodeology"):
            third_party_logger = logging.getLogger(logger_name)
            if debug_mode:
                # In debug mode, third-party loggers show WARNING and above
                third_party_logger.setLevel(logging.WARNING)
            else:
                # In production mode, third-party loggers show ERROR and above
                third_party_logger.setLevel(logging.ERROR)

    # Store handlers in logger for later cleanup
    root_logger.handlers_to_close = root_logger.handlers[:]


def cleanup_logging():
    """Properly clean up logging handlers to prevent resource leaks."""
    root_logger = logging.getLogger()

    # Close and remove any existing handlers
    if hasattr(root_logger, "handlers_to_close"):
        for handler in root_logger.handlers_to_close:
            try:
                handler.close()
            except:
                pass  # Ignore errors during cleanup
            if handler in root_logger.handlers:
                root_logger.removeHandler(handler)
        root_logger.handlers_to_close = []


# https://stackoverflow.com/questions/2183233/how-to-add-a-custom-loglevel-to-pythons-logging-facility/35804945#35804945
def add_logging_level(levelName, levelNum, methodName=None):
    """Add a new logging level to the logging module.

    Args:
        levelName (str): Name of the new level (e.g., 'TRACE')
        levelNum (int): Numeric value for the level
        methodName (str, optional): Method name to add. Defaults to levelName.lower()

    Raises:
        AttributeError: If levelName or methodName already exists
    """
    if not methodName:
        methodName = levelName.lower()

    if hasattr(logging, levelName):
        raise AttributeError("{} already defined in logging module".format(levelName))
    if hasattr(logging, methodName):
        raise AttributeError("{} already defined in logging module".format(methodName))
    if hasattr(logging.getLoggerClass(), methodName):
        raise AttributeError("{} already defined in logger class".format(methodName))

    # This method was inspired by the answers to Stack Overflow post
    # http://stackoverflow.com/q/2183233/2988730, especially
    # http://stackoverflow.com/a/13638084/2988730
    def logForLevel(self, message, *args, **kwargs):
        if self.isEnabledFor(levelNum):
            self._log(levelNum, message, args, **kwargs)

    def logToRoot(message, *args, **kwargs):
        logging.log(levelNum, message, *args, **kwargs)

    logging.addLevelName(levelNum, levelName)
    setattr(logging, levelName, levelNum)
    setattr(logging.getLoggerClass(), methodName, logForLevel)
    setattr(logging, methodName, logToRoot)


def log_print_color(text, color="", print_to_console=True):
    """Print colored text to console and log it to file.

    Args:
        text (str): Text to print and log
        color (str): Color name ('green', 'red', 'blue', 'yellow', or '' for white)
        print_to_console (bool): If True, print the text to console
    """
    # Define color codes as constants at the top of the function
    COLOR_CODES = {
        "green": "\033[92m",
        "red": "\033[91m",
        "blue": "\033[94m",
        "yellow": "\033[93m",
        "": "\033[97m",  # default white
    }

    # Get color code from dictionary, defaulting to white
    ansi_code = COLOR_CODES.get(color, COLOR_CODES[""])

    if print_to_console:
        print(ansi_code + text + "\033[0m")
    logger.logonly(text)



================================================
FILE: nodeology/node.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os
from string import Formatter
from inspect import signature
from typing import Optional, Annotated, List, Union, Dict, Callable, Any
import ast

from nodeology.state import State
from nodeology.log import log_print_color
from nodeology.client import LLM_Client, VLM_Client


def _process_state_with_transforms(
    state: State, transforms: Dict[str, Callable], client: LLM_Client, **kwargs
) -> State:
    """Helper function to apply transforms to state values.

    Args:
        state: Current state
        transforms: Dictionary mapping state keys to transformation functions
        client: LLM client (unused but kept for signature compatibility)
    """
    for key, transform in transforms.items():
        if key in state:
            try:
                state[key] = transform(state[key])
            except Exception as e:
                raise ValueError(f"Error applying transform to {key}: {str(e)}")
    return state


class Node:
    """Template for creating node functions that process data using LLMs or custom functions.

    A Node represents a processing unit in a workflow that can:
    - Execute LLM/VLM queries or custom functions
    - Manage state before and after execution
    - Handle pre/post processing steps
    - Process both text and image inputs
    - Format and validate outputs

    Args:
        prompt_template (str): Template string for the LLM prompt. Uses Python string formatting
            syntax (e.g., "{variable}"). Empty if using custom_function.
        node_type (Optional[str]): Unique identifier for the node.
        sink (Optional[Union[List[str], str]]): Where to store results in state. Can be:
            - Single string key
            - List of keys for multiple outputs
            - None (results won't be stored)
        sink_format (Optional[str]): Format specification for LLM output (e.g., "json", "list").
            Used to ensure consistent response structure.
        image_keys (Optional[List[str]]): List of keys for image file paths when using VLM.
            Must provide at least one image path in kwargs when these are specified.
        pre_process (Optional[Union[Callable, Dict[str, Callable]]]): Either a function to run
            before execution or a dictionary mapping state keys to transform functions.
        post_process (Optional[Union[Callable, Dict[str, Callable]]]): Either a function to run
            after execution or a dictionary mapping state keys to transform functions.
        sink_transform (Optional[Union[Callable, List[Callable]]]): Transform(s) to apply to
            sink value(s). If sink is a string, must be a single callable. If sink is a list,
            can be either a single callable (applied to all sinks) or a list of callables.
        custom_function (Optional[Callable]): Custom function to execute instead of LLM query.
            Function parameters become required keys for node execution.

    Attributes:
        required_keys (List[str]): Keys required from state/kwargs for node execution.
            Extracted from either prompt_template or custom_function signature.
        prompt_history (List[str]): History of prompt templates used by this node.

    Raises:
        ValueError: If required keys are missing or response format is invalid
        FileNotFoundError: If specified image files don't exist
        ValueError: If VLM operations are attempted without proper client

    Example:
        ```python
        # Create a simple text processing node
        node = Node(
            node_type="summarizer",
            prompt_template="Summarize this text: {text}",
            sink="summary"
        )

        # Create a node with custom function
        def process_data(x, y):
            return x + y

        node = Node(
            node_type="calculator",
            prompt_template="",
            sink="result",
            custom_function=process_data
        )
        ```
    """

    # Simplified set of allowed functions that return values
    ALLOWED_FUNCTIONS = {
        "len": len,  # Length of sequences
        "str": str,  # String conversion
        "int": int,  # Integer conversion
        "float": float,  # Float conversion
        "sum": sum,  # Sum of numbers
        "max": max,  # Maximum value
        "min": min,  # Minimum value
        "abs": abs,  # Absolute value
    }

    DISALLOWED_FUNCTION_NAMES = [
        "eval",
        "exec",
        "compile",
        "open",
        "print",
        "execfile",
        "exit",
        "quit",
        "help",
        "dir",
        "globals",
        "locals",
        "dir",
        "type",
        "hash",
        "repr",
        "filter",
        "enumerate",
        "reversed",
        "sorted",
        "any",
        "all",
    ]

    # String methods that return values
    ALLOWED_STRING_METHODS = {
        "upper": str.upper,
        "lower": str.lower,
        "strip": str.strip,
        "capitalize": str.capitalize,
    }

    def __init__(
        self,
        prompt_template: str,
        node_type: Optional[str] = None,
        sink: Optional[Union[List[str], str]] = None,
        sink_format: Optional[str] = None,
        image_keys: Optional[List[str]] = None,
        pre_process: Optional[
            Union[
                Callable[[State, LLM_Client, Any], Optional[State]], Dict[str, Callable]
            ]
        ] = None,
        post_process: Optional[
            Union[
                Callable[[State, LLM_Client, Any], Optional[State]], Dict[str, Callable]
            ]
        ] = None,
        sink_transform: Optional[Union[Callable, List[Callable]]] = None,
        custom_function: Optional[Callable[..., Any]] = None,
        use_conversation: Optional[bool] = False,
    ):
        # Set default node_type based on whether it's prompt or function-based
        if node_type is None:
            if custom_function:
                self.node_type = custom_function.__name__
            else:
                self.node_type = "prompt"
        else:
            self.node_type = node_type

        self.prompt_template = prompt_template
        self._escaped_sections = []  # Store escaped sections at instance level
        self.sink = sink
        self.image_keys = image_keys
        self.sink_format = sink_format
        self.custom_function = custom_function
        self.use_conversation = use_conversation

        # Handle pre_process
        if isinstance(pre_process, dict):
            transforms = pre_process
            self.pre_process = (
                lambda state, client, **kwargs: _process_state_with_transforms(
                    state, transforms, client, **kwargs
                )
            )
        else:
            self.pre_process = pre_process

        # Handle post_process
        if isinstance(post_process, dict):
            transforms = post_process
            self.post_process = (
                lambda state, client, **kwargs: _process_state_with_transforms(
                    state, transforms, client, **kwargs
                )
            )
        else:
            self.post_process = post_process

        # Handle sink_transform
        if sink_transform is not None:
            if isinstance(sink, str):
                if not callable(sink_transform):
                    raise ValueError(
                        "sink_transform must be callable when sink is a string"
                    )
                self._sink_transform = sink_transform
            elif isinstance(sink, list):
                if callable(sink_transform):
                    # If single transform provided for multiple sinks, apply it to all
                    self._sink_transform = [sink_transform] * len(sink)
                elif len(sink_transform) != len(sink):
                    raise ValueError("Number of transforms must match number of sinks")
                else:
                    self._sink_transform = sink_transform
            else:
                raise ValueError("sink must be specified to use sink_transform")
        else:
            self._sink_transform = None

        # Extract required keys from template or custom function signature
        if self.custom_function:
            # Get only required keys (those without default values) from function signature
            sig = signature(self.custom_function)
            self.required_keys = [
                param.name
                for param in sig.parameters.values()
                if param.default is param.empty
                and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)
                and param.name != "self"
            ]
        else:
            # Extract base variable names from expressions, excluding function names and escaped content
            self.required_keys = []
            # First, temporarily replace escaped content
            template = prompt_template

            # Replace {{{ }}} sections first
            import re

            triple_brace_pattern = (
                r"\{{3}[\s\S]*?\}{3}"  # Non-greedy match, including newlines
            )
            for i, match in enumerate(re.finditer(triple_brace_pattern, template)):
                placeholder = f"___ESCAPED_TRIPLE_{i}___"
                self._escaped_sections.append((placeholder, match.group(0)))
                template = template.replace(match.group(0), placeholder)

            # Then replace {{ }} sections
            double_brace_pattern = (
                r"\{{2}[\s\S]*?\}{2}"  # Non-greedy match, including newlines
            )
            for i, match in enumerate(re.finditer(double_brace_pattern, template)):
                placeholder = f"___ESCAPED_DOUBLE_{i}___"
                self._escaped_sections.append((placeholder, match.group(0)))
                template = template.replace(match.group(0), placeholder)

            self._template_with_placeholders = template  # Store modified template

            # Now parse the template normally
            for _, expr, _, _ in Formatter().parse(template):
                if expr is not None:
                    # Parse the expression to identify actual variables
                    try:
                        tree = ast.parse(expr, mode="eval")
                        variables = set()
                        for node in ast.walk(tree):
                            if (
                                isinstance(node, ast.Name)
                                and node.id not in self.ALLOWED_FUNCTIONS
                                and node.id not in self.DISALLOWED_FUNCTION_NAMES
                            ):
                                variables.add(node.id)
                        self.required_keys.extend(variables)
                    except SyntaxError:
                        # If parsing fails, fall back to basic extraction
                        base_var = expr.split("[")[0].split(".")[0].split("(")[0]
                        if (
                            base_var not in self.ALLOWED_FUNCTIONS
                            and base_var not in self.DISALLOWED_FUNCTION_NAMES
                            and base_var not in self.required_keys
                        ):
                            self.required_keys.append(base_var)

            # Remove duplicates while preserving order
            self.required_keys = list(dict.fromkeys(self.required_keys))

        self._prompt_history = [
            prompt_template
        ]  # Add prompt history as private attribute

    def _eval_expr(self, expr: str, context: dict) -> Any:
        """Safely evaluate a Python expression with limited scope."""
        try:
            # Add allowed functions to the context
            eval_context = {
                **context,
                **self.ALLOWED_FUNCTIONS,  # Include built-in functions
            }
            tree = ast.parse(expr, mode="eval")
            return self._eval_node(tree.body, eval_context)
        except SyntaxError as e:
            raise ValueError(f"Invalid Python syntax in expression: {str(e)}")
        except Exception as e:
            raise ValueError(f"Invalid expression: {str(e)}")

    def _eval_node(self, node: ast.AST, context: dict) -> Any:
        """Recursively evaluate an AST node with security constraints."""
        if isinstance(node, ast.Name):
            if node.id not in context:
                raise ValueError(f"Variable '{node.id}' not found in context")
            return context[node.id]

        elif isinstance(node, ast.Constant):
            return node.value

        elif isinstance(node, ast.UnaryOp):  # Add support for unary operations
            if isinstance(node.op, ast.USub):  # Handle negative numbers
                operand = self._eval_node(node.operand, context)
                return -operand
            raise ValueError(f"Unsupported unary operator: {type(node.op).__name__}")

        elif isinstance(node, ast.Call):
            if isinstance(node.func, ast.Attribute):  # Add support for method calls
                obj = self._eval_node(node.func.value, context)
                method_name = node.func.attr
                # List of allowed string methods
                allowed_string_methods = ["upper", "lower", "title", "strip"]
                if method_name in allowed_string_methods:
                    method = getattr(obj, method_name)
                    args = [self._eval_node(arg, context) for arg in node.args]
                    return method(*args)
                raise ValueError(f"String method not allowed: {method_name}")
            elif isinstance(node.func, ast.Name):
                func_name = node.func.id
                if func_name not in self.ALLOWED_FUNCTIONS:
                    raise ValueError(f"Function not allowed: {func_name}")
                func = context[
                    func_name
                ]  # Get function from context instead of globals
                args = [self._eval_node(arg, context) for arg in node.args]
                return func(*args)
            raise ValueError("Only simple function calls are allowed")

        elif isinstance(node, ast.Attribute):
            # Handle string methods (e.g., text.upper())
            if not isinstance(node.value, ast.Name):
                raise ValueError("Only simple string methods are allowed")

            obj = self._eval_node(node.value, context)
            if not isinstance(obj, str):
                raise ValueError("Methods are only allowed on strings")

            method_name = node.attr
            if method_name not in self.ALLOWED_STRING_METHODS:
                raise ValueError(f"String method not allowed: {method_name}")

            return self.ALLOWED_STRING_METHODS[method_name](obj)

        elif isinstance(node, (ast.List, ast.Tuple)):
            return [self._eval_node(elt, context) for elt in node.elts]

        elif isinstance(node, ast.Subscript):
            value = self._eval_node(node.value, context)
            if isinstance(node.slice, ast.Slice):
                lower = (
                    self._eval_node(node.slice.lower, context)
                    if node.slice.lower
                    else None
                )
                upper = (
                    self._eval_node(node.slice.upper, context)
                    if node.slice.upper
                    else None
                )
                step = (
                    self._eval_node(node.slice.step, context)
                    if node.slice.step
                    else None
                )
                return value[slice(lower, upper, step)]
            else:
                # Handle both numeric indices and string keys
                idx = self._eval_node(node.slice, context)
                try:
                    return value[idx]
                except (TypeError, KeyError) as e:
                    raise ValueError(f"Invalid subscript access: {str(e)}")

        elif isinstance(node, ast.Str):  # For string literals in subscripts
            return node.s

        raise ValueError(f"Unsupported expression type: {type(node).__name__}")

    @property
    def func(self):
        """Returns the node function without executing it"""

        def node_function(
            state: Annotated[State, "The current state"],
            client: Annotated[LLM_Client, "The LLM client"],
            sink: Optional[Union[List[str], str]] = None,
            source: Optional[Dict[str, str]] = None,
            **kwargs,
        ) -> State:
            return self(state, client, sink, source, **kwargs)

        # Attach the attributes to the function
        node_function.node_type = self.node_type
        node_function.prompt_template = self.prompt_template
        node_function.sink = self.sink
        node_function.image_keys = self.image_keys
        node_function.sink_format = self.sink_format
        node_function.pre_process = self.pre_process
        node_function.post_process = self.post_process
        node_function.required_keys = self.required_keys
        return node_function

    def __call__(
        self,
        state: State,
        client: Union[LLM_Client, VLM_Client],
        sink: Optional[Union[List[str], str]] = None,
        source: Optional[Union[Dict[str, str], str]] = None,
        debug: bool = False,
        use_conversation: Optional[bool] = None,
        **kwargs,
    ) -> State:
        """Creates and executes a node function from this template.

        Args:
            state: Current state object containing variables
            client: LLM or VLM client for making API calls
            sink: Optional override for where to store results
            source: Optional mapping of template keys to state keys
            **kwargs: Additional keyword arguments passed to function

        Returns:
            Updated state object with results stored in sink keys

        Raises:
            ValueError: If required keys are missing or response format is invalid
            FileNotFoundError: If specified image files don't exist
        """
        # Update node type
        state["previous_node_type"] = state.get("current_node_type", "")
        state["current_node_type"] = self.node_type

        # Pre-processing if defined
        if self.pre_process:
            pre_process_result = self.pre_process(state, client, **kwargs)
            if pre_process_result is None:
                return state
            state = pre_process_result

        # Get values from state or kwargs
        if isinstance(source, str):
            source = {"source": source}

        message_values = {}
        for key in self.required_keys:
            if source and key in source:
                source_key = source[key]
                if source_key not in state:
                    raise ValueError(
                        f"Source mapping key '{source_key}' not found in state"
                    )
                message_values[key] = state[source_key]
            elif key in state:
                message_values[key] = state[key]
            elif key in kwargs:
                message_values[key] = kwargs[key]
            else:
                raise ValueError(f"Required key '{key}' not found in state or kwargs")

        # Execute either custom function or LLM call
        if self.custom_function:
            # Get default values from function signature
            sig = signature(self.custom_function)
            default_values = {
                k: v.default
                for k, v in sig.parameters.items()
                if v.default is not v.empty
            }
            # Update message_values with defaults for missing parameters
            for key, default in default_values.items():
                if key not in message_values:
                    message_values[key] = default
            if "state" in sig.parameters and "state" not in message_values:
                message_values["state"] = state
            if "client" in sig.parameters and "client" not in message_values:
                message_values["client"] = client
            response = self.custom_function(**message_values)
        else:
            # Create a context with state variables for expression evaluation
            eval_context = {**message_values}

            # First fill the template with placeholders
            message = self._template_with_placeholders
            for _, expr, _, _ in Formatter().parse(self._template_with_placeholders):
                if expr is not None:
                    try:
                        result = self._eval_expr(expr, eval_context)
                        message = message.replace(f"{{{expr}}}", str(result))
                    except Exception as e:
                        raise ValueError(
                            f"Error evaluating expression '{expr}': {str(e)}"
                        )

            # Now restore the escaped sections
            for placeholder, original in self._escaped_sections:
                message = message.replace(placeholder, original)

            # Record the formatted message
            if "messages" not in state:
                state["messages"] = []
            state["messages"].append({"role": "user", "content": message})

            # Determine if we should use conversation mode
            should_use_conversation = (
                use_conversation
                if use_conversation is not None
                else self.use_conversation
            )
            if should_use_conversation:
                assert "conversation" in state and isinstance(
                    state["conversation"], list
                ), "Conversation does not exist in state or is not a list of messages"

            # Prepare messages for client call
            if should_use_conversation:
                if len(state["conversation"]) == 0 or state["end_conversation"]:
                    state["conversation"].append({"role": "user", "content": message})
                messages = state["conversation"]
            else:
                messages = [{"role": "user", "content": message}]

            # Handle VLM specific requirements
            if self.image_keys:
                if not isinstance(client, VLM_Client):
                    raise ValueError("VLM client required for image keys")

                # Check both state and kwargs for image keys
                image_paths = []
                for key in self.image_keys:
                    if key in state:
                        path = state[key]
                    elif key in kwargs:
                        path = kwargs[key]
                    else:
                        continue

                    if path is None:
                        raise TypeError(
                            f"Image path for '{key}' should be string, got None"
                        )
                    if not isinstance(path, str):
                        raise TypeError(
                            f"Image path for '{key}' should be string, got {type(path)}"
                        )
                    image_paths.append(path)

                if not image_paths:
                    raise ValueError(
                        "At least one image key must be provided in state or kwargs"
                    )

                # Verify all paths exist
                for path in image_paths:
                    if not os.path.exists(path):
                        raise FileNotFoundError(f"Image file not found: {path}")

                response = client(
                    messages=messages,
                    images=image_paths,
                    format=self.sink_format,
                    workflow=kwargs.get("workflow"),
                    node=self,
                    previous_node_type=state["previous_node_type"],
                )
            else:
                response = client(
                    messages=messages,
                    format=self.sink_format,
                    workflow=kwargs.get("workflow"),
                    node=self,
                    previous_node_type=state["previous_node_type"],
                )

        log_print_color(f"Response: {response}", "white", False)

        # Update state with response
        if sink is None:
            sink = self.sink

        if sink is None:
            log_print_color(
                f"Warning: No sink specified for {self.node_type} node", "yellow"
            )
            return state

        if isinstance(sink, str):
            state[sink] = (
                remove_markdown_blocks_formatting(response)
                if not self.custom_function
                else response
            )
        elif isinstance(sink, list):
            if not sink:
                log_print_color(
                    f"Warning: Empty sink list for {self.node_type} node", "yellow"
                )
                return state

            if len(sink) == 1:
                state[sink[0]] = (
                    remove_markdown_blocks_formatting(response)
                    if not self.custom_function
                    else response
                )
            else:
                if not isinstance(response, (list, tuple)):
                    raise ValueError(
                        f"Expected multiple responses for multiple sink in {self.node_type} node, but got a single response"
                    )
                if len(response) != len(sink):
                    raise ValueError(
                        f"Number of responses ({len(response)}) doesn't match number of sink ({len(sink)}) in {self.node_type} node"
                    )

                for key, value in zip(sink, response):
                    state[key] = (
                        remove_markdown_blocks_formatting(value)
                        if not self.custom_function
                        else value
                    )

        # After storing results but before post_process, apply sink transforms
        if self._sink_transform is not None:
            current_sink = sink or self.sink
            if isinstance(current_sink, str):
                state[current_sink] = self._sink_transform(state[current_sink])
            else:
                for key, transform in zip(current_sink, self._sink_transform):
                    state[key] = transform(state[key])

        # Post-processing if defined
        if self.post_process:
            post_process_result = self.post_process(state, client, **kwargs)
            if post_process_result is None:
                return state
            state = post_process_result

        return state

    def __str__(self):
        MAX_WIDTH = 80

        # Format prompt with highlighted keys
        prompt_lines = self.prompt_template.split("\n")
        # First make the whole prompt green
        prompt_lines = [f"\033[92m{line}\033[0m" for line in prompt_lines]  # Green
        # Then highlight the keys in red
        for key in self.required_keys:
            for i, line in enumerate(prompt_lines):
                prompt_lines[i] = line.replace(
                    f"{{{key}}}",
                    f"\033[91m{{{key}}}\033[0m\033[92m",  # Red keys, return to green after
                )

        # Calculate width for horizontal line (min of actual width and MAX_WIDTH)
        width = min(max(len(line) for line in prompt_lines), MAX_WIDTH)
        double_line = "â•" * width
        horizontal_line = "â”€" * width

        # Color formatting for keys in info section
        required_keys_colored = [
            f"\033[91m{key}\033[0m" for key in self.required_keys
        ]  # Red
        if isinstance(self.sink, str):
            sink_colored = [f"\033[94m{self.sink}\033[0m"]  # Blue
        elif isinstance(self.sink, list):
            sink_colored = [f"\033[94m{key}\033[0m" for key in self.sink]  # Blue
        else:
            sink_colored = ["None"]

        # Build the string representation
        result = [
            double_line,
            f"{self.node_type}",
            horizontal_line,
            *prompt_lines,
            horizontal_line,
            f"Required keys: {', '.join(required_keys_colored)}",
            f"Sink keys: {', '.join(sink_colored)}",
            f"Format: {self.sink_format or 'None'}",
            f"Image keys: {', '.join(self.image_keys) or 'None'}",
            f"Pre-process: {self.pre_process.__name__ if self.pre_process else 'None'}",
            f"Post-process: {self.post_process.__name__ if self.post_process else 'None'}",
            f"Custom function: {self.custom_function.__name__ if self.custom_function else 'None'}",
        ]

        return "\n".join(result)

    @property
    def prompt_history(self) -> list[str]:
        """Returns the history of prompt templates.

        Returns:
            list[str]: List of prompt templates, oldest to newest
        """
        return self._prompt_history.copy()


def as_node(
    sink: List[str],
    pre_process: Optional[Callable[[State, LLM_Client, Any], Optional[State]]] = None,
    post_process: Optional[Callable[[State, LLM_Client, Any], Optional[State]]] = None,
    as_function: bool = False,
):
    """Decorator to transform a regular Python function into a Node function.

    This decorator allows you to convert standard Python functions into Node objects
    that can be integrated into a nodeology workflow. The decorated function becomes
    the custom_function of the Node, with its parameters becoming required keys.

    Args:
        sink (List[str]): List of state keys where the function's results will be stored.
            The number of sink keys should match the number of return values from the function.
        pre_process (Optional[Callable]): Function to run before main execution.
            Signature: (state: State, client: LLM_Client, **kwargs) -> Optional[State]
        post_process (Optional[Callable]): Function to run after main execution.
            Signature: (state: State, client: LLM_Client, **kwargs) -> Optional[State]
        as_function (bool): If True, returns a callable node function. If False, returns
            the Node object itself. Default is False.

    Returns:
        Union[Node, Callable]: Either a Node object or a node function, depending on
        the as_function parameter.

    Example:
        ```python
        # Basic usage
        @as_node(sink=["result"])
        def multiply(x: int, y: int) -> int:
            return x * y

        # With pre and post processing
        def log_start(state, client, **kwargs):
            print("Starting calculation...")
            return state

        def log_result(state, client, **kwargs):
            print(f"Result: {state['result']}")
            return state

        @as_node(
            sink=["result"],
            pre_process=log_start,
            post_process=log_result
        )
        def add(x: int, y: int) -> int:
            return x + y

        # Multiple return values
        @as_node(sink=["mean", "std"])
        def calculate_stats(numbers: List[float]) -> Tuple[float, float]:
            return np.mean(numbers), np.std(numbers)
        ```

    Notes:
        - The decorated function's parameters become required keys for node execution
        - The function can access the state and client objects by including them
          as optional parameters
        - The number of sink keys should match the number of return values
        - When as_function=True, the decorator returns a callable that can be used
          directly in workflows
    """

    def decorator(func):
        # Create a Node instance with the custom function
        node = Node(
            prompt_template="",  # Empty template since we're using custom function
            node_type=func.__name__,
            sink=sink,
            pre_process=pre_process,
            post_process=post_process,
            custom_function=func,  # Pass the function to Node
        )

        # Get only required parameters (those without default values)
        sig = signature(func)
        node.required_keys = [
            param.name
            for param in sig.parameters.values()
            if param.default is param.empty
        ]

        return node.func if as_function else node

    return decorator


def remove_markdown_blocks_formatting(text: str) -> str:
    """Remove common markdown code block delimiters from text.

    Args:
        text: Input text containing markdown code blocks

    Returns:
        str: Text with code block delimiters removed
    """
    lines = text.split("\n")
    cleaned_lines = []

    for line in lines:
        stripped_line = line.strip()
        # Check if line starts with backticks (more robust than exact matches)
        if stripped_line.startswith("```"):
            continue
        else:
            cleaned_lines.append(line)

    return "\n".join(cleaned_lines)



================================================
FILE: nodeology/state.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import json
import logging
import numpy as np
import plotly.graph_objects as go
import plotly.io as pio

logger = logging.getLogger(__name__)
from typing import TypedDict, List, Dict, Union, Any, get_origin, get_args
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer
import msgpack

StateBaseT = Union[str, int, float, bool, np.ndarray]

"""
State management module for nodeology.
Handles type definitions, state processing, and state registry management.
"""


class State(TypedDict):
    """
    Base state class representing the core state structure.
    Contains node information, input/output data, and message history.
    """

    current_node_type: str
    previous_node_type: str
    human_input: str
    input: str
    output: str
    messages: List[dict]


def _split_by_top_level_comma(s: str) -> List[str]:
    """Helper function to split by comma while respecting brackets"""
    parts = []
    current = []
    bracket_count = 0

    for char in s:
        if char == "[":
            bracket_count += 1
        elif char == "]":
            bracket_count -= 1
        elif char == "," and bracket_count == 0:
            parts.append("".join(current).strip())
            current = []
            continue
        current.append(char)

    if current:
        parts.append("".join(current).strip())
    return parts


def _resolve_state_type(type_str: str):
    """
    Resolve string representations of types to actual Python types.
    """
    if not hasattr(_resolve_state_type, "_cache"):
        _resolve_state_type._cache = {}

    if type_str in _resolve_state_type._cache:
        return _resolve_state_type._cache[type_str]

    try:
        # Handle basic types
        if type_str in (
            "str",
            "int",
            "float",
            "bool",
            "dict",
            "list",
            "bytes",
            "tuple",
            "ndarray",
        ):
            if type_str == "ndarray":
                return np.ndarray
            return eval(type_str)

        if type_str.startswith("List[") and type_str.endswith("]"):
            inner_type = type_str[5:-1]
            return List[_resolve_state_type(inner_type)]

        elif type_str.startswith("Dict[") and type_str.endswith("]"):
            inner_str = type_str[5:-1]
            parts = _split_by_top_level_comma(inner_str)
            if len(parts) != 2:
                raise ValueError(f"Invalid Dict type format: {type_str}")

            key_type = _resolve_state_type(parts[0])
            value_type = _resolve_state_type(parts[1])
            return Dict[key_type, value_type]

        elif type_str.startswith("Union[") and type_str.endswith("]"):
            inner_str = type_str[6:-1]
            types = [
                _resolve_state_type(t) for t in _split_by_top_level_comma(inner_str)
            ]
            return Union[tuple(types)]

        else:
            raise ValueError(f"Unknown state type: {type_str}")

    except Exception as e:
        raise ValueError(f"Failed to resolve type '{type_str}': {str(e)}")


def _process_dict_state_def(state_def: Dict) -> tuple:
    """
    Process a dictionary-format state definition.

    Supports both formats:
    - {'name': 'type'} format
    - {'name': str, 'type': str} format

    Args:
        state_def (Dict): Dictionary containing state definition

    Returns:
        tuple: (name, resolved_type)

    Raises:
        ValueError: If state definition is missing required fields
    """
    if len(state_def) == 1:
        # Handle {'name': 'type'} format
        name, type_str = next(iter(state_def.items()))
    else:
        # Handle {'name': str, 'type': str} format
        name = state_def.get("name")
        type_str = state_def.get("type")
        if not name or not type_str:
            raise ValueError(f"Invalid state definition: {state_def}")

    state_type = _resolve_state_type(type_str)
    return (name, state_type)


def _process_list_state_def(state_def: List) -> List:
    """
    Process a list-format state definition.

    Supports two formats:
    1. Single definition: [name, type_str]
    2. Multiple definitions: [[name1, type_str1], [name2, type_str2], ...]

    Args:
        state_def (List): List containing state definitions

    Returns:
        List[tuple]: List of (name, resolved_type) tuples

    Raises:
        ValueError: If state definition format is invalid
    """
    if len(state_def) == 2 and isinstance(state_def[0], str):
        # Single list format [name, type_str]
        name, type_str = state_def
        state_type = _resolve_state_type(type_str)
        return [(name, state_type)]
    else:
        processed_lists = []
        for item in state_def:
            if not isinstance(item, list) or len(item) != 2:
                raise ValueError(f"Invalid state definition item: {item}")
            name, type_str = item
            state_type = _resolve_state_type(type_str)
            processed_lists.append((name, state_type))
        return processed_lists


def process_state_definitions(state_defs: List, state_registry: dict):
    """
    Process state definitions from template format to internal format.

    Supports multiple input formats:
    - Dictionary format: {'name': 'type'} or {'name': str, 'type': str}
    - List format: [name, type_str] or [[name1, type_str1], ...]
    - String format: References to pre-defined states in state_registry

    Args:
        state_defs (List): List of state definitions in various formats
        state_registry (dict): Registry of pre-defined states

    Returns:
        List[tuple]: List of processed (name, type) pairs

    Raises:
        ValueError: If state definition format is invalid or state type is unknown
    """
    processed_state_defs = []

    for state_def in state_defs:
        if isinstance(state_def, dict):
            processed_state_defs.append(_process_dict_state_def(state_def))
        elif isinstance(state_def, list):
            processed_state_defs.extend(_process_list_state_def(state_def))
        elif isinstance(state_def, str):
            if state_def in state_registry:
                processed_state_defs.append(state_registry[state_def])
            else:
                raise ValueError(f"Unknown state type: {state_def}")
        else:
            raise ValueError(
                f"Invalid state definition format: {state_def}. Must be a string, "
                "[name, type] list, or {'name': 'type'} dictionary"
            )

    return processed_state_defs


def _type_from_str(type_obj: type) -> str:
    """
    Convert a Python type object to a string representation that _resolve_state_type can parse.
    """
    # Add handling for numpy arrays
    if type_obj is np.ndarray:
        return "ndarray"

    # Handle basic types
    if type_obj in (str, int, float, bool, dict, list, bytes, tuple):
        return type_obj.__name__

    # Get the origin type
    origin = get_origin(type_obj)
    if origin is None:
        # More explicit handling of unknown types
        logger.warning(f"Unknown type {type_obj}, defaulting to None")
        return None

    # Handle List types
    if origin is list or origin is List:
        args = get_args(type_obj)
        if not args:
            return "list"  # Default to list if no type args
        inner_type = _type_from_str(args[0])
        if inner_type is None:
            return "list"
        return f"List[{inner_type}]"

    # Handle Dict types
    if origin is dict or origin is Dict:
        args = get_args(type_obj)
        if not args or len(args) != 2:
            return "dict"  # Default if no/invalid type args
        key_type = _type_from_str(args[0])  # Recursive call for key type
        value_type = _type_from_str(args[1])  # Recursive call for value type
        if key_type is None or value_type is None:
            return "dict"
        return f"Dict[{key_type}, {value_type}]"

    # Handle Union types
    if origin is Union:
        args = get_args(type_obj)
        if not args:
            return "tuple"
        types = [_type_from_str(arg) for arg in args]
        if any(t is None for t in types):
            return "tuple"
        return f"Union[{','.join(types)}]"

    # Default case
    return "str"


class StateEncoder(json.JSONEncoder):
    """Custom JSON encoder for serializing workflow states."""

    def default(self, obj):
        try:
            if isinstance(obj, np.ndarray):
                return {
                    "__type__": "ndarray",
                    "data": obj.tolist(),
                    "dtype": str(obj.dtype),
                }
            if isinstance(obj, go.Figure):
                return {
                    "__type__": "plotly_figure",
                    "data": pio.to_json(obj),
                }
            if hasattr(obj, "to_dict"):
                return obj.to_dict()
            if isinstance(obj, bytes):
                return obj.decode("utf-8")
            if isinstance(obj, set):
                return list(obj)
            if hasattr(obj, "__dict__"):
                return obj.__dict__
            return super().default(obj)
        except TypeError as e:
            logger.warning(f"Could not serialize object of type {type(obj)}: {str(e)}")
            return str(obj)


class CustomSerializer(JsonPlusSerializer):
    NDARRAY_EXT_TYPE = 42  # Ensure this code doesn't conflict with other ExtTypes
    PLOTLY_FIGURE_EXT_TYPE = 43  # New extension type for Plotly figures

    def _default(self, obj: Any) -> Union[str, Dict[str, Any]]:
        if isinstance(obj, np.ndarray):
            return {
                "lc": 2,
                "type": "ndarray",
                "data": obj.tolist(),
                "dtype": str(obj.dtype),
            }
        if isinstance(obj, go.Figure):
            return {
                "lc": 2,
                "type": "plotly_figure",
                "data": pio.to_json(obj),
            }
        return super()._default(obj)

    def _reviver(self, value: Dict[str, Any]) -> Any:
        if value.get("lc", None) == 2:
            if value.get("type", None) == "ndarray":
                return np.array(value["data"], dtype=value["dtype"])
            elif value.get("type", None) == "plotly_figure":
                return pio.from_json(value["data"])
        return super()._reviver(value)

    # Override dumps_typed to use instance method _msgpack_enc
    def dumps_typed(self, obj: Any) -> tuple[str, bytes]:
        if isinstance(obj, bytes):
            return "bytes", obj
        elif isinstance(obj, bytearray):
            return "bytearray", obj
        else:
            try:
                return "msgpack", self._msgpack_enc(obj)
            except UnicodeEncodeError:
                return "json", self.dumps(obj)

    # Provide instance-level _msgpack_enc
    def _msgpack_enc(self, data: Any) -> bytes:
        enc = msgpack.Packer(default=self._msgpack_default)
        return enc.pack(data)

    # Provide instance-level _msgpack_default
    def _msgpack_default(self, obj: Any) -> Any:
        if isinstance(obj, np.ndarray):
            # Prepare metadata for ndarray
            metadata = {
                "dtype": str(obj.dtype),
                "shape": obj.shape,
            }
            metadata_packed = msgpack.packb(metadata, use_bin_type=True)
            data_packed = obj.tobytes()
            combined = metadata_packed + data_packed
            return msgpack.ExtType(self.NDARRAY_EXT_TYPE, combined)
        elif isinstance(obj, np.number):
            # Handle NumPy scalar types
            return obj.item()
        elif isinstance(obj, go.Figure):
            figure_json = pio.to_json(obj)
            figure_packed = msgpack.packb(figure_json, use_bin_type=True)
            return msgpack.ExtType(self.PLOTLY_FIGURE_EXT_TYPE, figure_packed)

        return super()._msgpack_default(obj)

    # Provide instance-level loads_typed
    def loads_typed(self, data: tuple[str, bytes]) -> Any:
        type_, data_ = data
        if type_ == "bytes":
            return data_
        elif type_ == "bytearray":
            return bytearray(data_)
        elif type_ == "json":
            return self.loads(data_)
        elif type_ == "msgpack":
            return msgpack.unpackb(
                data_, ext_hook=self._msgpack_ext_hook, strict_map_key=False
            )
        else:
            raise NotImplementedError(f"Unknown serialization type: {type_}")

    # Provide instance-level _msgpack_ext_hook
    def _msgpack_ext_hook(self, code: int, data: bytes) -> Any:
        if code == self.NDARRAY_EXT_TYPE:
            # Unpack metadata
            unpacker = msgpack.Unpacker(use_list=False, raw=False)
            unpacker.feed(data)
            metadata = unpacker.unpack()
            buffer_offset = unpacker.tell()
            array_data = data[buffer_offset:]
            array = np.frombuffer(array_data, dtype=metadata["dtype"])
            array = array.reshape(metadata["shape"])
            return array
        elif code == self.PLOTLY_FIGURE_EXT_TYPE:
            figure_json = msgpack.unpackb(
                data, strict_map_key=False, ext_hook=self._msgpack_ext_hook
            )
            return pio.from_json(figure_json)
        else:
            return super()._msgpack_ext_hook(code, data)


def convert_serialized_objects(obj):
    """
    Convert serialized objects back to their original form.
    Currently handles:
    - NumPy arrays (serialized as {"__type__": "ndarray", "data": [...], "dtype": "..."})
    - Plotly figures (serialized as {"__type__": "plotly_figure", "data": "..."})

    Args:
        obj: The object to convert, which may contain serialized objects

    Returns:
        The object with any serialized objects converted back to their original form
    """
    if isinstance(obj, dict):
        if "__type__" in obj:
            if obj["__type__"] == "ndarray":
                return np.array(obj["data"], dtype=obj["dtype"])
            elif obj["__type__"] == "plotly_figure":
                return pio.from_json(obj["data"])
        return {k: convert_serialized_objects(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_serialized_objects(item) for item in obj]
    return obj


if __name__ == "__main__":
    serializer = CustomSerializer()
    original_data = {
        "array": np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64),
        "scalar": np.float32(7.5),
        "message": "Test serialization",
        "nested": {
            "array": np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64),
            "scalar": np.float32(7.5),
            "list_of_arrays": [
                np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64),
                np.array([[7, 8, 9], [10, 11, 12]], dtype=np.float64),
            ],
        },
    }

    # Create a more complex figure with multiple traces and customization
    fig = go.Figure()

    # Add a scatter plot with markers
    fig.add_trace(
        go.Scatter(
            x=[1, 2, 3, 4, 5],
            y=[4, 5.2, 6, 3.2, 8],
            mode="markers+lines",
            name="Series A",
            marker=dict(size=10, color="blue", symbol="circle"),
        )
    )

    # Add a bar chart
    fig.add_trace(
        go.Bar(
            x=[1, 2, 3, 4, 5], y=[2, 3, 1, 5, 3], name="Series B", marker_color="green"
        )
    )

    # Add a line plot with different style
    fig.add_trace(
        go.Scatter(
            x=[1, 2, 3, 4, 5],
            y=[7, 6, 9, 8, 7],
            mode="lines",
            name="Series C",
            line=dict(width=3, dash="dash", color="red"),
        )
    )

    # Update layout with title and axis labels
    fig.update_layout(
        title="Complex Test Figure",
        xaxis_title="X Axis",
        yaxis_title="Y Axis",
        legend_title="Legend",
        template="plotly_white",
    )
    original_data["figure"] = fig

    # Serialize the data
    _, serialized = serializer.dumps_typed(original_data)
    # Deserialize the data
    deserialized_data = serializer.loads_typed(("msgpack", serialized))

    # Assertions
    assert isinstance(deserialized_data["array"], np.ndarray)
    assert np.array_equal(deserialized_data["array"], original_data["array"])
    assert isinstance(deserialized_data["scalar"], float)
    assert deserialized_data["scalar"] == float(original_data["scalar"])
    assert deserialized_data["message"] == original_data["message"]
    assert isinstance(deserialized_data["nested"]["array"], np.ndarray)
    assert np.array_equal(
        deserialized_data["nested"]["array"], original_data["nested"]["array"]
    )
    assert isinstance(deserialized_data["nested"]["scalar"], float)
    assert deserialized_data["nested"]["scalar"] == float(
        original_data["nested"]["scalar"]
    )
    assert isinstance(deserialized_data["nested"]["list_of_arrays"], list)
    assert all(
        isinstance(arr, np.ndarray)
        for arr in deserialized_data["nested"]["list_of_arrays"]
    )
    assert all(
        np.array_equal(arr, original_arr)
        for arr, original_arr in zip(
            deserialized_data["nested"]["list_of_arrays"],
            original_data["nested"]["list_of_arrays"],
        )
    )

    assert isinstance(deserialized_data["figure"], go.Figure)
    assert len(deserialized_data["figure"].data) == len(fig.data)
    for i, trace in enumerate(fig.data):
        assert deserialized_data["figure"].data[i].type == trace.type
        # Compare x and y data if they exist
        if hasattr(trace, "x") and trace.x is not None:
            assert np.array_equal(deserialized_data["figure"].data[i].x, trace.x)
        if hasattr(trace, "y") and trace.y is not None:
            assert np.array_equal(deserialized_data["figure"].data[i].y, trace.y)

    # Compare layout properties
    assert deserialized_data["figure"].layout.title.text == fig.layout.title.text

    print("Serialization and deserialization test passed.")



================================================
FILE: nodeology/workflow.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os, logging
import json, yaml, re
import inspect
import getpass
from datetime import datetime
from jsonschema import validate
from typing import Dict, Any, Optional, List, Union, Callable
import ast, operator, traceback
from abc import ABC, abstractmethod
from collections import OrderedDict
import numpy as np

from nodeology.interface import run_chainlit_for_workflow

# Ensure that TypedDict is imported correctly for all Python versions
try:
    from typing import TypedDict, get_type_hints, is_typeddict
except ImportError:
    from typing_extensions import TypedDict, get_type_hints, is_typeddict

from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.types import StateSnapshot
from langgraph.checkpoint.base import BaseCheckpointSaver
from langgraph.checkpoint.memory import MemorySaver

from chainlit import Message, AskUserMessage, run_sync

from nodeology.log import (
    logger,
    log_print_color,
    add_logging_level,
    setup_logging,
)
from nodeology.client import get_client, LLM_Client, VLM_Client
from nodeology.state import (
    State,
    StateEncoder,
    CustomSerializer,
    process_state_definitions,
    _resolve_state_type,
    _type_from_str,
)
from nodeology.node import Node


class Workflow(ABC):
    """Abstract base class for workflow management.

    The Workflow class provides a framework for creating and managing stateful workflows
    that combine language models, vision models, and custom processing nodes. It handles
    state management, logging, error recovery, and workflow execution.

    Key Features:
        - State Management: Maintains workflow state with type validation and history
        - Error Recovery: Automatic state restoration on failures
        - Logging: Comprehensive logging with custom levels
        - Checkpointing: Automatic state checkpointing
        - Human Interaction: Handles user input and interrupts
        - Model Integration: Supports both LLM and VLM clients

    Attributes:
        name (str): Unique workflow identifier
        llm_client (LLM_Client): Language model client for text processing
        vlm_client (Optional[VLM_Client]): Vision model client for image processing
        exit_commands (List[str]): Commands that will trigger workflow termination
        save_artifacts (bool): Whether to save state artifacts to disk
        debug_mode (bool): Enable detailed debug logging
        max_history (int): Maximum number of states to keep in history
        state_schema (Type[State]): Type definition for workflow state
        state_history (List[StateSnapshot]): History of workflow states
        state_index (int): Current state index
        graph (CompiledStateGraph): Compiled workflow graph
        langgraph_config (dict): Configuration for langgraph execution

    Example:
        ```python
        class MyWorkflow(Workflow):
            def create_workflow(self):
                # Define workflow structure
                self.workflow = StateGraph(self.state_schema)
                self.workflow.add_node("start", start_node)
                self.workflow.add_node("process", process_node)
                self.workflow.add_edge("start", "process")
                self.workflow.set_entry_point("start")
                self.graph = self.workflow.compile()

        # Create and run workflow
        workflow = MyWorkflow(
            name="example",
            llm_name="gpt-4o",
            save_artifacts=True
        )
        result = workflow.run()
        ```
    """

    def __init__(
        self,
        name: Optional[str] = None,
        llm_name: Union[str, LLM_Client] = "gpt-4o",
        vlm_name: Optional[Union[str, VLM_Client]] = None,
        state_defs: Optional[Union[List, State]] = None,
        exit_commands: Optional[List[str]] = None,
        save_artifacts: bool = True,
        debug_mode: bool = False,
        max_history: int = 1000,
        checkpointer: Union[BaseCheckpointSaver, str] = "memory",
        tracing: bool = False,
        **kwargs,
    ) -> None:
        """Initialize workflow

        Args:
            name: Workflow name (defaults to class name + timestamp)
            llm_name: Name of LLM model to use
            vlm_name: Optional name of VLM model
            state_defs: State definitions (defaults to class state_schema or State)
            exit_commands: List of commands that will exit the workflow
            save_artifacts: Whether to save state artifacts
            debug_mode: Enable debug logging
            max_history: Maximum number of states to keep in history
            tracing: Whether to enable Langfuse tracing (defaults to False)
        """
        self._init_kwargs = {
            "name": name,
            "llm_name": llm_name,
            "vlm_name": vlm_name,
            "state_defs": state_defs,
            "exit_commands": exit_commands,
            "save_artifacts": save_artifacts,
            "debug_mode": debug_mode,
            "max_history": max_history,
            "checkpointer": checkpointer,
            "tracing": tracing,
        }
        # Add any additional kwargs
        self._init_kwargs.update(kwargs)

        # Generate default name if none provided
        self.name = (
            name
            or f"{self.__class__.__name__}_{datetime.now().strftime('%m_%d_%Y_%H_%M_%S')}"
        )
        if not self.name:
            raise ValueError("Workflow name cannot be empty")

        # Store tracing configuration
        self.tracing = tracing

        # Configure Langfuse if tracing is enabled
        if self.tracing:
            from nodeology.client import configure_langfuse

            configure_langfuse(enabled=True)

        # Create clients
        if isinstance(llm_name, str):
            self.llm_client = get_client(llm_name, tracing_enabled=self.tracing)
        elif isinstance(llm_name, LLM_Client):
            self.llm_client = llm_name
            # If it's a LiteLLM_Client, set tracing_enabled
            if hasattr(self.llm_client, "tracing_enabled"):
                self.llm_client.tracing_enabled = self.tracing
        else:
            raise ValueError("llm_name must be a string or LLM_Client instance")

        if vlm_name:
            if isinstance(vlm_name, str):
                self.vlm_client = get_client(vlm_name, tracing_enabled=self.tracing)
            elif isinstance(vlm_name, VLM_Client):
                self.vlm_client = vlm_name
                # If it's a LiteLLM_Client, set tracing_enabled
                if hasattr(self.vlm_client, "tracing_enabled"):
                    self.vlm_client.tracing_enabled = self.tracing
            else:
                raise ValueError("vlm_name must be a string or VLM_Client instance")
        else:
            self.vlm_client = None
            logger.warning(
                "VLM client not provided - vision features will be unavailable"
            )

        # Store configuration
        self.exit_commands = (
            exit_commands
            if exit_commands
            else [
                "stop workflow",
                "quit workflow",
                "terminate workflow",
            ]
        )
        self.save_artifacts = save_artifacts
        self.checkpointer = checkpointer
        self.debug_mode = debug_mode
        self.max_history = max_history
        self.kwargs = kwargs

        # Process state definitions
        if is_typeddict(state_defs):
            self.state_schema = state_defs
        elif isinstance(state_defs, list):
            self.state_schema = self._compile_state_definitions(state_defs)
        else:
            self.state_schema = getattr(self, "state_schema", State)

        # Setup logging and initialize workflow
        self._setup_logging()
        self._node_configs = {}
        self._entry_point = None
        self._interrupt_before = []
        self.create_workflow()
        self.initialize()

    def _compile_state_definitions(self, state_defs):
        """Compile state definitions into a State class"""
        annotations = {}

        for state_def in state_defs:
            if isinstance(state_def, tuple) and len(state_def) == 2:
                name, type_hint = state_def
                if isinstance(type_hint, str):
                    # Use _resolve_state_type to get the actual type
                    type_hint = _resolve_state_type(type_hint)
                annotations[name] = type_hint
            elif isinstance(state_def, type) and is_typeddict(state_def):
                # Use get_type_hints to retrieve annotations from the TypedDict
                annotations.update(get_type_hints(state_def))
            else:
                raise ValueError(f"Invalid state definition format: {state_def}")

        # Dynamically create a new TypedDict class with the collected annotations
        CompiledState = TypedDict("CompiledState", annotations)
        return CompiledState

    def _setup_logging(self, base_dir: Optional[str] = None) -> None:
        """Setup workflow-specific logging configuration.

        Configures logging with custom levels and file handlers.

        Args:
            base_dir: Optional base directory for log files
        """
        # Set up basic workflow information
        self.user_name = getpass.getuser()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_name = f"{self.name}_{timestamp}"
        self.log_path = os.path.join("logs", self.name)

        # Add custom logging levels if not already present
        if not hasattr(logging, "PRINTLOG"):
            add_logging_level("PRINTLOG", logging.INFO + 5)
        if not hasattr(logging, "LOGONLY"):
            add_logging_level("LOGONLY", logging.INFO + 1)

        # Setup logging using the log_utils configuration
        setup_logging(
            log_dir=self.log_path,
            log_name=self.log_name,
            debug_mode=self.debug_mode,
            base_dir=base_dir,
        )

        # Log initial workflow configuration
        logger.logonly("########## Settings ##########")
        logger.logonly(f"Workflow name: {self.name}")
        logger.logonly(f"User name: {self.user_name}")
        logger.logonly(f"Debug mode: {self.debug_mode}")
        logger.logonly("##############################")

    def save_state(self, current_state: Optional[StateSnapshot] = None) -> None:
        """Save the current workflow state to history and optionally to disk.

        Args:
            current_state: State snapshot to save (fetched if None)

        Maintains a rolling history window and saves state files if save_artifacts is enabled.
        """
        try:
            if current_state is None:
                current_state = self.graph.get_state(self.langgraph_config)

            # Add to history
            self.state_history.append(current_state)
            current_state_values = current_state.values

            # Maintain rolling history window
            if len(self.state_history) > self.max_history:
                self.state_history = self.state_history[-self.max_history :]

            # Save state file if enabled
            if self.save_artifacts and not self.debug_mode:
                state_file = os.path.join(
                    self.log_path, f"state_{self.state_index}.json"
                )
                with open(state_file, "w", encoding="utf-8") as f:
                    json.dump(current_state_values, f, indent=2, cls=StateEncoder)

            self.state_index += 1

        except Exception as e:
            logger.error(f"Failed to save state: {str(e)}")
            if self.debug_mode:
                raise

    def load_state(self, state_index: int) -> None:
        """Load a previous workflow state by index.

        Args:
            state_index: Index of state to load

        Raises:
            ValueError: If state file not found or schema mismatch
        """
        # Try loading from recent history first
        if state_index < self.state_index and state_index >= (
            self.state_index - self.max_history
        ):
            state = self.state_history[-self.state_index + state_index]
            state_values = state.values
        else:
            # Fall back to loading from file
            state_file = os.path.join(self.log_path, f"state_{state_index}.json")
            if os.path.exists(state_file):
                with open(state_file, "r", encoding="utf-8") as f:
                    state_values = json.load(f)
            else:
                raise ValueError(f"State file not found: {state_file}")

        # Validate loaded state against current schema
        annotations = get_type_hints(self.state_schema)
        for field in annotations:
            if field not in state_values:
                raise ValueError("Loaded state does not match current schema")

        # Update graph state and save
        self.graph.update_state(self.langgraph_config, state_values)
        self.save_state()

    def update_state(
        self,
        values: Optional[Dict[str, Any]] = None,
        current_state: Optional[StateSnapshot] = None,
        human_input: Optional[str] = None,
        as_node: Optional[Node] = None,
    ) -> None:
        """Update the workflow state with new values and/or human input.

        Handles nested updates, type validation, and error recovery.

        Args:
            values: Dictionary of state values to update
            current_state: Current state snapshot (fetched if None)
            human_input: Human input to add to messages/conversation
            as_node: Node to attribute the update to

        Raises:
            TypeError: If provided values don't match schema types
            ValueError: If invalid fields are provided in debug mode
        """
        try:
            current_state = (
                self.graph.get_state(self.langgraph_config)
                if current_state is None
                else current_state
            )
            new_state_values = current_state.values.copy()

            annotations = get_type_hints(self.state_schema)

            if values:
                # Validate fields before updating
                invalid_fields = [field for field in values if field not in annotations]
                if invalid_fields:
                    if self.debug_mode:  # Only raise in debug mode
                        raise ValueError(f"Invalid fields in update: {invalid_fields}")
                    else:
                        logger.warning(
                            f"Ignoring invalid fields in update: {invalid_fields}"
                        )
                        # Filter out invalid fields
                        values = {k: v for k, v in values.items() if k in annotations}

                def update_nested(current: dict, updates: dict):
                    """Recursively update nested dictionary with type validation."""
                    for k, v in updates.items():
                        if (
                            k in current
                            and isinstance(current[k], dict)
                            and isinstance(v, dict)
                        ):
                            update_nested(current[k], v)
                        else:
                            if k in annotations:
                                field_type = annotations[k]
                                if not self._validate_type(v, field_type):
                                    raise TypeError(
                                        f"Invalid type for {k}: expected {field_type}, got {type(v)}"
                                    )
                            current[k] = v

                update_nested(new_state_values, values)

            if human_input is not None:
                # Update message-related fields if they exist in schema
                for field, update in [
                    (
                        "messages",
                        lambda: new_state_values["messages"]
                        + [{"role": "user", "content": human_input}],
                    ),
                    (
                        "conversation",
                        lambda: new_state_values["conversation"]
                        + [{"role": "user", "content": human_input}],
                    ),
                    ("human_input", lambda: human_input),
                ]:
                    if field in annotations:
                        if field not in new_state_values:
                            new_state_values[field] = (
                                [] if field in ["messages", "conversation"] else ""
                            )
                        new_value = update()
                        field_type = annotations[field]
                        if not self._validate_type(new_value, field_type):
                            raise TypeError(
                                f"Invalid type for {field}: expected {field_type}, got {type(new_value)}"
                            )
                        new_state_values[field] = new_value

            self.graph.update_state(
                config=self.langgraph_config,
                values=new_state_values,
                as_node=as_node,
            )

        except Exception as e:
            logger.error(f"Error in update_state: {str(e)}\n{traceback.format_exc()}")
            if self.debug_mode:
                raise
            else:
                self._restore_last_valid_state()

    def _create_checkpoint(self) -> None:
        """Create a checkpoint of the current workflow state.

        Saves the current state to a checkpoint file if save_artifacts is enabled.
        """
        if self.save_artifacts and not self.debug_mode:
            state = self.graph.get_state(self.langgraph_config)
            checkpoint_file = os.path.join(self.log_path, "checkpoint.json")
            try:
                with open(checkpoint_file, "w", encoding="utf-8") as f:
                    json.dump(state.values, f, indent=2, cls=StateEncoder)
                logger.debug("Created checkpoint")
            except Exception as e:
                logger.error(f"Failed to create checkpoint: {str(e)}")

    def _restore_last_valid_state(self):
        """Attempt to restore the workflow to the last valid state.

        First tries recent history states, then falls back to checkpoint.
        Raises RuntimeError if no valid state can be restored.
        """
        # First try recent history
        for i in range(self.state_index - 1, max(-1, self.state_index - 4), -1):
            try:
                self.load_state(i)
                logger.info(f"Successfully restored to state {i}")
                return
            except Exception as e:
                logger.warning(f"Failed to restore state {i}: {str(e)}")

        # If that fails, try loading checkpoint
        checkpoint_file = os.path.join(self.log_path, "checkpoint.json")
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, "r", encoding="utf-8") as f:
                    checkpoint_state = json.load(f)
                self.graph.update_state(self.langgraph_config, checkpoint_state)
                self.save_state()
                logger.info("Successfully restored from checkpoint")
                return
            except Exception as e:
                logger.error(f"Failed to restore from checkpoint: {str(e)}")

        raise RuntimeError("Could not restore to any valid state")

    @abstractmethod
    def create_workflow(self):
        """Create the workflow graph structure"""
        pass

    def add_node(
        self, name: str, node: Optional[Node] = None, client_type: str = "llm", **kwargs
    ):
        """Add a node to the workflow with simplified syntax"""
        # If node has image_keys, automatically set client_type to vlm
        if node and hasattr(node, "image_keys") and node.image_keys:
            client_type = "vlm"
            kwargs["image_keys"] = node.image_keys

        # Initialize configurations if needed
        if not hasattr(self, "_workflow_configs"):
            self._workflow_configs = {
                "nodes": {},
                "edges": [],
                "conditionals": [],
                "entry": None,
            }
        if not hasattr(self, "_node_configs"):
            self._node_configs = {}

        # Store workflow configuration
        workflow_config = {
            "client_type": client_type,
            "node": node,
            "kwargs": kwargs.copy(),
        }
        self._workflow_configs["nodes"][name] = workflow_config

        # Keep original template configuration structure
        node_config = {
            "client_type": client_type,
        }

        if node is not None:
            if node.prompt_template is not None and len(node.prompt_template) > 0:
                node_config.update(
                    {
                        "type": "prompt",
                        "template": node.prompt_template,
                    }
                )
            else:
                node_config["type"] = name

            if node.sink:
                node_config["sink"] = node.sink
            if node.sink_format:
                node_config["sink_format"] = node.sink_format
            if node.image_keys:
                node_config["image_keys"] = node.image_keys
                node_config["client_type"] = "vlm"

        # Handle special kwargs as before
        processed_kwargs = {}
        if kwargs:
            for k, v in kwargs.items():
                if k in node_config:
                    pass
                elif callable(v):
                    processed_kwargs[k] = "${" + k + "}"
                else:
                    processed_kwargs[k] = v

            if processed_kwargs:
                node_config["kwargs"] = processed_kwargs

        self._node_configs[name] = node_config

    def add_flow(self, from_node: str, to_node: str):
        """Add a simple edge between nodes"""
        if not hasattr(self, "_workflow_configs"):
            self._workflow_configs = {
                "nodes": {},
                "edges": [],
                "conditionals": [],
                "entry": None,
            }

        # Store workflow edge configuration
        self._workflow_configs["edges"].append({"from": from_node, "to": to_node})

        # Keep original template configuration
        self._node_configs[from_node]["next"] = to_node if to_node != END else "END"

    def add_conditional_flow(
        self,
        from_node: str,
        condition: Union[str, Callable[[dict], bool]],
        then: str,
        otherwise: str,
    ):
        """Add a conditional edge with simplified syntax"""
        if not hasattr(self, "_workflow_configs"):
            self._workflow_configs = {
                "nodes": {},
                "edges": [],
                "conditionals": [],
                "entry": None,
            }

        # Process condition for tracking
        if isinstance(condition, str):
            condition_str = condition

            # Create a closure to avoid variable capture issues
            def make_condition(cond_str):
                return lambda state: state[cond_str]

            condition_func = make_condition(condition)
        elif callable(condition):
            condition_src = inspect.getsource(condition).strip()

            # Create a closure to avoid variable capture issues
            def make_condition(cond):
                return lambda state: cond(state)

            condition_func = make_condition(condition)

            if condition_src.startswith("lambda"):
                # Extract just the lambda function definition using regex
                lambda_pattern = r"lambda\s+[^:]+:\s*([^,\n]+)"
                match = re.search(lambda_pattern, condition_src)
                if match:
                    condition_str = match.group(1).strip()
                else:
                    raise ValueError(
                        f"Could not parse lambda condition: {condition_src}"
                    )
                # Replace state['var'], state["var"], state.get["var"] and state.get('var') with just var
                condition_str = re.sub(r"state\['([^']+)'\]", r"\1", condition_str)
                condition_str = re.sub(r'state\["([^"]+)"\]', r"\1", condition_str)
                condition_str = re.sub(r'state\.get\("([^"]+)"\)', r"\1", condition_str)
                condition_str = re.sub(r"state\.get\('([^']+)'\)", r"\1", condition_str)
            else:
                # For complex functions, use template variable for tracking
                condition_str = "${" + condition.__name__ + "}"

        # Store workflow conditional configuration
        self._workflow_configs["conditionals"].append(
            {
                "from": from_node,
                "condition": condition_func,
                "then": then,
                "otherwise": otherwise,
            }
        )

        # Keep original template configuration
        self._node_configs[from_node]["next"] = {
            "condition": condition_str,
            "then": "END" if then == END else then,
            "otherwise": "END" if otherwise == END else otherwise,
        }

    def set_entry(self, node: str):
        """Set the workflow entry point"""
        if not hasattr(self, "_workflow_configs"):
            self._workflow_configs = {
                "nodes": {},
                "edges": [],
                "conditionals": [],
                "entry": None,
            }

        self._workflow_configs["entry"] = node
        self._entry_point = node  # Keep original entry point storage

    def compile(
        self,
        interrupt_before: Optional[List[str]] = None,
        checkpointer: Optional[Union[str, BaseCheckpointSaver]] = None,
        auto_input_nodes: bool = True,
        interrupt_before_phrases: Optional[Dict[str, str]] = None,
    ):
        """Compile the workflow with optional interrupt points and checkpointing"""
        if not hasattr(self, "workflow"):
            self.workflow = StateGraph(self.state_schema)

        # Setup checkpointer
        checkpointer = checkpointer if checkpointer else self.checkpointer
        if checkpointer == "memory":
            checkpointer = MemorySaver(serde=CustomSerializer())
        elif not isinstance(checkpointer, BaseCheckpointSaver):
            raise ValueError(
                "checkpointer must be 'memory' or a BaseCheckpointSaver instance"
            )

        # Store interrupt_before_phrases
        if interrupt_before_phrases is None:
            interrupt_before_phrases = {}
        self._interrupt_before_phrases = interrupt_before_phrases

        # Track input nodes if auto creation is enabled
        input_nodes = set()
        node_mapping = {}  # Maps original nodes to their input nodes
        added_nodes = set()  # Track nodes that have been added

        if auto_input_nodes and interrupt_before:
            # Create input nodes for interrupted nodes (avoiding duplicates)
            for node_name in interrupt_before:
                input_node_name = f"{node_name}_input"

                # Skip if this input node was already created
                if input_node_name not in added_nodes:
                    input_nodes.add(input_node_name)
                    node_mapping[node_name] = input_node_name
                    # Add input node to workflow
                    self.workflow.add_node(input_node_name, lambda state: state)
                    added_nodes.add(input_node_name)

        # First pass: Create all nodes from workflow configs
        for node_name, config in self._workflow_configs["nodes"].items():
            if node_name not in added_nodes:
                # Create the actual node
                node = config.get("node")
                if node is None:
                    self.workflow.add_node(node_name, lambda state: state)
                else:
                    # Select appropriate client
                    client_type = config["client_type"].lower()
                    if client_type == "llm":
                        client = self.llm_client
                    elif client_type == "vlm":
                        if self.vlm_client is None:
                            raise ValueError("VLM client not available")
                        client = self.vlm_client
                    else:
                        raise ValueError(f"Invalid client_type: {client_type}")

                    # Create wrapped function with client injection
                    def wrapped_func(
                        state,
                        n=node,
                        c=client,
                        debug=self.debug_mode,
                        k=config["kwargs"],
                    ):
                        # Pass workflow and node to client for metadata tracking
                        return n(state, c, debug=debug, workflow=self, node=n, **k)

                    self.workflow.add_node(node_name, wrapped_func)
                    added_nodes.add(node_name)

        # Second pass: Create all edges with proper routing
        for edge in self._workflow_configs["edges"]:
            from_node = edge["from"]
            to_node = edge["to"]

            if auto_input_nodes and interrupt_before:
                # If the target node needs input, route through its input node
                if to_node in interrupt_before and to_node != END:
                    # Add edge from original source to target's input node
                    self.workflow.add_edge(from_node, node_mapping[to_node])
                    # Add edge from input node to actual target
                    self.workflow.add_edge(node_mapping[to_node], to_node)
                else:
                    # Regular edge
                    self.workflow.add_edge(
                        from_node, END if to_node == END else to_node
                    )
            else:
                # Regular edge without input node routing
                self.workflow.add_edge(from_node, END if to_node == END else to_node)

        # Third pass: Add conditional edges with proper routing
        for config in self._workflow_configs["conditionals"]:
            from_node = config["from"]
            condition = config["condition"]
            then_target = config["then"]
            otherwise_target = config["otherwise"]

            if auto_input_nodes and interrupt_before:
                # Route targets through input nodes if needed
                if then_target in interrupt_before and then_target != END:
                    # Add edge from input node to actual target for 'then' branch
                    self.workflow.add_edge(node_mapping[then_target], then_target)
                    then_target = node_mapping[then_target]

                if otherwise_target in interrupt_before and otherwise_target != END:
                    # Add edge from input node to actual target for 'otherwise' branch
                    self.workflow.add_edge(
                        node_mapping[otherwise_target], otherwise_target
                    )
                    otherwise_target = node_mapping[otherwise_target]

            # Use a function factory to create wrapped_condition_func
            def make_wrapped_condition_func(condition):
                if isinstance(condition, Callable):
                    return lambda state: "then" if condition(state) else "otherwise"
                elif isinstance(condition, str):
                    return lambda state: (
                        "then" if _eval_condition(condition, state) else "otherwise"
                    )
                else:
                    raise ValueError(f"Invalid condition type: {type(condition)}")

            wrapped_condition_func = make_wrapped_condition_func(condition)

            self.workflow.add_conditional_edges(
                from_node,
                wrapped_condition_func,
                {
                    "then": END if then_target == END else then_target,
                    "otherwise": END if otherwise_target == END else otherwise_target,
                },
            )

        # Set entry point with proper routing
        entry_point = self._workflow_configs["entry"]
        if entry_point is None:
            raise ValueError("No entry point set. Call set_entry first.")

        if auto_input_nodes and interrupt_before and entry_point in interrupt_before:
            # If entry point needs input, route through its input node
            self.workflow.add_edge(node_mapping[entry_point], entry_point)
            self.workflow.set_entry_point(node_mapping[entry_point])
        else:
            self.workflow.set_entry_point(entry_point)

        # Store interrupt configuration and compile
        self._interrupt_before = (
            list(input_nodes) if auto_input_nodes else (interrupt_before or [])
        )
        self.graph = self.workflow.compile(
            checkpointer=checkpointer, interrupt_before=self._interrupt_before
        )

    def _validate_type(self, value: Any, expected_type: Any) -> bool:
        """Validate that a value matches the expected type.

        Handles complex types including:
        - Union types
        - List types with element validation
        - Dict types with key/value validation
        - Numpy arrays

        Args:
            value: Value to validate
            expected_type: Type to validate against

        Returns:
            bool: Whether the value matches the expected type
        """
        from typing import get_origin, get_args, Union

        # Add numpy array handling
        if expected_type is np.ndarray:
            return isinstance(value, np.ndarray)

        origin_type = get_origin(expected_type)

        if origin_type is Union:
            return any(self._validate_type(value, t) for t in get_args(expected_type))
        elif origin_type is list:
            if not isinstance(value, list):
                return False
            elem_type = get_args(expected_type)[0]
            return all(self._validate_type(v, elem_type) for v in value)
        elif origin_type is dict:
            if not isinstance(value, dict):
                return False
            key_type, val_type = get_args(expected_type)
            return all(
                self._validate_type(k, key_type) and self._validate_type(v, val_type)
                for k, v in value.items()
            )
        else:
            return isinstance(value, expected_type)

    def initialize(
        self, init_values: Optional[Dict[str, Any]] = None, recursion_limit=99999999
    ) -> None:
        """Initialize the workflow state with proper None handling and type checking"""
        assert hasattr(self, "graph"), "Workflow graph must be defined"
        assert isinstance(
            self.graph, CompiledStateGraph
        ), "Graph must be a CompiledStateGraph instance"

        # Use get_type_hints to resolve actual types
        annotations = get_type_hints(self.state_schema)

        # Initialize default values for all state fields
        default_state = {}
        for field, field_type in annotations.items():
            if field_type in (str, int, float, bool):
                default_values = {str: "", int: 0, float: 0.0, bool: False}
                default_state[field] = default_values[field_type]
            elif field_type == list or (
                hasattr(field_type, "__origin__") and field_type.__origin__ is list
            ):
                default_state[field] = []
            elif field_type == dict or (
                hasattr(field_type, "__origin__") and field_type.__origin__ is dict
            ):
                default_state[field] = {}
            elif hasattr(field_type, "__origin__") and field_type.__origin__ is Union:
                # For Union types, use the first type's default value
                first_type = field_type.__args__[0]
                if first_type in (str, int, float, bool):
                    default_values = {str: "", int: 0, float: 0.0, bool: False}
                    default_state[field] = default_values[first_type]
                else:
                    default_state[field] = None
            else:
                default_state[field] = None

        # Validate input fields before updating
        if init_values:
            invalid_fields = [
                field for field in init_values if field not in annotations
            ]
            if invalid_fields and self.debug_mode:
                raise ValueError(f"Invalid fields in initialization: {invalid_fields}")

        # Update defaults with provided values
        if init_values:
            for field, value in init_values.items():
                if field in annotations:
                    if value is None:
                        # Keep the default value if None is provided
                        continue
                    field_type = annotations[field]
                    if not self._validate_type(value, field_type):
                        raise TypeError(
                            f"Invalid type for {field}: expected {field_type}, got {type(value)}"
                        )
                    default_state[field] = value

        self.langgraph_config = {
            "configurable": {"thread_id": self.log_name},
            "recursion_limit": recursion_limit,
        }
        self.graph.update_state(
            config=self.langgraph_config,
            values=default_state,
        )

        self.state_index = 0
        self.state_history = []
        self.save_state()

    def run(self, init_values: Optional[Dict] = None, ui: bool = False) -> Dict:
        """Run the workflow, optionally in Chainlit UI mode."""
        if not ui:
            log_print_color(
                f"Starting {self.name} workflow for {self.user_name}", "green"
            )
            return self._run(init_values, ui=False)
        else:
            return run_chainlit_for_workflow(self, init_values)

    def _run(self, init_values: Optional[Dict] = None, ui: bool = False) -> Dict:
        """Your existing run code that was previously in 'run' method."""
        # Initialize graph state
        graph_input = (
            self.graph.get_state(self.langgraph_config).values
            if init_values is None
            else init_values
        )
        error_state = None
        current_state = self.graph.get_state(self.langgraph_config)

        try:
            while True:
                # Run the graph until it needs input or reaches the end
                for _ in self.graph.stream(graph_input, self.langgraph_config):
                    current_state = self.graph.get_state(self.langgraph_config)
                    self.save_state()

                    # Check if we've reached the end
                    if (
                        current_state.next is None
                        or len(current_state.next) == 0
                        or END in current_state.next
                        or "END" in current_state.next
                    ):
                        return current_state.values if current_state else {}

                # Get human input when the graph needs it
                # Attempt to fetch an interrupt phrase if we have one
                # next_node might be something like "someNode_input"
                # so we strip "_input" to get the real node name
                real_node_name = current_state.next[0]
                if real_node_name.endswith("_input"):
                    real_node_name = real_node_name[:-6]

                prompt_text = f"{self.user_name}: "
                if (
                    self._interrupt_before_phrases
                    and real_node_name in self._interrupt_before_phrases
                ):
                    prompt_text = self._interrupt_before_phrases[real_node_name]

                human_input = self._get_human_input(ui, prompt_text)

                if self._should_exit(human_input):
                    return current_state.values if current_state else {}

                # Update state with human input and continue
                self.update_state(
                    human_input=human_input, as_node=current_state.next[0]
                )
                self.save_state()
                graph_input = None  # Reset input for next iteration

        except Exception as e:
            logger.error(f"Error during workflow execution: {str(e)}")
            error_state = {"error": str(e)}
            exc_info = traceback.format_exc()
            print(exc_info)
            if self.debug_mode:
                raise
            return (
                error_state
                if error_state
                else current_state.values if current_state else {}
            )

    def _should_exit(self, cmd_input: str) -> bool:
        """Check if a command should exit the workflow.

        Args:
            cmd_input: Command string to check

        Returns:
            bool: True if command matches any exit command
        """
        return any(cmd in cmd_input.lower() for cmd in self.exit_commands)

    def _get_human_input(self, ui: bool = False, prompt: Optional[str] = None) -> str:
        """Get and log input from the user.

        Args:
            ui: Whether we are in Chainlit UI mode
            prompt: Optional custom prompt to display (falls back to f"{self.user_name}: " if None)

        Returns:
            str: User input

        Raises:
            ValueError: If invalid input mode specified
        """
        if prompt is None:
            prompt = f"{self.user_name}: "

        if ui:
            human_input = run_sync(AskUserMessage(content=prompt).send())["output"]
        else:
            human_input = input(prompt)
        logger.logonly(f"{self.user_name}: {human_input}")
        return human_input

    def __enter__(self):
        """Context manager entry point.

        Returns:
            Workflow: Self for use in with statement
        """
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit point with cleanup.

        Creates final checkpoint and cleans up logging handlers.
        """
        try:
            # Create checkpoint before cleanup
            self._create_checkpoint()

            # Store a reference to the root logger
            root_logger = logging.getLogger()

            # Clean up logging handlers safely
            handlers = root_logger.handlers[:]  # Create a copy of the list
            for handler in handlers:
                try:
                    # Flush any remaining logs
                    handler.flush()
                    # Remove handler from logger first
                    root_logger.removeHandler(handler)
                    # Then close the handler
                    handler.close()
                except Exception as e:
                    # Log error without using file handler
                    print(f"Warning during handler cleanup: {str(e)}")

            # Clean up any graph resources
            if hasattr(self, "graph"):
                # Add graph cleanup if needed
                pass

        except Exception as e:
            # Print error since logging may not be available
            print(f"Error during workflow cleanup: {str(e)}")
            if self.debug_mode:
                raise

    def to_yaml(self, output_path: Optional[str] = None) -> Dict:
        """Export workflow configuration to YAML format.

        Args:
            output_path: Optional path to save YAML file. If not provided,
                        returns dictionary representation only.

        Returns:
            Dict: Template configuration dictionary

        Raises:
            ValueError: If workflow configuration is invalid for export
        """
        return export_workflow_to_template(self, output_path)


def _validate_template_structure(template: Dict) -> None:
    """Validate the basic structure of a workflow template against schema.

    Required fields:
    - name: string
    - state_defs: array of state definitions
    - nodes: object containing node definitions
    - entry_point: string

    Optional fields:
    - llm: string (default: "gpt-4o")
    - vlm: string
    - exit_commands: array of strings
    - intervene_before: array of strings
    - intervene_before_phrases: object mapping node names to prompt phrases
    """
    schema = {
        "type": "object",
        "required": ["name", "state_defs", "nodes", "entry_point"],
        "properties": {
            "name": {"type": "string", "minLength": 1},
            "state_defs": {"type": "array", "minItems": 1},
            "nodes": {"type": "object", "minProperties": 1},
            "entry_point": {"type": "string"},
            "llm": {"type": "string"},
            "vlm": {"type": "string"},
            "exit_commands": {"type": "array", "items": {"type": "string"}},
            "intervene_before": {"type": "array", "items": {"type": "string"}},
            "intervene_before_phrases": {
                "type": "object",
                "additionalProperties": {"type": "string"},
            },
        },
        "additionalProperties": False,
    }

    try:
        validate(instance=template, schema=schema)
    except Exception as e:
        raise ValueError(f"Invalid template structure: {str(e)}")


def _validate_nodes(nodes: Dict, node_registry: Dict) -> None:
    """Validate node configurations in the template.

    Checks:
    - Required fields presence
    - Node type validity
    - Prompt node specific configuration
    - Next/conditional logic validity

    Args:
        nodes: Dictionary of node configurations
        node_registry: Dictionary of available node types

    Raises:
        ValueError: If node configuration is invalid
    """
    for node_name, node_config in nodes.items():
        # Check required fields
        if "type" not in node_config:
            raise ValueError(f"Node {node_name} missing 'type' field")

        node_type = node_config["type"]

        # Validate by node type
        if node_type == "prompt":
            _validate_prompt_node(node_name, node_config)
        elif node_type not in node_registry:
            raise ValueError(
                f"Unknown node type: {node_type} (available: {list(node_registry.keys())})"
            )

        # Validate next/conditional logic
        if "next" not in node_config:
            raise ValueError(f"Node '{node_name}' missing 'next' field")

        # Convert string "END" to END constant
        if isinstance(node_config["next"], str) and node_config["next"] == "END":
            node_config["next"] = END
        elif isinstance(node_config["next"], dict):
            if node_config["next"].get("then") == "END":
                node_config["next"]["then"] = END
            if node_config["next"].get("otherwise") == "END":
                node_config["next"]["otherwise"] = END

        _validate_node_transitions(node_name, node_config["next"])


def _validate_prompt_node(node_name: str, config: Dict) -> None:
    """Validate prompt node specific configuration.

    Args:
        node_name: Name of the node
        config: Node configuration dictionary

    Raises:
        ValueError: If prompt node configuration is invalid
    """
    # Check required template field
    if "template" not in config:
        raise ValueError(f"Prompt node '{node_name}' missing 'template' field")

    # Validate optional image_keys field
    if "image_keys" in config:
        # Accept both string (single item) and list formats
        if not isinstance(config["image_keys"], (str, list)):
            raise ValueError(
                f"Prompt node '{node_name}' image_keys must be a string or list"
            )
        # Convert string to list internally
        if isinstance(config["image_keys"], str):
            config["image_keys"] = [config["image_keys"]]

    # Validate sink field if present
    if "sink" in config:
        # Accept both string (single item) and list formats
        if isinstance(config["sink"], str):
            config["sink"] = [config["sink"]]  # Convert single string to list
        elif not isinstance(config["sink"], list):
            raise ValueError(f"Prompt node '{node_name}' sink must be a string or list")

        # Validate each sink field name
        for sink in config["sink"]:
            if not isinstance(sink, str):
                raise ValueError(
                    f"Prompt node '{node_name}' sink values must be strings"
                )


def _validate_condition_expr(expr: str) -> bool:
    """Validate a conditional expression for security and correctness.

    Checks:
    - Python syntax validity
    - Allowed operations and functions
    - Security constraints

    Args:
        expr: Condition expression string

    Returns:
        bool: True if expression is valid

    Raises:
        ValueError: If expression is invalid or contains forbidden operations
    """
    import ast

    try:
        tree = ast.parse(expr, mode="eval")
        allowed_ops = (
            # Core expression nodes
            ast.Expression,  # Root node for eval mode
            ast.Name,  # Variable names
            ast.Constant,  # Literal values
            ast.List,  # List literals
            ast.Dict,  # Dictionary literals
            # Array/Dict access
            ast.Subscript,  # For array[index] or dict[key]
            ast.Index,  # For simple indexing
            ast.Slice,  # For slice operations
            # Comparison operators
            ast.Compare,
            ast.Eq,  # ==
            ast.NotEq,  # !=
            ast.Lt,  # <
            ast.LtE,  # <=
            ast.Gt,  # >
            ast.GtE,  # >=
            # Unary operators
            ast.UnaryOp,
            ast.Is,  # is
            ast.IsNot,  # is not
            ast.In,  # in
            ast.NotIn,  # not in
            # Boolean operators
            ast.BoolOp,
            ast.And,  # and
            ast.Or,  # or
            ast.Not,  # not
            # Function calls
            ast.Call,
            # Additional nodes for comprehensions
            ast.ListComp,  # List comprehensions
            ast.SetComp,  # Set comprehensions
            ast.DictComp,  # Dict comprehensions
            ast.GeneratorExp,  # Generator expressions
            ast.comprehension,  # The 'for' part of comprehensions
        )
        allowed_funcs = {
            "len",
            "upper",
            "lower",
            "str",
            "int",
            "float",
            "bool",
            "list",
            "dict",
            "all",
            "any",
            "filter",
            "map",
            "sum",
            "max",
            "min",
        }

        # Walk AST and validate each node
        for node in ast.walk(tree):
            # Skip context attributes
            if isinstance(node, (ast.Load, ast.Store)):
                continue

            if not isinstance(node, allowed_ops):
                raise ValueError(
                    f"Invalid operation in condition: {type(node).__name__}"
                )
            # Check function calls
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id not in allowed_funcs:
                        raise ValueError(
                            f"Function not allowed in condition: {node.func.id}"
                        )
                else:
                    raise ValueError("Only simple function calls are allowed")

        return True
    except SyntaxError as e:
        raise ValueError(f"Invalid Python syntax in condition: {str(e)}")
    except Exception as e:
        raise ValueError(f"Invalid condition expression: {str(e)}")


def _validate_node_transitions(node_name: str, next_config: Union[str, Dict]) -> None:
    """Validate node transition configuration.

    Checks transition configuration for both simple and conditional transitions.

    Args:
        node_name: Name of the node being validated
        next_config: Transition configuration (string for simple, dict for conditional)

    Raises:
        ValueError: If transition configuration is invalid
    """
    if isinstance(next_config, dict):
        # Validate conditional transition structure
        if "condition" not in next_config:
            raise ValueError(
                f"Conditional transition in node '{node_name}' missing 'condition'"
            )
        if "then" not in next_config or "otherwise" not in next_config:
            raise ValueError(
                f"Conditional transition in node '{node_name}' missing then/otherwise paths"
            )

        # Validate condition expression
        try:
            _validate_condition_expr(next_config["condition"])
        except ValueError as e:
            raise ValueError(f"Invalid condition in node '{node_name}': {str(e)}")


def _validate_state_definitions(state_defs: List, state_registry: Dict) -> None:
    """Validate state definitions in template.

    Supports multiple formats:
    1. String references to registered states: ["HilpState", "CustomState"]
    2. Tuple definitions: [["name", "type"], ["other", "str"]]
    3. Dictionary definitions: [{"name": "type"}, {"other": "str"}]

    Args:
        state_defs: List of state definitions
        state_registry: Dictionary of available state types

    Raises:
        ValueError: If state definitions are invalid
    """
    for state_def in state_defs:
        if isinstance(state_def, str):
            # Format 1: Validate reference to registered state
            if state_def not in state_registry:
                raise ValueError(f"Unknown state type: {state_def}")
        elif isinstance(state_def, list) and len(state_def) == 2:
            # Format 2: Validate (name, type) tuple format
            name, type_str = state_def
            if not isinstance(name, str):
                raise ValueError(f"State name must be a string: {name}")
            try:
                _resolve_state_type(type_str)
            except ValueError:
                raise ValueError(f"Cannot resolve state type: {type_str}")
        elif isinstance(state_def, dict) and len(state_def) == 1:
            # Format 3: Validate {"name": "type"} dictionary format
            name, type_str = next(iter(state_def.items()))
            if not isinstance(name, str):
                raise ValueError(f"State name must be a string: {name}")
            try:
                _resolve_state_type(type_str)
            except ValueError:
                raise ValueError(f"Cannot resolve state type: {type_str}")
        else:
            raise ValueError(
                f"Invalid state definition format: {state_def}. Must be a string, "
                "[name, type] list, or {'name': 'type'} dictionary"
            )


def _safe_read_template(template_path: str, node_registry: Dict, state_registry: Dict):
    """Safely load and perform initial validation of workflow template.

    Performs basic structural validation before any variable interpolation.

    Args:
        template_path: Path to YAML template file
        node_registry: Dictionary of available node types
        state_registry: Dictionary of available state types

    Returns:
        Dict: Loaded template dictionary

    Raises:
        ValueError: If template cannot be loaded or basic validation fails
    """
    # Load template
    try:
        with open(template_path, "r") as f:
            template = yaml.safe_load(f)
    except Exception as e:
        raise ValueError(f"Failed to load template: {str(e)}")

    # Check for required fields without validating their content yet
    required_fields = ["name", "state_defs", "nodes", "entry_point"]
    missing_fields = [field for field in required_fields if field not in template]
    if missing_fields:
        raise ValueError(f"Template missing required fields: {missing_fields}")

    # Basic type checking that won't be affected by variable interpolation
    if not isinstance(template.get("state_defs", []), list):
        raise ValueError("Template 'state_defs' must be a list")
    if not isinstance(template.get("nodes", {}), dict):
        raise ValueError("Template 'nodes' must be a dictionary")
    if not isinstance(template.get("exit_commands", []), list):
        raise ValueError("Template 'exit_commands' must be a list")
    if not isinstance(template.get("intervene_before", []), list):
        raise ValueError("Template 'intervene_before' must be a list")
    if not isinstance(template.get("intervene_before_phrases", {}), dict):
        raise ValueError(
            "Template 'intervene_before_phrases' must be an object/dict of {nodeName: phrase}"
        )

    # Validate node structure (but not content)
    for node_name, node_config in template["nodes"].items():
        if not isinstance(node_config, dict):
            raise ValueError(f"Node '{node_name}' configuration must be a dictionary")
        if "type" not in node_config:
            raise ValueError(f"Node '{node_name}' missing 'type' field")
        if "next" not in node_config:
            raise ValueError(f"Node '{node_name}' missing 'next' field")

    return template


def _eval_condition_expr(node, state, allowed_funcs, operators):
    """Evaluate a single AST node in a condition expression.

    Recursively evaluates AST nodes while enforcing security constraints.

    Args:
        node: AST node to evaluate
        state: Current workflow state
        allowed_funcs: Dictionary of allowed functions
        operators: Dictionary of allowed operators

    Returns:
        Any: Result of evaluating the node

    Raises:
        ValueError: If evaluation encounters forbidden operations
    """
    if isinstance(node, ast.BoolOp):
        values = [
            _eval_condition_expr(v, state, allowed_funcs, operators)
            for v in node.values
        ]
        return operators[type(node.op)](*values)
    elif isinstance(node, ast.UnaryOp):
        operand = _eval_condition_expr(node.operand, state, allowed_funcs, operators)
        return operators[type(node.op)](operand)
    elif isinstance(node, ast.Compare):
        left = _eval_condition_expr(node.left, state, allowed_funcs, operators)
        for op, comparator in zip(node.ops, node.comparators):
            right = _eval_condition_expr(comparator, state, allowed_funcs, operators)
            # Special handling for 'in' operator - swap operands
            if isinstance(op, (ast.In, ast.NotIn)):
                if not operators[type(op)](right, left):
                    return False
            else:
                if not operators[type(op)](left, right):
                    return False
            left = right
        return True
    elif isinstance(node, ast.Name):
        return state.get(node.id, False)
    elif isinstance(node, ast.Constant):
        return node.value
    elif isinstance(node, ast.Call):
        if not isinstance(node.func, ast.Name):
            raise ValueError("Only simple function calls are allowed")
        if node.func.id not in allowed_funcs:
            raise ValueError(f"Function not allowed: {node.func.id}")
        args = [
            _eval_condition_expr(arg, state, allowed_funcs, operators)
            for arg in node.args
        ]
        return allowed_funcs[node.func.id](*args)
    elif isinstance(node, ast.List):
        return [
            _eval_condition_expr(elt, state, allowed_funcs, operators)
            for elt in node.elts
        ]
    elif isinstance(node, ast.Dict):
        return {
            _eval_condition_expr(
                k, state, allowed_funcs, operators
            ): _eval_condition_expr(v, state, allowed_funcs, operators)
            for k, v in zip(node.keys, node.values)
        }
    elif isinstance(node, ast.Subscript):
        value = _eval_condition_expr(node.value, state, allowed_funcs, operators)
        if isinstance(node.slice, ast.Slice):
            # Handle slice operations
            lower = (
                _eval_condition_expr(node.slice.lower, state, allowed_funcs, operators)
                if node.slice.lower is not None
                else None
            )
            upper = (
                _eval_condition_expr(node.slice.upper, state, allowed_funcs, operators)
                if node.slice.upper is not None
                else None
            )
            step = (
                _eval_condition_expr(node.slice.step, state, allowed_funcs, operators)
                if node.slice.step is not None
                else None
            )
            return value[slice(lower, upper, step)]
        elif isinstance(node.slice, ast.Index):
            idx = _eval_condition_expr(
                node.slice.value, state, allowed_funcs, operators
            )
        else:
            idx = _eval_condition_expr(node.slice, state, allowed_funcs, operators)
        return value[idx]
    elif isinstance(node, (ast.Load, ast.Store)):
        return None
    else:
        raise ValueError(f"Unsupported expression: {ast.dump(node)}")


def _eval_condition(condition_expr: str, state: Dict) -> bool:
    """Evaluate a condition expression against the current state

    Args:
        condition_expr: String containing the condition expression
        state: Current workflow state dictionary

    Returns:
        bool: Result of evaluating the condition

    Raises:
        ValueError: If condition evaluation fails or result cannot be converted to bool
    """
    operators = {
        ast.And: operator.and_,
        ast.Or: operator.or_,
        ast.Not: operator.not_,
        ast.Eq: operator.eq,
        ast.NotEq: operator.ne,
        ast.Lt: operator.lt,
        ast.LtE: operator.le,
        ast.Gt: operator.gt,
        ast.GtE: operator.ge,
        ast.Is: operator.is_,
        ast.IsNot: operator.is_not,
        ast.In: operator.contains,
        ast.NotIn: lambda x, y: not operator.contains(y, x),
    }

    allowed_funcs = {
        "len": len,
        "upper": str.upper,
        "lower": str.lower,
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
        "list": list,
        "dict": dict,
        "all": all,
        "any": any,
        "filter": filter,
        "map": map,
        "sum": sum,
        "max": max,
        "min": min,
    }

    try:
        # First validate the expression
        _validate_condition_expr(condition_expr)

        # If validation passes, evaluate it
        expr_ast = ast.parse(condition_expr, mode="eval")
        result = _eval_condition_expr(expr_ast.body, state, allowed_funcs, operators)

        # Convert result to boolean
        try:
            return bool(result)
        except (TypeError, ValueError) as e:
            raise ValueError(
                f"Condition result cannot be converted to boolean: {result}"
            )

    except Exception as e:
        # Log the error but also re-raise it
        logger.error(f"Error evaluating condition '{condition_expr}': {str(e)}")
        raise ValueError(f"Invalid condition expression: {str(e)}")


def _interpolate_variables(template: Dict, kwargs: Dict) -> Dict:
    """Recursively interpolate variables in template with values from kwargs

    Args:
        template: Template dictionary containing ${var} placeholders
        kwargs: Dictionary of variable values

    Returns:
        Dict with interpolated values
    """

    def _interpolate_value(value):
        if isinstance(value, str):
            # Handle string interpolation
            import re

            pattern = r"\${([^}]+)}"
            matches = re.finditer(pattern, value)
            result = value

            for match in matches:
                var_name = match.group(1)
                if var_name not in kwargs:
                    raise ValueError(
                        f"Required variable '{var_name}' not found in kwargs"
                    )
                # Convert kwargs[var_name] to string for replacement
                replacement = str(kwargs[var_name])
                result = result.replace(f"${{{var_name}}}", replacement)

            return result
        elif isinstance(value, dict):
            return {k: _interpolate_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [_interpolate_value(v) for v in value]
        return value

    return _interpolate_value(template)


def load_workflow_from_template(
    template_path: str,
    node_registry: Optional[Dict[str, Node]] = {},
    state_registry: Optional[Dict[str, State]] = {},
    **kwargs,
) -> Workflow:
    """Create a Workflow instance from a YAML template file."""
    # Load and validate template
    template = _safe_read_template(template_path, node_registry, state_registry)
    template = _interpolate_variables(template, kwargs)
    _validate_template_structure(template)
    _validate_state_definitions(template["state_defs"], state_registry)
    _validate_nodes(template["nodes"], node_registry)

    class UserWorkflow(Workflow):
        def __init__(self_, **kwargs):
            # Store template
            self_.template = template

            # Process state definitions
            state_defs = process_state_definitions(
                template["state_defs"], state_registry
            )

            # Create kwargs dictionary prioritizing template values
            workflow_kwargs = {}
            param_mappings = {
                "name": "name",
                "llm": "llm_name",
                "vlm": "vlm_name",
                "exit_commands": "exit_commands",
                "save_artifacts": "save_artifacts",
                "debug_mode": "debug_mode",
                "max_history": "max_history",
            }

            # Process template values first, then kwargs
            for template_key, param_name in param_mappings.items():
                if template_key in template:
                    workflow_kwargs[param_name] = template[template_key]
                elif param_name in kwargs:
                    workflow_kwargs[param_name] = kwargs[param_name]

            # Set defaults for required parameters
            workflow_kwargs.setdefault("llm_name", "gpt-4o")
            workflow_kwargs.setdefault("vlm_name", None)
            workflow_kwargs.setdefault("tracing", False)  # Always default to False
            workflow_kwargs["state_defs"] = state_defs

            # Add remaining kwargs
            remaining_kwargs = {
                k: v
                for k, v in kwargs.items()
                if k not in workflow_kwargs and k not in param_mappings.values()
            }
            workflow_kwargs.update(remaining_kwargs)

            # Initialize workflow
            super().__init__(**workflow_kwargs)

        def create_workflow(self_):
            """Create workflow structure from template configuration"""
            nodes_config = template["nodes"]
            interrupt_before = template.get("intervene_before", [])
            interrupt_before_phrases = template.get("intervene_before_phrases", {})

            # Store original template configuration
            self_._node_configs = nodes_config.copy()

            # Initialize workflow configuration
            self_._workflow_configs = {
                "nodes": {},
                "edges": [],
                "conditionals": [],
                "entry": None,
            }

            # First pass: Create all nodes
            for node_name, node_config in nodes_config.items():
                node_type = node_config["type"]

                # Create node instance
                if node_type == "prompt":
                    node = Node(
                        prompt_template=node_config["template"],
                        node_type=node_name,
                        sink=node_config.get("sink", []),
                        sink_format=node_config.get("sink_format"),
                        image_keys=node_config.get("image_keys", []),
                    )
                else:
                    node = node_registry[node_type]

                # Determine client type
                client_type = (
                    "vlm"
                    if (
                        "image_keys" in node_config
                        or (node_type == "prompt" and node_config.get("image_keys"))
                    )
                    else "llm"
                )

                # Extract kwargs
                kwargs = {
                    k: v
                    for k, v in node_config.items()
                    if k
                    not in [
                        "type",
                        "template",
                        "next",
                        "sink",
                        "sink_format",
                        "image_keys",
                    ]
                }

                # Store workflow configuration
                self_._workflow_configs["nodes"][node_name] = {
                    "node": node,
                    "client_type": client_type,
                    "kwargs": kwargs,
                }

            # Second pass: Create edges and conditionals
            for node_name, node_config in nodes_config.items():
                next_config = node_config.get("next")
                if isinstance(next_config, str):
                    # Simple edge
                    self_._workflow_configs["edges"].append(
                        {
                            "from": node_name,
                            "to": END if next_config == "END" else next_config,
                        }
                    )
                elif isinstance(next_config, dict):
                    # Conditional edge
                    self_._workflow_configs["conditionals"].append(
                        {
                            "from": node_name,
                            "condition": next_config["condition"],
                            "then": next_config["then"],
                            "otherwise": next_config["otherwise"],
                        }
                    )

            # Set entry point
            self_._workflow_configs["entry"] = template["entry_point"]
            self_._entry_point = template["entry_point"]

            # Compile workflow
            self_.compile(
                interrupt_before=interrupt_before,
                auto_input_nodes=True,
                checkpointer=MemorySaver(serde=CustomSerializer()),
                interrupt_before_phrases=interrupt_before_phrases,
            )

    return UserWorkflow(**kwargs)


def export_workflow_to_template(
    workflow: Workflow, output_path: Optional[str] = None
) -> Dict:
    """Export a workflow configuration to a YAML template."""
    assert isinstance(workflow, Workflow), "Workflow instance required"
    assert hasattr(workflow, "_node_configs"), "Workflow must have node configurations"

    # Start with required fields
    template = {
        "name": workflow.name or "unnamed_workflow",
    }

    # Export state definitions
    state_hints = get_type_hints(workflow.state_schema)
    if not state_hints:
        template["state_defs"] = [{"state": "Dict[str, Any]"}]
    else:
        template["state_defs"] = [
            {name: _type_from_str(type_hint)} for name, type_hint in state_hints.items()
        ]

    # Export nodes configuration using original node_configs
    template["nodes"] = workflow._node_configs

    # Add entry point
    template["entry_point"] = workflow._entry_point

    # Add optional fields
    if workflow.llm_client and workflow.llm_client.model_name:
        template["llm"] = workflow.llm_client.model_name

    if workflow.vlm_client and workflow.vlm_client.model_name:
        template["vlm"] = workflow.vlm_client.model_name

    if workflow.exit_commands:
        template["exit_commands"] = workflow.exit_commands

    if workflow._interrupt_before:
        template["intervene_before"] = [
            node[:-6] if node.endswith("_input") else node
            for node in workflow._interrupt_before
        ]

    # If phrases are stored, export them to 'intervene_before_phrases'
    if (
        hasattr(workflow, "_interrupt_before_phrases")
        and workflow._interrupt_before_phrases
    ):
        template["intervene_before_phrases"] = {
            k: v for k, v in workflow._interrupt_before_phrases.items()
        }

    # Validate template
    node_registry = {}
    for node_name, node_config in workflow._node_configs.items():
        if "type" in node_config:
            if (
                not node_config["type"] in node_registry
                and node_config["type"] != "prompt"
            ):
                node_registry[node_config["type"]] = node_config["type"]
    try:
        _validate_template_structure(template)
        _validate_nodes(template["nodes"], node_registry)
        _validate_state_definitions(template["state_defs"], {})
    except ValueError as e:
        raise ValueError(f"Generated template failed validation: {str(e)}")

    # Save to file if path provided
    if output_path:
        output_template = template.copy()
        output_template["nodes"] = {}
        for node_name, node_config in workflow._node_configs.items():
            # Create node configuration
            node_template = OrderedDict()

            # 1. Type (always first)
            node_template["type"] = node_config.get("type", "prompt")

            # 2. Template (if present)
            if "template" in node_config:
                template_text = node_config["template"]
                # Clean up template text
                template_text = template_text.replace("\\n", "\n")
                template_text = template_text.replace("\t", "\n")
                template_text = template_text.replace("\\t", "\t")
                template_text = " ".join(template_text.split())
                template_text = template_text.strip()
                node_template["template"] = template_text

            # 3. Image keys (if present)
            if "image_keys" in node_config:
                image_keys = node_config["image_keys"]
                # Use direct value for single item
                node_template["image_keys"] = (
                    image_keys[0] if len(image_keys) == 1 else image_keys
                )

            # 4. Sink (if present)
            if "sink" in node_config:
                sink = node_config["sink"]
                # Use direct value for single item
                node_template["sink"] = (
                    sink[0] if isinstance(sink, list) and len(sink) == 1 else sink
                )

            # 5. Additional kwargs (if any)
            if "kwargs" in node_config:
                for k, v in node_config["kwargs"].items():
                    if k not in ["type", "template", "next"] and v is not None:
                        if callable(v):
                            node_template[k] = f"${{{k}}}"
                        else:
                            node_template[k] = v

            # 6. Next (always last)
            next_config = node_config.get("next", "END")
            if isinstance(next_config, dict):
                node_template["next"] = {
                    "condition": next_config.get("condition", "True"),
                    "then": (
                        "END"
                        if next_config.get("then") == END
                        else next_config.get("then", "END")
                    ),
                    "otherwise": (
                        "END"
                        if next_config.get("otherwise") == END
                        else next_config.get("otherwise", "END")
                    ),
                }
            else:
                node_template["next"] = (
                    "END" if next_config == END else (next_config or "END")
                )

            # Ensure the keys are in the desired order
            ordered_node_template = OrderedDict()
            for key in ["type", "template", "sink", "image_keys", "next"]:
                if key in node_template:
                    ordered_node_template[key] = node_template[key]
            # Add any remaining keys
            for key in node_template:
                if key not in ordered_node_template:
                    ordered_node_template[key] = node_template[key]

            output_template["nodes"][node_name] = ordered_node_template

        with open(output_path, "w") as f:

            class CustomDumper(yaml.SafeDumper):
                pass

            def str_presenter(dumper, data):
                return dumper.represent_scalar("tag:yaml.org,2002:str", data)

            def list_presenter(dumper, data):
                if len(data) <= 3:
                    return dumper.represent_sequence(
                        "tag:yaml.org,2002:seq", data, flow_style=True
                    )
                return dumper.represent_sequence(
                    "tag:yaml.org,2002:seq", data, flow_style=False
                )

            def ordered_dict_presenter(dumper, data):
                return dumper.represent_mapping("tag:yaml.org,2002:map", data)

            def dict_presenter(dumper, data):
                return dumper.represent_mapping("tag:yaml.org,2002:map", data)

            CustomDumper.add_representer(str, str_presenter)
            CustomDumper.add_representer(list, list_presenter)
            CustomDumper.add_representer(OrderedDict, ordered_dict_presenter)
            CustomDumper.add_representer(dict, dict_presenter)

            yaml.dump(
                output_template,
                f,
                Dumper=CustomDumper,
                sort_keys=False,
                default_flow_style=False,
                width=80,
                indent=2,
                allow_unicode=True,
            )

    return template



================================================
FILE: tests/test_node.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os, logging
import pytest
from typing import Any, Dict, List
import numpy as np
from nodeology.client import VLM_Client
from nodeology.node import (
    Node,
    as_node,
    remove_markdown_blocks_formatting,
)
from nodeology.state import State
from nodeology.log import add_logging_level

add_logging_level("PRINTLOG", logging.INFO + 5)
add_logging_level("LOGONLY", logging.INFO + 1)


# Basic Node Tests
class TestBasicNodeFunctionality:
    def test_node_creation(self):
        """Test basic node creation and configuration"""
        node = Node(
            node_type="test_node",
            prompt_template="Test prompt with {key1} and {key2}",
            sink=["output"],
        )

        assert node.node_type == "test_node"
        assert "key1" in node.required_keys
        assert "key2" in node.required_keys
        assert node.sink == ["output"]

    def test_node_error_handling(self):
        """Test node error handling for missing required keys"""
        node = Node(
            node_type="test_node", prompt_template="Test {required_key}", sink="output"
        )

        state = State()
        state["messages"] = []

        with pytest.raises(ValueError, match="Required key 'required_key' not found"):
            node(state, None)

    def test_empty_sink_list(self):
        """Test node behavior with empty sink list"""
        node = Node(node_type="test_node", prompt_template="Test", sink=[])

        state = State()
        state["messages"] = []

        result_state = node(state, lambda **k: "response")
        assert result_state == state  # State should remain unchanged

    def test_state_type_tracking_chain(self):
        """Test node type tracking through multiple nodes"""
        node1 = Node(node_type="node1", prompt_template="Test", sink="output1")
        node2 = Node(node_type="node2", prompt_template="Test", sink="output2")

        state = State()
        state["messages"] = []

        state = node1(state, lambda **k: "response1")
        assert state["current_node_type"] == "node1"
        assert state["previous_node_type"] == ""

        state = node2(state, lambda **k: "response2")
        assert state["current_node_type"] == "node2"
        assert state["previous_node_type"] == "node1"

    def test_state_preservation(self):
        """Test that unrelated state data is preserved"""
        node = Node(node_type="test_node", prompt_template="Test", sink="output")

        state = State()
        state["messages"] = []
        state["preserved_key"] = "preserved_value"
        state["input"] = "test_input"

        result_state = node(state, lambda **k: "response")
        assert result_state["preserved_key"] == "preserved_value"
        assert result_state["input"] == "test_input"

    def test_state_immutability(self):
        """Test that original state is not modified"""
        node = Node(node_type="test_node", prompt_template="Test", sink="output")

        original_state = State()
        original_state["messages"] = []
        original_state["value"] = "original"

        state_copy = original_state.copy()
        result_state = node(state_copy, lambda **k: "response")

        assert original_state["value"] == "original"
        assert original_state != result_state


# Execution Tests
class TestNodeExecution:
    class MockClient:
        def __call__(self, messages, **kwargs):
            return "Test response"

    @pytest.fixture
    def basic_state(self):
        state = State()
        state["input"] = "value"
        state["messages"] = []
        return state

    def test_basic_execution(self, basic_state):
        """Test basic node execution"""
        node = Node(
            node_type="test_node", prompt_template="Test {input}", sink="output"
        )

        result_state = node(basic_state, self.MockClient())
        assert result_state["output"] == "Test response"

    def test_multiple_sinks(self, basic_state):
        """Test node with multiple output sinks"""

        class MultiResponseMock:
            def __call__(self, messages, **kwargs):
                return ["Response 1", "Response 2"]

        node = Node(
            node_type="test_node",
            prompt_template="Test {input}",
            sink=["output1", "output2"],
        )

        result_state = node(basic_state, MultiResponseMock())
        assert result_state["output1"] == "Response 1"
        assert result_state["output2"] == "Response 2"

    def test_sink_format(self, basic_state):
        """Test node with specific sink format"""

        class FormatMock:
            def __call__(self, messages, **kwargs):
                assert kwargs.get("format") == "json"
                return '{"key": "value"}'

        node = Node(
            node_type="test_node",
            prompt_template="Test {input}",
            sink="output",
            sink_format="json",
        )

        result_state = node(basic_state, FormatMock())
        assert result_state["output"] == '{"key": "value"}'

    def test_invalid_sink_format(self):
        """Test handling of invalid sink format specification"""
        node = Node(
            node_type="test_node",
            prompt_template="Test",
            sink="output",
            sink_format="invalid_format",
        )

        state = State()
        state["messages"] = []

        # The client should handle invalid format
        class FormatTestClient:
            def __call__(self, messages, **kwargs):
                assert kwargs.get("format") == "invalid_format"
                return "response"

        result_state = node(state, FormatTestClient())
        assert result_state["output"] == "response"

    def test_mismatched_sink_response(self):
        """Test handling of mismatched sink and response count"""
        node = Node(
            node_type="test_node", prompt_template="Test", sink=["output1", "output2"]
        )

        state = State()
        state["messages"] = []

        with pytest.raises(
            ValueError, match="Number of responses .* doesn't match number of sink"
        ):
            node(state, lambda **k: ["single_response"])


# Expression Evaluation Tests
class TestExpressionEvaluation:
    @pytest.fixture
    def sample_state(self):
        class SampleState(State):
            items: List[str]
            numbers: np.ndarray
            text: str
            data: Dict[str, Any]
            key: str

        state = SampleState()
        state["messages"] = []
        state["items"] = ["a", "b", "c", "d"]
        state["numbers"] = np.array([1, 2, 3, 4, 5])
        state["text"] = "Hello World"
        state["data"] = {"name": "John", "age": 30}
        state["key"] = "name"
        return state

    def test_basic_indexing(self, sample_state):
        """Test basic list indexing"""
        node = Node(
            node_type="test_node",
            prompt_template="First: {items[0]}, Last: {items[-1]}",
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        assert "First: a, Last: d" in result_state["messages"][-1]["content"]

    def test_list_slicing(self, sample_state):
        """Test list slicing operations"""
        node = Node(
            node_type="test_node",
            prompt_template="Slice: {items[1:3]}, Reverse: {items[::-1]}",
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        assert (
            "Slice: ['b', 'c'], Reverse: ['d', 'c', 'b', 'a']"
            in result_state["messages"][-1]["content"]
        )

    def test_string_methods(self, sample_state):
        """Test string method calls"""
        node = Node(
            node_type="test_node",
            prompt_template="""
            Upper: {text.upper()}
            Lower: {text.lower()}
            Title: {text.title()}
            Strip: {text.strip()}
            """,
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        message = result_state["messages"][-1]["content"]
        assert "Upper: HELLO WORLD" in message
        assert "Lower: hello world" in message
        assert "Title: Hello World" in message

    def test_built_in_functions(self, sample_state):
        """Test allowed built-in function calls"""
        node = Node(
            node_type="test_node",
            prompt_template="""
            Length: {len(items)}
            Maximum: {max(numbers)}
            Minimum: {min(numbers)}
            Sum: {sum(numbers)}
            Absolute: {abs(-42)}
            """,
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        message = result_state["messages"][-1]["content"]
        assert "Length: 4" in message
        assert "Maximum: 5" in message
        assert "Minimum: 1" in message
        assert "Sum: 15" in message
        assert "Absolute: 42" in message

    def test_dict_access(self, sample_state):
        """Test dictionary access methods"""
        node = Node(
            node_type="test_node",
            prompt_template="""
            Direct key: {data['name']}
            Variable key: {data[key]}
            """,
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        message = result_state["messages"][-1]["content"]
        assert "Direct key: John" in message
        assert "Variable key: John" in message

    def test_type_conversions(self, sample_state):
        """Test type conversion functions"""
        sample_state["value"] = "42"
        node = Node(
            node_type="test_node",
            prompt_template="""
            Integer: {int(value)}
            Float: {float(value)}
            String: {str(numbers[0])}
            """,
            sink="output",
        )

        result_state = node(sample_state, lambda **k: "response")
        message = result_state["messages"][-1]["content"]
        assert "Integer: 42" in message
        assert "Float: 42.0" in message
        assert "String: 1" in message

    def test_invalid_expressions(self, sample_state):
        """Test error handling for invalid expressions"""
        # Test invalid function
        with pytest.raises(ValueError, match="Function not allowed: print"):
            node = Node(
                node_type="test_node", prompt_template="{print(text)}", sink="output"
            )
            node(sample_state, lambda **k: "response")

        # Test invalid method
        with pytest.raises(ValueError, match="String method not allowed: split"):
            node = Node(
                node_type="test_node", prompt_template="{text.split()}", sink="output"
            )
            node(sample_state, lambda **k: "response")

        # Test invalid index
        with pytest.raises(ValueError):
            node = Node(
                node_type="test_node", prompt_template="{items[10]}", sink="output"
            )
            node(sample_state, lambda **k: "response")

        # Test invalid key
        with pytest.raises(ValueError):
            node = Node(
                node_type="test_node",
                prompt_template="{data['invalid_key']}",
                sink="output",
            )
            node(sample_state, lambda **k: "response")


# Pre/Post Processing Tests
class TestPrePostProcessing:
    @pytest.fixture
    def processed_list(self):
        return []

    def test_pre_post_processing(self, processed_list):
        """Test node with pre and post processing functions"""

        def pre_process(state, client, **kwargs):
            processed_list.append("pre")
            return state

        def post_process(state, client, **kwargs):
            processed_list.append("post")
            return state

        node = Node(
            node_type="test_node",
            prompt_template="Test {input}",
            sink="output",
            pre_process=pre_process,
            post_process=post_process,
        )

        state = State()
        state["input"] = "value"
        state["messages"] = []

        node(state, lambda **k: "response")
        assert processed_list == ["pre", "post"]

    def test_none_pre_post_process(self):
        """Test node behavior when pre/post process returns None"""

        def pre_process(state, client, **kwargs):
            return None

        def post_process(state, client, **kwargs):
            return None

        node = Node(
            node_type="test_node",
            prompt_template="Test {input}",
            sink="output",
            pre_process=pre_process,
            post_process=post_process,
        )

        state = State()
        state["input"] = "value"
        state["messages"] = []

        result_state = node(state, lambda **k: "response")
        assert result_state == state


# Source Mapping Tests
class TestSourceMapping:
    @pytest.fixture
    def state_with_mapping(self):
        state = State()
        state["different_key"] = "mapped value"
        state["input_key"] = "value"
        state["messages"] = []
        return state

    def test_dict_source_mapping(self, state_with_mapping):
        """Test node with source key mapping"""
        node = Node(
            node_type="test_node", prompt_template="Test {value}", sink="output"
        )

        result_state = node(
            state_with_mapping,
            lambda **k: "Test response",
            source={"value": "different_key"},
        )
        assert result_state["output"] == "Test response"

    def test_string_source_mapping(self, state_with_mapping):
        """Test node with string source mapping"""
        node = Node(
            node_type="test_node", prompt_template="Test {source}", sink="output"
        )

        result_state = node(
            state_with_mapping, lambda **k: "response", source="input_key"
        )
        assert result_state["output"] == "response"

    def test_invalid_source_mapping(self):
        """Test handling of invalid source mapping"""
        node = Node(
            node_type="test_node", prompt_template="Test {value}", sink="output"
        )

        state = State()
        state["messages"] = []

        with pytest.raises(ValueError):
            node(state, None, source={"value": "nonexistent_key"})


# VLM Integration Tests
class TestVLMIntegration:
    class MockVLMClient(VLM_Client):
        def __init__(self):
            super().__init__()

        def process_images(self, messages, images, **kwargs):
            # Verify images are valid paths (for testing)
            assert all(isinstance(img, str) for img in images)
            return messages

        def __call__(self, messages, images=None, **kwargs):
            if images is not None:
                messages = self.process_images(messages, images)
            return "Image description response"

    def test_vlm_execution(self):
        """Test node execution with VLM client"""
        node = Node(
            node_type="test_vlm_node",
            prompt_template="Describe this image",
            sink="output",
            image_keys=["image_path"],
        )

        state = State()
        state["messages"] = []

        # Create a temporary test image file
        test_image_path = "test_image.jpg"
        with open(test_image_path, "w") as f:
            f.write("dummy image content")

        try:
            result_state = node(state, self.MockVLMClient(), image_path=test_image_path)
            assert result_state["output"] == "Image description response"
        finally:
            # Clean up the test image
            if os.path.exists(test_image_path):
                os.remove(test_image_path)

    def test_vlm_multiple_images(self):
        """Test VLM node with multiple image inputs"""
        node = Node(
            node_type="test_vlm_node",
            prompt_template="Describe these images",
            sink="output",
            image_keys=["image1", "image2"],
        )

        state = State()
        state["messages"] = []

        # Create temporary test image files
        test_images = ["test1.jpg", "test2.jpg"]
        for img_path in test_images:
            with open(img_path, "w") as f:
                f.write(f"dummy image content for {img_path}")

        try:
            result_state = node(
                state,
                self.MockVLMClient(),
                image1=test_images[0],
                image2=test_images[1],
            )
            assert result_state["output"] == "Image description response"
        finally:
            # Clean up test images
            for img_path in test_images:
                if os.path.exists(img_path):
                    os.remove(img_path)

    def test_vlm_missing_image(self):
        """Test VLM node execution without required image"""
        node = Node(
            node_type="test_vlm_node",
            prompt_template="Describe this image",
            sink="output",
            image_keys=["image_path"],
        )

        state = State()
        state["messages"] = []

        with pytest.raises(ValueError, match="At least one image key must be provided"):
            node(state, self.MockVLMClient())

    def test_vlm_invalid_image_path(self):
        """Test VLM node with invalid image path"""
        node = Node(
            node_type="test_vlm_node",
            prompt_template="Test",
            sink="output",
            image_keys=["image"],
        )

        state = State()
        state["messages"] = []

        with pytest.raises(TypeError, match="should be string"):
            node(state, self.MockVLMClient(), image=None)


# Decorator Tests
class TestDecorators:
    def test_as_node_decorator(self):
        """Test @as_node decorator functionality"""

        @as_node(["output"])
        def test_function(input_value):
            return f"Processed {input_value}"

        state = State()
        state["input_value"] = "test"
        state["messages"] = []

        result_state = test_function(state, None)
        assert result_state["output"] == "Processed test"

    def test_custom_function_defaults(self):
        """Test node with custom function having default parameters"""

        def custom_func(required_param, optional_param="default"):
            return f"{required_param}-{optional_param}"

        node = Node(
            node_type="test_node",
            prompt_template="",
            sink="output",
            custom_function=custom_func,
        )

        state = State()
        state["required_param"] = "value"
        state["messages"] = []

        result_state = node(state, None)
        assert result_state["output"] == "value-default"

    def test_invalid_custom_function_args(self):
        """Test handling of custom function with missing required arguments"""

        def custom_func(required_arg):
            return required_arg

        node = Node(
            node_type="test_node",
            prompt_template="",
            sink="output",
            custom_function=custom_func,
        )

        state = State()
        state["messages"] = []

        with pytest.raises(ValueError, match="Required key 'required_arg' not found"):
            node(state, None)

    def test_as_node_with_multiple_sinks(self):
        """Test @as_node decorator with multiple output sinks"""

        @as_node(["output1", "output2"])
        def multi_output_function(value):
            return [f"First {value}", f"Second {value}"]

        state = State()
        state["value"] = "test"
        state["messages"] = []

        result_state = multi_output_function(state, None)
        assert result_state["output1"] == "First test"
        assert result_state["output2"] == "Second test"

    def test_as_node_with_pre_post_processing(self):
        """Test @as_node decorator with pre and post processing"""
        processed = []

        def pre_process(state, client, **kwargs):
            processed.append("pre")
            return state

        def post_process(state, client, **kwargs):
            processed.append("post")
            return state

        @as_node(["output"], pre_process=pre_process, post_process=post_process)
        def test_function(value):
            processed.append("main")
            return f"Result: {value}"

        state = State()
        state["value"] = "test"
        state["messages"] = []

        result_state = test_function(state, None)
        assert result_state["output"] == "Result: test"
        assert processed == ["pre", "main", "post"]

    def test_as_node_with_multiple_parameters(self):
        """Test @as_node decorator with function having multiple parameters"""

        @as_node(["output"])
        def multi_param_function(param1, param2, param3="default"):
            return f"{param1}-{param2}-{param3}"

        state = State()
        state["param1"] = "value1"
        state["param2"] = "value2"
        state["messages"] = []

        result_state = multi_param_function(state, None)
        assert result_state["output"] == "value1-value2-default"

    def test_as_node_as_function_flag(self):
        """Test @as_node decorator with as_function flag"""

        @as_node(["output"], as_function=True)
        def test_function(value):
            return f"Processed {value}"

        assert callable(test_function)
        assert hasattr(test_function, "node_type")
        assert hasattr(test_function, "sink")
        assert test_function.node_type == "test_function"
        assert test_function.sink == ["output"]

    def test_as_node_error_handling(self):
        """Test @as_node decorator error handling for missing parameters"""

        @as_node(["output"])
        def error_function(required_param):
            return f"Value: {required_param}"

        state = State()
        state["messages"] = []

        with pytest.raises(ValueError, match="Required key 'required_param' not found"):
            error_function(state, None)


# Utility Function Tests
class TestUtilityFunctions:
    def test_remove_markdown_blocks(self):
        """Test markdown block removal"""
        text = "```python\ndef test():\n    pass\n```"
        result = remove_markdown_blocks_formatting(text)
        assert result == "def test():\n    pass"



================================================
FILE: tests/test_state.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import numpy as np
from typing import List, Dict, Union
import pytest
from nodeology.state import (
    process_state_definitions,
    _resolve_state_type,
    _type_from_str,
)


class TestTypeResolution:
    """Tests for basic type resolution functionality"""

    def test_primitive_types(self):
        """Test resolution of primitive types"""
        assert _resolve_state_type("str") == str
        assert _resolve_state_type("int") == int
        assert _resolve_state_type("float") == float
        assert _resolve_state_type("bool") == bool
        assert _resolve_state_type("ndarray") == np.ndarray

    def test_list_types(self):
        """Test resolution of List types"""
        assert _resolve_state_type("List[str]") == List[str]
        assert _resolve_state_type("List[int]") == List[int]
        assert _resolve_state_type("List[bool]") == List[bool]

    def test_dict_types(self):
        """Test resolution of Dict types"""
        assert _resolve_state_type("Dict[str, int]") == Dict[str, int]
        assert _resolve_state_type("Dict[str, List[int]]") == Dict[str, List[int]]
        assert (
            _resolve_state_type("Dict[str, Dict[str, bool]]")
            == Dict[str, Dict[str, bool]]
        )

    def test_nested_types(self):
        """Test resolution of deeply nested types"""
        complex_type = "Dict[str, List[Dict[str, Union[int, str]]]]"
        expected = Dict[str, List[Dict[str, Union[int, str]]]]
        assert _resolve_state_type(complex_type) == expected

    def test_numpy_composite_types(self):
        """Test resolution of composite types involving numpy arrays"""
        assert _resolve_state_type("List[ndarray]") == List[np.ndarray]
        assert _resolve_state_type("Dict[str, ndarray]") == Dict[str, np.ndarray]
        assert (
            _resolve_state_type("Dict[str, List[ndarray]]")
            == Dict[str, List[np.ndarray]]
        )
        assert _resolve_state_type("Union[ndarray, int]") == Union[np.ndarray, int]

    def test_type_conversion_symmetry(self):
        """Test that type conversion is symmetrical"""
        test_cases = [
            str,
            int,
            float,
            bool,
            List[str],
            List[int],
            Dict[str, int],
            Dict[str, List[str]],
            Union[str, int],
            Union[str, List[int]],
            np.ndarray,
            List[np.ndarray],
            Dict[str, np.ndarray],
            Dict[str, List[np.ndarray]],
            Union[np.ndarray, int],
            Union[List[np.ndarray], Dict[str, np.ndarray]],
        ]

        for type_obj in test_cases:
            # Convert type to string
            type_str = _type_from_str(type_obj)
            # Convert string back to type
            resolved_type = _resolve_state_type(type_str)
            # Verify they're equivalent
            assert str(resolved_type) == str(
                type_obj
            ), f"Type conversion failed for {type_obj}"


class TestErrorHandling:
    """Tests for error handling in type resolution"""

    def test_invalid_type_names(self):
        """Test handling of invalid type names"""
        with pytest.raises(ValueError, match="Unknown state type"):
            _resolve_state_type("InvalidType")

        with pytest.raises(ValueError, match="Unknown state type"):
            _resolve_state_type("List[InvalidType]")

    def test_malformed_type_strings(self):
        """Test handling of malformed type strings"""
        invalid_types = [
            "List[str",  # Missing closing bracket
            "Dict[str]",  # Missing value type
            "Dict[str,]",  # Empty value type
            "Union[]",  # Empty union
            "List[]",  # Empty list type
            "[str]",  # Invalid format
        ]

        for invalid_type in invalid_types:
            with pytest.raises(ValueError):
                _resolve_state_type(invalid_type)

    def test_invalid_dict_formats(self):
        """Test handling of invalid dictionary formats"""
        with pytest.raises(ValueError):
            _resolve_state_type("Dict")

        with pytest.raises(ValueError):
            _resolve_state_type("Dict[str, int, bool]")


class TestStateDefinitionProcessing:
    """Tests for state definition processing"""

    def test_dict_state_definition(self):
        """Test processing of dictionary state definitions"""
        state_def = {"name": "test_field", "type": "str"}
        result = process_state_definitions([state_def], {})
        assert result == [("test_field", str)]

    def test_custom_type_processing(self):
        """Test processing with custom types in registry"""

        class CustomType:
            pass

        registry = {"CustomType": CustomType}

        # Test direct custom type reference
        assert process_state_definitions(["CustomType"], registry) == [CustomType]

        # Test custom type in dictionary definition
        state_def = {
            "name": "custom_field",
            "type": "str",
        }  # Can't use CustomType directly in type string
        result = process_state_definitions([state_def], registry)
        assert result == [("custom_field", str)]

    def test_list_state_definition(self):
        """Test processing of list format state definitions"""
        # Single list definition
        state_def = ["test_field", "List[int]"]
        result = process_state_definitions([state_def], {})
        assert result == [("test_field", List[int])]

        # Multiple list definitions
        state_defs = [["field1", "str"], ["field2", "List[int]"]]
        result = process_state_definitions(state_defs, {})
        assert result == [("field1", str), ("field2", List[int])]

    def test_process_mixed_definitions(self):
        """Test processing mixed format state definitions"""
        state_defs = [
            {"name": "field1", "type": "str"},
            ["field2", "List[int]"],
            {"name": "field3", "type": "Dict[str, bool]"},
        ]
        result = process_state_definitions(state_defs, {})
        assert result == [
            ("field1", str),
            ("field2", List[int]),
            ("field3", Dict[str, bool]),
        ]

    def test_numpy_array_state_definition(self):
        """Test processing of numpy array state definitions"""
        # Test direct ndarray type
        state_def = {"name": "array_field", "type": "ndarray"}
        result = process_state_definitions([state_def], {})
        assert result == [("array_field", np.ndarray)]

        # Test in list format
        state_def = ["array_field2", "ndarray"]
        result = process_state_definitions([state_def], {})
        assert result == [("array_field2", np.ndarray)]

        # Test in mixed definitions
        state_defs = [
            {"name": "field1", "type": "str"},
            ["array_field", "ndarray"],
            {"name": "field3", "type": "Dict[str, bool]"},
        ]
        result = process_state_definitions(state_defs, {})
        assert result == [
            ("field1", str),
            ("array_field", np.ndarray),
            ("field3", Dict[str, bool]),
        ]

    def test_numpy_composite_state_definition(self):
        """Test processing of composite state definitions with numpy arrays"""
        state_defs = [
            {"name": "array_list", "type": "List[ndarray]"},
            {"name": "array_dict", "type": "Dict[str, ndarray]"},
            ["nested_arrays", "Dict[str, List[ndarray]]"],
            {"name": "mixed_type", "type": "Union[ndarray, int]"},
        ]

        result = process_state_definitions(state_defs, {})
        assert result == [
            ("array_list", List[np.ndarray]),
            ("array_dict", Dict[str, np.ndarray]),
            ("nested_arrays", Dict[str, List[np.ndarray]]),
            ("mixed_type", Union[np.ndarray, int]),
        ]



================================================
FILE: tests/test_workflow.py
================================================
"""
Copyright (c) 2024, UChicago Argonne, LLC. All rights reserved.

Copyright 2024. UChicago Argonne, LLC. This software was produced
under U.S. Government contract DE-AC02-06CH11357 for Argonne National
Laboratory (ANL), which is operated by UChicago Argonne, LLC for the
U.S. Department of Energy. The U.S. Government has rights to use,
reproduce, and distribute this software.  NEITHER THE GOVERNMENT NOR
UChicago Argonne, LLC MAKES ANY WARRANTY, EXPRESS OR IMPLIED, OR
ASSUMES ANY LIABILITY FOR THE USE OF THIS SOFTWARE.  If software is
modified to produce derivative works, such modified software should
be clearly marked, so as not to confuse it with the version available
from ANL.

Additionally, redistribution and use in source and binary forms, with
or without modification, are permitted provided that the following
conditions are met:

    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.

    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in
      the documentation and/or other materials provided with the
      distribution.

    * Neither the name of UChicago Argonne, LLC, Argonne National
      Laboratory, ANL, the U.S. Government, nor the names of its
      contributors may be used to endorse or promote products derived
      from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY UChicago Argonne, LLC AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL UChicago
Argonne, LLC OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
"""

### Initial Author <2024>: Xiangyu Yin

import os
import tempfile
import shutil
from pathlib import Path
import json
import yaml
import numpy as np

try:
    from typing import get_type_hints
except ImportError:
    from typing_extensions import get_type_hints

import pytest
from unittest.mock import patch

from langgraph.graph import END
from langgraph.graph.state import CompiledStateGraph
from nodeology.workflow import (
    Workflow,
    load_workflow_from_template,
    export_workflow_to_template,
    _validate_template_structure,
    _validate_nodes,
    _validate_condition_expr,
    _validate_state_definitions,
    _validate_node_transitions,
    _validate_prompt_node,
    _eval_condition,
    _interpolate_variables,
)
from nodeology.client import LLM_Client
from nodeology.state import State
from nodeology.node import Node, as_node


class HilpState(State):
    questions: str
    response: str


class TestWorkflowBase:
    """Base test class with common fixtures"""

    @pytest.fixture
    def temp_dir(self):
        """Create a temporary directory for test artifacts"""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def mock_llm_client(self, mocker):
        """Mock LLM client for testing"""
        mock_client = mocker.Mock()
        mock_client.generate.return_value = "mocked response"
        return mock_client

    @pytest.fixture
    def basic_workflow(self):
        """Basic workflow configuration fixture"""

        class TestWorkflowImpl(Workflow):
            def create_workflow(self):
                self.add_node("start")
                self.set_entry("start")
                self.compile()

        return TestWorkflowImpl(
            name="test_workflow",
            llm_name="mock",
            debug_mode=True,
        )

    @pytest.fixture(autouse=True)
    def setup_teardown(self):
        """Setup and teardown for each test"""
        original_env = os.environ.copy()
        yield
        from nodeology.log import cleanup_logging

        cleanup_logging()  # Clean up logging handlers
        os.environ.clear()
        os.environ.update(original_env)


class TestWorkflowInitialization(TestWorkflowBase):
    """Tests for workflow initialization and basic functionality"""

    def test_basic_workflow_creation(self, basic_workflow):
        """Test basic workflow initialization"""
        assert basic_workflow.name == "test_workflow"
        assert isinstance(basic_workflow.graph, CompiledStateGraph)
        assert isinstance(basic_workflow.llm_client, LLM_Client)
        assert basic_workflow.vlm_client is None

    def test_workflow_with_custom_config(self):
        """Test workflow initialization with custom configuration"""

        class CustomState(State):
            field1: str
            field2: int
            field3: np.ndarray

        class CustomWorkflow(Workflow):
            def create_workflow(self):
                self.add_node("start")
                self.set_entry("start")
                self.compile()

        workflow = CustomWorkflow(
            name="custom",
            state_defs=CustomState,
            llm_name="mock",
            vlm_name="mock_vlm",
            exit_commands=["quit"],
            debug_mode=True,
        )

        assert workflow.name == "custom"
        assert workflow.vlm_client is not None
        assert workflow.exit_commands == ["quit"]
        annotations = get_type_hints(workflow.state_schema)
        assert "field1" in annotations
        assert "field2" in annotations

    def test_workflow_name_generation(self):
        """Test automatic workflow name generation"""

        class UnnamedWorkflow(Workflow):
            def create_workflow(self):
                self.add_node("start")
                self.set_entry("start")
                self.compile()

        workflow = UnnamedWorkflow(debug_mode=True, save_artifacts=False)
        assert workflow.name.startswith("UnnamedWorkflow_")

    def test_invalid_workflow_creation(self):
        """Test workflow creation with invalid parameters"""
        with pytest.raises(AssertionError):

            class InvalidWorkflow(Workflow):
                def create_workflow(self):
                    pass  # No graph defined

            InvalidWorkflow(name="test", debug_mode=True)


class TestWorkflowStateManagement(TestWorkflowBase):
    """Tests for state management functionality"""

    def test_state_initialization(self, basic_workflow):
        """Test state initialization with valid values"""
        basic_workflow.initialize({"input": "test", "output": "initial"})
        assert basic_workflow.state_history[0].values["input"] == "test"
        assert basic_workflow.state_history[0].values["output"] == "initial"

    def test_state_updates_and_history(self, basic_workflow):
        """Test state updates and history management"""
        # Initialize state (creates first state)
        basic_workflow.initialize({"input": "initial"})
        basic_workflow.save_state()  # Second state

        # Update state multiple times
        basic_workflow.update_state({"input": "update1"})
        basic_workflow.save_state()  # Third state
        basic_workflow.update_state({"input": "update2"})
        basic_workflow.save_state()  # Fourth state

        # Verify history
        assert len(basic_workflow.state_history) == 4
        assert basic_workflow.state_history[0].values["input"] == "initial"
        assert basic_workflow.state_history[1].values["input"] == "initial"
        assert basic_workflow.state_history[2].values["input"] == "update1"
        assert basic_workflow.state_history[3].values["input"] == "update2"

    def test_state_history_limits(self, basic_workflow):
        """Test state history size limits"""
        basic_workflow.max_history = 2
        basic_workflow.initialize({"input": "initial"})  # First state

        for i in range(5):
            basic_workflow.update_state({"input": f"update{i}"})
            basic_workflow.save_state()  # Explicit save after each update

        assert len(basic_workflow.state_history) == 2  # Only keeps last 2 states
        assert basic_workflow.state_history[-1].values["input"] == "update4"

    def test_state_type_validation(self, basic_workflow):
        """Test state type validation"""
        with pytest.raises(TypeError):
            basic_workflow.initialize({"input": 123})  # Should be string

        with pytest.raises(TypeError):
            basic_workflow.update_state(
                {"output": ["invalid", "type"]}
            )  # Should be string

    def test_state_checkpointing(self, basic_workflow, temp_dir):
        """Test state checkpointing functionality"""
        basic_workflow.log_path = temp_dir
        basic_workflow.save_artifacts = True
        basic_workflow.debug_mode = False

        basic_workflow.initialize({"input": "test_checkpoint"})
        basic_workflow._create_checkpoint()

        checkpoint_file = Path(temp_dir) / "checkpoint.json"
        assert checkpoint_file.exists()

        with open(checkpoint_file) as f:
            checkpoint = json.load(f)
            assert checkpoint["input"] == "test_checkpoint"

    def test_state_restoration(self, basic_workflow):
        """Test state restoration functionality"""
        # Initialize and create history (creates first state)
        basic_workflow.initialize({"input": "initial"})
        basic_workflow.save_state()  # Second state
        basic_workflow.update_state({"input": "modified"})
        basic_workflow.save_state()  # Third state

        # Restore to initial state
        basic_workflow.load_state(0)
        assert basic_workflow.state_history[-1].values["input"] == "initial"

        # Test invalid state index
        with pytest.raises(ValueError, match="State file not found"):
            basic_workflow.load_state(999)


class TestWorkflowExecution(TestWorkflowBase):
    """Tests for workflow execution functionality"""

    @pytest.fixture
    def conditional_workflow(self):
        """Workflow with conditional branching"""

        class ConditionalWorkflow(Workflow):
            def create_workflow(self):
                @as_node(["output"])
                def path_a():
                    return "Path A"

                @as_node(["output"])
                def path_b():
                    return "Path B"

                self.add_node("check")
                self.add_node("path_a", path_a)
                self.add_node("path_b", path_b)

                self.add_conditional_flow(
                    "check",
                    lambda state: state["input"].startswith("a:"),
                    then="path_a",
                    otherwise="path_b",
                )

                self.set_entry("check")
                self.compile()

        return ConditionalWorkflow(name="conditional", debug_mode=True)

    def test_basic_execution(self, basic_workflow):
        """Test basic workflow execution"""
        result = basic_workflow.run({"input": "test"})
        assert "input" in result
        assert result["input"] == "test"
        assert len(basic_workflow.state_history) == 2  # initial state + final state

    def test_simple_execution(self):
        """Test that state updates within a simplenode are applied and recorded correctly."""

        class SimpleWorkflow(Workflow):
            def create_workflow(self):
                @as_node(sink=["output"])
                def track_states(input):
                    return f"Processed {input}"

                self.add_node("track", track_states)
                self.set_entry("track")
                self.compile()

        # Initialize the workflow
        workflow = SimpleWorkflow(
            name="simple_workflow",
            llm_name="mock",
            debug_mode=True,
        )

        # Run the workflow
        result = workflow.run({"input": "test"})

        # Verify the output
        assert result["output"] == "Processed test"
        assert (
            len(workflow.state_history) == 2
        )  # initial + final state after track node

    def test_conditional_execution(self, conditional_workflow):
        """Test conditional workflow execution"""
        # Test path A
        result = conditional_workflow.run({"input": "a: test"})
        assert result["output"] == "Path A"

        # Test path B
        result = conditional_workflow.run({"input": "b: test"})
        assert result["output"] == "Path B"

    def test_human_in_the_loop_workflow(self):
        """Test workflow execution with human-in-the-loop interactions"""

        class HumanInLoopWorkflow(Workflow):
            def create_workflow(self):
                @as_node(["output"])
                def process_input(human_input):
                    return f"Processed input: {human_input}"

                self.add_node("receive_input")
                self.add_node("process_input", process_input)

                self.add_flow("receive_input", "process_input")
                self.add_flow("process_input", END)

                self.set_entry("receive_input")
                self.compile(interrupt_before=["receive_input"], auto_input_nodes=False)

        workflow = HumanInLoopWorkflow(
            name="human_in_loop_workflow",
            llm_name="mock",
            debug_mode=True,
        )

        with patch("builtins.input", return_value="Test input"):
            result = workflow.run({"human_input": "initial input"})

        assert result["output"] == "Processed input: Test input"
        assert result["human_input"] == "Test input"
        assert len(workflow.state_history) == 4

    def test_multiple_human_inputs_workflow(self):
        """Test workflow with multiple human-in-the-loop interactions"""

        class MultiHumanInputWorkflow(Workflow):
            def create_workflow(self):
                @as_node(["output"])
                def node_a(human_input):
                    return f"Node A seen: {human_input}"

                @as_node(["output"])
                def node_b(human_input):
                    return f"Node B seen: {human_input}"

                @as_node(["output"])
                def node_c(human_input):
                    return f"Node C seen: {human_input}"

                self.add_node("input_1")
                self.add_node("node_a", node_a)
                self.add_node("node_b", node_b)
                self.add_node("input_2")
                self.add_node("node_c", node_c)

                self.add_flow("input_1", "node_a")
                self.add_flow("node_a", "node_b")
                self.add_flow("node_b", "input_2")
                self.add_flow("input_2", "node_c")
                self.add_flow("node_c", END)

                self.set_entry("input_1")
                self.compile(
                    interrupt_before=["input_1", "input_2"], auto_input_nodes=False
                )

        workflow = MultiHumanInputWorkflow(
            name="multi_human_input_workflow",
            llm_name="mock",
            debug_mode=True,
        )

        inputs = iter(["First input", "Second input"])
        with patch("builtins.input", lambda _: next(inputs)):
            result = workflow.run({"human_input": "start input"})

        assert result["output"] == "Node C seen: Second input"
        assert len(workflow.state_history) == 8

    def test_workflow_exit(self):
        """Test workflow handles both custom exit condition and built-in exit_commands correctly"""

        class ExitWorkflow(Workflow):
            def create_workflow(self):
                @as_node(["output"])
                def continue_or_end():
                    return "Continue or end"

                @as_node(["output"])
                def process_input(human_input):
                    return f"Processed input: {human_input}"

                self.add_node("recieve_input")
                self.add_node("process_input", process_input)
                self.add_node("continue_or_end", continue_or_end)

                self.add_flow("recieve_input", "continue_or_end")
                self.add_conditional_flow(
                    "continue_or_end",
                    lambda state: not state["human_input"].lower() == "magic_word",
                    then="process_input",
                    otherwise=END,
                )

                self.set_entry("recieve_input")
                self.compile(interrupt_before=["recieve_input"], auto_input_nodes=False)

        # Test Case 1: Exit via magic_word
        workflow1 = ExitWorkflow(
            name="exit_1",
            exit_commands=["stop workflow", "quit now"],  # Custom exit commands
            debug_mode=True,
        )

        # Add more inputs to prevent StopIteration
        with patch("builtins.input", lambda _: "magic_word"):
            result1 = workflow1.run({"human_input": "initial"})

        # Verify magic_word exit
        assert result1["output"] == "Continue or end"
        assert (
            len(workflow1.state_history) == 4
        )  # initial + receive_input + continue_or_end + final

        # Test Case 2: Exit via built-in exit command
        workflow2 = ExitWorkflow(
            name="exit_2",
            exit_commands=["stop workflow", "quit now"],
            debug_mode=True,
        )

        with patch("builtins.input", lambda _: "quit now"):
            result2 = workflow2.run({"human_input": "initial", "output": "initial"})

        # Verify exit_command exit (should exit before processing)
        assert result2["output"] == "initial"
        assert len(workflow2.state_history) == 2  # initial + exit

        # Test Case 3: Exit until end
        workflow3 = ExitWorkflow(
            name="exit_3",
            debug_mode=True,
        )

        with patch("builtins.input", lambda _: "continue"):
            result3 = workflow3.run({"human_input": "initial", "output": "initial"})

        assert result3["output"] == "Processed input: continue"
        assert (
            len(workflow3.state_history) == 5
        )  # initial + receive_input + continue_or_end + process_input + final


class TestWorkflowErrorHandling(TestWorkflowBase):
    """Tests for error handling and recovery"""

    @pytest.fixture
    def error_workflow(self):
        """Workflow that raises errors"""

        class ErrorWorkflow(Workflow):
            def create_workflow(self):
                @as_node(["output"])
                def error_node():
                    raise ValueError("Test error")

                self.add_node("error", error_node)
                self.set_entry("error")
                self.compile()

        return ErrorWorkflow(name="error_test", debug_mode=True)

    def test_error_handling_debug_mode(self, error_workflow):
        """Test error handling in debug mode"""
        with pytest.raises(ValueError, match="Test error"):
            error_workflow.run({"input": "test"})

    def test_error_handling_production_mode(self, error_workflow):
        """Test error handling in production mode"""
        error_workflow.debug_mode = False
        result = error_workflow.run({"input": "test"})
        assert "error" in result
        assert "Test error" in result["error"]

    def test_state_recovery_after_error(self, basic_workflow):
        """Test state recovery after errors"""
        basic_workflow.initialize({"input": "initial"})
        basic_workflow.save_state()

        # Simulate error during state update
        with pytest.raises(ValueError):
            basic_workflow.update_state({"invalid_field": "value"})

        # State should be restored
        assert basic_workflow.state_history[-1].values["input"] == "initial"

    def test_checkpoint_recovery(self, basic_workflow, temp_dir):
        """Test recovery from checkpoint after error"""
        basic_workflow.log_path = temp_dir
        basic_workflow.save_artifacts = True

        # Create initial state and checkpoint
        basic_workflow.initialize({"input": "checkpoint_test"})
        basic_workflow._create_checkpoint()

        # Simulate error and verify recovery
        try:
            basic_workflow.update_state({"invalid": "state"})
        except:
            basic_workflow._restore_last_valid_state()

        assert basic_workflow.state_history[-1].values["input"] == "checkpoint_test"

    def test_error_logging(self, error_workflow, temp_dir):
        """Test error logging functionality"""
        # Set log path and debug mode before any logging setup
        error_workflow.log_path = temp_dir
        error_workflow.debug_mode = False
        error_workflow.save_artifacts = True

        # Re-initialize logging with new settings
        error_workflow._setup_logging(base_dir=temp_dir)

        result = error_workflow.run({"input": "test"})

        # Check log file exists and contains error
        log_files = list(Path(temp_dir).glob("**/*.log"))
        assert len(log_files) > 0
        with open(log_files[0]) as f:
            log_content = f.read()
            assert "Test error" in log_content


class TestTemplateValidation(TestWorkflowBase):
    """Tests for template validation functionality"""

    @pytest.fixture
    def basic_template(self):
        """Basic valid template fixture"""
        return {
            "name": "test_workflow",
            "state_defs": [("input", "str"), ("output", "str")],
            "nodes": {
                "start": {
                    "type": "prompt",
                    "template": "Process: {input}",
                    "next": "end",
                },
                "end": {"type": "prompt", "template": "Final: {output}", "next": "END"},
            },
            "entry_point": "start",
        }

    def test_template_structure_validation(self, basic_template):
        """Test basic template structure validation"""
        # Test valid template
        _validate_template_structure(basic_template)

        # Test missing required fields
        for field in ["name", "state_defs", "nodes", "entry_point"]:
            invalid = basic_template.copy()
            del invalid[field]
            with pytest.raises(ValueError, match=f".*{field}.*"):
                _validate_template_structure(invalid)

    def test_node_validation(self, basic_template):
        """Test node configuration validation"""
        nodes = basic_template["nodes"]
        node_registry = {"prompt": lambda x: x}

        # Test valid nodes
        _validate_nodes(nodes, node_registry)

        # Test missing type field
        invalid_nodes = {"bad_node": {"template": "test", "next": "end"}}
        with pytest.raises(ValueError, match="missing 'type' field"):
            _validate_nodes(invalid_nodes, node_registry)

        # Test unknown node type
        invalid_nodes = {"bad_node": {"type": "unknown_type", "next": "end"}}
        with pytest.raises(ValueError, match="Unknown node type"):
            _validate_nodes(invalid_nodes, node_registry)

    def test_condition_expression_validation(self):
        """Test condition expression validation"""
        # Test valid expressions
        valid_expressions = [
            "status == 'active'",
            "count > 0",
            "len(messages) > 0",
            "all(x > 0 for x in numbers)",
            "'key' in data",
            "status in ['success', 'pending']",
        ]

        for expr in valid_expressions:
            assert _validate_condition_expr(expr) is True

        # Test invalid expressions
        invalid_expressions = [
            "import os",
            "os.system('cmd')",
            "__import__('os')",
            "globals()",
            "eval('1+1')",
            "lambda x: x",
            "def func(): pass",
        ]

        for expr in invalid_expressions:
            with pytest.raises(ValueError):
                _validate_condition_expr(expr)

    def test_prompt_node_validation(self):
        """Test prompt node configuration validation"""
        # Test valid configuration
        valid_config = {
            "type": "prompt",
            "template": "Test prompt",
            "next": "end",
            "image_keys": ["img1", "img2"],
        }
        _validate_prompt_node("test_node", valid_config)

        # Test missing template
        invalid_config = {"type": "prompt", "next": "end"}
        with pytest.raises(ValueError, match="missing 'template' field"):
            _validate_prompt_node("test_node", invalid_config)

    def test_validate_state_definitions(self):
        """Test state definition validation"""

        class DemoState(State):
            demo_field: str

        # Valid state definitions
        valid_states = [
            ["input", "str"],
            ["count", "int"],
            "DemoState",  # Referencing existing state
        ]
        _validate_state_definitions(valid_states, {"DemoState": DemoState})

        # Invalid state definitions
        with pytest.raises(ValueError, match="Invalid state definition format"):
            _validate_state_definitions([123], {})  # Invalid format

        with pytest.raises(ValueError, match="Unknown state type"):
            _validate_state_definitions(["UnknownState"], {})  # Unknown state

        with pytest.raises(ValueError, match="Cannot resolve state type"):
            _validate_state_definitions([["field", "invalid_type"]], {})

    def test_validate_node_transitions(self):
        """Test node transition validation"""
        # Test simple transition
        _validate_node_transitions("test_node", "next_node")

        # Test valid conditional transition
        valid_condition = {
            "condition": "count > 0",
            "then": "path_a",
            "otherwise": "path_b",
        }
        _validate_node_transitions("test_node", valid_condition)

        # Test missing condition
        with pytest.raises(ValueError, match="missing 'condition'"):
            _validate_node_transitions("test_node", {"then": "a", "otherwise": "b"})

        # Test missing paths
        with pytest.raises(ValueError, match="missing then/otherwise paths"):
            _validate_node_transitions("test_node", {"condition": "x > 0"})

        # Test invalid condition
        with pytest.raises(ValueError, match="Invalid condition"):
            _validate_node_transitions(
                "test_node",
                {
                    "condition": "import os",  # Unsafe expression
                    "then": "a",
                    "otherwise": "b",
                },
            )

    def test_eval_condition(self):
        """Test condition evaluation"""
        test_state = {
            "count": 5,
            "status": "active",
            "items": ["a", "b", "c"],
            "data": {"key": "value"},
        }

        # Test basic comparisons
        assert _eval_condition("count > 3", test_state) == True
        assert _eval_condition("count < 3", test_state) == False
        assert _eval_condition("status == 'active'", test_state) == True

        # Test list operations
        assert _eval_condition("len(items) == 3", test_state) == True
        assert _eval_condition("'a' in items", test_state) == True

        # Test dict operations
        assert _eval_condition("'key' in data", test_state) == True
        assert _eval_condition("data['key'] == 'value'", test_state) == True

        # Test complex conditions
        assert _eval_condition("count > 0 and status == 'active'", test_state) == True
        assert _eval_condition("len(items) > 5 or count < 10", test_state) == True

        # Test invalid expressions
        with pytest.raises(ValueError):
            _eval_condition("os.system('ls')", test_state)  # Unsafe operation

        with pytest.raises(ValueError):
            _eval_condition("lambda x: x", test_state)  # Lambda not allowed

    def test_interpolate_variables(self):
        """Test variable interpolation in templates"""
        template = {
            "name": "${workflow_name}",
            "config": {"model": "${model_name}", "temperature": 0.7},
            "nodes": {
                "start": {
                    "template": "Using ${model_name} with ${temperature}",
                    "next": "END",
                }
            },
            "list_values": ["${value1}", "${value2}"],
        }

        variables = {
            "workflow_name": "test_workflow",
            "model_name": "mock",
            "temperature": 0.5,
            "value1": "a",
            "value2": "b",
        }

        result = _interpolate_variables(template, variables)

        assert result["name"] == "test_workflow"
        assert result["config"]["model"] == "mock"
        assert result["config"]["temperature"] == 0.7  # Unchanged
        assert "mock" in result["nodes"]["start"]["template"]
        assert result["list_values"] == ["a", "b"]

        # Test missing variable
        with pytest.raises(ValueError, match="Required variable .* not found"):
            _interpolate_variables({"name": "${missing_var}"}, {})

        # Test nested interpolation
        nested = {"outer": {"inner": "${var}", "list": ["${var}"]}}
        result = _interpolate_variables(nested, {"var": "value"})
        assert result["outer"]["inner"] == "value"
        assert result["outer"]["list"][0] == "value"


class TestTemplateLoading(TestWorkflowBase):
    """Tests for template loading functionality"""

    @pytest.fixture
    def template_files(self, temp_dir):
        """Create test template files in temporary directory"""
        templates = {}

        # Basic template
        basic = {
            "name": "basic_workflow",
            "state_defs": [["input", "str"], ["output", "str"]],
            "nodes": {
                "start": {
                    "type": "prompt",
                    "template": "Input: {input}",
                    "next": "end",
                },
                "end": {
                    "type": "prompt",
                    "template": "Output: {output}",
                    "next": "END",
                },
            },
            "entry_point": "start",
        }
        basic_path = os.path.join(temp_dir, "basic.yaml")
        with open(basic_path, "w") as f:
            yaml.dump(basic, f)
        templates["basic"] = basic_path

        # Template with variables
        variable = {
            "name": "${workflow_name}",
            "llm": "${model_name}",
            "state_defs": [["input", "str"], ["output", "str"]],
            "nodes": {
                "process": {
                    "type": "prompt",
                    "template": "Using ${model_name}",
                    "next": "END",
                }
            },
            "entry_point": "process",
        }
        variable_path = os.path.join(temp_dir, "variable.yaml")
        with open(variable_path, "w") as f:
            yaml.dump(variable, f)
        templates["variable"] = variable_path

        # Complex template with intervention points and conditional branching
        complex_template = {
            "name": "${workflow_name}",
            "state_defs": ["HilpState"],
            "nodes": {
                "start": {
                    "type": "prompt",
                    "template": "Initial prompt: {human_input}",
                    "sink": "output",
                    "next": {
                        "condition": "'path_a' in output",
                        "then": "path_a",
                        "otherwise": "path_b",
                    },
                },
                "path_a": {
                    "type": "prompt",
                    "template": "Processing path A: {human_input}",
                    "sink": "output",
                    "next": "final",
                },
                "path_b": {
                    "type": "prompt",
                    "template": "Processing path B: {human_input}",
                    "sink": "output",
                    "next": "final",
                },
                "final": {
                    "type": "prompt",
                    "template": "Final output: {human_input}",
                    "sink": "output",
                    "next": "END",
                },
            },
            "entry_point": "start",
            "intervene_before": ["start", "final"],
        }
        complex_path = os.path.join(temp_dir, "complex.yaml")
        with open(complex_path, "w") as f:
            yaml.dump(complex_template, f)
        templates["complex"] = complex_path

        # Custom components template
        custom_template = {
            "name": "custom_workflow",
            "state_defs": [["value", "int"], ["status", "str"], ["output", "str"]],
            "nodes": {
                "start": {
                    "type": "prompt",
                    "template": "Test prompt",
                    "sink": "output",
                    "next": "process",
                },
                "process": {"type": "custom_node", "next": "validate"},
                "validate": {
                    "type": "custom_validator",
                    "next": {
                        "condition": "value > 10",
                        "then": "success",
                        "otherwise": "start",
                    },
                },
                "success": {
                    "type": "prompt",
                    "template": "Success! Final value: {value}",
                    "sink": "output",
                    "next": "END",
                },
            },
            "entry_point": "start",
        }
        custom_path = os.path.join(temp_dir, "custom.yaml")
        with open(custom_path, "w") as f:
            yaml.dump(custom_template, f)
        templates["custom"] = custom_path

        # Return both templates dict and temp_dir for cleanup
        yield templates

        # Cleanup is handled by temp_dir fixture

    def test_basic_template_loading(self, template_files, temp_dir):
        """Test loading basic template"""
        workflow = load_workflow_from_template(
            template_files["basic"],
            debug_mode=True,  # Add debug_mode to prevent log file creation
        )

        # Test basic workflow properties
        assert workflow.name == "basic_workflow"
        assert isinstance(workflow.graph, CompiledStateGraph)
        assert hasattr(workflow, "template")

        # Test template structure
        assert "nodes" in workflow.template
        assert "state_defs" in workflow.template
        assert "entry_point" in workflow.template

        # Test nodes configuration
        assert len(workflow.template["nodes"]) == 2
        assert "start" in workflow.template["nodes"]
        assert "end" in workflow.template["nodes"]

        # Test node structure
        start_node = workflow.template["nodes"]["start"]
        assert start_node["type"] == "prompt"
        assert start_node["template"] == "Input: {input}"
        assert start_node["next"] == "end"

        end_node = workflow.template["nodes"]["end"]
        assert end_node["type"] == "prompt"
        assert end_node["template"] == "Output: {output}"
        assert end_node["next"] == END

        # Test state definitions
        assert len(workflow.template["state_defs"]) == 2
        assert ["input", "str"] in workflow.template["state_defs"]
        assert ["output", "str"] in workflow.template["state_defs"]

        # Test entry point
        assert workflow.template["entry_point"] == "start"

        # Test workflow execution
        result = workflow.run({"input": "test input", "output": ""})
        assert "input" in result
        assert "output" in result
        assert result["input"] == "test input"

        # Test state schema
        annotations = get_type_hints(workflow.state_schema)
        assert "input" in annotations
        assert "output" in annotations
        assert annotations["input"] == str
        assert annotations["output"] == str

    def test_interpolate_variables(self, temp_dir):
        """Test variable interpolation when loading workflow templates"""
        # Create template with variables
        template = {
            "name": "${workflow_name}",
            "llm": "${llm}",
            "vlm": "${vlm}",
            "state_defs": [["input", "str"], ["output", "str"]],
            "nodes": {
                "start": {
                    "type": "prompt",
                    "template": "Using ${llm} and ${vlm} to process ${data_type} data",
                    "next": "END",
                }
            },
            "entry_point": "start",
            "exit_commands": ["${exit_cmd1}", "${exit_cmd2}"],
        }

        # Write template to YAML file
        template_path = os.path.join(temp_dir, "variable_template.yaml")
        with open(template_path, "w") as f:
            yaml.dump(template, f)

        # Test successful interpolation
        workflow = load_workflow_from_template(
            template_path,
            workflow_name="test_workflow",
            llm="mock",
            vlm="mock_vlm",
            data_type="video",
            exit_cmd1="stop",
            exit_cmd2="quit",
            intervene_before1="node_a",
            intervene_before2="node_b",
            debug_mode=True,  # Add debug_mode
        )

        # Verify interpolated values in loaded workflow
        assert workflow.name == "test_workflow"
        assert workflow.llm_client.model_name == "mock"
        assert workflow.vlm_client.model_name == "mock_vlm"
        assert (
            workflow.template["nodes"]["start"]["template"]
            == "Using mock and mock_vlm to process video data"
        )
        assert workflow.template["exit_commands"] == ["stop", "quit"]

    def test_complex_template_loading(self, template_files, temp_dir):
        """Test loading complex template with intervention points and conditional branching"""
        workflow = load_workflow_from_template(
            template_files["complex"],
            state_registry={"HilpState": HilpState},
            workflow_name="complex_workflow",
            llm_name="mock",
            debug_mode=True,  # Add debug_mode
        )

        # Test path A execution
        inputs = iter(["path_a:test", "final input"])
        with patch("builtins.input", lambda _: next(inputs)):
            result_a = workflow.run({"human_input": "", "output": ""})

        # Verify path A execution
        assert result_a["current_node_type"] == "final"
        assert result_a["previous_node_type"] == "path_a"
        assert len(result_a["messages"]) == 5
        assert result_a["messages"][0]["role"] == "user"
        assert result_a["messages"][0]["content"] == "path_a:test"
        assert result_a["messages"][1]["content"] == "Initial prompt: path_a:test"
        assert result_a["messages"][2]["content"] == "Processing path A: path_a:test"
        assert result_a["messages"][3]["content"] == "final input"
        assert result_a["messages"][4]["content"] == "Final output: final input"

        # Update workflow for path B
        workflow.update_state({"human_input": "", "output": ""})

        # Test path B execution
        inputs = iter(["path_b:test", "final input"])
        with patch("builtins.input", lambda _: next(inputs)):
            result_b = workflow.run({"human_input": "", "output": ""})

        # Verify path B execution
        assert result_b["current_node_type"] == "final"
        assert result_b["previous_node_type"] == "path_b"
        assert len(result_b["messages"]) == 10
        assert result_b["messages"][5]["role"] == "user"
        assert result_b["messages"][5]["content"] == "path_b:test"
        assert result_b["messages"][6]["content"] == "Initial prompt: path_b:test"
        assert result_b["messages"][7]["content"] == "Processing path B: path_b:test"
        assert result_b["messages"][8]["content"] == "final input"
        assert result_b["messages"][9]["content"] == "Final output: final input"

        # Verify state history length for both paths
        assert (
            len(workflow.state_history) >= 6
        )  # Should have multiple state transitions for both paths

    def test_custom_components_loading(self, template_files, temp_dir):
        """Test loading template with custom nodes and states"""

        @as_node(["value", "status"])
        def custom_node(value):
            """Custom node that doubles the value"""
            return value * 2, "processed"

        @as_node(["value", "status"])
        def custom_validator(value):
            """Custom validator that checks if value > 10"""
            if value > 10:
                return value, "validated"
            else:
                return value, "failed_validation"

        # Create registry with custom nodes
        custom_nodes = {
            "custom_node": custom_node,
            "custom_validator": custom_validator,
        }

        # Load workflow with custom nodes
        workflow = load_workflow_from_template(
            template_files["custom"],
            node_registry={**custom_nodes},
            llm_name="mock",
            debug_mode=True,  # Add debug_mode
            save_artifacts=False,  # Add save_artifacts
            log_path=temp_dir,  # Add log_path to use temp directory
        )

        # Test case 1: Value that needs to loop (5 * 2 * 2 = 20 > 10)
        result1 = workflow.run({"value": 5, "status": "initial", "output": ""})

        # Verify final state for test case 1
        assert result1["value"] == 20
        assert result1["status"] == "validated"
        assert result1["output"] == "user: Success! Final value: 20"

        # Verify state history length for the loop case
        # Initial -> start -> process -> validate -> start -> process -> validate -> success
        assert len(workflow.state_history) >= 8

        # Test case 2: Value that passes validation directly (6 * 2 = 12 > 10)
        result2 = workflow.run({"value": 6, "status": "initial", "output": ""})

        # Verify final state for test case 2
        assert result2["value"] == 12
        assert result2["status"] == "validated"
        assert result2["output"] == "user: Success! Final value: 12"

        # Verify state history for direct pass case
        # Initial -> start -> process -> validate -> success
        assert len(workflow.state_history) >= 5


class TestTemplateExport(TestWorkflowBase):
    """Tests for template export and recovery functionality"""

    @pytest.fixture
    def sample_workflow(self):
        """Create a sample workflow with various node types for testing export"""

        class SampleState(HilpState):
            processed: bool
            image_analysis: str
            image_input: str

        class SampleWorkflow(Workflow):
            def create_workflow(self):
                # Add initial prompt node
                self.add_node(
                    "start",
                    Node(
                        prompt_template="Initial prompt: {human_input}", sink="output"
                    ),
                )

                # Add function-based processing node
                @as_node(sink=["output", "processed"])
                def process_data(output: str) -> tuple[str, bool]:
                    """Process the input data and return result with processing flag"""
                    processed_text = f"Processed: {output.upper()}"
                    return processed_text, True

                self.add_node("process", process_data)

                # Add VLM-based image analysis node
                self.add_node(
                    "analyze_image",
                    Node(
                        prompt_template="Describe this image in detail: {image_input}",
                        sink="image_analysis",
                        image_keys=["image_input"],
                    ),
                    client_type="vlm",  # Specify VLM client
                )

                final_template = """Summarize the results: 
Regular processing: {processed}
Image analysis: {image_analysis}
Final output: {output}"""
                # Add final summary node
                self.add_node(
                    "final",
                    Node(
                        prompt_template=final_template,
                        sink="output",
                    ),
                )

                # Add conditional routing
                self.add_conditional_flow(
                    "start",
                    lambda state: "image" in state["output"],
                    then="analyze_image",
                    otherwise="process",
                )

                self.add_flow("process", "final")
                self.add_flow("analyze_image", "final")
                self.add_flow("final", END)

                # Set entry and compile with intervention points
                self.set_entry("start")
                self.compile(interrupt_before=["start", "final"], auto_input_nodes=True)

        return SampleWorkflow(
            name="test_export",
            llm_name="mock",
            vlm_name="mock_vlm",
            state_defs=SampleState,
            exit_commands=["quit", "exit"],
            debug_mode=True,
        )

    def test_basic_export(self, sample_workflow, temp_dir):
        """Test basic template export functionality"""
        export_path = os.path.join(temp_dir, "exported.yaml")

        # Export template
        template = export_workflow_to_template(sample_workflow, export_path)

        # Verify template structure
        assert template["name"] == "test_export"
        assert template["llm"] == "mock"
        assert template["vlm"] == "mock_vlm"
        assert template["exit_commands"] == ["quit", "exit"]
        assert "state_defs" in template
        assert "nodes" in template
        assert set(template["nodes"].keys()) == {
            "start",
            "process",
            "analyze_image",
            "final",
        }
        assert template["entry_point"] == "start"
        assert set(template["intervene_before"]) == set(["start", "final"])

        # Verify file was created
        assert os.path.exists(export_path)

    def test_yaml_format(self, sample_workflow, temp_dir):
        """Test that YAML is exported in the exact desired format"""
        export_path = os.path.join(temp_dir, "clean_format.yaml")
        export_workflow_to_template(sample_workflow, export_path)

        with open(export_path) as f:
            content = f.read()

        # Verify state_defs format
        assert "- current_node_type: str" in content

        # Verify single-item format
        assert "sink: output" in content  # Not sink: [output]
        assert "image_keys: image_input" in content  # Not image_keys: [image_input]

        # Verify inline lists
        assert (
            "exit_commands: [quit, exit]" in content
            or "exit_commands: [exit, quit]" in content
        )
        assert (
            "intervene_before: [start, final]" in content
            or "intervene_before: [final, start]" in content
        )

        # Verify node field ordering
        node_content = content[content.index("nodes:") :]
        type_index = node_content.index("type: prompt")
        next_index = node_content.index("next:")
        assert type_index < next_index, "type should come before next"

        # Verify the exported YAML is valid and loadable
        yaml_content = yaml.safe_load(content)
        assert yaml_content["name"] == "test_export"
        assert len(yaml_content["state_defs"]) > 0
        assert len(yaml_content["nodes"]) > 0

    def test_round_trip(self, sample_workflow, temp_dir):
        """Test that exporting and re-importing a workflow preserves its structure"""
        # Export the workflow to a template
        export_path = os.path.join(temp_dir, "round_trip.yaml")
        _ = export_workflow_to_template(sample_workflow, export_path)

        # Define the custom nodes that were used in the original workflow
        @as_node(sink=["output", "processed"])
        def process_data(output: str) -> tuple[str, bool]:
            """Process the input data and return result with processing flag"""
            processed_text = f"Processed: {output.upper()}"
            return processed_text, True

        # Load the workflow back from the template
        reloaded_workflow = load_workflow_from_template(
            export_path, node_registry={"process": process_data}
        )

        # Verify basic properties
        assert reloaded_workflow.name == sample_workflow.name
        assert (
            reloaded_workflow.llm_client.model_name
            == sample_workflow.llm_client.model_name
        )
        assert (
            reloaded_workflow.vlm_client.model_name
            == sample_workflow.vlm_client.model_name
        )
        assert reloaded_workflow.exit_commands == sample_workflow.exit_commands

        # Compare interrupt_before nodes without the '_input' suffix
        original_nodes = set(
            node.replace("_input", "") for node in sample_workflow._interrupt_before
        )
        reloaded_nodes = set(
            node.replace("_input", "") for node in reloaded_workflow._interrupt_before
        )
        assert original_nodes == reloaded_nodes

        # Verify nodes configuration
        for node_name in original_nodes:
            orig_config = sample_workflow._node_configs[node_name]
            reload_config = reloaded_workflow._node_configs[node_name]

            # Compare essential node properties
            assert orig_config.get("type") == reload_config.get("type")
            # Normalize and compare templates
            orig_template = orig_config.get("template", "").replace("\n", " ").strip()
            reload_template = (
                reload_config.get("template", "").replace("\n", " ").strip()
            )
            while "  " in orig_template:  # Remove double spaces
                orig_template = orig_template.replace("  ", " ")
            while "  " in reload_template:
                reload_template = reload_template.replace("  ", " ")
            assert orig_template == reload_template
            assert orig_config.get("sink") == reload_config.get("sink")
            assert orig_config.get("image_keys") == reload_config.get("image_keys")

            # Compare next configurations
            orig_next = orig_config.get("next")
            reload_next = reload_config.get("next")
            if isinstance(orig_next, dict):
                assert isinstance(reload_next, dict)
                assert orig_next.get("condition") == reload_next.get("condition")
                assert (orig_next.get("then") == END) == (
                    reload_next.get("then") == END
                )
                assert (orig_next.get("otherwise") == END) == (
                    reload_next.get("otherwise") == END
                )
            else:
                assert (orig_next == END) == (reload_next == END)

        # Verify state schema
        orig_hints = get_type_hints(sample_workflow.state_schema)
        reload_hints = get_type_hints(reloaded_workflow.state_schema)
        assert set(orig_hints.keys()) == set(reload_hints.keys())
        for k in orig_hints:
            assert orig_hints[k] == reload_hints[k]

        # Verify entry point
        assert reloaded_workflow._entry_point == sample_workflow._entry_point

        # Test execution with same input
        test_input = {
            "human_input": "test input",
            "output": "",
            "processed": False,
            "image_analysis": "",
            "image_input": "test_image.jpg",
        }

        with patch(
            "builtins.input",
            side_effect=["continue", "continue", "continue", "continue", "continue"],
        ):
            original_result = sample_workflow.run(test_input.copy())
            reloaded_result = reloaded_workflow.run(test_input.copy())

        # Compare execution results
        assert original_result.keys() == reloaded_result.keys()
        for k in original_result:
            assert original_result[k].__class__ == reloaded_result[k].__class__


