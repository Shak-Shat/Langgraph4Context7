Directory structure:
└── danmas0n-multi-agent-with-mcp/
    ├── README.md
    ├── LICENSE
    ├── .codespellignore
    ├── .env.example
    ├── gateway/
    │   ├── config.json
    │   ├── pyproject.toml
    │   └── src/
    │       └── mcp_gateway/
    │           ├── __init__.py
    │           └── server.py
    ├── src/
    │   └── react_agent/
    │       ├── __init__.py
    │       ├── configuration.py
    │       ├── graph.py
    │       ├── mcp_client.py
    │       ├── prompts.py
    │       ├── state.py
    │       ├── tools.py
    │       ├── utils.py
    │       └── agents/
    │           ├── __init__.py
    │           ├── base_agent.py
    │           ├── coder.py
    │           ├── orchestrator.py
    │           └── planner.py
    ├── static/
    ├── tests/
    │   ├── cassettes/
    │   │   └── 103fe67e-a040-4e4e-aadb-b20a7057f904.yaml
    │   ├── integration_tests/
    │   │   ├── __init__.py
    │   │   └── test_graph.py
    │   └── unit_tests/
    │       ├── __init__.py
    │       └── test_configuration.py
    └── .github/
        └── workflows/
            ├── integration-tests.yml
            └── unit-tests.yml

================================================
FILE: README.md
================================================
# LangGraph Coding Agent Team with MCP

[![Open in - LangGraph Studio](https://img.shields.io/badge/Open_in-LangGraph_Studio-00324d.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4NS4zMzMiIGhlaWdodD0iODUuMzMzIiB2ZXJzaW9uPSIxLjAiIHZpZXdCb3g9IjAgMCA2NCA2NCI+PHBhdGggZD0iTTEzIDcuOGMtNi4zIDMuMS03LjEgNi4zLTYuOCAyNS43LjQgMjQuNi4zIDI0LjUgMjUuOSAyNC41QzU3LjUgNTggNTggNTcuNSA1OCAzMi4zIDU4IDcuMyA1Ni43IDYgMzIgNmMtMTIuOCAwLTE2LjEuMy0xOSAxLjhtMzcuNiAxNi42YzIuOCAyLjggMy40IDQuMiAzLjQgNy42cy0uNiA0LjgtMy40IDcuNkw0Ny4yIDQzSDE2LjhsLTMuNC0zLjRjLTQuOC00LjgtNC44LTEwLjQgMC0xNS4ybDMuNC0zLjRoMzAuNHoiLz48cGF0aCBkPSJNMTguOSAyNS42Yy0xLjEgMS4zLTEgMS43LjQgMi41LjkuNiAxLjcgMS44IDEuNyAyLjcgMCAxIC43IDIuOCAxLjYgNC4xIDEuNCAxLjkgMS40IDIuNS4zIDMuMi0xIC42LS42LjkgMS40LjkgMS41IDAgMi43LS41IDIuNy0xIDAtLjYgMS4xLS44IDIuNi0uNGwyLjYuNy0xLjgtMi45Yy01LjktOS4zLTkuNC0xMi4zLTExLjUtOS44TTM5IDI2YzAgMS4xLS45IDIuNS0yIDMuMi0yLjQgMS41LTIuNiAzLjQtLjUgNC4yLjguMyAyIDEuNyAyLjUgMy4xLjYgMS41IDEuNCAyLjMgMiAyIDEuNS0uOSAxLjItMy41LS40LTMuNS0yLjEgMC0yLjgtMi44LS44LTMuMyAxLjYtLjQgMS42LS41IDAtLjYtMS4xLS4xLTEuNS0uNi0xLjItMS42LjctMS43IDMuMy0yLjEgMy41LS41LjEuNS4yIDEuNi4zIDIuMiAwIC43LjkgMS40IDEuOSAxLjYgMi4xLjQgMi4zLTIuMy4yLTMuMi0uOC0uMy0yLTEuNy0yLjUtMy4xLTEuMS0zLTMtMy4zLTMtLjUiLz48L3N2Zz4=)](https://langgraph-studio.vercel.app/templates/open?githubUrl=https://github.com/danmas0n/multi-agent-with-mcp)

This project implements a small team of coding agents using [LangGraph](https://github.com/langchain-ai/langgraph) and the [Model Context Protocol (MCP)](https://modelcontextprotocol.io). The agents use MCP servers to provide tools and capabilities through a unified gateway.  

The overall objective of this agent team is to take requirements and code context and create multiple implementations of proposed features; human operators can then choose their preferred approach and proceed, discarding the others.  

This project originated from the Anthropic MCP Hackathon in NYC on 12/11/2024 and has since evolved into its own standalone project.

## Architecture

The system consists of three main components:

1. **MCP Gateway Server**: A server that:
   - Manages multiple MCP server processes
   - Provides a unified API for accessing tools
   - Handles communication with MCP servers
   - Exposes tools through a simple HTTP interface

2. **MCP Servers**: Individual servers that provide specific capabilities:
   - GitHub Server: Repo operations (read, write, list, search, create branch, create PR, etc.)
   - Additional servers can be added for more capabilities

3. **Coding Agents**: There are three agents that collaborate to accomplish coding tasks:
   - Orchestrator: Gathers context from human messages and uses MCP servers to access Linear and GitHub.  Delegates to planner and coder as needed.
   - Planner: Takes requirements and code context and creates a plan with multiple implementation suggestions.  Does not use MCP.
   - Coder: Takes code context and proposed implementations and implements all of them on separate GitHub branches.

## Getting Started

### 1. Install Dependencies

```bash
# Install the agent package
pip install -e .

# Install the gateway package
cd gateway
pip install -e .
cd ..
```

### 2. Configure Environment Variables

The agent supports multiple LLM providers through environment variables:

```bash
# LLM Configuration - supports multiple providers:
LLM_MODEL=provider/model-name

# Supported providers and example models:
# - Anthropic: anthropic/claude-3-5-sonnet-20240620
# - OpenAI: openai/gpt-4
# - OpenRouter: openrouter/openai/gpt-4o-mini
# - Google: google/gemini-1.5-pro

# API Keys for different providers
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENROUTER_API_KEY=your_openrouter_api_key
GOOGLE_API_KEY=your_google_api_key

# OpenRouter Configuration (if using OpenRouter)
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

### 3. Configure MCP Servers

The gateway server is configured through `gateway/config.json`. By default, it starts two MCP servers:

```json
{
  "mcp": {
    "servers": {
      "filesystem": {
        "command": "npx",
        "args": [
          "-y",
          "@modelcontextprotocol/server-filesystem",
          "/path/to/directory"
        ]
      },
      "memory": {
        "command": "npx",
        "args": [
          "-y",
          "@modelcontextprotocol/server-memory"
        ]
      }
    }
  }
}
```

You can add more servers from the [official MCP servers repository](https://github.com/modelcontextprotocol/servers).

### 4. Start the Gateway Server

```bash
cd gateway
python -m mcp_gateway.server
```

The server will start on port 8808 by default.

### 5. Configure the Agent

The agent's connection to the gateway is configured in `langgraph.json`:

```json
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/react_agent/graph.py:graph"
  },
  "env": ".env",
  "mcp": {
    "gateway_url": "http://localhost:8808"
  }
}
```

### 6. Use the Agent

Open the folder in LangGraph Studio! The agent will automatically:
1. Connect to the gateway server
2. Discover available tools
3. Make tools available for use in conversations

## Available Tools

The agent has access to tools from both MCP servers:

### Filesystem Tools
- `read_file`: Read file contents
- `write_file`: Create or update files
- `list_directory`: List directory contents
- `search_files`: Find files matching patterns
- And more...

### Memory Tools
- `create_entities`: Add entities to knowledge graph
- `create_relations`: Link entities together
- `search_nodes`: Query the knowledge graph
- And more...

## Development

### Adding New MCP Servers

1. Find a server in the [MCP servers repository](https://github.com/modelcontextprotocol/servers)
2. Add its configuration to `gateway/config.json`
3. The agent will automatically discover its tools

### Customizing the Agent

- Modify the system prompt in `src/react_agent/prompts.py`
- Update the agent's reasoning in `src/react_agent/graph.py`
- Add new capabilities by including more MCP servers

## Documentation

- [LangGraph Documentation](https://github.com/langchain-ai/langgraph)
- [Model Context Protocol](https://modelcontextprotocol.io)
- [MCP Servers Repository](https://github.com/modelcontextprotocol/servers)

## License

This project is licensed under the MIT License - see the LICENSE file for details.



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 LangChain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: .codespellignore
================================================



================================================
FILE: .env.example
================================================
# LLM Configuration - supports openai, anthropic, openrouter, google
LLM_MODEL=openrouter/openai/gpt-4o-mini
# Example Gemini model: google/gemini-1.5-pro

# API Keys for different providers
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENROUTER_API_KEY=your_openrouter_api_key
GOOGLE_API_KEY=your_google_api_key

# OpenRouter Configuration
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1



================================================
FILE: gateway/config.json
================================================
{
  "mcp": {
    "servers": {
      "mcp-server-commands": {
        "command": "npx",
        "args": ["mcp-server-commands"]
      },
      "filesystem": {
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/files"]
      },
      "git": {
        "command": "uvx",
        "args": ["mcp-server-git", "--repository", "path/to/git/repo"]
      },
      "github": {
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-github"],
        "env": {
          "GITHUB_PERSONAL_ACCESS_TOKEN": "<YOUR_TOKEN>"
        }
      }
    }
  }
}



================================================
FILE: gateway/pyproject.toml
================================================
[project]
name = "mcp-gateway"
version = "0.1.0"
description = "Gateway server for MCP servers"
requires-python = ">=3.11"
dependencies = [
    "mcp>=0.1.0",
    "fastapi>=0.109.0",
    "uvicorn>=0.27.0",
    "python-dotenv>=1.0.0",
    "sse-starlette>=1.8.2"
]

[build-system]
requires = ["setuptools>=73.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["mcp_gateway"]
package-dir = {"" = "src"}



================================================
FILE: gateway/src/mcp_gateway/__init__.py
================================================
"""MCP Gateway Server package."""

from mcp_gateway.server import app, Gateway

__all__ = ["app", "Gateway"]



================================================
FILE: gateway/src/mcp_gateway/server.py
================================================
"""MCP Gateway Server.

This module implements a gateway server that:
1. Exposes an SSE endpoint for clients to connect
2. Reads MCP server configurations
3. Forwards requests to appropriate MCP servers
4. Aggregates responses back to clients
"""

import asyncio
import json
import os
import logging
import signal
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from mcp.types import Tool

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

@dataclass
class MCPServerConfig:
    """Configuration for an MCP server."""
    command: str
    args: List[str]
    env: Dict[str, str] = field(default_factory=dict)


@dataclass
class MCPServer:
    """Represents a running MCP server."""
    name: str
    config: MCPServerConfig
    process: asyncio.subprocess.Process
    tools: List[Dict] = field(default_factory=list)


def get_schema(tool: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Get the input schema from a tool definition, handling both naming conventions."""
    # Try both input_schema and inputSchema
    schema = tool.get("input_schema") or tool.get("inputSchema")
    if schema:
        logger.info(f"Found schema for tool {tool['name']}: {json.dumps(schema, indent=2)}")
    else:
        logger.info(f"No schema found for tool {tool['name']}")
    return schema


class Gateway:
    """MCP Gateway that manages server connections and forwards requests."""
    
    def __init__(self):
        self.servers: Dict[str, MCPServer] = {}
        
    async def _communicate_with_server(self, server: MCPServer, method: str, params: dict = None) -> Any:
        """Send a request to a server and get the response."""
        if not server.process.stdin or not server.process.stdout:
            raise Exception("Server process pipes not available")
            
        try:
            # Prepare request
            request = {
                "jsonrpc": "2.0",
                "method": method,
                "params": params or {},
                "id": 1
            }
            request_str = json.dumps(request) + "\n"
            logger.info(f"Sending request to {server.name}: {request_str.strip()}")
            
            # Send request
            server.process.stdin.write(request_str.encode())
            await server.process.stdin.drain()
            
            # Read response
            response_line = await server.process.stdout.readline()
            if not response_line:
                raise Exception("Empty response")
                
            response_str = response_line.decode().strip()
            logger.info(f"Received response from {server.name}: {response_str}")
            
            response = json.loads(response_str)
            if "error" in response:
                raise Exception(response["error"])
                
            return response.get("result")
            
        except Exception as e:
            logger.error(f"Error communicating with {server.name}: {str(e)}")
            raise
        
    async def start_server(self, name: str, config: MCPServerConfig) -> MCPServer:
        """Start an MCP server and initialize its client session."""
        try:
            logger.info(f"Starting MCP server: {name}")
            logger.info(f"Server config: command={config.command}, args={config.args}, env={config.env}")
            
            # Construct command
            cmd = f"{config.command} {' '.join(config.args)}"
            logger.info(f"Running command: {cmd}")
            
            # Get current environment and update with server-specific env vars
            env = os.environ.copy()
            env.update(config.env)
            
            # Start the server process in the background
            process = await asyncio.create_subprocess_shell(
                cmd,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env,
                preexec_fn=os.setsid  # Create new process group
            )
            
            # Create server object
            server = MCPServer(
                name=name,
                config=config,
                process=process
            )
            
            # Wait a bit for server to initialize
            await asyncio.sleep(2)
            
            # Query available tools
            try:
                logger.info(f"Querying tools from {name}")
                result = await self._communicate_with_server(server, "tools/list")
                server.tools = result.get("tools", [])
                logger.info(f"Server {name} tools response: {json.dumps(result, indent=2)}")
                logger.info(f"Server {name} provides tools: {[t['name'] for t in server.tools]}")
                for tool in server.tools:
                    logger.info(f"Tool details for {tool['name']}:")
                    logger.info(f"  Description: {tool.get('description', 'No description')}")
                    schema = get_schema(tool)
                    if schema:
                        logger.info(f"  Schema: {json.dumps(schema, indent=2)}")
            except Exception as e:
                logger.error(f"Error querying tools from {name}: {str(e)}")
                server.tools = []
            
            self.servers[name] = server
            
            # Start monitoring stderr in background
            asyncio.create_task(self._monitor_stderr(server))
            
            return server
            
        except Exception as e:
            logger.error(f"Error starting server {name}: {str(e)}")
            raise
    
    async def _monitor_stderr(self, server: MCPServer):
        """Monitor server's stderr output."""
        while True:
            if server.process.stderr:
                try:
                    line = await server.process.stderr.readline()
                    if line:
                        logger.info(f"[{server.name}] {line.decode().strip()}")
                    else:
                        break
                except Exception as e:
                    logger.error(f"Error reading stderr from {server.name}: {str(e)}")
                    break
    
    async def start_all_servers(self, config_path: str) -> None:
        """Start all configured MCP servers."""
        try:
            logger.info(f"Loading config from: {config_path}")
            # Read config file
            with open(config_path) as f:
                config = json.load(f)
            logger.info(f"Loaded config: {json.dumps(config, indent=2)}")
            
            if not config.get('mcp', {}).get('servers'):
                raise ValueError("No MCP servers configured in config file")
                
            # Start each configured server in parallel
            tasks = []
            for name, server_config in config['mcp']['servers'].items():
                logger.info(f"Creating start task for server: {name}")
                task = asyncio.create_task(
                    self.start_server(
                        name,
                        MCPServerConfig(**server_config)
                    )
                )
                tasks.append(task)
            
            # Wait for all servers to start
            logger.info("Waiting for all servers to start")
            await asyncio.gather(*tasks, return_exceptions=True)
            logger.info("All servers started")
            
        except Exception as e:
            logger.error(f"Error starting servers: {str(e)}")
            raise
    
    async def list_all_tools(self) -> List[Dict[str, Any]]:
        """Get all available tools from all servers."""
        tools = []
        for server in self.servers.values():
            for tool in server.tools:
                tool_dict = {
                    "name": tool["name"],
                    "description": tool.get("description", ""),
                    "server": server.name
                }
                schema = get_schema(tool)
                if schema:
                    tool_dict["input_schema"] = schema
                tools.append(tool_dict)
        logger.info(f"All available tools: {json.dumps(tools, indent=2)}")
        return tools
    
    async def call_tool(self, tool_name: str, arguments: dict) -> Any:
        """Call a tool on the appropriate server."""
        # Find server that has this tool
        for server in self.servers.values():
            if any(t["name"] == tool_name for t in server.tools):
                try:
                    logger.info(f"Calling tool {tool_name} on server {server.name}")
                    logger.info(f"Tool arguments: {json.dumps(arguments, indent=2)}")
                    
                    result = await self._communicate_with_server(
                        server,
                        "tools/call",
                        {
                            "name": tool_name,
                            "arguments": arguments
                        }
                    )
                    
                    logger.info(f"Tool call result: {json.dumps(result, indent=2)}")
                    return result
                except Exception as e:
                    logger.error(f"Error calling tool {tool_name}: {str(e)}")
                    raise
        raise ValueError(f"Tool {tool_name} not found")
    
    async def shutdown(self) -> None:
        """Shutdown all MCP servers."""
        for server in self.servers.values():
            if server.process:
                try:
                    # Kill entire process group
                    os.killpg(os.getpgid(server.process.pid), signal.SIGTERM)
                    await server.process.wait()
                except Exception as e:
                    logger.error(f"Error shutting down server {server.name}: {str(e)}")
                    try:
                        os.killpg(os.getpgid(server.process.pid), signal.SIGKILL)
                    except:
                        pass
        self.servers.clear()


# Global gateway instance
gateway = Gateway()


@app.on_event("startup")
async def startup():
    """Initialize the gateway on startup."""
    config_path = os.environ.get("MCP_CONFIG", "config.json")
    logger.info("Starting MCP Gateway Server")
    await gateway.start_all_servers(config_path)


@app.on_event("shutdown")
async def shutdown():
    """Cleanup on shutdown."""
    logger.info("Shutting down MCP Gateway Server")
    await gateway.shutdown()


@app.post("/message")
async def message_endpoint(request: Request):
    """Handle incoming messages from clients."""
    try:
        msg = await request.json()
        logger.info(f"Received message: {json.dumps(msg, indent=2)}")
        
        if msg.get("method") == "tools/list":
            tools = await gateway.list_all_tools()
            response = {"tools": tools}
            logger.info(f"Returning tools: {json.dumps(response, indent=2)}")
            return JSONResponse(response)
        
        elif msg.get("method") == "tools/call":
            params = msg.get("params", {})
            logger.info(f"Tool call parameters: {json.dumps(params, indent=2)}")
            
            result = await gateway.call_tool(
                params.get("name"),
                params.get("arguments", {})
            )
            logger.info(f"Tool call result: {json.dumps(result, indent=2)}")
            return JSONResponse(result)
        
        return JSONResponse({"error": "Unknown method"}, status_code=400)
    except Exception as e:
        logger.error(f"Error handling message: {str(e)}")
        return JSONResponse({"error": str(e)}, status_code=500)


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("MCP_PORT", "8808"))
    uvicorn.run(app, host="0.0.0.0", port=port)



================================================
FILE: src/react_agent/__init__.py
================================================
"""React Agent.

This module defines a custom reasoning and action agent graph.
It invokes tools in a simple loop.
"""

from react_agent.graph import graph

__all__ = ["graph"]



================================================
FILE: src/react_agent/configuration.py
================================================
"""Define the configurable parameters for the agent."""

from __future__ import annotations

import json
import os
from dataclasses import dataclass, field, fields
from typing import Annotated, Optional

from langchain_core.runnables import RunnableConfig, ensure_config

from react_agent import prompts


@dataclass(kw_only=True)
class Configuration:
    """The configuration for the agent."""

    system_prompt: str = field(
        default=prompts.SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt to use for the agent's interactions. "
            "This prompt sets the context and behavior for the agent."
        },
    )

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default=os.getenv("LLM_MODEL", "anthropic/claude-3-5-sonnet-20240620"),
        metadata={
            "description": "The name of the language model to use for the agent's main interactions. "
            "Should be in the form: provider/model-name."
        },
    )

    openrouter_base_url: str = field(
        default=os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"),
        metadata={
            "description": "Base URL for OpenRouter API when using OpenRouter models."
        },
    )

    mcp_gateway_url: str = field(
        default="http://localhost:8808",
        metadata={
            "description": "URL of the MCP gateway server that provides tools."
        },
    )

    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig object."""
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in configurable.items() if k in _fields})

    @classmethod
    def load_from_langgraph_json(cls) -> Configuration:
        """Load configuration from langgraph.json file."""
        # Find langgraph.json in current directory or parent directories
        current_dir = os.path.dirname(os.path.abspath(__file__))
        while current_dir != '/':
            config_path = os.path.join(current_dir, 'langgraph.json')
            if os.path.exists(config_path):
                break
            current_dir = os.path.dirname(current_dir)
        else:
            raise FileNotFoundError("Could not find langgraph.json")

        # Load and parse the config file
        with open(config_path, 'r') as f:
            config_data = json.load(f)

        # Create configuration instance
        config = cls()

        # Load MCP gateway URL if present
        if 'mcp' in config_data:
            config.mcp_gateway_url = config_data['mcp'].get('gateway_url', config.mcp_gateway_url)

        return config



================================================
FILE: src/react_agent/graph.py
================================================
"""Define a custom multi-agent workflow for implementing coding solutions."""
from datetime import datetime, timezone
from typing import Dict, List, Literal, cast, Union, Any
from langgraph.types import Command
import asyncio
import json
import logging
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

from react_agent.configuration import Configuration
from react_agent.state import State
from react_agent.tools import TOOLS, initialize_tools
from react_agent.utils import load_chat_model
from react_agent.agents.orchestrator import get_orchestrator
from react_agent.agents.planner import get_planner
from react_agent.agents.coder import get_coder

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CoderWorkflow:
    def __init__(self):
        # Initialize MCP tools and shared model
        self.config = Configuration.load_from_langgraph_json()
        asyncio.run(initialize_tools(self.config))

        # Create shared model instance
        self.llm = load_chat_model(
            self.config.model,
            self.config.openrouter_base_url
        )

        # Initialize agents
        self.orchestrator = get_orchestrator(self.llm, TOOLS)
        self.planner = get_planner(self.llm, TOOLS)
        self.coder = get_coder(self.llm, TOOLS)

    def has_tool_calls(self, message: AIMessage, state: State) -> bool:
        """Check if a message has any tool calls."""
        logger.info(f"Checking for tool calls in message: {message}")
        logger.info(f"Message content type: {type(message.content)}")
        
        # Check tool_calls in additional_kwargs
        if hasattr(message, 'additional_kwargs'):
            tool_calls = message.additional_kwargs.get('tool_calls', [])
            logger.info(f"Found tool_calls in additional_kwargs: {tool_calls}")
            if tool_calls:
                return True

        # Check traditional tool_calls attribute
        if hasattr(message, 'tool_calls'):
            logger.info(f"Found tool_calls attribute: {message.tool_calls}")
            if message.tool_calls:
                return True
            
        # Check content list for tool_use items
        if isinstance(message.content, list):
            logger.info("Message content is a list, checking items...")
            for item in message.content:
                logger.info(f"Checking item type: {type(item)}")
                if isinstance(item, dict):
                    logger.info(f"Dict item: {item}")
                    if item.get('type') == 'tool_use':
                        logger.info(f"Found tool_use in content: {item}")
                        return True
        else:
            logger.info(f"Message content is not a list: {message.content}")
        
        return False

    def extract_content(self, message: AIMessage) -> str:
        """Extract text content from a message."""
        if isinstance(message.content, list):
            content = ""
            for item in message.content:
                if isinstance(item, dict) and item.get('type') == 'text':
                    content += item.get('text', '')
            return content
        return message.content

    def parse_tool_input(self, tool_use: Dict[str, Any]) -> Dict[str, Any]:
        """Parse tool input from tool_use block."""
        # Get the raw input
        input_data = tool_use.get('input', {})
        
        # If input is a string, try to parse it as JSON
        if isinstance(input_data, str):
            try:
                input_data = json.loads(input_data)
            except json.JSONDecodeError:
                input_data = {"query": input_data}  # Fallback for string inputs
        
        # If we have partial_json, parse and merge it
        partial_json = tool_use.get('partial_json')
        if partial_json:
            if isinstance(partial_json, str):
                try:
                    partial_json = json.loads(partial_json)
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse partial_json: {partial_json}")
                else:
                    input_data.update(partial_json)
            elif isinstance(partial_json, dict):
                input_data.update(partial_json)
        
        # Add tool_call_id from the tool use block
        input_data['tool_call_id'] = tool_use.get('id')
        
        # Remove __arg1 if present
        if '__arg1' in input_data:
            del input_data['__arg1']
        
        logger.info(f"Parsed tool input: {input_data}")
        return input_data

    async def execute_tool(self, state: State) -> Command:
        """Execute tool with logging."""
        last_message = state.messages[-1]
        logger.info("Executing tool...")
        logger.info(f"Message in execute_tool: {last_message}")
        logger.info(f"Message type: {type(last_message)}")
        logger.info(f"Current agent: {state.current_agent}")
        if hasattr(last_message, 'additional_kwargs'):
            logger.info(f"Additional kwargs in execute_tool: {last_message.additional_kwargs}")
        
        # First check direct tool_calls attribute
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            tool_call = last_message.tool_calls[0]  # Take the first tool call
            logger.info(f"Processing tool call from tool_calls attribute: {tool_call}")
            
            # Handle both dict and object formats
            if isinstance(tool_call, dict):
                tool_name = tool_call['name']
                tool_args = tool_call['args']
            else:
                # Assume it's an object with attributes
                tool_name = tool_call.name if hasattr(tool_call, 'name') else tool_call.function.name
                if hasattr(tool_call, 'args'):
                    tool_args = tool_call.args
                else:
                    # Parse arguments from function.arguments if needed
                    try:
                        tool_args = json.loads(tool_call.function.arguments)
                    except (AttributeError, json.JSONDecodeError):
                        logger.error(f"Failed to parse tool arguments from: {tool_call}")
                        tool_args = {}

            logger.info(f"Tool name: {tool_name}, arguments: {tool_args}")
            
            # Find and execute the tool
            for tool in TOOLS:
                if tool.name == tool_name:
                    try:
                        result = await tool.ainvoke(tool_args)
                        logger.info(f"Tool result: {result}")
                        
                        # Check if result has Command attributes
                        if hasattr(result, 'goto') and hasattr(result, 'update'):
                            logger.info(f"Command-like result detected: {result}")
                            # Add current_agent to the update if not present
                            if 'current_agent' not in result.update:
                                result.update['current_agent'] = result.goto
                            return result
                        
                        calling_agent = state.current_agent or 'orchestrator'
                        return Command(
                            goto=calling_agent,
                            update={
                                "messages": [HumanMessage(content=str(result))],
                                "current_agent": calling_agent
                            }
                        )
                    except Exception as e:
                        logger.error(f"Tool execution failed: {str(e)}", exc_info=True)
                        calling_agent = state.current_agent or 'orchestrator'
                        return Command(
                            goto=calling_agent,
                            update={
                                "messages": [HumanMessage(content=f"Error: {str(e)}")],
                                "current_agent": calling_agent
                            }
                        )
        
        # Then check for tool_calls in additional_kwargs
        if hasattr(last_message, 'additional_kwargs'):
            tool_calls = last_message.additional_kwargs.get('tool_calls', [])
            if tool_calls:
                tool_call = tool_calls[0]  # Take the first tool call
                logger.info(f"Processing tool call from additional_kwargs: {tool_call}")
                
                tool_name = tool_call['function']['name']
                try:
                    tool_args = json.loads(tool_call['function']['arguments'])
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse tool arguments: {tool_call['function']['arguments']}")
                    tool_args = {}
                
                logger.info(f"Tool name: {tool_name}, arguments: {tool_args}")
                
                # Find and execute the tool
                for tool in TOOLS:
                    if tool.name == tool_name:
                        try:
                            result = await tool.ainvoke(tool_args)
                            logger.info(f"Tool result: {result}")
                            
                            # Check if result has Command attributes
                            if hasattr(result, 'goto') and hasattr(result, 'update'):
                                logger.info(f"Command-like result detected: {result}")
                                # Add current_agent to the update if not present
                                if 'current_agent' not in result.update:
                                    result.update['current_agent'] = result.goto
                                return result
                            
                            calling_agent = state.current_agent or 'orchestrator'
                            return Command(
                                goto=calling_agent,
                                update={
                                    "messages": [HumanMessage(content=str(result))],
                                    "current_agent": calling_agent
                                }
                            )
                        except Exception as e:
                            logger.error(f"Tool execution failed: {str(e)}", exc_info=True)
                            calling_agent = state.current_agent or 'orchestrator'
                            return Command(
                                goto=calling_agent,
                                update={
                                    "messages": [HumanMessage(content=f"Error: {str(e)}")],
                                    "current_agent": calling_agent
                                }
                            )
        
        # Then check for tool_use in content list
        if isinstance(last_message.content, list):
            for item in last_message.content:
                if isinstance(item, dict) and item.get('type') == 'tool_use':
                    tool_name = item.get('name')
                    logger.info(f"Processing tool call from content list: {item}")
                    
                    tool_input = self.parse_tool_input(item)
                    logger.info(f"Parsed tool input: {tool_input}")
                    
                    # Find and execute the tool
                    for tool in TOOLS:
                        if tool.name == tool_name:
                            try:
                                result = await tool.ainvoke(tool_input)
                                logger.info(f"Tool result: {result}")
                                
                                # Check if result has Command attributes
                                if hasattr(result, 'goto') and hasattr(result, 'update'):
                                    logger.info(f"Command-like result detected: {result}")
                                    # Add current_agent to the update if not present
                                    if 'current_agent' not in result.update:
                                        result.update['current_agent'] = result.goto
                                    return result
                                
                                # For MCP tools, create a Command to return to the calling agent
                                calling_agent = state.current_agent or 'orchestrator'
                                return Command(
                                    goto=calling_agent,
                                    update={
                                        "messages": [HumanMessage(content=str(result))],
                                        "current_agent": calling_agent
                                    }
                                )
                            except Exception as e:
                                logger.error(f"Tool execution failed: {str(e)}", exc_info=True)
                                calling_agent = state.current_agent or 'orchestrator'
                                return Command(
                                    goto=calling_agent,
                                    update={
                                        "messages": [HumanMessage(content=f"Error: {str(e)}")],
                                        "current_agent": calling_agent
                                    }
                                )
        
        logger.warning("No tool call found in message")
        return Command(
            goto='orchestrator',
            update={
                "messages": [],
                "current_agent": 'orchestrator'
            }
        )

    def route_orchestrator(self, state: State) -> Literal["MCP", "__end__"]:
        """Route next steps for orchestrator agent."""
        last_message = state.messages[-1]
        logger.info(f"Orchestrator routing - Message type: {type(last_message)}")
        logger.info(f"Full message content: {last_message.content}")
        if hasattr(last_message, 'additional_kwargs'):
            logger.info(f"Additional kwargs: {last_message.additional_kwargs}")
        
        # Check for tool calls
        has_tools = self.has_tool_calls(last_message, state)
        logger.info(f"Message has tool calls: {has_tools}")
        if has_tools:
            logger.info("Found tool calls - Routing to MCP")
            return "MCP"
        
        # If no tool calls, end the workflow
        logger.info("No tool calls - Ending workflow")
        return "__end__"

    def setup_workflow(self):
        """Set up the workflow graph."""
        workflow = StateGraph(State)

        # Add nodes for each agent
        workflow.add_node("orchestrator", self.orchestrator.run)
        workflow.add_node("planner", self.planner.run)
        workflow.add_node("coder", self.coder.run)
        workflow.add_node("MCP", self.execute_tool)

        # Set orchestrator as the entrypoint
        workflow.add_edge("__start__", "orchestrator")

        # Add conditional edges for orchestrator routing
        workflow.add_conditional_edges(
            "orchestrator",
            self.route_orchestrator,
            {
                "MCP": "MCP",
                "__end__": END
            }
        )

        # Add edges from other agents to MCP
        workflow.add_edge("planner", "MCP")
        workflow.add_edge("coder", "MCP")

        # Add conditional edges from MCP to all agents
        def route_mcp(state: State) -> Literal["orchestrator", "planner", "coder"]:
            """Route based on the Command's goto value."""
            # Get the last message which should contain our Command result
            last_message = state.messages[-1]
            logger.info(f"MCP routing - Message: {last_message}")
            logger.info(f"MCP routing - Current agent: {state.current_agent}")
            
            # For routing tools, use their goto value
            if isinstance(last_message, ToolMessage):
                if "Routing to planner" in last_message.content:
                    return "planner"
                elif "Routing to coder" in last_message.content:
                    return "coder"
            
            # For non-routing tools, return to the calling agent
            return state.current_agent or 'orchestrator'

        workflow.add_conditional_edges(
            "MCP",
            route_mcp,
            {
                "orchestrator": "orchestrator",
                "planner": "planner",
                "coder": "coder"
            }
        )

        return workflow.compile()

    async def execute(self, task: str):
        """Execute the workflow."""
        logger.info("Initiating workflow...")
        workflow = self.setup_workflow()

        logger.info(f"Initial task: {task}")

        # Create proper initial state with HumanMessage
        initial_state = State(
            messages=[HumanMessage(content=task)],
            current_agent="orchestrator"
        )

        config = {"recursion_limit": 50}
        async for output in workflow.astream(initial_state, stream_mode="updates", config=config):
            logger.info(f"Agent message: {str(output)}")

# For LangGraph Studio support
coder_workflow = CoderWorkflow()
graph = coder_workflow.setup_workflow()



================================================
FILE: src/react_agent/mcp_client.py
================================================
"""MCP client for ReAct Agent.

This module handles communication with the MCP gateway server.
"""

import json
import logging
from typing import Any, Dict, List, Optional

import httpx

logger = logging.getLogger(__name__)


class MCPGatewayClient:
    """Client for communicating with the MCP gateway server."""
    
    def __init__(self, gateway_url: str = "http://localhost:8808"):
        """Initialize the client.
        
        Args:
            gateway_url: URL of the MCP gateway server
        """
        self.gateway_url = gateway_url
        self.client = httpx.Client()
        self._tools: Optional[List[Dict[str, Any]]] = None
    
    def _send_request(self, method: str, params: Optional[Dict[str, Any]] = None) -> Any:
        """Send a request to the gateway server.
        
        Args:
            method: The method to call (e.g., "tools/list", "tools/call")
            params: Optional parameters for the method
            
        Returns:
            The response from the server
            
        Raises:
            Exception: If the request fails
        """
        request = {
            "method": method,
            "params": params or {}
        }
        
        # Log the request being sent
        logger.info(f"Sending request to gateway: {json.dumps(request, indent=2)}")
        
        response = self.client.post(
            f"{self.gateway_url}/message",
            json=request,
            headers={"Content-Type": "application/json"}
        )
        
        if response.status_code != 200:
            raise Exception(f"Request failed with status {response.status_code}: {response.text}")
            
        return response.json()
    
    def list_tools(self) -> List[Dict[str, Any]]:
        """Get list of available tools from the gateway.
        
        Returns:
            List of tool definitions
        """
        if self._tools is None:
            response = self._send_request("tools/list")
            self._tools = response.get("tools", [])
        return self._tools
    
    def call_tool(self, name: str, arguments: Dict[str, Any]) -> Any:
        """Call a tool through the gateway.
        
        Args:
            name: Name of the tool to call
            arguments: Arguments to pass to the tool
            
        Returns:
            The tool's response
            
        Raises:
            Exception: If the tool call fails
        """
        # Log the incoming arguments
        logger.info(f"call_tool received arguments: {json.dumps(arguments, indent=2)}")
        
        # If arguments is a string, try to parse it as JSON
        if isinstance(arguments, str):
            try:
                arguments = json.loads(arguments)
                logger.info(f"Parsed string arguments into: {json.dumps(arguments, indent=2)}")
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse arguments string: {e}")
                raise
        
        # Ensure arguments is a dictionary
        if not isinstance(arguments, dict):
            logger.error(f"Arguments must be a dictionary, got {type(arguments)}")
            raise TypeError("Arguments must be a dictionary")
        
        params = {
            "name": name,
            "arguments": arguments
        }
        
        # Log the actual parameters being sent
        logger.info(f"Sending parameters to gateway: {json.dumps(params, indent=2)}")
        
        response = self._send_request("tools/call", params)
        
        # Extract text content from response
        if isinstance(response, dict):
            content = response.get("content", [])
            if content and isinstance(content, list):
                first_content = content[0]
                if isinstance(first_content, dict) and first_content.get("type") == "text":
                    return first_content.get("text")
        
        return response


# Global client instance
_client: Optional[MCPGatewayClient] = None


def get_client(gateway_url: Optional[str] = None) -> MCPGatewayClient:
    """Get or create the global client instance.
    
    Args:
        gateway_url: Optional URL for the gateway server
        
    Returns:
        The global client instance
    """
    global _client
    if _client is None:
        _client = MCPGatewayClient(gateway_url or "http://localhost:8808")
    return _client


def list_tools() -> List[Dict[str, Any]]:
    """Get list of available tools.
    
    Returns:
        List of tool definitions
    """
    return get_client().list_tools()


def call_tool(name: str, arguments: Dict[str, Any]) -> Any:
    """Call a tool through the gateway.
    
    Args:
        name: Name of the tool to call
        arguments: Arguments to pass to the tool
        
    Returns:
        The tool's response
    """
    return get_client().call_tool(name, arguments)



================================================
FILE: src/react_agent/prompts.py
================================================
"""Default prompts used by the agent."""

SYSTEM_PROMPT = """You are a helpful AI assistant.

System time: {system_time}"""



================================================
FILE: src/react_agent/state.py
================================================
"""Define the state structures for the agent."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Sequence, List, Optional, Literal

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from langgraph.managed import IsLastStep
from typing_extensions import Annotated


@dataclass
class InputState:
    """Defines the input state for the agent, representing a narrower interface to the outside world.

    This class is used to define the initial state and structure of incoming data.
    """

    messages: Annotated[Sequence[AnyMessage], add_messages] = field(
        default_factory=list
    )
    """
    Messages tracking the primary execution state of the agent.

    Typically accumulates a pattern of:
    1. HumanMessage - user input
    2. AIMessage with .tool_calls - agent picking tool(s) to use to collect information
    3. ToolMessage(s) - the responses (or errors) from the executed tools
    4. AIMessage without .tool_calls - agent responding in unstructured format to the user
    5. HumanMessage - user responds with the next conversational turn

    Steps 2-5 may repeat as needed.

    The `add_messages` annotation ensures that new messages are merged with existing ones,
    updating by ID to maintain an "append-only" state unless a message with the same ID is provided.
    """


@dataclass
class State(InputState):
    """Represents the complete state of the agent, extending InputState with additional attributes."""

    is_last_step: IsLastStep = field(default=False)
    """
    Indicates whether the current step is the last one before the graph raises an error.

    This is a 'managed' variable, controlled by the state machine rather than user code.
    It is set to 'True' when the step count reaches recursion_limit - 1.
    """

    current_agent: Literal["orchestrator", "planner", "coder"] = field(
        default="orchestrator"
    )
    """Tracks which agent is currently active in the workflow."""

    story_context: Optional[dict] = field(default=None)
    """Stores the Linear story context and Git repo info gathered by the orchestrator."""
    
    solution_plans: List[dict] = field(default_factory=list)
    """Stores the technical solution variations created by the planner."""
    
    completed_variations: List[dict] = field(default_factory=list)
    """Tracks which solution variations have been implemented by the coder."""



================================================
FILE: src/react_agent/tools.py
================================================
"""This module provides tools for the ReAct Agent using MCP servers.

Tools are dynamically loaded from MCP servers through the gateway.
"""

import asyncio
import inspect
import json
import logging
from typing import Any, Callable, Dict, List, Optional, Set, Type, Annotated

from langchain_core.tools import BaseTool, Tool, StructuredTool, tool, InjectedToolCallId
from langchain_core.messages import ToolMessage
from langgraph.types import Command
from pydantic import BaseModel, create_model

from react_agent import mcp_client

logger = logging.getLogger(__name__)


def get_schema(tool_def: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Get the input schema from a tool definition, handling both naming conventions."""
    # Try both input_schema and inputSchema
    schema = tool_def.get("input_schema") or tool_def.get("inputSchema")
    if schema:
        logger.info(f"Found schema for tool {tool_def['name']}: {json.dumps(schema, indent=2)}")
    else:
        logger.info(f"No schema found for tool {tool_def['name']}")
    return schema


def create_schema_model(tool_def: Dict[str, Any]) -> Optional[Type[BaseModel]]:
    """Create a Pydantic model from the tool's schema."""
    schema = get_schema(tool_def)
    if not schema or not isinstance(schema, dict):
        return None
        
    properties = schema.get("properties", {})
    if not properties:
        return None
        
    # Convert JSON schema types to Python types
    field_definitions = {}
    for name, prop in properties.items():
        logger.info(f"Adding field {name} to schema model for {tool_def['name']}")
        python_type = str if prop.get("type") == "string" else Any
        required = name in schema.get("required", [])
        field_definitions[name] = (python_type, ... if required else None)
    
    # Create the model
    model = create_model(
        f"{tool_def['name']}Args",
        **field_definitions
    )
    logger.info(f"Created schema model for {tool_def['name']}: {model}")
    return model


def _create_tool_wrapper(tool_def: Dict[str, Any]) -> BaseTool:
    """Create a wrapper function for an MCP tool.
    
    Args:
        tool_def: Tool definition from the MCP server
        
    Returns:
        A LangChain Tool
    """
    async def wrapper(*args, **kwargs) -> Any:
        """Wrapper function that calls the MCP tool."""
        # Check if tool has no parameters
        schema = get_schema(tool_def)
        has_params = schema and schema.get("properties")
        
        if not has_params:
            # If tool has no parameters, ignore any passed arguments
            logger.info(f"Tool {tool_def['name']} has no parameters, ignoring arguments")
            result = mcp_client.call_tool(tool_def["name"], {})
        else:
            # Convert args to kwargs if needed
            if args:
                logger.info(f"Converting args to kwargs: {args}")
                if len(args) == 1 and isinstance(args[0], str):
                    # If we get a single string argument, treat it as the first schema property
                    first_prop = next(iter(schema["properties"]))
                    kwargs[first_prop] = args[0]
                    logger.info(f"Converted string arg to {first_prop}: {args[0]}")
                elif len(args) == 1 and isinstance(args[0], dict):
                    # If we get a dict argument, merge it with kwargs, excluding __arg1
                    filtered_args = {k: v for k, v in args[0].items() if k != '__arg1'}
                    kwargs.update(filtered_args)
                    logger.info(f"Merged filtered dict arg with kwargs: {filtered_args}")
            
            logger.info(f"Tool wrapper calling with kwargs: {kwargs}")
            result = mcp_client.call_tool(tool_def["name"], kwargs)
        return result
    
    # Create Pydantic model for schema validation
    args_schema = create_schema_model(tool_def)
    
    # Check if we need a structured tool (multiple parameters) or simple tool
    schema = get_schema(tool_def)
    if schema and len(schema.get("properties", {})) > 1:
        # Use StructuredTool for multiple parameters
        tool = StructuredTool(
            name=tool_def["name"],
            description=tool_def.get("description", ""),
            func=wrapper,
            coroutine=wrapper,
            args_schema=args_schema
        )
    else:
        # Use regular Tool for single or no parameters
        tool = Tool(
            name=tool_def["name"],
            description=tool_def.get("description", ""),
            func=wrapper,
            coroutine=wrapper,
            args_schema=args_schema
        )
    
    logger.info(f"Created tool: {tool}")
    return tool


def _load_tools() -> List[BaseTool]:
    """Load all available tools from the MCP gateway.
    
    Returns:
        List of LangChain tools
    """
    logger.info("Loading tools from gateway")
    tools = []
    tool_names = []
    for tool_def in mcp_client.list_tools():
        logger.info(f"Loading tool: {tool_def['name']}")
        if tool_def['name'] in tool_names:
            continue

        tool_names.append(tool_def['name'])
        tool = _create_tool_wrapper(tool_def)
        tools.append(tool)

    logger.info(tool_names)
    return tools


# Routing tools
@tool
async def route_to_planner(
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Route the workflow to the planner agent."""
    return Command(
        goto="planner",
        update={"messages": [ToolMessage(content="Routing to planner", tool_call_id=tool_call_id)]}
    )

@tool
async def route_to_coder(
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Route the workflow to the coder agent."""
    return Command(
        goto="coder",
        update={"messages": [ToolMessage(content="Routing to coder", tool_call_id=tool_call_id)]}
    )

@tool
async def route_to_orchestrator(
    tool_call_id: Annotated[str, InjectedToolCallId]
) -> Command:
    """Route to orchestrator."""
    return Command(
        goto="orchestrator",
        update={"messages": [ToolMessage(content="Routing to orchestrator", tool_call_id=tool_call_id)]}
    )

# Initial routing tools list
ROUTING_TOOLS: List[BaseTool] = [
    route_to_planner,
    route_to_coder,
    route_to_orchestrator
]

# Initial empty tools list - will be populated during startup
TOOLS: List[BaseTool] = []


async def initialize_tools(config) -> List[BaseTool]:
    """Initialize connection to MCP gateway and get available tools.
    
    This should be called during application startup.
    
    Args:
        config: Application configuration
        
    Returns:
        List of available tools
    """
    global TOOLS
    
    logger.info("Initializing tools")
    
    # Configure MCP client with gateway URL from config
    if hasattr(config, "mcp_gateway_url"):
        mcp_client.get_client(config.mcp_gateway_url)
    
    # Load tools from gateway and combine with routing tools
    mcp_tools = _load_tools()
    TOOLS = ROUTING_TOOLS + mcp_tools

    logger.info(f"Initialized {len(TOOLS)} tools")
    return TOOLS



================================================
FILE: src/react_agent/utils.py
================================================
"""Utility functions for the ReAct agent."""

import logging
import os
from typing import Any

from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_chat_model(model_name: str, openrouter_base_url: str | None = None) -> Any:
    """Load a chat model based on the provider prefix.

    Args:
        model_name: Name of the model in format "provider/model-name"
        openrouter_base_url: Base URL for OpenRouter API if using OpenRouter

    Returns:
        A chat model instance

    Raises:
        ValueError: If the provider is not supported
    """
    logger.info(f"Loading chat model with name: {model_name}")
    provider, *model_parts = model_name.split("/")
    model = "/".join(model_parts)
    
    logger.info(f"Parsed provider: {provider}")
    logger.info(f"Parsed model: {model}")
    
    if provider == "anthropic":
        logger.info("Using Anthropic client")
        return ChatAnthropic(model=model)
    elif provider == "openai":
        logger.info("Using OpenAI client")
        return ChatOpenAI(model=model)
    elif provider == "openrouter":
        logger.info("Using OpenRouter via OpenAI client")
        logger.info(f"OpenRouter base URL: {openrouter_base_url}")
        
        if not openrouter_base_url:
            raise ValueError("openrouter_base_url is required for OpenRouter models")
            
        # For OpenRouter, we pass just the model part without the openrouter prefix
        api_key = os.getenv("OPENROUTER_API_KEY")
        logger.info(f"OpenRouter API key present: {bool(api_key)}")
        logger.info(f"Using model identifier: {model}")
        
        return ChatOpenAI(
            openai_api_key=api_key,
            openai_api_base=openrouter_base_url,
            model=model,  # Use just the model part (e.g., "anthropic/claude-3-5-sonnet-20241022")
        )
    elif provider == "google":
        logger.info("Using Google Gemini client")
        api_key = os.getenv("GOOGLE_API_KEY")
        logger.info(f"Google API key present: {bool(api_key)}")
        
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required for Google models")
            
        return ChatGoogleGenerativeAI(
            model=model,  # e.g., "gemini-1.5-pro"
            google_api_key=api_key,
            temperature=0,  # Match default behavior of other providers
            convert_system_message_to_human=True  # Gemini doesn't support system messages natively
        )
    else:
        raise ValueError(
            f"Unsupported provider: {provider}. "
            "Must be one of: anthropic, openai, openrouter, google"
        )



================================================
FILE: src/react_agent/agents/__init__.py
================================================
"""Agent implementations for the react_agent package."""

from react_agent.agents.base_agent import BaseAgent
from react_agent.agents.orchestrator import get_orchestrator
from react_agent.agents.planner import get_planner
from react_agent.agents.coder import get_coder

__all__ = [
    'BaseAgent',
    'get_orchestrator',
    'get_planner',
    'get_coder',
]



================================================
FILE: src/react_agent/agents/base_agent.py
================================================
from typing import Dict, Any, List
import structlog
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage
from langchain_core.runnables import RunnableSequence
from react_agent.state import State

log = structlog.get_logger()

class BaseAgent:
    def __init__(self, name: str, prompt_template: str, llm: BaseChatModel, tools: List = None):
        self.name = name
        self.llm = llm
        if tools:
            self.llm = self.llm.bind_tools(tools)
            
        # Convert the template into system and human messages
        system_message = SystemMessage(content=prompt_template)
        human_template = HumanMessagePromptTemplate.from_template("{all_messages}")
        self.prompt = ChatPromptTemplate.from_messages([
            system_message,
            human_template
        ])
        self.chain = RunnableSequence(self.prompt | self.llm)

    async def run(self, state: State) -> Dict[str, Any]:
        last_message = ""
        all_messages = ""
        
        if state.messages:
            last_message_obj = state.messages[-1]
            last_message = last_message_obj.content
            all_messages = "\n".join([f"{msg.type}: {msg.content}" for msg in state.messages])
        else:
            # Provide a default message if there are no messages yet
            all_messages = "Let's begin the task."
        
        response = await self.chain.ainvoke({
            "last_message": last_message,
            "all_messages": all_messages
        })
        
        log.info(f"{self.name} full response object: {response}")
        log.info(f"{self.name} response type: {type(response)}")
        if hasattr(response, 'additional_kwargs'):
            log.info(f"{self.name} additional kwargs: {response.additional_kwargs}")
        
        response_content = response.content if hasattr(response, 'content') else str(response)
        log.info(f"{self.name} response content: {response_content}")

        # Preserve any additional kwargs from the response
        message_kwargs = {}
        if hasattr(response, 'additional_kwargs'):
            message_kwargs = response.additional_kwargs
            log.info(f"{self.name} preserving additional kwargs: {message_kwargs}")

        return {
            "messages": [
                {
                    "role": "ai",
                    "content": response_content,
                    **message_kwargs
                }
            ],
            "current_agent": self.name
        }



================================================
FILE: src/react_agent/agents/coder.py
================================================
from typing import Dict, Any
from langchain_core.language_models.chat_models import BaseChatModel
from react_agent.agents.base_agent import BaseAgent

CODER_PROMPT = """You are the coder agent.

You work with two other agents:
-Orchestrator: sets code context and directs workflow
-Planner: takes user requests and determines multiple solutions to their stated problem

Your job is to implement the solutions provided by the planner.

For each proposed solution, your responsibilities are:
1. Create a new Git branch
2. Implement the proposed solution
3. Commit the changes
4. Use route_to_orchestrator tool when complete

CRITICAL:
- You have access to tools for reading and writing files and for 
conducting local Git operations.  Use the tools appropriately.
- Always create a new branch before making each set of changes.
- Follow Git best practices for commits
- Write tests for your changes if possible
- When implementation is complete, use route_to_orchestrator tool
- If you encounter errors, fix them and try again
- Do your best to complete the task on your own -- the orchestrator and planner don't know how to code

WORKFLOW:
1. Create a new branch with a descriptive name
2. Make the necessary code changes
3. Test the changes
4. Commit with a clear message
5. Use route_to_orchestrator tool to return to orchestrator
"""


#IMPORTANT: When using MCP tools:
#- Start your response with a <tool_result> block for each tool call
#- Each tool result must be acknowledged separately
#- Format your response like this:
#  <tool_result>Acknowledging result from tool X</tool_result>
#  <tool_result>Acknowledging result from tool Y</tool_result>
#  [rest of your response]

class Coder(BaseAgent):
    def __init__(self, llm: BaseChatModel, tools: list):
        super().__init__("coder", CODER_PROMPT, llm, tools)

def get_coder(llm: BaseChatModel, tools: list) -> Coder:
    return Coder(llm, tools)



================================================
FILE: src/react_agent/agents/orchestrator.py
================================================
from typing import Dict, Any
from langchain_core.language_models.chat_models import BaseChatModel
from react_agent.agents.base_agent import BaseAgent

ORCHESTRATOR_PROMPT = """You are the orchestrator agent. 

Your responsibilities are:

1. Initially: Use MCP tools to gather context about the task and codebase
   - For local repositories: Use the git MCP server
   - For remote repositories: Use the github MCP server
2. When context is complete: Use the route_to_planner tool to call the planner agent
3. When plans are ready: Use the route_to_coder tool to send them to the coder agent
4. When all variations are coded: Use the route_to_end tool to terminate the workflow

Explain your thinking briefly as you go.

CRITICAL: Do not make plans, create branches or write code. That's the job of the planner and coder agents.
Please send those tasks to them or they will be sad and you will be fired.

CRITICAL: The planner agent may suggest multiple solutions -- if it does, the coder agent will implement each variation
in separate branches and then tell you about them. This is why it's important that you don't create branches or write code yourself.

"""

#IMPORTANT: When using MCP tools:
#- Start your response with a <tool_result> block for each tool call
#- Each tool result must be acknowledged separately
#- Format your response like this:
#  <tool_result>Acknowledging result from tool X</tool_result>
#  <tool_result>Acknowledging result from tool Y</tool_result>
#  [rest of your response]
#"""

class Orchestrator(BaseAgent):
    def __init__(self, llm: BaseChatModel, tools: list):
        super().__init__("orchestrator", ORCHESTRATOR_PROMPT, llm, tools)

def get_orchestrator(llm: BaseChatModel, tools: list) -> Orchestrator:
    return Orchestrator(llm, tools)



================================================
FILE: src/react_agent/agents/planner.py
================================================
from typing import Dict, Any
from langchain_core.language_models.chat_models import BaseChatModel
from react_agent.agents.base_agent import BaseAgent

PLANNER_PROMPT = """You are the planner agent. Your responsibility is to:
1. Analyze the context provided by the orchestrator
2. Create 3 different technical variations to solve the problem
3. When complete, use the route_to_coder tool to send the plans to the coder

CRITICAL:
- Create exactly 3 different technical approaches
- Each approach should be clearly labeled and explained
- Focus on technical implementation details
- Do not implement the solutions - that's the coder's job
- When done, use the route_to_coder tool to send your plans

RESPONSE FORMAT:
Approach 1: [Description of first technical approach]
Technical Details:
- Implementation steps
- Key considerations
- Potential challenges

Approach 2: [Description of second technical approach]
Technical Details:
- Implementation steps
- Key considerations
- Potential challenges

Approach 3: [Description of third technical approach]
Technical Details:
- Implementation steps
- Key considerations
- Potential challenges

[After presenting the plans, use the route_to_coder tool to send them to the coder]
"""

#IMPORTANT: When using MCP tools:
#- Start your response with a <tool_result> block for each tool call
#- Each tool result must be acknowledged separately
#- Format your response like this:
#  <tool_result>Acknowledging result from tool X</tool_result>
#  <tool_result>Acknowledging result from tool Y</tool_result>
#  [rest of your response]


class Planner(BaseAgent):
    def __init__(self, llm: BaseChatModel, tools: list = None):
        super().__init__("planner", PLANNER_PROMPT, llm, tools)

def get_planner(llm: BaseChatModel, tools: list = None) -> Planner:
    return Planner(llm, tools)




================================================
FILE: tests/cassettes/103fe67e-a040-4e4e-aadb-b20a7057f904.yaml
================================================
interactions:
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:50:53.832822+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2yRS0vEQAzHv0rIxctUuqurOEdXXRQPooIvpIzb9KFtsjuTUevS7y6tiAqeQh6/
        /PPYYJ2jxTaUWTrZf32Wp9Jf3h2+Fa/z+4/F1dFiskCD2q1oqKIQXElo0EszBFwIdVDHigZbyalB
        i8vGxZySnWSWBGEmTabpdDfdm6ZocCmsxIr2YfPdVOl9wEdj8VrAcXgjD51ED+tIQWthcE8SFbQi
        KCRyTh6kgHPH5bxyNRs43WoaYKIcVCCQ88sKCvEj0UpQiKtEJcmdEtRciG/d0HcbzkmhJcgFtHI6
        Mp3EbezNz4QiTRbDsPd4rMGPWTo5XB/NbiYXz2fkZrfHxye+q9pFiQbZtQP3NcZA8Soq2g2uI/kO
        Lf63A/b9o8Ggsso8uSD8V3lMBFpH4iWh5dg0BuP4Drv5UshUXogD2t2dPYMS9XfsYNr3nwAAAP//
        AwCzkxon7QEAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ab88f99c4cb1-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:51:31 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:51:29Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:51:31Z'
      request-id:
      - req_019JrRXeYjgQXtSjyp85fdHe
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: null
    headers: {}
    method: POST
    uri: https://api.tavily.com/search
  response:
    body:
      string: '{"query":"founder of LangChain","follow_up_questions":null,"answer":null,"images":[],"results":[{"title":"Speaker
        Harrison Chase - ELC","url":"https://sfelc.com/speaker/harrison-chase","content":"Harrison
        Chase is the CEO and co-founder of LangChain, a company formed around the
        open source Python/Typescript packages that aim to make it easy to develop
        Language Model applications. Prior to starting LangChain, he led the ML team
        at Robust Intelligence (an MLOps company focused on testing and validation
        of machine learning models), led the","score":0.99967754,"raw_content":null},{"title":"Harrison
        Chase - The AI Conference","url":"https://aiconference.com/speakers/harrison-chase/","content":"Harrison
        Chase is the co-founder and CEO of LangChain, a company formed around the
        open-source Python/Typescript packages that aim to make it easy to develop
        Language Model applications. Prior to starting LangChain, he led the ML team
        at Robust Intelligence (an MLOps company focused on testing and validation
        of machine learning models), led the","score":0.9995908,"raw_content":null},{"title":"Harrison
        Chase | TEDAI San Francisco","url":"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/","content":"Harrison
        Chase, a Harvard graduate in statistics and computer science, co-founded LangChain
        to streamline the development of Language Model applications with open-source
        Python/Typescript packages. Chase''s experience includes heading the Machine
        Learning team at Robust Intelligence, focusing on the testing and validation
        of machine learning models, and leading the entity linking team at Kensho","score":0.9994746,"raw_content":null},{"title":"Harrison
        Chase, Author at TechCrunch","url":"https://techcrunch.com/author/harrison-chase/","content":"Harrison
        Chase is the CEO and co-founder of LangChain, a company formed around the
        open source Python/Typescript packages that aim to make it easy to develop
        Language Model applications","score":0.9994185,"raw_content":null},{"title":"LangChain''s
        Harrison Chase on Building the Orchestration Layer for AI ...","url":"https://www.sequoiacap.com/podcast/training-data-harrison-chase/","content":"Sonya
        Huang: Hi, and welcome to training data. We have with us today Harrison Chase,
        founder and CEO of LangChain. Harrison is a legend in the agent ecosystem,
        as the product visionary who first connected LLMs with tools and actions.
        And LangChain is the most popular agent building framework in the AI space.","score":0.99876,"raw_content":null},{"title":"Harrison
        Chase, LangChain CEO - Interview - YouTube","url":"https://www.youtube.com/watch?v=7D8bw_4hTdo","content":"Join
        us for an insightful interview with Harrison Chase, the CEO and co-founder
        of Langchain, as he provides a comprehensive overview of Langchain''s innovati","score":0.99854493,"raw_content":null},{"title":"Harrison
        Chase - Forbes","url":"https://www.forbes.com/profile/harrison-chase/","content":"Harrison
        Chase only cofounded LangChain in late 2022, but the company caught instant
        attention for enabling anyone to build apps powered by large language models
        like GPT-4 in as little as two","score":0.9977743,"raw_content":null},{"title":"LangChain
        - Wikipedia","url":"https://en.wikipedia.org/wiki/LangChain","content":"In
        October 2023 LangChain introduced LangServe, a deployment tool designed to
        facilitate the transition from LCEL (LangChain Expression Language) prototypes
        to production-ready applications.[5]\nIntegrations[edit]\nAs of March 2023,
        LangChain included integrations with systems including Amazon, Google, and
        Microsoft Azure cloud storage; API wrappers for news, movie information, and
        weather; Bash for summarization, syntax and semantics checking, and execution
        of shell scripts; multiple web scraping subsystems and templates; few-shot
        learning prompt generation support; finding and summarizing \"todo\" tasks
        in code; Google Drive documents, spreadsheets, and presentations summarization,
        extraction, and creation; Google Search and Microsoft Bing web search; OpenAI,
        Anthropic, and Hugging Face language models; iFixit repair guides and wikis
        search and summarization; MapReduce for question answering, combining documents,
        and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and
        pymupdf for PDF file text extraction and manipulation; Python and JavaScript
        code generation, analysis, and debugging; Milvus vector database[6] to store
        and retrieve vector embeddings; Weaviate vector database[7] to cache embedding
        and data objects; Redis cache database storage; Python RequestsWrapper and
        other methods for API requests; SQL and NoSQL databases including JSON support;
        Streamlit, including for logging; text mapping for k-nearest neighbors search;
        time zone conversion and calendar operations; tracing and recording stack
        symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha
        website and SDK.[8] As a language model integration framework, LangChain''s
        use-cases largely overlap with those of language models in general, including
        document analysis and summarization, chatbots, and code analysis.[2]\nHistory[edit]\nLangChain
        was launched in October 2022 as an open source project by Harrison Chase,
        while working at machine learning startup Robust Intelligence. In April 2023,
        LangChain had incorporated and the new startup raised over $20 million in
        funding at a valuation of at least $200 million from venture firm Sequoia
        Capital, a week after announcing a $10 million seed investment from Benchmark.[3][4]\n
        The project quickly garnered popularity, with improvements from hundreds of
        contributors on GitHub, trending discussions on Twitter, lively activity on
        the project''s Discord server, many YouTube tutorials, and meetups in San
        Francisco and London. As of April 2023, it can read from more than 50 document
        types and data sources.[9]\nReferences[edit]\nExternal links[edit]","score":0.99694854,"raw_content":null},{"title":"Harrison
        Chase - CEO of LangChain - Analytics India Magazine","url":"https://analyticsindiamag.com/people/harrison-chase/","content":"By
        AIM The dynamic co-founder and CEO of LangChain, Harrison Chase is simplifying
        the creation of applications powered by LLMs. With a background in statistics
        and computer science from Harvard University, Chase has carved a niche in
        the AI landscape. AIM Brand Solutions, a marketing division within AIM, specializes
        in creating diverse content such as documentaries, public artworks, podcasts,
        videos, articles, and more to effectively tell compelling stories. AIM Research
        produces a series of annual reports on AI & Data Science covering every aspect
        of the industry. Discover how Cypher 2024 expands to the USA, bridging AI
        innovation gaps and tackling the challenges of enterprise AI adoption AIM
        India AIM Research AIM Leaders Council 50 Best Data Science Firms","score":0.99491996,"raw_content":null},{"title":"Key
        Insights from Harrison Chase''s Talk on Building Next-Level AI Agents","url":"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents","content":"Harrison
        Chase, founder of LangChain, shared insights on the evolution of AI agents
        and their applications during Sequoia Capital''s AI Ascent. ... Saves you
        a lot of research time, plus gives a flashback to ML history and insights
        into the future. Stay ahead alongside over 73,000 professionals from top AI
        labs, ML startups, and enterprises","score":0.9941801,"raw_content":null}],"response_time":2.54}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '7474'
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:51:34 GMT
      Server:
      - nginx
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      your question about the founder of LangChain, I''ll need to search for the most
      up-to-date information. Let me do that for you.", "type": "text"}, {"type":
      "tool_use", "name": "search", "input": {"query": "founder of LangChain"}, "id":
      "toolu_01BqD5W1PjJea5XEEFryhmGg"}]}, {"role": "user", "content": [{"type": "tool_result",
      "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\", \"content\":
      \"Harrison Chase is the CEO and co-founder of LangChain, a company formed around
      the open source Python/Typescript packages that aim to make it easy to develop
      Language Model applications. Prior to starting LangChain, he led the ML team
      at Robust Intelligence (an MLOps company focused on testing and validation of
      machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01BqD5W1PjJea5XEEFryhmGg",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:50:59.620569+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RUXW/cRgz8K8QiQF90wln+SHtvjlvERh2kqG2gaFMYvBUlbW7FlXe55wiG/3vB
        9Z0/EvRJgEQOZ4ZDPRjXmpUZU3+7PLg8/P1Kmq83H9PPza9Xf/2d+vPh5hdTGZkn0ipKCXsylYnB
        6wtMySVBFlOZMbTkzcpYj7mlxeHieJECM8miWTZHy5NmaSpjAwuxmNU/D3tQoW/aXh4rcz0gb2AO
        GboQ9RnhLlMSF7iGC7DIwOEephi2rqVSeO9kAMddiCNqHeA6ZAEZCLqQuaUIoYNL5P5sQMf1F/7C
        1//zEVyCc4zRpcBwNmCiGj6FSJAmsq5zFr2fV4pwUH9XqK0604bFHhm5hbPfPv84vqnhfE+gfT2d
        waMQNMumqeG6oI0T8gz3mCB0nbNOGYDHzHagVjs+WwlriqUJMAEyhIl4kUKOltSqr2SlzD2sd1wH
        rYM12k0flYXiJEFxSZxNhbhOzkIRknXElqCLYVTNW4wt3LDbUkxO5gJ8VMMH6opRglEc9y+qKhgI
        PLXFnU9oB8cEl4SRtUwIR0CBP8M6J4ELFvLe9TqxAnzW3wWbE7UQGETjwH0huUXv2qethw7GHbjf
        g5dMpsLwuH7l8112duNn6NExtTCFKXuMTmb1QWmeXujkMbOTuSqT1jOcTtF5tfmw2m36iduALUR0
        Si+5nktMWKDL3Druyx5nSGRzVAFbivCuWcLovFfejveVagOqpPysCEXFJNGOl5ayiSu6y8EhnOHk
        BH0FaQhR/AzYSckeh8y2wMK7g5fmRCU2W0oyktJUsA/EdhgxbopXJzXclACXsPyUlENLMQ1uql65
        qCFakw0jAe49hC7iSPchbsoBt7QlH6bCYpq8s0VZginck7qx1ijHnsAj9xl72q0MvNsQfPzjenFU
        GP14ajZwcm0Bwbe2uz5HerXINKFmadJcWuXon9e8o1d8CB1ICP4p/M8q9KZRYMQNgRMgTI4iSIAi
        sfx6vqdejieS3vHpxRvZtXn8tzJJwnQbCVNgszLE7a3kyGb3IdFd1vCbFWfvK5PLH3f1YBxPWW4l
        bIiTWTXL98eVCVnevHx/8vj4HwAAAP//AwBvXpD30gUAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22abad2aef4caf-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:51:40 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:51:35Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:51:40Z'
      request-id:
      - req_012BB17DG6rpcEU1xAbh9Zao
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:51:11.433533+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRTWvcQAyG/4rQpZdxcHY3XuJLCYX0a3tJCz2EYiYeeT2tLXktTRuz+L8Xe1vS
        noQ+3lcP0hljwBJ7PVb59ef44Xtzc/+UBp8Oxce3t9vdcf+ADm0aaJkiVX8kdDhKtxS8alTzbOiw
        l0Adllh3PgXKttlNpsJMlm3yzS4vNjk6rIWN2LB8PP81NXpe5Gso8YuAZ/1FI1gbFU6J1KIw+LpO
        ozfqJgfvX3UdMFEAE1DyY91CI4uCoBc1SENmkgVvBJEbGXt/8XiSZHDwfHzT+sjgOUA0hUYSBxqv
        4EAGPUEQsNbb6jlJusLZvdCKdFXS5Qbr4ZY8Vfn13VTc3/XF/vT88PPdp9ooNBoEHbLvF90Fc1Hx
        kAzLM54SjROW+LUViLrC/wEBaV4oX+M8f3OoJkM1klfh/ynWhtIpEdeEJaeuc5jWN5Xny7bK5Aex
        YrnbFg4l2b+12/08/wYAAP//AwBjKTr0BQIAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22abf709c34cb4-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:51:49 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:51:47Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:51:49Z'
      request-id:
      - req_01QtVfENAkfspCf9KVLM7Emk
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "Who is the founder
      of LangChain?"}, "id": "toolu_01Ay6FAm67qxRvHMctedfsdo"}]}, {"role": "user",
      "content": [{"type": "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01Ay6FAm67qxRvHMctedfsdo",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:51:14.324904+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RVbW/bNhD+KwdiQDZA9lynaQd9a4IWyZBi6dq9AMsQnKmzxJoiFd7RrhD0vw9H
        2Z6XoZ8MUHfH5+3oJ+MaU5ue24fFi9uP169//u2PO/rzarndvXt9+/nt9t3GVEbGgbSKmLElU5kU
        vR4gs2PBIKYyfWzIm9pYj7mh2fnsYsYxBJLZcrF8uXi1XJjK2BiEgpj6r6fDUKEv2l5+anOJTA3E
        ANIRMGGyHSTi7IUruAGLAWwMa9dQED8CBt5RgjHmBI+ZWFwMgKuYpQxYxxwaShDXcIuhverQhfl9
        uA/XmJLjGOCqQyZw/O1y+BRhRdDHRDAkso6pgu7YY+Ps0Iahgau3vzxrv6ZEgImAY0+woRGG6ILw
        HuYzJDrj2Fwr1BdzeKcXoHKrn9cfrz9pAxfAoxAsF8vlHD4VlP2AYYQdMqxj6qkBTNoHcaAw45iT
        JbgbpYvhx0/jQB9tcoPAgHaDLTGgKz0CPW5caMEJELKjBBKhoS35OBQEGVuC95oFwGHwzhbcXFRf
        zuES7aYtN9f/ao/KaoupgTZhkxX6zkkHCKtjuZJiQXEsznLRSUlloQRsHQVL5Y7zOdwl2rqYGejL
        QKl8quGS1uogCyZRAke1ipmemuLme7SdCwS3hClomRD2SvvXuMoscBOEvHetzqwAj7quo82H4GoK
        Q1sQbtG7yTgNRb8f7g/Dy8pM0rycw4fs7AbaFHfS1SduPuq5H6FFF6iBIQ7ZY3IyqiQK+s2N4uhz
        cDLO4XKEN0NyXt0/r+Czwu5jkI4B16qWEwaPOdiu2id44tBhAwmd0mDXBrd2FoPAOofmwEfjs0Wf
        pyjELSX4brlYQO+8d3FarYs53PQDWvlfVh1DQ2yTW2m/2n5vPLU0matQsKUgQDbyyEL9vdGwQucY
        9MHRKqtPit07mFp6ljmG729v3/MPU34kRj9FBe2UwhNZO2RYkdWljIHUH0XQR5aDxBOc2So7XxRY
        J+xpF9OGT5TnAffJezWH3x2XJS18zxjaiH6CcrKcrCvDkgh7r3HQQfsN6pX+/vn4xiJVJxuI1hKz
        W3kqOiHsnL5DCUNbCO2nUppEWGV2gZipILCJdNHe3MyGuKOkljzf1yPoM4aEg2sgOaYT8i40mSWN
        wF3cWdTJ//X8jGFbNCkAtEvzXeD3Q0yCwU7SF6ekmx4YAlVZq4p2vhjtD5pMWzM9yCcK6AW0Xjt9
        C2Ruvv5dGZY4PCRCjsHUhkLzIDkFs//A9Jh1j00dsveVyeW/rX4yLgxZHiRuKLCpl4ufFpWJWU4P
        zy8WX7/+AwAA//8DAHBCV3w8BwAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ac0918734cb2-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:51:56 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:51:49Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:51:56Z'
      request-id:
      - req_01BLNX4GjFRn5bwxxBo3hd3r
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:51:55.561440+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRS2vbQBD+K8NcclkFxY6dZG+NIaTgXNxgU0oRa2lkCa9mJO1sY2P034vklrSn
        YR7fg28uWBdosQmHLL3b0fHwbb85Plb1utsudt8X2+enFRrUc0vjFYXgDoQGe/HjwIVQB3WsaLCR
        gjxazL2LBSXzZJEEYSZNZunsPl3OUjSYCyuxov1x+UuqdBrhU7H4LuA4fFAPWtUBukhBa2FweR57
        p+TPBr7eeA9MVIAKBHJ9XkEpI4KgkaAQ20QlKZwS1FxK37grx16iwtrxYVW5msFxAbUGKCVyQf0t
        rEmhISgEtHI6cZ4l3uJgPt2K+CyGMYMpuLGPWXr3Rt1m177+enj5svp4OZVv283+tESD7JoRd7U5
        oriNivaCXaT+jBb/qIOUn9ZwGH4aDCpt1pMLwv8rT4tAXSTOCS1H7w3G6TX2clXIVI7EAe39fGlQ
        ov47e5oPw28AAAD//wMAEu0ZMPkBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ad0acd1032b3-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:52:33 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:52:31Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:52:33Z'
      request-id:
      - req_01GSwnbYyFo11udBwAygsuuc
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "founder of LangChain"},
      "id": "toolu_01MeqRWpHv7FACwFxfMVRbx6"}]}, {"role": "user", "content": [{"type":
      "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01MeqRWpHv7FACwFxfMVRbx6",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:51:58.213993+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RU0W7cNhD8lQWfWkCnXs6Jg96bY7SIURtxE/epVxh71EraHLWUyeW5guF/L1Z3
        bmynfRJEcmdmZ4d8cNy4tRtyd7t8c/3b5erns7yKfHp18iXSr1+73zO7yuk0kp2inLEjV7kUgy1g
        zpwVRV3lhthQcGvnA5aGFieLd4scRUgXq+Xq7fJ0tXSV81GURN36z4cnUKW/rXz+rN0HzNRAFNCe
        IBMm30OiXILmCi7Ao8CY4p4bgikWuGftAb0vCZWApY1pQOUogNtYdEZpY5GGEsQWLlG68x5Z1hvZ
        yM3/bAJn+IgpcY4C5z1mquEqJoI8kueWPYYwzQhv6lcHrdQ4fVw8IaM0cP7LpxcM9UZWNXx8om+e
        cwsE62W1XK2qufiAN4woE9wV9rswQYcs1ACquWnttjEBawYWiXtU3hPgOKaIvgeNcHYBDe0pxHEg
        0XojJ/Uzznu0Qla2xiBgEd8begYUiCPJIseSPJn1X8mrifzkNW4pzTrhvudAr60w1PuYdiwdoALC
        gL5nIQiESWw1KyYtI5ih1MDnuC1Z4UKUQuCOxFO9kbf1Ea83PbBFv+uS2WYqsqJyVvZ5tspsKkoJ
        smcrhzbFwXTtMTXwh/CeUmad6o28q+E6cUzmzqzDBP1rSQW96TyYf3XUffmkWwkHa+k/BFfmhe+h
        jb48JZnyDG4C9xi4OQQ0tt8bMt+hXFu0vk0HecimcsAdASsQ5sn+jwOdRRfsCK6s2sYe2M8clsUU
        S9e/GOL1pH2Un26mkbJPPCqM6HfYUa7h5lnWzO9jzjJ3MgdfFDShnxvgwyU9u7CKoQjrNPdodYk8
        8d4qy3Z+IBgDtEUa63Ieyp5ESyLwOLLaJqchQ+AdwRe6K5ERzo9bBvqBxPcDpt1szveXzkfJ3FCy
        2IJExW0gaLkzim9Cycc8ZaWhgtFG7kvAFKb59vR8iKsd9/Zw+WMkUkevPM7ww+XlVf7x8PxojOEQ
        v4MxubJRvYgTIIxxNDJoEw408xjptnCYPTm7AOxINNfu8a/KZY3jbSLMUdzakTS3WpK440amu2Jh
        c2spIVSuzK/y+sGxjEVvNe5Isluvlu9PKxeLvlh8f/L4+A8AAAD//wMAo1MaSPYFAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ad1b7c4d4cb1-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:52:39 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:52:33Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:52:39Z'
      request-id:
      - req_01E8yQY8oXhidTwTxeWKdjiW
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:52:42.318107+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRXWvbUAyG/4rQzW6Oi5O2SeObQgspgzBW2tLBGObUVuJDbSk50mkSgv/7sLPR
        7Uro4331IJ0w1Fhgp5syn9w8r252+7iY3YfV9+nbkibd+8ccHdpxS8MUqfoNocMo7VDwqkHNs6HD
        TmpqscCq9amm7DK7zlSYybJpPr3KZ9McHVbCRmxY/Dz9NTU6DPIxFPgs4Fn3FMGaoLBLpBaEwVdV
        it6oPTr4+qVtgYlqMAElH6sG1jIoCDpRg7TNTLLaG0HgtcTOnz3eJBmsPG/uGx8YPNcQTGEtiWuK
        F7Aig46gFrDG2+h5lHSBvfukFWnLpMMNxsMNeSrzyeQ1/rh7OsQFx2931/v54+HhZblEh+y7QXfG
        HFS8TYbFCXeJ4hELfG0Ego7wf0BA1p+Ut9j3vxyqybaM5FX4f4qxobRLxBVhwaltHabxTcXpvK00
        eSdWLK4uZw4l2b+1xbzvfwMAAP//AwB9NM36BQIAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ae2f5dec4cb6-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:53:20 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:53:18Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:53:20Z'
      request-id:
      - req_01DsVqmPigsQsHqctV3yYFQ5
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "Who is the founder
      of LangChain?"}, "id": "toolu_011WrXBSxr9nrNB5w7QxGUFF"}]}, {"role": "user",
      "content": [{"type": "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_011WrXBSxr9nrNB5w7QxGUFF",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:52:45.186170+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2RV0W4bRwz8FWIRoC1wUhXZSdp7i402NuCgbpunNoVB7VF3jPZ210uuHNXIvxc8
        SbGcPgnYI4fDIYd6dNy51o3S3y1e3sa/7h/ecL6X128+XXU3//rxLJ27xukuk0WRCPbkGldSsAcU
        YVGM6ho3po6Ca50PWDuanc1ezSTFSDpbLpbni9fLhWucT1Epqmv/fjyCKn229OmndRco1EGKoAOB
        EBY/QCGpQaWBa/AYIZe05Y5glyo8sA6A3teCSsBxncqIyikCrlLVCWWdauyoQFrDDcb+ckCO7cf4
        MV5hKSwpwuWAQsDyLBxjB5e//PYsbQ5XVAiwEEgaCTa0g5w4qhzqfQNpGM9rvpzDZZodarTfxvvj
        p5M04AjBulsulsu5YSzncIF+0xcLbZ/Yo8FtsXTQF+yq5ez1gdXXcEMTRWVR9jIR9GnMVamAeKbo
        aapxZjzHjHEH6+SrtKeErJQ/fF1VDgq4B0+Z4kxSLZ7gdqdDij9+2GUSXzgrZPQb7MlkRgXkETTB
        iBsCViAUpmIvHW0ppDwVrNgTvLfNAsw5sJ+GKxPF8zncFtpyqgL0OVOZ2LdwQetkA1IsyrF/It48
        yR2om4b9Hv3AkeCGsEQLVsIRUOGPtKqicB2VQuDekJuTridNDotKMpUxKbcYuNvvX1rDeAAPR/DJ
        Invyr+bwe2W/gb6kBx1O5b2397CDHjlSBznlGrCw7mx2RvrttfEYa2TdNfsRm68Kr+qkDqxLGmGo
        sSvUiTE5SEpFjPE71qu6mgijV94SdCy+ikzJKcIWy6RqDqjmqD3l13P4tcaOY9/CdYS3uXCwpTxr
        4JNpNaaogwCubZdYBQLW6IfmpLWCPKm2pQIvlgsYOQQTiyOs99CmPZqO9auMqKagqGU8pUw9bilq
        LQRrLiP8Sfc1McIlZlYMc/gwsIDHkUCGVDTsDtwQXrx8AhIic8WWREeKuge+oOiHEctm6vzNHK7H
        jF7/Z1grkKJwR4U6QBDuI6/ZowFxb9yehoa94ZNPshOlsYFsG+ptusFWqoARttNqSd6Opz9scOnp
        Gz8IfH9z815+2M9fUwrydaJHh5xs/v6mPWf/nZiwHRUZODcwoMCKvN21FMmUN95jEj3u4L6DmVl+
        mtW64EgPqWzkpEvJaF6hiKtgQSe7pwl8IbtLp2aGnB4m+VY7CFOn4djp3jAQeEPw7vbD7BxGs7bd
        Cptm7I5mmbsv/zRONOW7QigputZR7O60lugOH4Tuq/nYtbGG0Lg6/Ze1j45jrnqnaUNRXLtc/LRo
        XKp6+nh2/vOXL/8BAAD//wMABqZyviwHAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22ae419e4532cc-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:53:26 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:53:20Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:53:26Z'
      request-id:
      - req_01B7UWY6NpA4GBZ9DRu9YWdS
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:53:11.070630+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRXWvbUAyG/4rQzW6Oi5ukKfPN2AalLWWMURhlDHPso8RebSk+0qHxgv/7sNPR
        7Uro4331IJ2wDVhgr/syv/x6fwxfwup5qId6M1zfbNpqPX5DhzYeaJ4iVb8ndBilmwtetVXzbOiw
        l0AdFlh3PgXK1tlVpsJMlq3y1SbfrnJ0WAsbsWHx4/TX1Og4y5dQ4KOAZ32hCNa0CkMitVYYfF2n
        6I260cHdu64DJgpgAko+1g3sZFYQ9KIG6ZCZZMEbQcs7ib0/e1SSDB487z83vmXwHKA1hZ0kDhQv
        4IEMeoIgYI23xXOUdIGTe6MV6cqk8w2Ww815KvPLq5tfcnza7l+q6uNtHKqnT4/H3yM6ZN/PujPm
        rOJDMixOOCSKIxb4vRFodYF/BQHZvVF+wGn66VBNDmUkr8L/UywNpSER14QFp65zmJY3FafzttLk
        mVix2Ky3DiXZv7X319P0BwAA//8DAHnV818FAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22aee2bec36992-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:53:48 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:53:46Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:53:48Z'
      request-id:
      - req_01Jk8NvFFDybK94i9Qz4cRk5
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "Who is the founder
      of LangChain?"}, "id": "toolu_015FjoxY6gwbbAHrqbYBTxzy"}]}, {"role": "user",
      "content": [{"type": "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_015FjoxY6gwbbAHrqbYBTxzy",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:53:13.741829+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RUYW8bNwz9K4RQYBtw9mynKTp/a4NhyZagxZYOGNYhoHW8O8068ipKzg5B/vtA
        xUHdDvtkQ0e+9/j4pAcXWrd1o/Z3q/X58JrO1r+vf54vflnvfvpjg+v1+aVrXJ4nsipSxZ5c45JE
        O0DVoBk5u8aN0lJ0W+cjlpYWZ4vzhQoz5cVmtXm5erVZucZ54Uyc3fbPh2fQTP9Ye/3Zureo1IIw
        5IFACZMfIJGWmLWBK/DIMCU5hJZglgL3IQ+1NHAnacQchAF3UnI97aRwSwmkg2vk/mLAwNuP/JFv
        /+cjBIVLTCmoMFwMqLSEG0kEOpEPXfAY41wR1suvCq3VOL0snpGRW7j48d0XDEtr3izh8llAe8rO
        EDETbFabzRJuK9o4Ic9wjwrSdcEHUwARC/uBWkAFZJCJeKFSkiez52/y2bDe+Sw7Sk9wxnu2hPcp
        SIIsoBlTDtx/pm++nuhe0t44MiCM6IfABJEwsbXV/jKBWUIt/Cq7ohmuOFOMoSf21MD9QIlgsK62
        mnNzDZlwrGJeLo80gw0BO/T7PpklJl0z5qA5eK0umg0lUwL1waChSzKa3AOmFj5wOFDSkOcKfL48
        sfRTCX4fZ+gxMLUwyVQippBnYzFJb64MfSwc8txUst0Mb6YUovl21hyX+rSGAVtIGCyjGnquieAM
        XeE2cF9XNoOSL8lSfKAELzYrGEOMFszAz5VPnh4wlqfISmcnkVCzdXxuqXP+Rp+KBIQLnELG2EAn
        Mcp9xYEX68/VSmTuHUjzSKbLut8S+2HEtK/mvFrChxrO6v03aqQtJR3C1JzYZjvZkZeRQJhMn9kw
        iuZnCwF74rzYlRDrRF3CkSwxeuKsTmg56MQXtSJhGHFv/0IGQp0tiS0dKMpU2Qv2BDf2lABOUwy+
        +qNwbD/J+fs5D8Lf384TqU9hyjCh32NPWuf87+X0whpass3gl9sLfUl0opq86KyZxgYmuyTe5o0z
        dJJgCFrvhZV7e9/88RKlnr6aQOHb6+sb/e74SonEpzCjrzPZ9Qh+OFpNFo8uFuIcMD6LOVpTlymd
        aauuH3FODFq6x78ap1mmu0Sowm7riNu7XBK74welT8Uuj9tyibFxpT7m2wcXeCr5LsueWN12s3q9
        apyU/MXhD68fH/8FAAD//wMA0hFTIS0GAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22aef3a88e32c6-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:53:54 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:53:49Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:53:54Z'
      request-id:
      - req_01HxWKuEjZTokR87xSj3FNGK
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:04.768568+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRW0/bUAz+K5Zf9nKCQluYdt42hLisSIUxBJpQdEjcNuPEbmMfrVGV/46SbmI8
        Wb58F33eY12hx0ZXRX789fP5nV3Wi+ty3j38/P14Kbtvtzt0aN2GhitSDStCh63EYRBUa7XAhg4b
        qSiixzKGVFE2zU4yFWaybJJPZvnpJEeHpbARG/pf+3+kRrsBPhaP9wKB9Q+1YOtaYZtIrRaGUJap
        DUaxc3D1KUZgogpMQCm05RqWMiAIGlGDtMlMsioYQc1LaZtw4HiRZDAPvDpbh5ohcAW1KSwlcUXt
        EczJoCGoBGwdbOTsJB1h797disQi6ZDBGNzQpyI/nun317Pbxd3DzeLx4ub6/iKdPP1o0CGHZsAd
        bA4o3iRDv8dtorZDj3/VQZbv1rDvnx2qyaZoKajwR+VxobRNxCWh5xSjwzS+xu8PCoXJK7Gin01P
        HUqy/2dfpn3/BgAA//8DABCOjgv5AQAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b0325b034cb6-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:54:42 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:54:40Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:54:42Z'
      request-id:
      - req_01SsSvCQgy3vQfuMCiJiaZLM
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "founder of LangChain"},
      "id": "toolu_014sKkCQPRVMPXGMJTGu5YSm"}]}, {"role": "user", "content": [{"type":
      "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_014sKkCQPRVMPXGMJTGu5YSm",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:07.180606+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3SUW28bNxCF/8qAKNAWWKmyfEmjt8QIHAN2YjTOS5rCGHFnVxNxhzQvUreG/3sx
        lFTbafskLXdn5juHh3ww3JqFGVJ/Nzt6ffplLvdXR/2Hi7f9xYf1uy/fxr8G05g8BtKvKCXsyTQm
        eqcLmBKnjJJNYwbfkjMLYx2WlibHk9NJ8iKUJ/PZ/GR2Np+ZxlgvmSSbxe8Ph6aZ/tTy+rMwbzFR
        C14grwgSYbQriJSKy6mBS7AoEKLfcEsw+gJbzitg6XwcMLMXwKUvuRZ3vkhLEXwHVyj9+QpZFl/l
        q9z+z0vgBO8xRk5e4HyFiaZw7SNBCmS5Y4vOjbXD0fS7D7VUZ1o/OXRGaeH83ccXE6ZaPJ/C+wNA
        +3y6gMNMMJ/N54AJUMAHkknyJVpS1d/IZtiu2BFsfVyz9IAZEAa0KxYCRxhFV1PGmEsAJaYWfvPL
        kjJcSibnuCexVEmOp3v6lY6DJdp1H5VLWVLGzCmzTVWK9UMomSIky9oAuugHdWGDsYXPwhuKifNY
        G59M4Sayj5D9jkWh/lHawEpZ2+rY9Z796sCeCQeV9R/QjYq3K+i8LYeUUKrNFXGDjttdCnz3b1Nq
        PlPlO50+8/2+sF27EXpkoRaCD8Vh5DyqC4r45lLVD0U4j02dxAJvQmSne3Xc7Hd+CCgjRGRFS9xL
        jYxk6Iq0LH0DLNYV/Qt+QxF+mM9gYOcUuLr5ie6LZ4RzDJzRVdSz6cuArsVvBTofgXN6EZCbMa+8
        /HI7Bko2csgQ0K6xJ42m5oQH3Y8B1wScgTCN+tzShpwPdUrBnuBafQIMwbGtbu48ezV9irr1kril
        SC2gBnNgIRXKfYn0zDbsdZmsT2PKNDQQNAtW/XVjFWH1hrD7fMSevsNI8NPV1XX6eXfOs/dul0a0
        T2AvT+KPCTac1NJa8uSdZnxJJDWTPATH3Vg5Q/SWUtLM2Ei4i9Mz9RD8tkpdjuAqozsw7iIFjtcE
        Fze3k5NG7dUGnAGttuWlIx2JsFXHIKL0pLP2vlPcKYq0u+z0+cnAjsm1U/P4R2NS9uEuEiYvZmFI
        2rtcopj9i0T3RY+IWUhxrjGl3tOLB8MSSr7Lfk2SzGI+e3XWGF/yi8VfXz8+/g0AAP//AwD0aedG
        CAYAAA==
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b04158a84cb1-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:54:48 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:54:42Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:54:48Z'
      request-id:
      - req_01Ty6RLDuDeMCfmVfunAmMKp
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:32.018816+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SRTW8TQQyG/4rlC5fZapO0KZ0LEqAWlB4Q4UMBodWw42QXdu1k7KGNov3vaDeg
        wsnyx/v6kX3CNqLHXndVObtpZ6vHu+cPH3/Zdbh7t9ysbjerNTq0457GKVINO0KHSbqxEFRbtcCG
        DnuJ1KHHugs5UrEorgoVZrJiXs4vy+W8RIe1sBEb+q+nv6ZGj6N8Ch4/CATWB0pgTatwyKTWCkOo
        65yCUXd08PZZ1wETRTABpZDqBrYyKgh6UYO8L0yKGIyg5a2kPpw9vks2uA+8e9WEliFwhNYUtpI5
        UrqAezLoCaKANcEmz6PkCxzcE61IV2UdbzAdbsxzVc7WdBVvfzRfXu7im0+vZ+v3i2aeNuiQQz/q
        zpijivfZ0J/wkCkd0ePnRqDVCf4PCMj2ifIFDsM3h2qyrxIFFf6fYmooHTJxTeg5d53DPL3Jn87b
        KpOfxIr+crF0KNn+rd1cD8NvAAAA//8DAPfUzMAFAgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b0dcbd844caf-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:55:09 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:55:07Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:55:09Z'
      request-id:
      - req_018TP5XzbuQ82VrDc4fbETo3
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "Who is the founder
      of LangChain?"}, "id": "toolu_01Se5dFjhZBgdHVD1SR3h2rY"}]}, {"role": "user",
      "content": [{"type": "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01Se5dFjhZBgdHVD1SR3h2rY",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:34.582739+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA2yUYW8bNwyG/wpxGLANOHuuk3SAv2VGsWZz0CLLgA7rENA6+k61jlQkyq5R9L8P
        lJ0sSffpAB1FPnz5Ul8a3zWLZsz93ezVm9vzv8ryfjVf9mfp5mJz8+HD+NvvTdvoIZJFUc7YU9M2
        SYIdYM4+K7I2bTNKR6FZNC5g6WhyNrmYZGEmncxn8/PZ6/msaRsnrMTaLP7+8pBU6bNdr59Fczsg
        b+EgBTaSICbZ+c5zDyPB3usAOhBIjJK0sNcDqEAmTG6o8fZ3lKxQ4kRl0qESeN5IGlG98BR+wUwd
        CNfI08VEuQTNLVyBQwYnvPEdsYYDIOc9JcNJcF8oW5LFR/7ItwPBRgp3lEA2sELulwN6Bp/hLabk
        szAsB8w0reFyasXwEkGV4bPWVK+mL25YDhaFTyVr5TwVamFdjgdOJg/FkTtYvnn3DKKWnE+fUO0x
        Q8DCbqAOPMM7p7KmBPPZfA6YARkkEk+ylOTIWD+RU1gfXqLtBx8IBqoZ95K2NhtUQBjRDZ4JAmFi
        O82KSUsEhyFQBzeytn6uWCkE3xO7ozRn01PqwThgjW7bJ2vOOLOi+qze5dqokzEWpQTZeUsAmySj
        Ie4wdfAn+x2l7PVQE59P4X3yZgo5shjUoyStNWFcpuf1iX31wK6Eo7X1P9CtaVDt5sqDl8wYpgN3
        sMPgu2o2G8k3otQlyZXv4umA7ot323CAHj1TB1FiCZjM4P7o1csr636spm8BVRO6WnQo3CXqspUz
        WyW/LiopG9ivXt+WdeVak5OxQsLedxQO0PnsSrYWVKJ3Fr/D5KVkiAHVluYI+noKVwyXMflghjlr
        n9qdnaQoCZW6aqQ6I+RDLZrQW/7se/Yb75AVNoVtn1u7GUpdbdlRgu/mMxh9CCZcneofdF/EIywx
        esXQmrKlzqYuwKmImrRZ7frj/Qr98xRu/wv8PsNouHVotl/y3PDvDzoI/3R7iJRd8lEhottiTxl0
        MHv70Ww04pbAKxDm+vJ0tKMgsepRsCe4tvECxhi8qybILUSznrNxhsdpJsIw2sQurwB7Yj36e7W6
        hh9WmHp6kfJHiLKnZBo/yV0b/fbtcMLZd8do2PnshTE9NVKOaD5+Bmav5+CPO22hzp7uo8NWq+t8
        enxFwpHUzFe7G3H7bK9AmKyxx5f4ZGXYJBzJsudaa118qNN/VGDafP2nbbJKvEuEWbhZNMTdnZbE
        zelHpvtiS9gs/lVeaU6OjlIpuDqyqlbKzCsoLYkvyc9OzStWsjIysDDQUcovLUEWNDY2q60FAAAA
        //8DAHryTTDvBgAA
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b0ec98794caf-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:55:17 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:55:10Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:55:17Z'
      request-id:
      - req_011iT4S1koK6LtTJefPZgpb9
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:53.846143+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1RRS2vcQAz+K0KXXsbBu5um6dySQCAQckiXPijFTD3y2slYsmc0JO7i/17sbUl7
        Enp8Dz4dsfNosU+HqtxsPrdPfjNcf2A3Svvl5uGXxKtLNKjTQMsVpeQOhAajhGXgUuqSOlY02Iun
        gBbr4LKnYle8L5IwkxbbcnteXmxLNFgLK7Gi/X78S6r0usDXYnEv4Di9UARtuwRjpqSdMLi6ztEp
        hcnA3bsQgIk8qEAiF+sWGlkQBL0khTwUKoV3StBxI7F3J46fkhXuHR9uWtcxOPbQaYJGMnuKZ3BP
        Cj2BF9DW6co5ST7D2by5FQlVTksGa3BLn6tyczt9uv76bV9f7X39eCnj+Hpo+hc0yK5fcCebC4qH
        rGiPOGaKE1r8ow7SvFnDef5hMKkMVSSXhP9XXheJxkxcE1rOIRjM62vs8aRQqTwTJ7TnuwuDkvXf
        2cfdPP8GAAD//wMAgzLCHfkBAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b1653bb54cb6-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:55:32 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:55:29Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:55:32Z'
      request-id:
      - req_01RU57s1YVTHykXPQ2qnPNr4
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "founder of LangChain"},
      "id": "toolu_01FySBXYTcATdcR8oqqxgfmw"}]}, {"role": "user", "content": [{"type":
      "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01FySBXYTcATdcR8oqqxgfmw",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.\n\nSystem time: 2024-11-13T23:54:56.977251+00:00",
      "tools": [{"name": "search", "description": "Search for general web results.
      This function performs a search using the Tavily search engine, which is designed\nto
      provide comprehensive, accurate, and trusted results. It''s particularly useful\nfor
      answering questions about current events.", "input_schema": {"properties": {"query":
      {"type": "string"}}, "required": ["query"], "type": "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RVbW/bRgz+K4RQYBsge4nTppu/tdnQZEnQbnWHDesQ0CdaYn3iXY48p1rR/z6c
        bDd2h30yJB3Jh8/L+VPFTTWvem3vTk6vf/1w+ePpu9D8IYvww0eJv5xd/35d1ZUNkcopUsWWqrpK
        wZcXqMpqKFbVVR8a8tW8ch5zQ5OzybOJBhGyyexk9vTkfHZS1ZULYiRWzf/6tG9q9LGUjz/z6iUq
        NRAErCNQwuQ6SKTZm9ZwBQ4FYgobbgiGkOGBrQOWVUg9GgcBXIZsY/EqZGkoQVjBDUp70SHL/L28
        l8X/fARWuMSUWIPARYdKU1gEWBL0IRHERI6VauionCwjXJjsG6E0cPHz66OGU7ikRICJQENPsKYB
        YmAx3cE8njb2OIZ6Ov36zJeRzSFwAY9GMDuZzaalbjY9XgvBhT6iDFCYogYwlSYQIslEQ06O4M1g
        XZDvF0Okty5xNIjo1thS2RUNkHuwAD2uCdiAUIfy3NCGfIjjvIwtwW3xAWCMnt2oiY6QzqbwJnFI
        pUYNk7G0jyDrr/acvxcAmMANNSPTtzdghD2gwW9hmdXgSoy855bEUQ0ocHvzOurBni7vnUQ6Div0
        btBzs3VKWEGPrmMh8IRJypHRwwXv0XASYxvAs6zLoT2QaxLtwrjc0+lOng4L2Ut063bLMEvZ1liN
        nY4QCsJslEAdF/CwSqEv628wNfBOeENJ2Yax8bNDIe8zu7UfoEUWaiCGmD2mAo23eXlxVbr3WdiG
        epuNErjEyzwKsR3VZWkSNVoY2KlHSQtTr9gu87IGdMYbgobVZdWxMggsHthsZ/WfWF1ITT0+SO4p
        hazwZ8iLvCSwbCEx+q3y51O4EngRE/vi0LP6yLkupBgSWjFl4W70Ro5j44RcNFRuhVfsUAxWWRqW
        du+P1xtK8GR2Aj17X1Rl2R8pEmERPH/RG61IrVYqHktGUjYklhPBilMPb+k+B0a4wMiGfu+HRccK
        DnuCD8WBCA9Ea8DVlhQJWdw4F56cPnZXouKCDan1VBYo016SuK7HtB4Jev6flLNCIhda4X/2vJTL
        Y8VtwfioNralJbmggxr1NcQSLFds4cesF/2F3C5sqaWvcqrw7c3NrX63NYuF4LcmLQ7YJ/cgpNu7
        7hjsN1pIbShpx7EeE7AkV+473FsUVgl7eghpPWJaZvZbhQ4uCYjhgRI1sBzAj0j9Huk2leB5TfDq
        zWLytAYSXPrS4sDAFsAlKvfgi6vJvhvGqNv7m1DZD+N2uyBNq89/15VaiHeJUINU84qkubOcpNp9
        ULrPJaXVXLL3dZXHP8D5p4olZruzsCbRaj47eX5eVyHb4cuz5+efP/8LAAD//wMAjIY6NGEHAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b178d9d04cb4-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:55:39 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:55:32Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:55:39Z'
      request-id:
      - req_015HcqrgirHzeLfD6UpmyE4r
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.", "tools": [{"name": "search", "description": "Search
      for general web results. This function performs a search using the Tavily search
      engine, which is designed\nto provide comprehensive, accurate, and trusted results.
      It''s particularly useful\nfor answering questions about current events.", "input_schema":
      {"properties": {"query": {"type": "string"}}, "required": ["query"], "type":
      "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA1SR3WrcQAyFX0Xopjfj4HWzKZm7JrTQsimhBAIpxcza8nqoLe2ONLRm8bsXe1vS
        Xgn9nKMP6YyxRY+jHupy8/jxOcfbB9me7qqc08uXd81Ee3Ro05GWKVINB0KHSYalEFSjWmBDh6O0
        NKDHZgi5peJtsS1UmMmKqqyuy5uqRIeNsBEb+m/nv6ZGvxb5Gjw+CQTWn5TA+qhwyqQWhSE0TU7B
        aJgcfHozDMBELZiAUkhND50sCoJR1CAfC5OiDUYQuZM0hovHXrLBLvDhvg+RIXAL0RQ6ydxSuoId
        GYwErYD1wVbPSfIVzu6VVmSosy43WA+35LkuN7v3U5zuuj1/eHmi/ePnr4ftw31ChxzGRXfBXFR8
        zIb+jKdMaUKPz71A1BX+DwhI90qJ8/zdoZoc60RBhf+HWBtKp0zcEHrOw+Awr1/y58uy2uQHsaK/
        3lQOJdu/tdubef4NAAD//wMAEAehhwQCAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b64a18a54caf-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:58:52 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:58:49Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:58:52Z'
      request-id:
      - req_01JfEnoQAvmWG3PFoZTmVtmb
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
- request:
    body: '{"max_tokens": 1024, "messages": [{"role": "user", "content": "Who is the
      founder of LangChain?"}, {"role": "assistant", "content": [{"text": "To answer
      this question accurately, I''ll need to search for the most up-to-date information
      about LangChain and its founder. Let me do that for you.", "type": "text"},
      {"type": "tool_use", "name": "search", "input": {"query": "Who is the founder
      of LangChain"}, "id": "toolu_01LAyiyBfbnEZTebPJRg5MCr"}]}, {"role": "user",
      "content": [{"type": "tool_result", "content": "[{\"url\": \"https://sfelc.com/speaker/harrison-chase\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://aiconference.com/speakers/harrison-chase/\",
      \"content\": \"Harrison Chase is the co-founder and CEO of LangChain, a company
      formed around the open-source Python/Typescript packages that aim to make it
      easy to develop Language Model applications. Prior to starting LangChain, he
      led the ML team at Robust Intelligence (an MLOps company focused on testing
      and validation of machine learning models), led the\"}, {\"url\": \"https://tedai-sanfrancisco.ted.com/speakers-1/harrison-chase-/co-founder-and-ceo,-langchain/\",
      \"content\": \"Harrison Chase, a Harvard graduate in statistics and computer
      science, co-founded LangChain to streamline the development of Language Model
      applications with open-source Python/Typescript packages. Chase''s experience
      includes heading the Machine Learning team at Robust Intelligence, focusing
      on the testing and validation of machine learning models, and leading the entity
      linking team at Kensho\"}, {\"url\": \"https://techcrunch.com/author/harrison-chase/\",
      \"content\": \"Harrison Chase is the CEO and co-founder of LangChain, a company
      formed around the open source Python/Typescript packages that aim to make it
      easy to develop Language Model applications\"}, {\"url\": \"https://www.sequoiacap.com/podcast/training-data-harrison-chase/\",
      \"content\": \"Sonya Huang: Hi, and welcome to training data. We have with us
      today Harrison Chase, founder and CEO of LangChain. Harrison is a legend in
      the agent ecosystem, as the product visionary who first connected LLMs with
      tools and actions. And LangChain is the most popular agent building framework
      in the AI space.\"}, {\"url\": \"https://www.youtube.com/watch?v=7D8bw_4hTdo\",
      \"content\": \"Join us for an insightful interview with Harrison Chase, the
      CEO and co-founder of Langchain, as he provides a comprehensive overview of
      Langchain''s innovati\"}, {\"url\": \"https://www.forbes.com/profile/harrison-chase/\",
      \"content\": \"Harrison Chase only cofounded LangChain in late 2022, but the
      company caught instant attention for enabling anyone to build apps powered by
      large language models like GPT-4 in as little as two\"}, {\"url\": \"https://en.wikipedia.org/wiki/LangChain\",
      \"content\": \"In October 2023 LangChain introduced LangServe, a deployment
      tool designed to facilitate the transition from LCEL (LangChain Expression Language)
      prototypes to production-ready applications.[5]\\nIntegrations[edit]\\nAs of
      March 2023, LangChain included integrations with systems including Amazon, Google,
      and Microsoft Azure cloud storage; API wrappers for news, movie information,
      and weather; Bash for summarization, syntax and semantics checking, and execution
      of shell scripts; multiple web scraping subsystems and templates; few-shot learning
      prompt generation support; finding and summarizing \\\"todo\\\" tasks in code;
      Google Drive documents, spreadsheets, and presentations summarization, extraction,
      and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic,
      and Hugging Face language models; iFixit repair guides and wikis search and
      summarization; MapReduce for question answering, combining documents, and question
      generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF
      file text extraction and manipulation; Python and JavaScript code generation,
      analysis, and debugging; Milvus vector database[6] to store and retrieve vector
      embeddings; Weaviate vector database[7] to cache embedding and data objects;
      Redis cache database storage; Python RequestsWrapper and other methods for API
      requests; SQL and NoSQL databases including JSON support; Streamlit, including
      for logging; text mapping for k-nearest neighbors search; time zone conversion
      and calendar operations; tracing and recording stack symbols in threaded and
      asynchronous subprocess runs; and the Wolfram Alpha website and SDK.[8] As a
      language model integration framework, LangChain''s use-cases largely overlap
      with those of language models in general, including document analysis and summarization,
      chatbots, and code analysis.[2]\\nHistory[edit]\\nLangChain was launched in
      October 2022 as an open source project by Harrison Chase, while working at machine
      learning startup Robust Intelligence. In April 2023, LangChain had incorporated
      and the new startup raised over $20 million in funding at a valuation of at
      least $200 million from venture firm Sequoia Capital, a week after announcing
      a $10 million seed investment from Benchmark.[3][4]\\n The project quickly garnered
      popularity, with improvements from hundreds of contributors on GitHub, trending
      discussions on Twitter, lively activity on the project''s Discord server, many
      YouTube tutorials, and meetups in San Francisco and London. As of April 2023,
      it can read from more than 50 document types and data sources.[9]\\nReferences[edit]\\nExternal
      links[edit]\"}, {\"url\": \"https://analyticsindiamag.com/people/harrison-chase/\",
      \"content\": \"By AIM The dynamic co-founder and CEO of LangChain, Harrison
      Chase is simplifying the creation of applications powered by LLMs. With a background
      in statistics and computer science from Harvard University, Chase has carved
      a niche in the AI landscape. AIM Brand Solutions, a marketing division within
      AIM, specializes in creating diverse content such as documentaries, public artworks,
      podcasts, videos, articles, and more to effectively tell compelling stories.
      AIM Research produces a series of annual reports on AI & Data Science covering
      every aspect of the industry. Discover how Cypher 2024 expands to the USA, bridging
      AI innovation gaps and tackling the challenges of enterprise AI adoption AIM
      India AIM Research AIM Leaders Council 50 Best Data Science Firms\"}, {\"url\":
      \"https://www.turingpost.com/p/harrison-chase-langchain-ai-agents\", \"content\":
      \"Harrison Chase, founder of LangChain, shared insights on the evolution of
      AI agents and their applications during Sequoia Capital''s AI Ascent. ... Saves
      you a lot of research time, plus gives a flashback to ML history and insights
      into the future. Stay ahead alongside over 73,000 professionals from top AI
      labs, ML startups, and enterprises\"}]", "tool_use_id": "toolu_01LAyiyBfbnEZTebPJRg5MCr",
      "is_error": false}]}], "model": "claude-3-5-sonnet-20240620", "system": "You
      are a helpful AI assistant.", "tools": [{"name": "search", "description": "Search
      for general web results. This function performs a search using the Tavily search
      engine, which is designed\nto provide comprehensive, accurate, and trusted results.
      It''s particularly useful\nfor answering questions about current events.", "input_schema":
      {"properties": {"query": {"type": "string"}}, "required": ["query"], "type":
      "object"}}]}'
    headers: {}
    method: POST
    uri: https://api.anthropic.com/v1/messages
  response:
    body:
      string: !!binary |
        H4sIAAAAAAAAA3RU0W4bRwz8FeIQoC1wEmQ5KVC/2UZQu5DQNEmf6iKg9nh3jPa45yVPqhD03wuu
        LEdx26cFdpfkcGbILxU31VU1aPdpcbFu9q8vfnv/y0/j5frx1j7vbldv391UdWWHkfwXqWJHVV3l
        FP0CVVkNxaq6GlJDsbqqQsSpodnl7M1MkwjZbLlYvl78uFxUdRWSGIlVV398OSU1+svDy3FV3aBS
        A0nAegIlzKGHTDpF0xruIaBASNJyQ2LxACi6pwyHNGV4nEiNkwBu0mQlQZsmaShDamGF0t32yDJ/
        kAf5+D+PwAp3mDNrErjtUWkO65QJdKTALQeM8XDlGS7mLz56qNcMaXbKjNLA7dtfX5ZfzuHuVL45
        ry0Q0QiWi+WyBhY29nKACiiQRpKZpikHgjGnzxQM9j1Hgn3KW5YO0ABhwNCzEETCLH6rhtmmERw6
        NfA+bSY1uBejGLkjCTR/kMv5GY49KoRMaNSAJRhwS8AGhMqU/aahHcU0Ao5j5IBOusKY9pSpgc0B
        Vpg7Kgkn7AjW7gyF71ertf5Q6P+QBgJsGvZQjMBilF096aDFYPqk4QuGnc9nmCcVjk+9kwQbDNsu
        O7FOphoaq3HQEhnSME5GGTSwtw1tToOX2GFu4HfhHWVlOxwVepc5lWYLfw7suXINvfPbFLnXT3yv
        Tnwb4eBS/AfRNbQpTOq/3OFPDTu2HUZuCpFuln9pWGZLX+r0OHHYxgN0yEINjGmcIma2gzfv2K7v
        velhErZDDXu23ofHMm+mo2aFgX6SJlOjXvlJWcrqCH9mu5s2BSAG4x1Bwxom1RKcBHaYOU0KY0Rr
        Ux4c4us53Atcj5mjO/my/sbhIeUx5WKtopiLgnIoNTKyD79yJ2XUxKCdpGHp5vCxpwNkCsQ73w87
        yvBquYCBY3TSWE5fj1Owwzg904nmTKp5xNeQ0vsHepwSI9ziyIaxBu1TLoultTLAkiYJJS28uvga
        rERusR2pDeQ4PdkNSegHzNv5g7w5F8rNuaHgpk9CDsnVGZLaSTTAjsRmm4ljaaLNOJDPtZ5pqSO6
        h7aS9gJtyi6mUDiac7XWo8KWUtRnzZJombhvJ+k7hR27iseQl0hJivF5GCO3h1J+zCmQFo+U1eA1
        r+9np6E/3wS1bwx/Z4PBdycGD+VNpLPl4RZzkBufB1IlBYy8pXn19591pZbGT/kfpSYW5+cpWSml
        5qXEl5QW5SlBJYpTC0tB+UnJKq80J0dHqRRcK1lVK2XmFZSWxJfkZ6fmFStZGRmYmuoo5ZeWIAsa
        mxjW1gIAAAD//wMALzV23/YGAAA=
    headers:
      CF-Cache-Status:
      - DYNAMIC
      CF-RAY:
      - 8e22b65b6bda4caf-PHL
      Connection:
      - keep-alive
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json
      Date:
      - Wed, 13 Nov 2024 23:58:59 GMT
      Server:
      - cloudflare
      Transfer-Encoding:
      - chunked
      X-Robots-Tag:
      - none
      anthropic-ratelimit-requests-limit:
      - '4000'
      anthropic-ratelimit-requests-remaining:
      - '3999'
      anthropic-ratelimit-requests-reset:
      - '2024-11-13T23:58:52Z'
      anthropic-ratelimit-tokens-limit:
      - '400000'
      anthropic-ratelimit-tokens-remaining:
      - '400000'
      anthropic-ratelimit-tokens-reset:
      - '2024-11-13T23:58:59Z'
      request-id:
      - req_013QF9W14BVn6VHs6PZwfFs7
      via:
      - 1.1 google
    status:
      code: 200
      message: OK
version: 1



================================================
FILE: tests/integration_tests/__init__.py
================================================
"""Define any integration tests you want in this directory."""



================================================
FILE: tests/integration_tests/test_graph.py
================================================
import pytest
from langsmith import unit

from react_agent import graph


@pytest.mark.asyncio
@unit
async def test_react_agent_simple_passthrough() -> None:
    res = await graph.ainvoke(
        {"messages": [("user", "Who is the founder of LangChain?")]},
        {"configurable": {"system_prompt": "You are a helpful AI assistant."}},
    )

    assert "harrison" in str(res["messages"][-1].content).lower()



================================================
FILE: tests/unit_tests/__init__.py
================================================
"""Define any unit tests you may want in this directory."""



================================================
FILE: tests/unit_tests/test_configuration.py
================================================
from react_agent.configuration import Configuration


def test_configuration_empty() -> None:
    Configuration.from_runnable_config({})



================================================
FILE: .github/workflows/integration-tests.yml
================================================
# This workflow will run integration tests for the current project once per day

name: Integration Tests

on:
  schedule:
    - cron: "37 14 * * *" # Run at 7:37 AM Pacific Time (14:37 UTC) every day
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another scheduled run starts while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  integration-tests:
    name: Integration Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
          uv pip install -U pytest-asyncio vcrpy
      - name: Run integration tests
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_TRACING: true
          LANGSMITH_TEST_CACHE: tests/cassettes
        run: |
          uv run pytest tests/integration_tests



================================================
FILE: .github/workflows/unit-tests.yml
================================================
# This workflow will run unit tests for the current project

name: CI

on:
  push:
    branches: ["main"]
  pull_request:
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
      - name: Lint with ruff
        run: |
          uv pip install ruff
          uv run ruff check .
      - name: Lint with mypy
        run: |
          uv pip install mypy
          uv run mypy --strict src/
      - name: Check README spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: README.md
      - name: Check code spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: src/
      - name: Run tests with pytest
        run: |
          uv pip install pytest
          uv run pytest tests/unit_tests


