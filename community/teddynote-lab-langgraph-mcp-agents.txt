Directory structure:
â””â”€â”€ teddynote-lab-langgraph-mcp-agents/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ app.py
    â”œâ”€â”€ app_KOR.py
    â”œâ”€â”€ config.json
    â”œâ”€â”€ example_config.json
    â”œâ”€â”€ MCP-HandsOn-ENG.ipynb
    â”œâ”€â”€ MCP-HandsOn-KOR.ipynb
    â”œâ”€â”€ mcp_server_local.py
    â”œâ”€â”€ mcp_server_rag.py
    â”œâ”€â”€ mcp_server_remote.py
    â”œâ”€â”€ mcp_server_time.py
    â”œâ”€â”€ packages.txt
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ README_KOR.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ assets/
    â””â”€â”€ dockers/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ docker-compose-KOR-mac.yaml
        â”œâ”€â”€ docker-compose-KOR.yaml
        â”œâ”€â”€ docker-compose-mac.yaml
        â”œâ”€â”€ docker-compose.yaml
        â””â”€â”€ .env.example

================================================
FILE: README.md
================================================
# LangGraph Agents + MCP

[![English](https://img.shields.io/badge/Language-English-blue)](README.md) [![Korean](https://img.shields.io/badge/Language-í•œêµ­ì–´-red)](README_KOR.md)

[![GitHub](https://img.shields.io/badge/GitHub-langgraph--mcp--agents-black?logo=github)](https://github.com/teddylee777/langgraph-mcp-agents)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-â‰¥3.12-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-0.1.0-orange)](https://github.com/teddylee777/langgraph-mcp-agents)

![project demo](./assets/project-demo.png)

## Project Overview

![project architecture](./assets/architecture.png)

`LangChain-MCP-Adapters` is a toolkit provided by **LangChain AI** that enables AI agents to interact with external tools and data sources through the Model Context Protocol (MCP). This project provides a user-friendly interface for deploying ReAct agents that can access various data sources and APIs through MCP tools.

### Features

- **Streamlit Interface**: A user-friendly web interface for interacting with LangGraph `ReAct Agent` with MCP tools
- **Tool Management**: Add, remove, and configure MCP tools through the UI (Smithery JSON format supported). This is done dynamically without restarting the application
- **Streaming Responses**: View agent responses and tool calls in real-time
- **Conversation History**: Track and manage conversations with the agent

## MCP Architecture

The Model Context Protocol (MCP) consists of three main components:

1. **MCP Host**: Programs seeking to access data through MCP, such as Claude Desktop, IDEs, or LangChain/LangGraph.

2. **MCP Client**: A protocol client that maintains a 1:1 connection with the server, acting as an intermediary between the host and server.

3. **MCP Server**: A lightweight program that exposes specific functionalities through a standardized model context protocol, serving as the primary data source.

## Quick Start with Docker

You can easily run this project using Docker without setting up a local Python environment.

### Requirements (Docker Desktop)

Install Docker Desktop from the link below:

- [Install Docker Desktop](https://www.docker.com/products/docker-desktop/)

### Run with Docker Compose

1. Navigate to the `dockers` directory

```bash
cd dockers
```

2. Create a `.env` file with your API keys in the project root directory.

```bash
cp .env.example .env
```

Enter your obtained API keys in the `.env` file.

(Note) Not all API keys are required. Only enter the ones you need.
- `ANTHROPIC_API_KEY`: If you enter an Anthropic API key, you can use "claude-3-7-sonnet-latest", "claude-3-5-sonnet-latest", "claude-3-haiku-latest" models.
- `OPENAI_API_KEY`: If you enter an OpenAI API key, you can use "gpt-4o", "gpt-4o-mini" models.
- `LANGSMITH_API_KEY`: If you enter a LangSmith API key, you can use LangSmith tracing.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENAI_API_KEY=your_openai_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=LangGraph-MCP-Agents
```

When using the login feature, set `USE_LOGIN` to `true` and enter `USER_ID` and `USER_PASSWORD`.

```bash
USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin123
```

If you don't want to use the login feature, set `USE_LOGIN` to `false`.

```bash
USE_LOGIN=false
```

3. Select the Docker Compose file that matches your system architecture.

**AMD64/x86_64 Architecture (Intel/AMD Processors)**

```bash
# Run container
docker compose -f docker-compose.yaml up -d
```

**ARM64 Architecture (Apple Silicon M1/M2/M3/M4)**

```bash
# Run container
docker compose -f docker-compose-mac.yaml up -d
```

4. Access the application in your browser at http://localhost:8585

(Note)
- If you need to modify ports or other settings, edit the docker-compose.yaml file before building.

## Install Directly from Source Code

1. Clone this repository

```bash
git clone https://github.com/teddynote-lab/langgraph-mcp-agents.git
cd langgraph-mcp-agents
```

2. Create a virtual environment and install dependencies using uv

```bash
uv venv
uv pip install -r requirements.txt
source .venv/bin/activate  # For Windows: .venv\Scripts\activate
```

3. Create a `.env` file with your API keys (copy from `.env.example`)

```bash
cp .env.example .env
```

Enter your obtained API keys in the `.env` file.

(Note) Not all API keys are required. Only enter the ones you need.
- `ANTHROPIC_API_KEY`: If you enter an Anthropic API key, you can use "claude-3-7-sonnet-latest", "claude-3-5-sonnet-latest", "claude-3-haiku-latest" models.
- `OPENAI_API_KEY`: If you enter an OpenAI API key, you can use "gpt-4o", "gpt-4o-mini" models.
- `LANGSMITH_API_KEY`: If you enter a LangSmith API key, you can use LangSmith tracing.
```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENAI_API_KEY=your_openai_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=LangGraph-MCP-Agents
```

4. (New) Use the login/logout feature

When using the login feature, set `USE_LOGIN` to `true` and enter `USER_ID` and `USER_PASSWORD`.

```bash
USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin123
```

If you don't want to use the login feature, set `USE_LOGIN` to `false`.

```bash
USE_LOGIN=false
```

## Usage

1. Start the Streamlit application.

```bash
streamlit run app.py
```

2. The application will run in the browser and display the main interface.

3. Use the sidebar to add and configure MCP tools

Visit [Smithery](https://smithery.ai/) to find useful MCP servers.

First, select the tool you want to use.

Click the COPY button in the JSON configuration on the right.

![copy from Smithery](./assets/smithery-copy-json.png)

Paste the copied JSON string in the `Tool JSON` section.

<img src="./assets/add-tools.png" alt="tool json" style="width: auto; height: auto;">

Click the `Add Tool` button to add it to the "Registered Tools List" section.

Finally, click the "Apply" button to apply the changes to initialize the agent with the new tools.

<img src="./assets/apply-tool-configuration.png" alt="tool json" style="width: auto; height: auto;">

4. Check the agent's status.

![check status](./assets/check-status.png)

5. Interact with the ReAct agent that utilizes the configured MCP tools by asking questions in the chat interface.

![project demo](./assets/project-demo.png)

## Hands-on Tutorial

For developers who want to learn more deeply about how MCP and LangGraph integration works, we provide a comprehensive Jupyter notebook tutorial:

- Link: [MCP-HandsOn-KOR.ipynb](./MCP-HandsOn-KOR.ipynb)

This hands-on tutorial covers:

1. **MCP Client Setup** - Learn how to configure and initialize the MultiServerMCPClient to connect to MCP servers
2. **Local MCP Server Integration** - Connect to locally running MCP servers via SSE and Stdio methods
3. **RAG Integration** - Access retriever tools using MCP for document retrieval capabilities
4. **Mixed Transport Methods** - Combine different transport protocols (SSE and Stdio) in a single agent
5. **LangChain Tools + MCP** - Integrate native LangChain tools alongside MCP tools

This tutorial provides practical examples with step-by-step explanations that help you understand how to build and integrate MCP tools into LangGraph agents.

## License

MIT License 

## References

- https://github.com/langchain-ai/langchain-mcp-adapters


================================================
FILE: app.py
================================================
import streamlit as st
import asyncio
import nest_asyncio
import json
import os
import platform

if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# Apply nest_asyncio: Allow nested calls within an already running event loop
nest_asyncio.apply()

# Create and reuse global event loop (create once and continue using)
if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)

from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from utils import astream_graph, random_uuid
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# Load environment variables (get API keys and settings from .env file)
load_dotenv(override=True)

# config.json file path setting
CONFIG_FILE_PATH = "config.json"

# Function to load settings from JSON file
def load_config_from_json():
    """
    Loads settings from config.json file.
    Creates a file with default settings if it doesn't exist.

    Returns:
        dict: Loaded settings
    """
    default_config = {
        "get_current_time": {
            "command": "python",
            "args": ["./mcp_server_time.py"],
            "transport": "stdio"
        }
    }
    
    try:
        if os.path.exists(CONFIG_FILE_PATH):
            with open(CONFIG_FILE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # Create file with default settings if it doesn't exist
            save_config_to_json(default_config)
            return default_config
    except Exception as e:
        st.error(f"Error loading settings file: {str(e)}")
        return default_config

# Function to save settings to JSON file
def save_config_to_json(config):
    """
    Saves settings to config.json file.

    Args:
        config (dict): Settings to save
    
    Returns:
        bool: Save success status
    """
    try:
        with open(CONFIG_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        st.error(f"Error saving settings file: {str(e)}")
        return False

# Initialize login session variables
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# Check if login is required
use_login = os.environ.get("USE_LOGIN", "false").lower() == "true"

# Change page settings based on login status
if use_login and not st.session_state.authenticated:
    # Login page uses default (narrow) layout
    st.set_page_config(page_title="Agent with MCP Tools", page_icon="ğŸ§ ")
else:
    # Main app uses wide layout
    st.set_page_config(page_title="Agent with MCP Tools", page_icon="ğŸ§ ", layout="wide")

# Display login screen if login feature is enabled and not yet authenticated
if use_login and not st.session_state.authenticated:
    st.title("ğŸ” Login")
    st.markdown("Login is required to use the system.")

    # Place login form in the center of the screen with narrow width
    with st.form("login_form"):
        username = st.text_input("Username")
        password = st.text_input("Password", type="password")
        submit_button = st.form_submit_button("Login")

        if submit_button:
            expected_username = os.environ.get("USER_ID")
            expected_password = os.environ.get("USER_PASSWORD")

            if username == expected_username and password == expected_password:
                st.session_state.authenticated = True
                st.success("âœ… Login successful! Please wait...")
                st.rerun()
            else:
                st.error("âŒ Username or password is incorrect.")

    # Don't display the main app on the login screen
    st.stop()

# Add author information at the top of the sidebar (placed before other sidebar elements)
st.sidebar.markdown("### âœï¸ Made by [TeddyNote](https://youtube.com/c/teddynote) ğŸš€")
st.sidebar.markdown(
    "### ğŸ’» [Project Page](https://github.com/teddynote-lab/langgraph-mcp-agents)"
)

st.sidebar.divider()  # Add divider

# Existing page title and description
st.title("ğŸ’¬ MCP Tool Utilization Agent")
st.markdown("âœ¨ Ask questions to the ReAct agent that utilizes MCP tools.")

SYSTEM_PROMPT = """<ROLE>
You are a smart agent with an ability to use tools. 
You will be given a question and you will use the tools to answer the question.
Pick the most relevant tool to answer the question. 
If you are failed to answer the question, try different tools to get context.
Your answer should be very polite and professional.
</ROLE>

----

<INSTRUCTIONS>
Step 1: Analyze the question
- Analyze user's question and final goal.
- If the user's question is consist of multiple sub-questions, split them into smaller sub-questions.

Step 2: Pick the most relevant tool
- Pick the most relevant tool to answer the question.
- If you are failed to answer the question, try different tools to get context.

Step 3: Answer the question
- Answer the question in the same language as the question.
- Your answer should be very polite and professional.

Step 4: Provide the source of the answer(if applicable)
- If you've used the tool, provide the source of the answer.
- Valid sources are either a website(URL) or a document(PDF, etc).

Guidelines:
- If you've used the tool, your answer should be based on the tool's output(tool's output is more important than your own knowledge).
- If you've used the tool, and the source is valid URL, provide the source(URL) of the answer.
- Skip providing the source if the source is not URL.
- Answer in the same language as the question.
- Answer should be concise and to the point.
- Avoid response your output with any other information than the answer and the source.  
</INSTRUCTIONS>

----

<OUTPUT_FORMAT>
(concise answer to the question)

**Source**(if applicable)
- (source1: valid URL)
- (source2: valid URL)
- ...
</OUTPUT_FORMAT>
"""

OUTPUT_TOKEN_INFO = {
    "claude-3-5-sonnet-latest": {"max_tokens": 8192},
    "claude-3-5-haiku-latest": {"max_tokens": 8192},
    "claude-3-7-sonnet-latest": {"max_tokens": 64000},
    "gpt-4o": {"max_tokens": 16000},
    "gpt-4o-mini": {"max_tokens": 16000},
}

# Initialize session state
if "session_initialized" not in st.session_state:
    st.session_state.session_initialized = False  # Session initialization flag
    st.session_state.agent = None  # Storage for ReAct agent object
    st.session_state.history = []  # List for storing conversation history
    st.session_state.mcp_client = None  # Storage for MCP client object
    st.session_state.timeout_seconds = (
        120  # Response generation time limit (seconds), default 120 seconds
    )
    st.session_state.selected_model = (
        "claude-3-7-sonnet-latest"  # Default model selection
    )
    st.session_state.recursion_limit = 100  # Recursion call limit, default 100

if "thread_id" not in st.session_state:
    st.session_state.thread_id = random_uuid()


# --- Function Definitions ---


async def cleanup_mcp_client():
    """
    Safely terminates the existing MCP client.

    Properly releases resources if an existing client exists.
    """
    if "mcp_client" in st.session_state and st.session_state.mcp_client is not None:
        try:

            await st.session_state.mcp_client.__aexit__(None, None, None)
            st.session_state.mcp_client = None
        except Exception as e:
            import traceback

            # st.warning(f"Error while terminating MCP client: {str(e)}")
            # st.warning(traceback.format_exc())


def print_message():
    """
    Displays chat history on the screen.

    Distinguishes between user and assistant messages on the screen,
    and displays tool call information within the assistant message container.
    """
    i = 0
    while i < len(st.session_state.history):
        message = st.session_state.history[i]

        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(message["content"])
            i += 1
        elif message["role"] == "assistant":
            # Create assistant message container
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                # Display assistant message content
                st.markdown(message["content"])

                # Check if the next message is tool call information
                if (
                    i + 1 < len(st.session_state.history)
                    and st.session_state.history[i + 1]["role"] == "assistant_tool"
                ):
                    # Display tool call information in the same container as an expander
                    with st.expander("ğŸ”§ Tool Call Information", expanded=False):
                        st.markdown(st.session_state.history[i + 1]["content"])
                    i += 2  # Increment by 2 as we processed two messages together
                else:
                    i += 1  # Increment by 1 as we only processed a regular message
        else:
            # Skip assistant_tool messages as they are handled above
            i += 1


def get_streaming_callback(text_placeholder, tool_placeholder):
    """
    Creates a streaming callback function.

    This function creates a callback function to display responses generated from the LLM in real-time.
    It displays text responses and tool call information in separate areas.

    Args:
        text_placeholder: Streamlit component to display text responses
        tool_placeholder: Streamlit component to display tool call information

    Returns:
        callback_func: Streaming callback function
        accumulated_text: List to store accumulated text responses
        accumulated_tool: List to store accumulated tool call information
    """
    accumulated_text = []
    accumulated_tool = []

    def callback_func(message: dict):
        nonlocal accumulated_text, accumulated_tool
        message_content = message.get("content", None)

        if isinstance(message_content, AIMessageChunk):
            content = message_content.content
            # If content is in list form (mainly occurs in Claude models)
            if isinstance(content, list) and len(content) > 0:
                message_chunk = content[0]
                # Process text type
                if message_chunk["type"] == "text":
                    accumulated_text.append(message_chunk["text"])
                    text_placeholder.markdown("".join(accumulated_text))
                # Process tool use type
                elif message_chunk["type"] == "tool_use":
                    if "partial_json" in message_chunk:
                        accumulated_tool.append(message_chunk["partial_json"])
                    else:
                        tool_call_chunks = message_content.tool_call_chunks
                        tool_call_chunk = tool_call_chunks[0]
                        accumulated_tool.append(
                            "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                        )
                    with tool_placeholder.expander(
                        "ğŸ”§ Tool Call Information", expanded=True
                    ):
                        st.markdown("".join(accumulated_tool))
            # Process if tool_calls attribute exists (mainly occurs in OpenAI models)
            elif (
                hasattr(message_content, "tool_calls")
                and message_content.tool_calls
                and len(message_content.tool_calls[0]["name"]) > 0
            ):
                tool_call_info = message_content.tool_calls[0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander(
                    "ğŸ”§ Tool Call Information", expanded=True
                ):
                    st.markdown("".join(accumulated_tool))
            # Process if content is a simple string
            elif isinstance(content, str):
                accumulated_text.append(content)
                text_placeholder.markdown("".join(accumulated_text))
            # Process if invalid tool call information exists
            elif (
                hasattr(message_content, "invalid_tool_calls")
                and message_content.invalid_tool_calls
            ):
                tool_call_info = message_content.invalid_tool_calls[0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander(
                    "ğŸ”§ Tool Call Information (Invalid)", expanded=True
                ):
                    st.markdown("".join(accumulated_tool))
            # Process if tool_call_chunks attribute exists
            elif (
                hasattr(message_content, "tool_call_chunks")
                and message_content.tool_call_chunks
            ):
                tool_call_chunk = message_content.tool_call_chunks[0]
                accumulated_tool.append(
                    "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                )
                with tool_placeholder.expander(
                    "ğŸ”§ Tool Call Information", expanded=True
                ):
                    st.markdown("".join(accumulated_tool))
            # Process if tool_calls exists in additional_kwargs (supports various model compatibility)
            elif (
                hasattr(message_content, "additional_kwargs")
                and "tool_calls" in message_content.additional_kwargs
            ):
                tool_call_info = message_content.additional_kwargs["tool_calls"][0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander(
                    "ğŸ”§ Tool Call Information", expanded=True
                ):
                    st.markdown("".join(accumulated_tool))
        # Process if it's a tool message (tool response)
        elif isinstance(message_content, ToolMessage):
            accumulated_tool.append(
                "\n```json\n" + str(message_content.content) + "\n```\n"
            )
            with tool_placeholder.expander("ğŸ”§ Tool Call Information", expanded=True):
                st.markdown("".join(accumulated_tool))
        return None

    return callback_func, accumulated_text, accumulated_tool


async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=60):
    """
    Processes user questions and generates responses.

    This function passes the user's question to the agent and streams the response in real-time.
    Returns a timeout error if the response is not completed within the specified time.

    Args:
        query: Text of the question entered by the user
        text_placeholder: Streamlit component to display text responses
        tool_placeholder: Streamlit component to display tool call information
        timeout_seconds: Response generation time limit (seconds)

    Returns:
        response: Agent's response object
        final_text: Final text response
        final_tool: Final tool call information
    """
    try:
        if st.session_state.agent:
            streaming_callback, accumulated_text_obj, accumulated_tool_obj = (
                get_streaming_callback(text_placeholder, tool_placeholder)
            )
            try:
                response = await asyncio.wait_for(
                    astream_graph(
                        st.session_state.agent,
                        {"messages": [HumanMessage(content=query)]},
                        callback=streaming_callback,
                        config=RunnableConfig(
                            recursion_limit=st.session_state.recursion_limit,
                            thread_id=st.session_state.thread_id,
                        ),
                    ),
                    timeout=timeout_seconds,
                )
            except asyncio.TimeoutError:
                error_msg = f"â±ï¸ Request time exceeded {timeout_seconds} seconds. Please try again later."
                return {"error": error_msg}, error_msg, ""

            final_text = "".join(accumulated_text_obj)
            final_tool = "".join(accumulated_tool_obj)
            return response, final_text, final_tool
        else:
            return (
                {"error": "ğŸš« Agent has not been initialized."},
                "ğŸš« Agent has not been initialized.",
                "",
            )
    except Exception as e:
        import traceback

        error_msg = f"âŒ Error occurred during query processing: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""


async def initialize_session(mcp_config=None):
    """
    Initializes MCP session and agent.

    Args:
        mcp_config: MCP tool configuration information (JSON). Uses default settings if None

    Returns:
        bool: Initialization success status
    """
    with st.spinner("ğŸ”„ Connecting to MCP server..."):
        # First safely clean up existing client
        await cleanup_mcp_client()

        if mcp_config is None:
            # Load settings from config.json file
            mcp_config = load_config_from_json()
        client = MultiServerMCPClient(mcp_config)
        await client.__aenter__()
        tools = client.get_tools()
        st.session_state.tool_count = len(tools)
        st.session_state.mcp_client = client

        # Initialize appropriate model based on selection
        selected_model = st.session_state.selected_model

        if selected_model in [
            "claude-3-7-sonnet-latest",
            "claude-3-5-sonnet-latest",
            "claude-3-5-haiku-latest",
        ]:
            model = ChatAnthropic(
                model=selected_model,
                temperature=0.1,
                max_tokens=OUTPUT_TOKEN_INFO[selected_model]["max_tokens"],
            )
        else:  # Use OpenAI model
            model = ChatOpenAI(
                model=selected_model,
                temperature=0.1,
                max_tokens=OUTPUT_TOKEN_INFO[selected_model]["max_tokens"],
            )
        agent = create_react_agent(
            model,
            tools,
            checkpointer=MemorySaver(),
            prompt=SYSTEM_PROMPT,
        )
        st.session_state.agent = agent
        st.session_state.session_initialized = True
        return True


# --- Sidebar: System Settings Section ---
with st.sidebar:
    st.subheader("âš™ï¸ System Settings")

    # Model selection feature
    # Create list of available models
    available_models = []

    # Check Anthropic API key
    has_anthropic_key = os.environ.get("ANTHROPIC_API_KEY") is not None
    if has_anthropic_key:
        available_models.extend(
            [
                "claude-3-7-sonnet-latest",
                "claude-3-5-sonnet-latest",
                "claude-3-5-haiku-latest",
            ]
        )

    # Check OpenAI API key
    has_openai_key = os.environ.get("OPENAI_API_KEY") is not None
    if has_openai_key:
        available_models.extend(["gpt-4o", "gpt-4o-mini"])

    # Display message if no models are available
    if not available_models:
        st.warning(
            "âš ï¸ API keys are not configured. Please add ANTHROPIC_API_KEY or OPENAI_API_KEY to your .env file."
        )
        # Add Claude model as default (to show UI even without keys)
        available_models = ["claude-3-7-sonnet-latest"]

    # Model selection dropdown
    previous_model = st.session_state.selected_model
    st.session_state.selected_model = st.selectbox(
        "ğŸ¤– Select model to use",
        options=available_models,
        index=(
            available_models.index(st.session_state.selected_model)
            if st.session_state.selected_model in available_models
            else 0
        ),
        help="Anthropic models require ANTHROPIC_API_KEY and OpenAI models require OPENAI_API_KEY to be set as environment variables.",
    )

    # Notify when model is changed and session needs to be reinitialized
    if (
        previous_model != st.session_state.selected_model
        and st.session_state.session_initialized
    ):
        st.warning(
            "âš ï¸ Model has been changed. Click 'Apply Settings' button to apply changes."
        )

    # Add timeout setting slider
    st.session_state.timeout_seconds = st.slider(
        "â±ï¸ Response generation time limit (seconds)",
        min_value=60,
        max_value=300,
        value=st.session_state.timeout_seconds,
        step=10,
        help="Set the maximum time for the agent to generate a response. Complex tasks may require more time.",
    )

    st.session_state.recursion_limit = st.slider(
        "â±ï¸ Recursion call limit (count)",
        min_value=10,
        max_value=200,
        value=st.session_state.recursion_limit,
        step=10,
        help="Set the recursion call limit. Setting too high a value may cause memory issues.",
    )

    st.divider()  # Add divider

    # Tool settings section
    st.subheader("ğŸ”§ Tool Settings")

    # Manage expander state in session state
    if "mcp_tools_expander" not in st.session_state:
        st.session_state.mcp_tools_expander = False

    # MCP tool addition interface
    with st.expander("ğŸ§° Add MCP Tools", expanded=st.session_state.mcp_tools_expander):
        # Load settings from config.json file
        loaded_config = load_config_from_json()
        default_config_text = json.dumps(loaded_config, indent=2, ensure_ascii=False)
        
        # Create pending config based on existing mcp_config_text if not present
        if "pending_mcp_config" not in st.session_state:
            try:
                st.session_state.pending_mcp_config = loaded_config
            except Exception as e:
                st.error(f"Failed to set initial pending config: {e}")

        # UI for adding individual tools
        st.subheader("Add Tool(JSON format)")
        st.markdown(
            """
        Please insert **ONE tool** in JSON format.

        [How to Set Up?](https://teddylee777.notion.site/MCP-Tool-Setup-Guide-English-1d324f35d1298030a831dfb56045906a)

        âš ï¸ **Important**: JSON must be wrapped in curly braces (`{}`).
        """
        )

        # Provide clearer example
        example_json = {
            "github": {
                "command": "npx",
                "args": [
                    "-y",
                    "@smithery/cli@latest",
                    "run",
                    "@smithery-ai/github",
                    "--config",
                    '{"githubPersonalAccessToken":"your_token_here"}',
                ],
                "transport": "stdio",
            }
        }

        default_text = json.dumps(example_json, indent=2, ensure_ascii=False)

        new_tool_json = st.text_area(
            "Tool JSON",
            default_text,
            height=250,
        )

        # Add button
        if st.button(
            "Add Tool",
            type="primary",
            key="add_tool_button",
            use_container_width=True,
        ):
            try:
                # Validate input
                if not new_tool_json.strip().startswith(
                    "{"
                ) or not new_tool_json.strip().endswith("}"):
                    st.error("JSON must start and end with curly braces ({}).")
                    st.markdown('Correct format: `{ "tool_name": { ... } }`')
                else:
                    # Parse JSON
                    parsed_tool = json.loads(new_tool_json)

                    # Check if it's in mcpServers format and process accordingly
                    if "mcpServers" in parsed_tool:
                        # Move contents of mcpServers to top level
                        parsed_tool = parsed_tool["mcpServers"]
                        st.info(
                            "'mcpServers' format detected. Converting automatically."
                        )

                    # Check number of tools entered
                    if len(parsed_tool) == 0:
                        st.error("Please enter at least one tool.")
                    else:
                        # Process all tools
                        success_tools = []
                        for tool_name, tool_config in parsed_tool.items():
                            # Check URL field and set transport
                            if "url" in tool_config:
                                # Set transport to "sse" if URL exists
                                tool_config["transport"] = "sse"
                                st.info(
                                    f"URL detected in '{tool_name}' tool, setting transport to 'sse'."
                                )
                            elif "transport" not in tool_config:
                                # Set default "stdio" if URL doesn't exist and transport isn't specified
                                tool_config["transport"] = "stdio"

                            # Check required fields
                            if (
                                "command" not in tool_config
                                and "url" not in tool_config
                            ):
                                st.error(
                                    f"'{tool_name}' tool configuration requires either 'command' or 'url' field."
                                )
                            elif "command" in tool_config and "args" not in tool_config:
                                st.error(
                                    f"'{tool_name}' tool configuration requires 'args' field."
                                )
                            elif "command" in tool_config and not isinstance(
                                tool_config["args"], list
                            ):
                                st.error(
                                    f"'args' field in '{tool_name}' tool must be an array ([]) format."
                                )
                            else:
                                # Add tool to pending_mcp_config
                                st.session_state.pending_mcp_config[tool_name] = (
                                    tool_config
                                )
                                success_tools.append(tool_name)

                        # Success message
                        if success_tools:
                            if len(success_tools) == 1:
                                st.success(
                                    f"{success_tools[0]} tool has been added. Click 'Apply Settings' button to apply."
                                )
                            else:
                                tool_names = ", ".join(success_tools)
                                st.success(
                                    f"Total {len(success_tools)} tools ({tool_names}) have been added. Click 'Apply Settings' button to apply."
                                )
                            # Collapse expander after adding
                            st.session_state.mcp_tools_expander = False
                            st.rerun()
            except json.JSONDecodeError as e:
                st.error(f"JSON parsing error: {e}")
                st.markdown(
                    f"""
                **How to fix**:
                1. Check that your JSON format is correct.
                2. All keys must be wrapped in double quotes (").
                3. String values must also be wrapped in double quotes (").
                4. When using double quotes within a string, they must be escaped (\\").
                """
                )
            except Exception as e:
                st.error(f"Error occurred: {e}")

    # Display registered tools list and add delete buttons
    with st.expander("ğŸ“‹ Registered Tools List", expanded=True):
        try:
            pending_config = st.session_state.pending_mcp_config
        except Exception as e:
            st.error("Not a valid MCP tool configuration.")
        else:
            # Iterate through keys (tool names) in pending config
            for tool_name in list(pending_config.keys()):
                col1, col2 = st.columns([8, 2])
                col1.markdown(f"- **{tool_name}**")
                if col2.button("Delete", key=f"delete_{tool_name}"):
                    # Delete tool from pending config (not applied immediately)
                    del st.session_state.pending_mcp_config[tool_name]
                    st.success(
                        f"{tool_name} tool has been deleted. Click 'Apply Settings' button to apply."
                    )

    st.divider()  # Add divider

# --- Sidebar: System Information and Action Buttons Section ---
with st.sidebar:
    st.subheader("ğŸ“Š System Information")
    st.write(
        f"ğŸ› ï¸ MCP Tools Count: {st.session_state.get('tool_count', 'Initializing...')}"
    )
    selected_model_name = st.session_state.selected_model
    st.write(f"ğŸ§  Current Model: {selected_model_name}")

    # Move Apply Settings button here
    if st.button(
        "Apply Settings",
        key="apply_button",
        type="primary",
        use_container_width=True,
    ):
        # Display applying message
        apply_status = st.empty()
        with apply_status.container():
            st.warning("ğŸ”„ Applying changes. Please wait...")
            progress_bar = st.progress(0)

            # Save settings
            st.session_state.mcp_config_text = json.dumps(
                st.session_state.pending_mcp_config, indent=2, ensure_ascii=False
            )

            # Save settings to config.json file
            save_result = save_config_to_json(st.session_state.pending_mcp_config)
            if not save_result:
                st.error("âŒ Failed to save settings file.")
            
            progress_bar.progress(15)

            # Prepare session initialization
            st.session_state.session_initialized = False
            st.session_state.agent = None

            # Update progress
            progress_bar.progress(30)

            # Run initialization
            success = st.session_state.event_loop.run_until_complete(
                initialize_session(st.session_state.pending_mcp_config)
            )

            # Update progress
            progress_bar.progress(100)

            if success:
                st.success("âœ… New settings have been applied.")
                # Collapse tool addition expander
                if "mcp_tools_expander" in st.session_state:
                    st.session_state.mcp_tools_expander = False
            else:
                st.error("âŒ Failed to apply settings.")

        # Refresh page
        st.rerun()

    st.divider()  # Add divider

    # Action buttons section
    st.subheader("ğŸ”„ Actions")

    # Reset conversation button
    if st.button("Reset Conversation", use_container_width=True, type="primary"):
        # Reset thread_id
        st.session_state.thread_id = random_uuid()

        # Reset conversation history
        st.session_state.history = []

        # Notification message
        st.success("âœ… Conversation has been reset.")

        # Refresh page
        st.rerun()

    # Show logout button only if login feature is enabled
    if use_login and st.session_state.authenticated:
        st.divider()  # Add divider
        if st.button("Logout", use_container_width=True, type="secondary"):
            st.session_state.authenticated = False
            st.success("âœ… You have been logged out.")
            st.rerun()

# --- Initialize default session (if not initialized) ---
if not st.session_state.session_initialized:
    st.info(
        "MCP server and agent are not initialized. Please click the 'Apply Settings' button in the left sidebar to initialize."
    )


# --- Print conversation history ---
print_message()

# --- User input and processing ---
user_query = st.chat_input("ğŸ’¬ Enter your question")
if user_query:
    if st.session_state.session_initialized:
        st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(user_query)
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            tool_placeholder = st.empty()
            text_placeholder = st.empty()
            resp, final_text, final_tool = (
                st.session_state.event_loop.run_until_complete(
                    process_query(
                        user_query,
                        text_placeholder,
                        tool_placeholder,
                        st.session_state.timeout_seconds,
                    )
                )
            )
        if "error" in resp:
            st.error(resp["error"])
        else:
            st.session_state.history.append({"role": "user", "content": user_query})
            st.session_state.history.append(
                {"role": "assistant", "content": final_text}
            )
            if final_tool.strip():
                st.session_state.history.append(
                    {"role": "assistant_tool", "content": final_tool}
                )
            st.rerun()
    else:
        st.warning(
            "âš ï¸ MCP server and agent are not initialized. Please click the 'Apply Settings' button in the left sidebar to initialize."
        )



================================================
FILE: app_KOR.py
================================================
import streamlit as st
import asyncio
import nest_asyncio
import json
import os
import platform

if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# nest_asyncio ì ìš©: ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ë‚´ì—ì„œ ì¤‘ì²© í˜¸ì¶œ í—ˆìš©
nest_asyncio.apply()

# ì „ì—­ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ì¬ì‚¬ìš© (í•œë²ˆ ìƒì„±í•œ í›„ ê³„ì† ì‚¬ìš©)
if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)

from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
from langchain_mcp_adapters.client import MultiServerMCPClient
from utils import astream_graph, random_uuid
from langchain_core.messages.ai import AIMessageChunk
from langchain_core.messages.tool import ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ API í‚¤ ë“±ì˜ ì„¤ì •ì„ ê°€ì ¸ì˜´)
load_dotenv(override=True)

# config.json íŒŒì¼ ê²½ë¡œ ì„¤ì •
CONFIG_FILE_PATH = "config.json"

# JSON ì„¤ì • íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_config_from_json():
    """
    config.json íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë°˜í™˜ê°’:
        dict: ë¡œë“œëœ ì„¤ì •
    """
    default_config = {
        "get_current_time": {
            "command": "python",
            "args": ["./mcp_server_time.py"],
            "transport": "stdio"
        }
    }
    
    try:
        if os.path.exists(CONFIG_FILE_PATH):
            with open(CONFIG_FILE_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        else:
            # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ íŒŒì¼ ìƒì„±
            save_config_to_json(default_config)
            return default_config
    except Exception as e:
        st.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return default_config

# JSON ì„¤ì • íŒŒì¼ ì €ì¥ í•¨ìˆ˜
def save_config_to_json(config):
    """
    ì„¤ì •ì„ config.json íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        config (dict): ì €ì¥í•  ì„¤ì •
    
    ë°˜í™˜ê°’:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        with open(CONFIG_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        st.error(f"ì„¤ì • íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# ë¡œê·¸ì¸ ì„¸ì…˜ ë³€ìˆ˜ ì´ˆê¸°í™”
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# ë¡œê·¸ì¸ í•„ìš” ì—¬ë¶€ í™•ì¸
use_login = os.environ.get("USE_LOGIN", "false").lower() == "true"

# ë¡œê·¸ì¸ ìƒíƒœì— ë”°ë¼ í˜ì´ì§€ ì„¤ì • ë³€ê²½
if use_login and not st.session_state.authenticated:
    # ë¡œê·¸ì¸ í˜ì´ì§€ëŠ” ê¸°ë³¸(narrow) ë ˆì´ì•„ì›ƒ ì‚¬ìš©
    st.set_page_config(page_title="Agent with MCP Tools", page_icon="ğŸ§ ")
else:
    # ë©”ì¸ ì•±ì€ wide ë ˆì´ì•„ì›ƒ ì‚¬ìš©
    st.set_page_config(page_title="Agent with MCP Tools", page_icon="ğŸ§ ", layout="wide")

# ë¡œê·¸ì¸ ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆê³  ì•„ì§ ì¸ì¦ë˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ì¸ í™”ë©´ í‘œì‹œ
if use_login and not st.session_state.authenticated:
    st.title("ğŸ” ë¡œê·¸ì¸")
    st.markdown("ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # ë¡œê·¸ì¸ í¼ì„ í™”ë©´ ì¤‘ì•™ì— ì¢ê²Œ ë°°ì¹˜
    with st.form("login_form"):
        username = st.text_input("ì•„ì´ë””")
        password = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password")
        submit_button = st.form_submit_button("ë¡œê·¸ì¸")

        if submit_button:
            expected_username = os.environ.get("USER_ID")
            expected_password = os.environ.get("USER_PASSWORD")

            if username == expected_username and password == expected_password:
                st.session_state.authenticated = True
                st.success("âœ… ë¡œê·¸ì¸ ì„±ê³µ! ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
                st.rerun()
            else:
                st.error("âŒ ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ë¡œê·¸ì¸ í™”ë©´ì—ì„œëŠ” ë©”ì¸ ì•±ì„ í‘œì‹œí•˜ì§€ ì•ŠìŒ
    st.stop()

# ì‚¬ì´ë“œë°” ìµœìƒë‹¨ì— ì €ì ì •ë³´ ì¶”ê°€ (ë‹¤ë¥¸ ì‚¬ì´ë“œë°” ìš”ì†Œë³´ë‹¤ ë¨¼ì € ë°°ì¹˜)
st.sidebar.markdown("### âœï¸ Made by [í…Œë””ë…¸íŠ¸](https://youtube.com/c/teddynote) ğŸš€")
st.sidebar.markdown(
    "### ğŸ’» [Project Page](https://github.com/teddynote-lab/langgraph-mcp-agents)"
)

st.sidebar.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

# ê¸°ì¡´ í˜ì´ì§€ íƒ€ì´í‹€ ë° ì„¤ëª…
st.title("ğŸ’¬ MCP ë„êµ¬ í™œìš© ì—ì´ì „íŠ¸")
st.markdown("âœ¨ MCP ë„êµ¬ë¥¼ í™œìš©í•œ ReAct ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")

SYSTEM_PROMPT = """<ROLE>
You are a smart agent with an ability to use tools. 
You will be given a question and you will use the tools to answer the question.
Pick the most relevant tool to answer the question. 
If you are failed to answer the question, try different tools to get context.
Your answer should be very polite and professional.
</ROLE>

----

<INSTRUCTIONS>
Step 1: Analyze the question
- Analyze user's question and final goal.
- If the user's question is consist of multiple sub-questions, split them into smaller sub-questions.

Step 2: Pick the most relevant tool
- Pick the most relevant tool to answer the question.
- If you are failed to answer the question, try different tools to get context.

Step 3: Answer the question
- Answer the question in the same language as the question.
- Your answer should be very polite and professional.

Step 4: Provide the source of the answer(if applicable)
- If you've used the tool, provide the source of the answer.
- Valid sources are either a website(URL) or a document(PDF, etc).

Guidelines:
- If you've used the tool, your answer should be based on the tool's output(tool's output is more important than your own knowledge).
- If you've used the tool, and the source is valid URL, provide the source(URL) of the answer.
- Skip providing the source if the source is not URL.
- Answer in the same language as the question.
- Answer should be concise and to the point.
- Avoid response your output with any other information than the answer and the source.  
</INSTRUCTIONS>

----

<OUTPUT_FORMAT>
(concise answer to the question)

**Source**(if applicable)
- (source1: valid URL)
- (source2: valid URL)
- ...
</OUTPUT_FORMAT>
"""

OUTPUT_TOKEN_INFO = {
    "claude-3-5-sonnet-latest": {"max_tokens": 8192},
    "claude-3-5-haiku-latest": {"max_tokens": 8192},
    "claude-3-7-sonnet-latest": {"max_tokens": 64000},
    "gpt-4o": {"max_tokens": 16000},
    "gpt-4o-mini": {"max_tokens": 16000},
}

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "session_initialized" not in st.session_state:
    st.session_state.session_initialized = False  # ì„¸ì…˜ ì´ˆê¸°í™” ìƒíƒœ í”Œë˜ê·¸
    st.session_state.agent = None  # ReAct ì—ì´ì „íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.history = []  # ëŒ€í™” ê¸°ë¡ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    st.session_state.mcp_client = None  # MCP í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ì €ì¥ ê³µê°„
    st.session_state.timeout_seconds = 120  # ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ), ê¸°ë³¸ê°’ 120ì´ˆ
    st.session_state.selected_model = "claude-3-7-sonnet-latest"  # ê¸°ë³¸ ëª¨ë¸ ì„ íƒ
    st.session_state.recursion_limit = 100  # ì¬ê·€ í˜¸ì¶œ ì œí•œ, ê¸°ë³¸ê°’ 100

if "thread_id" not in st.session_state:
    st.session_state.thread_id = random_uuid()


# --- í•¨ìˆ˜ ì •ì˜ ë¶€ë¶„ ---


async def cleanup_mcp_client():
    """
    ê¸°ì¡´ MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤.

    ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì •ìƒì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ë¥¼ í•´ì œí•©ë‹ˆë‹¤.
    """
    if "mcp_client" in st.session_state and st.session_state.mcp_client is not None:
        try:

            await st.session_state.mcp_client.__aexit__(None, None, None)
            st.session_state.mcp_client = None
        except Exception as e:
            import traceback

            # st.warning(f"MCP í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # st.warning(traceback.format_exc())


def print_message():
    """
    ì±„íŒ… ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.

    ì‚¬ìš©ìì™€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë©”ì‹œì§€ë¥¼ êµ¬ë¶„í•˜ì—¬ í™”ë©´ì— í‘œì‹œí•˜ê³ ,
    ë„êµ¬ í˜¸ì¶œ ì •ë³´ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ë‚´ì— í‘œì‹œí•©ë‹ˆë‹¤.
    """
    i = 0
    while i < len(st.session_state.history):
        message = st.session_state.history[i]

        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(message["content"])
            i += 1
        elif message["role"] == "assistant":
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì»¨í…Œì´ë„ˆ ìƒì„±
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ë‚´ìš© í‘œì‹œ
                st.markdown(message["content"])

                # ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë„êµ¬ í˜¸ì¶œ ì •ë³´ì¸ì§€ í™•ì¸
                if (
                    i + 1 < len(st.session_state.history)
                    and st.session_state.history[i + 1]["role"] == "assistant_tool"
                ):
                    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ë™ì¼í•œ ì»¨í…Œì´ë„ˆ ë‚´ì— expanderë¡œ í‘œì‹œ
                    with st.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=False):
                        st.markdown(st.session_state.history[i + 1]["content"])
                    i += 2  # ë‘ ë©”ì‹œì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 2 ì¦ê°€
                else:
                    i += 1  # ì¼ë°˜ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 1 ì¦ê°€
        else:
            # assistant_tool ë©”ì‹œì§€ëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ ê±´ë„ˆëœ€
            i += 1


def get_streaming_callback(text_placeholder, tool_placeholder):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” LLMì—ì„œ ìƒì„±ë˜ëŠ” ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œí•˜ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í…ìŠ¤íŠ¸ ì‘ë‹µê³¼ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ê°ê° ë‹¤ë¥¸ ì˜ì—­ì— í‘œì‹œí•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        text_placeholder: í…ìŠ¤íŠ¸ ì‘ë‹µì„ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        tool_placeholder: ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸

    ë°˜í™˜ê°’:
        callback_func: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
        accumulated_text: ëˆ„ì ëœ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
        accumulated_tool: ëˆ„ì ëœ ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
    """
    accumulated_text = []
    accumulated_tool = []

    def callback_func(message: dict):
        nonlocal accumulated_text, accumulated_tool
        message_content = message.get("content", None)

        if isinstance(message_content, AIMessageChunk):
            content = message_content.content
            # ì½˜í…ì¸ ê°€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (Claude ëª¨ë¸ ë“±ì—ì„œ ì£¼ë¡œ ë°œìƒ)
            if isinstance(content, list) and len(content) > 0:
                message_chunk = content[0]
                # í…ìŠ¤íŠ¸ íƒ€ì…ì¸ ê²½ìš° ì²˜ë¦¬
                if message_chunk["type"] == "text":
                    accumulated_text.append(message_chunk["text"])
                    text_placeholder.markdown("".join(accumulated_text))
                # ë„êµ¬ ì‚¬ìš© íƒ€ì…ì¸ ê²½ìš° ì²˜ë¦¬
                elif message_chunk["type"] == "tool_use":
                    if "partial_json" in message_chunk:
                        accumulated_tool.append(message_chunk["partial_json"])
                    else:
                        tool_call_chunks = message_content.tool_call_chunks
                        tool_call_chunk = tool_call_chunks[0]
                        accumulated_tool.append(
                            "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                        )
                    with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                        st.markdown("".join(accumulated_tool))
            # tool_calls ì†ì„±ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (OpenAI ëª¨ë¸ ë“±ì—ì„œ ì£¼ë¡œ ë°œìƒ)
            elif (
                hasattr(message_content, "tool_calls")
                and message_content.tool_calls
                and len(message_content.tool_calls[0]["name"]) > 0
            ):
                tool_call_info = message_content.tool_calls[0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                    st.markdown("".join(accumulated_tool))
            # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° ì²˜ë¦¬
            elif isinstance(content, str):
                accumulated_text.append(content)
                text_placeholder.markdown("".join(accumulated_text))
            # ìœ íš¨í•˜ì§€ ì•Šì€ ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            elif (
                hasattr(message_content, "invalid_tool_calls")
                and message_content.invalid_tool_calls
            ):
                tool_call_info = message_content.invalid_tool_calls[0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander(
                    "ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´ (ìœ íš¨í•˜ì§€ ì•ŠìŒ)", expanded=True
                ):
                    st.markdown("".join(accumulated_tool))
            # tool_call_chunks ì†ì„±ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            elif (
                hasattr(message_content, "tool_call_chunks")
                and message_content.tool_call_chunks
            ):
                tool_call_chunk = message_content.tool_call_chunks[0]
                accumulated_tool.append(
                    "\n```json\n" + str(tool_call_chunk) + "\n```\n"
                )
                with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                    st.markdown("".join(accumulated_tool))
            # additional_kwargsì— tool_callsê°€ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (ë‹¤ì–‘í•œ ëª¨ë¸ í˜¸í™˜ì„± ì§€ì›)
            elif (
                hasattr(message_content, "additional_kwargs")
                and "tool_calls" in message_content.additional_kwargs
            ):
                tool_call_info = message_content.additional_kwargs["tool_calls"][0]
                accumulated_tool.append("\n```json\n" + str(tool_call_info) + "\n```\n")
                with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                    st.markdown("".join(accumulated_tool))
        # ë„êµ¬ ë©”ì‹œì§€ì¸ ê²½ìš° ì²˜ë¦¬ (ë„êµ¬ì˜ ì‘ë‹µ)
        elif isinstance(message_content, ToolMessage):
            accumulated_tool.append(
                "\n```json\n" + str(message_content.content) + "\n```\n"
            )
            with tool_placeholder.expander("ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì •ë³´", expanded=True):
                st.markdown("".join(accumulated_tool))
        return None

    return callback_func, accumulated_text, accumulated_tool


async def process_query(query, text_placeholder, tool_placeholder, timeout_seconds=60):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì—ì´ì „íŠ¸ì— ì „ë‹¬í•˜ê³ , ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ í‘œì‹œí•©ë‹ˆë‹¤.
    ì§€ì •ëœ ì‹œê°„ ë‚´ì— ì‘ë‹µì´ ì™„ë£Œë˜ì§€ ì•Šìœ¼ë©´ íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        query: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        text_placeholder: í…ìŠ¤íŠ¸ ì‘ë‹µì„ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        tool_placeholder: ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í‘œì‹œí•  Streamlit ì»´í¬ë„ŒíŠ¸
        timeout_seconds: ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ)

    ë°˜í™˜ê°’:
        response: ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ ê°ì²´
        final_text: ìµœì¢… í…ìŠ¤íŠ¸ ì‘ë‹µ
        final_tool: ìµœì¢… ë„êµ¬ í˜¸ì¶œ ì •ë³´
    """
    try:
        if st.session_state.agent:
            streaming_callback, accumulated_text_obj, accumulated_tool_obj = (
                get_streaming_callback(text_placeholder, tool_placeholder)
            )
            try:
                response = await asyncio.wait_for(
                    astream_graph(
                        st.session_state.agent,
                        {"messages": [HumanMessage(content=query)]},
                        callback=streaming_callback,
                        config=RunnableConfig(
                            recursion_limit=st.session_state.recursion_limit,
                            thread_id=st.session_state.thread_id,
                        ),
                    ),
                    timeout=timeout_seconds,
                )
            except asyncio.TimeoutError:
                error_msg = f"â±ï¸ ìš”ì²­ ì‹œê°„ì´ {timeout_seconds}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                return {"error": error_msg}, error_msg, ""

            final_text = "".join(accumulated_text_obj)
            final_tool = "".join(accumulated_tool_obj)
            return response, final_text, final_tool
        else:
            return (
                {"error": "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."},
                "ğŸš« ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "",
            )
    except Exception as e:
        import traceback

        error_msg = f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n{traceback.format_exc()}"
        return {"error": error_msg}, error_msg, ""


async def initialize_session(mcp_config=None):
    """
    MCP ì„¸ì…˜ê³¼ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        mcp_config: MCP ë„êµ¬ ì„¤ì • ì •ë³´(JSON). Noneì¸ ê²½ìš° ê¸°ë³¸ ì„¤ì • ì‚¬ìš©

    ë°˜í™˜ê°’:
        bool: ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    """
    with st.spinner("ğŸ”„ MCP ì„œë²„ì— ì—°ê²° ì¤‘..."):
        # ë¨¼ì € ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì •ë¦¬
        await cleanup_mcp_client()

        if mcp_config is None:
            # config.json íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
            mcp_config = load_config_from_json()
        client = MultiServerMCPClient(mcp_config)
        await client.__aenter__()
        tools = client.get_tools()
        st.session_state.tool_count = len(tools)
        st.session_state.mcp_client = client

        # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì´ˆê¸°í™”
        selected_model = st.session_state.selected_model

        if selected_model in [
            "claude-3-7-sonnet-latest",
            "claude-3-5-sonnet-latest",
            "claude-3-5-haiku-latest",
        ]:
            model = ChatAnthropic(
                model=selected_model,
                temperature=0.1,
                max_tokens=OUTPUT_TOKEN_INFO[selected_model]["max_tokens"],
            )
        else:  # OpenAI ëª¨ë¸ ì‚¬ìš©
            model = ChatOpenAI(
                model=selected_model,
                temperature=0.1,
                max_tokens=OUTPUT_TOKEN_INFO[selected_model]["max_tokens"],
            )
        agent = create_react_agent(
            model,
            tools,
            checkpointer=MemorySaver(),
            prompt=SYSTEM_PROMPT,
        )
        st.session_state.agent = agent
        st.session_state.session_initialized = True
        return True


# --- ì‚¬ì´ë“œë°”: ì‹œìŠ¤í…œ ì„¤ì • ì„¹ì…˜ ---
with st.sidebar:
    st.subheader("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")

    # ëª¨ë¸ ì„ íƒ ê¸°ëŠ¥
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ìƒì„±
    available_models = []

    # Anthropic API í‚¤ í™•ì¸
    has_anthropic_key = os.environ.get("ANTHROPIC_API_KEY") is not None
    if has_anthropic_key:
        available_models.extend(
            [
                "claude-3-7-sonnet-latest",
                "claude-3-5-sonnet-latest",
                "claude-3-5-haiku-latest",
            ]
        )

    # OpenAI API í‚¤ í™•ì¸
    has_openai_key = os.environ.get("OPENAI_API_KEY") is not None
    if has_openai_key:
        available_models.extend(["gpt-4o", "gpt-4o-mini"])

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ í‘œì‹œ
    if not available_models:
        st.warning(
            "âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— ANTHROPIC_API_KEY ë˜ëŠ” OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”."
        )
        # ê¸°ë³¸ê°’ìœ¼ë¡œ Claude ëª¨ë¸ ì¶”ê°€ (í‚¤ê°€ ì—†ì–´ë„ UIë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•¨)
        available_models = ["claude-3-7-sonnet-latest"]

    # ëª¨ë¸ ì„ íƒ ë“œë¡­ë‹¤ìš´
    previous_model = st.session_state.selected_model
    st.session_state.selected_model = st.selectbox(
        "ğŸ¤– ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ",
        options=available_models,
        index=(
            available_models.index(st.session_state.selected_model)
            if st.session_state.selected_model in available_models
            else 0
        ),
        help="Anthropic ëª¨ë¸ì€ ANTHROPIC_API_KEYê°€, OpenAI ëª¨ë¸ì€ OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.",
    )

    # ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆì„ ë•Œ ì„¸ì…˜ ì´ˆê¸°í™” í•„ìš” ì•Œë¦¼
    if (
        previous_model != st.session_state.selected_model
        and st.session_state.session_initialized
    ):
        st.warning(
            "âš ï¸ ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•˜ì„¸ìš”."
        )

    # íƒ€ì„ì•„ì›ƒ ì„¤ì • ìŠ¬ë¼ì´ë” ì¶”ê°€
    st.session_state.timeout_seconds = st.slider(
        "â±ï¸ ì‘ë‹µ ìƒì„± ì œí•œ ì‹œê°„(ì´ˆ)",
        min_value=60,
        max_value=300,
        value=st.session_state.timeout_seconds,
        step=10,
        help="ì—ì´ì „íŠ¸ê°€ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ìµœëŒ€ ì‹œê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤. ë³µì¡í•œ ì‘ì—…ì€ ë” ê¸´ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )

    st.session_state.recursion_limit = st.slider(
        "â±ï¸ ì¬ê·€ í˜¸ì¶œ ì œí•œ(íšŸìˆ˜)",
        min_value=10,
        max_value=200,
        value=st.session_state.recursion_limit,
        step=10,
        help="ì¬ê·€ í˜¸ì¶œ ì œí•œ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë„ˆë¬´ ë†’ì€ ê°’ì„ ì„¤ì •í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    )

    st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

    # ë„êµ¬ ì„¤ì • ì„¹ì…˜ ì¶”ê°€
    st.subheader("ğŸ”§ ë„êµ¬ ì„¤ì •")

    # expander ìƒíƒœë¥¼ ì„¸ì…˜ ìƒíƒœë¡œ ê´€ë¦¬
    if "mcp_tools_expander" not in st.session_state:
        st.session_state.mcp_tools_expander = False

    # MCP ë„êµ¬ ì¶”ê°€ ì¸í„°í˜ì´ìŠ¤
    with st.expander("ğŸ§° MCP ë„êµ¬ ì¶”ê°€", expanded=st.session_state.mcp_tools_expander):
        # config.json íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œí•˜ì—¬ í‘œì‹œ
        loaded_config = load_config_from_json()
        default_config_text = json.dumps(loaded_config, indent=2, ensure_ascii=False)
        
        # pending configê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ mcp_config_text ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
        if "pending_mcp_config" not in st.session_state:
            try:
                st.session_state.pending_mcp_config = loaded_config
            except Exception as e:
                st.error(f"ì´ˆê¸° pending config ì„¤ì • ì‹¤íŒ¨: {e}")

        # ê°œë³„ ë„êµ¬ ì¶”ê°€ë¥¼ ìœ„í•œ UI
        st.subheader("ë„êµ¬ ì¶”ê°€")
        st.markdown(
            """
            [ì–´ë–»ê²Œ ì„¤ì • í•˜ë‚˜ìš”?](https://teddylee777.notion.site/MCP-1d324f35d12980c8b018e12afdf545a1?pvs=4)

            âš ï¸ **ì¤‘ìš”**: JSONì„ ë°˜ë“œì‹œ ì¤‘ê´„í˜¸(`{}`)ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤."""
        )

        # ë³´ë‹¤ ëª…í™•í•œ ì˜ˆì‹œ ì œê³µ
        example_json = {
            "github": {
                "command": "npx",
                "args": [
                    "-y",
                    "@smithery/cli@latest",
                    "run",
                    "@smithery-ai/github",
                    "--config",
                    '{"githubPersonalAccessToken":"your_token_here"}',
                ],
                "transport": "stdio",
            }
        }

        default_text = json.dumps(example_json, indent=2, ensure_ascii=False)

        new_tool_json = st.text_area(
            "ë„êµ¬ JSON",
            default_text,
            height=250,
        )

        # ì¶”ê°€í•˜ê¸° ë²„íŠ¼
        if st.button(
            "ë„êµ¬ ì¶”ê°€",
            type="primary",
            key="add_tool_button",
            use_container_width=True,
        ):
            try:
                # ì…ë ¥ê°’ ê²€ì¦
                if not new_tool_json.strip().startswith(
                    "{"
                ) or not new_tool_json.strip().endswith("}"):
                    st.error("JSONì€ ì¤‘ê´„í˜¸({})ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ì•¼ í•©ë‹ˆë‹¤.")
                    st.markdown('ì˜¬ë°”ë¥¸ í˜•ì‹: `{ "ë„êµ¬ì´ë¦„": { ... } }`')
                else:
                    # JSON íŒŒì‹±
                    parsed_tool = json.loads(new_tool_json)

                    # mcpServers í˜•ì‹ì¸ì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
                    if "mcpServers" in parsed_tool:
                        # mcpServers ì•ˆì˜ ë‚´ìš©ì„ ìµœìƒìœ„ë¡œ ì´ë™
                        parsed_tool = parsed_tool["mcpServers"]
                        st.info(
                            "'mcpServers' í˜•ì‹ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."
                        )

                    # ì…ë ¥ëœ ë„êµ¬ ìˆ˜ í™•ì¸
                    if len(parsed_tool) == 0:
                        st.error("ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ë„êµ¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ëª¨ë“  ë„êµ¬ì— ëŒ€í•´ ì²˜ë¦¬
                        success_tools = []
                        for tool_name, tool_config in parsed_tool.items():
                            # URL í•„ë“œ í™•ì¸ ë° transport ì„¤ì •
                            if "url" in tool_config:
                                # URLì´ ìˆëŠ” ê²½ìš° transportë¥¼ "sse"ë¡œ ì„¤ì •
                                tool_config["transport"] = "sse"
                                st.info(
                                    f"'{tool_name}' ë„êµ¬ì— URLì´ ê°ì§€ë˜ì–´ transportë¥¼ 'sse'ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤."
                                )
                            elif "transport" not in tool_config:
                                # URLì´ ì—†ê³  transportë„ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ "stdio" ì„¤ì •
                                tool_config["transport"] = "stdio"

                            # í•„ìˆ˜ í•„ë“œ í™•ì¸
                            if (
                                "command" not in tool_config
                                and "url" not in tool_config
                            ):
                                st.error(
                                    f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'command' ë˜ëŠ” 'url' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
                                )
                            elif "command" in tool_config and "args" not in tool_config:
                                st.error(
                                    f"'{tool_name}' ë„êµ¬ ì„¤ì •ì—ëŠ” 'args' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
                                )
                            elif "command" in tool_config and not isinstance(
                                tool_config["args"], list
                            ):
                                st.error(
                                    f"'{tool_name}' ë„êµ¬ì˜ 'args' í•„ë“œëŠ” ë°˜ë“œì‹œ ë°°ì—´([]) í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
                                )
                            else:
                                # pending_mcp_configì— ë„êµ¬ ì¶”ê°€
                                st.session_state.pending_mcp_config[tool_name] = (
                                    tool_config
                                )
                                success_tools.append(tool_name)

                        # ì„±ê³µ ë©”ì‹œì§€
                        if success_tools:
                            if len(success_tools) == 1:
                                st.success(
                                    f"{success_tools[0]} ë„êµ¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                                )
                            else:
                                tool_names = ", ".join(success_tools)
                                st.success(
                                    f"ì´ {len(success_tools)}ê°œ ë„êµ¬({tool_names})ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                                )
                            # ì¶”ê°€ë˜ë©´ expanderë¥¼ ì ‘ì–´ì¤Œ
                            st.session_state.mcp_tools_expander = False
                            st.rerun()
            except json.JSONDecodeError as e:
                st.error(f"JSON íŒŒì‹± ì—ëŸ¬: {e}")
                st.markdown(
                    f"""
                **ìˆ˜ì • ë°©ë²•**:
                1. JSON í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.
                2. ëª¨ë“  í‚¤ëŠ” í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
                3. ë¬¸ìì—´ ê°’ë„ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.
                4. ë¬¸ìì—´ ë‚´ì—ì„œ í°ë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•  ê²½ìš° ì´ìŠ¤ì¼€ì´í”„(\\")í•´ì•¼ í•©ë‹ˆë‹¤.
                """
                )
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ë“±ë¡ëœ ë„êµ¬ ëª©ë¡ í‘œì‹œ ë° ì‚­ì œ ë²„íŠ¼ ì¶”ê°€
    with st.expander("ğŸ“‹ ë“±ë¡ëœ ë„êµ¬ ëª©ë¡", expanded=True):
        try:
            pending_config = st.session_state.pending_mcp_config
        except Exception as e:
            st.error("ìœ íš¨í•œ MCP ë„êµ¬ ì„¤ì •ì´ ì•„ë‹™ë‹ˆë‹¤.")
        else:
            # pending configì˜ í‚¤(ë„êµ¬ ì´ë¦„) ëª©ë¡ì„ ìˆœíšŒí•˜ë©° í‘œì‹œ
            for tool_name in list(pending_config.keys()):
                col1, col2 = st.columns([8, 2])
                col1.markdown(f"- **{tool_name}**")
                if col2.button("ì‚­ì œ", key=f"delete_{tool_name}"):
                    # pending configì—ì„œ í•´ë‹¹ ë„êµ¬ ì‚­ì œ (ì¦‰ì‹œ ì ìš©ë˜ì§€ëŠ” ì•ŠìŒ)
                    del st.session_state.pending_mcp_config[tool_name]
                    st.success(
                        f"{tool_name} ë„êµ¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ì ìš©í•˜ë ¤ë©´ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                    )

    st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

# --- ì‚¬ì´ë“œë°”: ì‹œìŠ¤í…œ ì •ë³´ ë° ì‘ì—… ë²„íŠ¼ ì„¹ì…˜ ---
with st.sidebar:
    st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    st.write(f"ğŸ› ï¸ MCP ë„êµ¬ ìˆ˜: {st.session_state.get('tool_count', 'ì´ˆê¸°í™” ì¤‘...')}")
    selected_model_name = st.session_state.selected_model
    st.write(f"ğŸ§  í˜„ì¬ ëª¨ë¸: {selected_model_name}")

    # ì„¤ì • ì ìš©í•˜ê¸° ë²„íŠ¼ì„ ì—¬ê¸°ë¡œ ì´ë™
    if st.button(
        "ì„¤ì • ì ìš©í•˜ê¸°",
        key="apply_button",
        type="primary",
        use_container_width=True,
    ):
        # ì ìš© ì¤‘ ë©”ì‹œì§€ í‘œì‹œ
        apply_status = st.empty()
        with apply_status.container():
            st.warning("ğŸ”„ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
            progress_bar = st.progress(0)

            # ì„¤ì • ì €ì¥
            st.session_state.mcp_config_text = json.dumps(
                st.session_state.pending_mcp_config, indent=2, ensure_ascii=False
            )

            # config.json íŒŒì¼ì— ì„¤ì • ì €ì¥
            save_result = save_config_to_json(st.session_state.pending_mcp_config)
            if not save_result:
                st.error("âŒ ì„¤ì • íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            progress_bar.progress(15)

            # ì„¸ì…˜ ì´ˆê¸°í™” ì¤€ë¹„
            st.session_state.session_initialized = False
            st.session_state.agent = None

            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            progress_bar.progress(30)

            # ì´ˆê¸°í™” ì‹¤í–‰
            success = st.session_state.event_loop.run_until_complete(
                initialize_session(st.session_state.pending_mcp_config)
            )

            # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            progress_bar.progress(100)

            if success:
                st.success("âœ… ìƒˆë¡œìš´ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                # ë„êµ¬ ì¶”ê°€ expander ì ‘ê¸°
                if "mcp_tools_expander" in st.session_state:
                    st.session_state.mcp_tools_expander = False
            else:
                st.error("âŒ ì„¤ì • ì ìš©ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤.")

        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()

    st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€

    # ì‘ì—… ë²„íŠ¼ ì„¹ì…˜
    st.subheader("ğŸ”„ ì‘ì—…")

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True, type="primary"):
        # thread_id ì´ˆê¸°í™”
        st.session_state.thread_id = random_uuid()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        st.session_state.history = []

        # ì•Œë¦¼ ë©”ì‹œì§€
        st.success("âœ… ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()

    # ë¡œê·¸ì¸ ê¸°ëŠ¥ì´ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ í‘œì‹œ
    if use_login and st.session_state.authenticated:
        st.divider()  # êµ¬ë¶„ì„  ì¶”ê°€
        if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True, type="secondary"):
            st.session_state.authenticated = False
            st.success("âœ… ë¡œê·¸ì•„ì›ƒ ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()

# --- ê¸°ë³¸ ì„¸ì…˜ ì´ˆê¸°í™” (ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°) ---
if not st.session_state.session_initialized:
    st.info(
        "MCP ì„œë²„ì™€ ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì˜ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
    )


# --- ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ---
print_message()

# --- ì‚¬ìš©ì ì…ë ¥ ë° ì²˜ë¦¬ ---
user_query = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_query:
    if st.session_state.session_initialized:
        st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(user_query)
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            tool_placeholder = st.empty()
            text_placeholder = st.empty()
            resp, final_text, final_tool = (
                st.session_state.event_loop.run_until_complete(
                    process_query(
                        user_query,
                        text_placeholder,
                        tool_placeholder,
                        st.session_state.timeout_seconds,
                    )
                )
            )
        if "error" in resp:
            st.error(resp["error"])
        else:
            st.session_state.history.append({"role": "user", "content": user_query})
            st.session_state.history.append(
                {"role": "assistant", "content": final_text}
            )
            if final_tool.strip():
                st.session_state.history.append(
                    {"role": "assistant_tool", "content": final_tool}
                )
            st.rerun()
    else:
        st.warning(
            "âš ï¸ MCP ì„œë²„ì™€ ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì™¼ìª½ ì‚¬ì´ë“œë°”ì˜ 'ì„¤ì • ì ìš©í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
        )



================================================
FILE: config.json
================================================
{
  "get_current_time": {
    "command": "python",
    "args": [
      "./mcp_server_time.py"
    ],
    "transport": "stdio"
  }
}


================================================
FILE: example_config.json
================================================
{
  "get_current_time": {
    "command": "python",
    "args": ["./mcp_server_time.py"],
    "transport": "stdio"
  }
}


================================================
FILE: MCP-HandsOn-ENG.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# MCP + LangGraph Hands-On Tutorial

- Author: [Teddy Notes](https://youtube.com/c/teddynote)
- Lecture: [Fastcampus RAG trick notes](https://fastcampus.co.kr/data_online_teddy)

**References**
- https://modelcontextprotocol.io/introduction
- https://github.com/langchain-ai/langchain-mcp-adapters
"""

"""
## configure

Refer to the installation instructions below to install `uv`.

**How to install `uv`**

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
irm https://astral.sh/uv/install.ps1 | iex
```

Install **dependencies**

```bash
uv pip install -r requirements.txt
```
"""

"""
Gets the environment variables.
"""

from dotenv import load_dotenv

load_dotenv(override=True)

"""
## MultiServerMCPClient
"""

"""
Run `mcp_server_remote.py` in advance. Open a terminal with the virtual environment activated and run the server.

> Command
```bash
source .venv/bin/activate
python mcp_server_remote.py
```

Create and terminate a temporary Session connection using `async with`
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from utils import ainvoke_graph, astream_graph
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

async with MultiServerMCPClient(
    {
        "weather": {
            # Must match the server's port (port 8005)
            "url": "http://localhost:8005/sse",
            "transport": "sse",
        }
    }
) as client:
    print(client.get_tools())
    agent = create_react_agent(model, client.get_tools())
    answer = await astream_graph(
        agent, {"messages": "What's the weather like in Seoul?"}
    )

"""
You might notice that you can't access the tool because the session is closed.
"""

await astream_graph(agent, {"messages": "What's the weather like in Seoul?"})

"""
Now let's change that to accessing the tool while maintaining an Async Session.
"""

# 1. Create client
client = MultiServerMCPClient(
    {
        "weather": {
            "url": "http://localhost:8005/sse",
            "transport": "sse",
        }
    }
)


# 2. Explicitly initialize connection (this part is necessary)
# Initialize
await client.__aenter__()

# Now tools are loaded
print(client.get_tools())  # Tools are displayed

"""
Create an agent with langgraph(`create_react_agent`).
"""

# Create agent
agent = create_react_agent(model, client.get_tools())

"""
Run the graph to see the results.
"""

await astream_graph(agent, {"messages": "What's the weather like in Seoul?"})

"""
## Stdio method

The Stdio method is intended for use in a local environment.

- Use standard input/output for communication
"""

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_anthropic import ChatAnthropic

# Initialize Anthropic's Claude model
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# Set up StdIO server parameters
# - command: Path to Python interpreter
# - args: MCP server script to execute
server_params = StdioServerParameters(
    command="./.venv/bin/python",
    args=["mcp_server_local.py"],
)

# Use StdIO client to communicate with the server
async with stdio_client(server_params) as (read, write):
    # Create client session
    async with ClientSession(read, write) as session:
        # Initialize connection
        await session.initialize()

        # Load MCP tools
        tools = await load_mcp_tools(session)
        print(tools)

        # Create agent
        agent = create_react_agent(model, tools)

        # Stream agent responses
        await astream_graph(agent, {"messages": "What's the weather like in Seoul?"})

"""
## Use MCP server with RAG deployed

- File: `mcp_server_rag.py`

Use the `mcp_server_rag.py` file that we built with langchain in advance.

It uses stdio communication to get information about the tools, where it gets the `retriever` tool, which is the tool defined in `mcp_server_rag.py`. This file **doesn't** need to be running on the server beforehand.
"""

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from utils import astream_graph

# Initialize Anthropic's Claude model
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# Set up StdIO server parameters for the RAG server
server_params = StdioServerParameters(
    command="./.venv/bin/python",
    args=["./mcp_server_rag.py"],
)

# Use StdIO client to communicate with the RAG server
async with stdio_client(server_params) as (read, write):
    # Create client session
    async with ClientSession(read, write) as session:
        # Initialize connection
        await session.initialize()

        # Load MCP tools (in this case, the retriever tool)
        tools = await load_mcp_tools(session)

        # Create and run the agent
        agent = create_react_agent(model, tools)

        # Stream agent responses
        await astream_graph(
            agent,
            {
                "messages": "Search for the name of the generative AI developed by Samsung Electronics"
            },
        )

"""
## Use a mix of SSE and Stdio methods

- File: `mcp_server_rag.py` communicates over Stdio
- `langchain-dev-docs` communicates via SSE

Use a mix of SSE and Stdio methods.
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic

# Initialize Anthropic's Claude model
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# 1. Create multi-server MCP client
client = MultiServerMCPClient(
    {
        "document-retriever": {
            "command": "./.venv/bin/python",
            # Update with the absolute path to mcp_server_rag.py file
            "args": ["./mcp_server_rag.py"],
            # Communicate via stdio (using standard input/output)
            "transport": "stdio",
        },
        "langchain-dev-docs": {
            # Make sure the SSE server is running
            "url": "https://teddynote.io/mcp/langchain/sse",
            # Communicate via SSE (Server-Sent Events)
            "transport": "sse",
        },
    }
)


# 2. Initialize connection explicitly through async context manager
await client.__aenter__()

"""
Create an agent using `create_react_agent` in langgraph.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

prompt = (
    "You are a smart agent. "
    "Use `retriever` tool to search on AI related documents and answer questions."
    "Use `langchain-dev-docs` tool to search on langchain / langgraph related documents and answer questions."
    "Answer in English."
)
agent = create_react_agent(
    model, client.get_tools(), prompt=prompt, checkpointer=MemorySaver()
)

"""
Use the `retriever` tool defined in `mcp_server_rag.py` that you built to perform the search.
"""

config = RunnableConfig(recursion_limit=30, thread_id=1)
await astream_graph(
    agent,
    {
        "messages": "Use the `retriever` tool to search for the name of the generative AI developed by Samsung Electronics"
    },
    config=config,
)

"""
This time, we'll use the `langchain-dev-docs` tool to perform the search.
"""

config = RunnableConfig(recursion_limit=30, thread_id=1)
await astream_graph(
    agent,
    {
        "messages": "Please tell me about the definition of self-rag by referring to the langchain-dev-docs"
    },
    config=config,
)

"""
Use `MemorySaver` to maintain short-term memory, so multi-turn conversations are possible.
"""

await astream_graph(
    agent,
    {"messages": "Summarize the previous content in bullet points"},
    config=config,
)

"""
## LangChain-integrated tools + MCP tools

Here we confirm that tools integrated into LangChain can be used in conjunction with existing MCP-only tools.
"""

from langchain_community.tools.tavily_search import TavilySearchResults

# Initialize the Tavily search tool (news type, news from the last 3 days)
tavily = TavilySearchResults(max_results=3, topic="news", days=3)

# Use it together with existing MCP tools
tools = client.get_tools() + [tavily]

"""
Create an agent using `create_react_agent` in langgraph.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

prompt = "You are a smart agent with various tools. Answer questions in English."
agent = create_react_agent(model, tools, prompt=prompt, checkpointer=MemorySaver())

"""
Perform a search using the newly added `tavily` tool.
"""

await astream_graph(
    agent, {"messages": "Tell me about today's news for me"}, config=config
)

"""
You can see that the `retriever` tool is working smoothly.
"""

await astream_graph(
    agent,
    {
        "messages": "Use the `retriever` tool to search for the name of the generative AI developed by Samsung Electronics"
    },
    config=config,
)

"""
## Smithery MCP Server

- Link: https://smithery.ai/

List of tools used:

- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking
  - MCP server providing tools for dynamic and reflective problem-solving through structured thinking processes
- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander
  - Run terminal commands and manage files with various editing capabilities. Coding, shell and terminal, task automation

**Note**

- When importing tools provided by smithery in JSON format, you must set `"transport": "stdio"` as shown in the example below.
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic

# Initialize LLM model
model = ChatAnthropic(model="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000)

# 1. Create client
client = MultiServerMCPClient(
    {
        "server-sequential-thinking": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@smithery-ai/server-sequential-thinking",
                "--key",
                "your_smithery_api_key",
            ],
            "transport": "stdio",  # Add communication using stdio method
        },
        "desktop-commander": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@wonderwhy-er/desktop-commander",
                "--key",
                "your_smithery_api_key",
            ],
            "transport": "stdio",  # Add communication using stdio method
        },
        "document-retriever": {
            "command": "./.venv/bin/python",
            # Update with the absolute path to the mcp_server_rag.py file
            "args": ["./mcp_server_rag.py"],
            # Communication using stdio (standard input/output)
            "transport": "stdio",
        },
    }
)


# 2. Explicitly initialize connection
await client.__aenter__()

"""
Create an agent using `create_react_agent` in langgraph.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# Set up configuration
config = RunnableConfig(recursion_limit=30, thread_id=3)

# Create agent
agent = create_react_agent(model, client.get_tools(), checkpointer=MemorySaver())

"""
`Desktop Commander` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

await astream_graph(
    agent,
    {
        "messages": "Draw the folder structure including the current path as a tree. However, exclude the .venv folder from the output."
    },
    config=config,
)

"""
We'll use the `Sequential Thinking` tool to see if we can accomplish a relatively complex task.
"""

await astream_graph(
    agent,
    {
        "messages": (
            "Use the `retriever` tool to search for information about generative AI developed by Samsung Electronics, "
            "and then use the `Sequential Thinking` tool to write a report."
        )
    },
    config=config,
)



================================================
FILE: MCP-HandsOn-KOR.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# MCP + LangGraph í•¸ì¦ˆì˜¨ íŠœí† ë¦¬ì–¼

- ì‘ì„±ì: [í…Œë””ë…¸íŠ¸](https://youtube.com/c/teddynote)
- ê°•ì˜: [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)

**ì°¸ê³ ìë£Œ**
- https://modelcontextprotocol.io/introduction
- https://github.com/langchain-ai/langchain-mcp-adapters
"""

"""
## í™˜ê²½ì„¤ì •

ì•„ë˜ ì„¤ì¹˜ ë°©ë²•ì„ ì°¸ê³ í•˜ì—¬ `uv` ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.

**uv ì„¤ì¹˜ ë°©ë²•**

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
irm https://astral.sh/uv/install.ps1 | iex
```

**ì˜ì¡´ì„± ì„¤ì¹˜**

```bash
uv pip install -r requirements.txt
```
"""

"""
í™˜ê²½ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

from dotenv import load_dotenv

load_dotenv(override=True)

"""
## MultiServerMCPClient
"""

"""
ì‚¬ì „ì— `mcp_server_remote.py` ë¥¼ ì‹¤í–‰í•´ë‘¡ë‹ˆë‹¤. í„°ë¯¸ë„ì„ ì—´ê³  ê°€ìƒí™˜ê²½ì´ í™œì„±í™” ë˜ì–´ ìˆëŠ” ìƒíƒœì—ì„œ ì„œë²„ë¥¼ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.

> ëª…ë ¹ì–´
```bash
source .venv/bin/activate
python mcp_server_remote.py
```

`async with` ë¡œ ì¼ì‹œì ì¸ Session ì—°ê²°ì„ ìƒì„± í›„ í•´ì œ
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from utils import ainvoke_graph, astream_graph
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

async with MultiServerMCPClient(
    {
        "weather": {
            # ì„œë²„ì˜ í¬íŠ¸ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.(8005ë²ˆ í¬íŠ¸)
            "url": "http://localhost:8005/sse",
            "transport": "sse",
        }
    }
) as client:
    print(client.get_tools())
    agent = create_react_agent(model, client.get_tools())
    answer = await astream_graph(agent, {"messages": "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?"})

"""
ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” session ì´ ë‹«í˜”ê¸° ë•Œë¬¸ì— ë„êµ¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

await astream_graph(agent, {"messages": "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?"})

"""
ì´ì œ ê·¸ëŸ¼ Async Session ì„ ìœ ì§€í•˜ë©° ë„êµ¬ì— ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•´ ë³´ê² ìŠµë‹ˆë‹¤.
"""

# 1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = MultiServerMCPClient(
    {
        "weather": {
            "url": "http://localhost:8005/sse",
            "transport": "sse",
        }
    }
)


# 2. ëª…ì‹œì ìœ¼ë¡œ ì—°ê²° ì´ˆê¸°í™” (ì´ ë¶€ë¶„ì´ í•„ìš”í•¨)
# ì´ˆê¸°í™”
await client.__aenter__()

# ì´ì œ ë„êµ¬ê°€ ë¡œë“œë¨
print(client.get_tools())  # ë„êµ¬ê°€ í‘œì‹œë¨

"""
langgraph ì˜ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_react_agent(model, client.get_tools())

"""
ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
"""

await astream_graph(agent, {"messages": "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?"})

"""
## Stdio í†µì‹  ë°©ì‹

Stdio í†µì‹  ë°©ì‹ì€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.

- í†µì‹ ì„ ìœ„í•´ í‘œì¤€ ì…ë ¥/ì¶œë ¥ ì‚¬ìš©

ì°¸ê³ : ì•„ë˜ì˜ python ê²½ë¡œëŠ” ìˆ˜ì •í•˜ì„¸ìš”!
"""

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_anthropic import ChatAnthropic

# Anthropicì˜ Claude ëª¨ë¸ ì´ˆê¸°í™”
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •
# - command: Python ì¸í„°í”„ë¦¬í„° ê²½ë¡œ
# - args: ì‹¤í–‰í•  MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸
server_params = StdioServerParameters(
    command="./.venv/bin/python",
    args=["mcp_server_local.py"],
)

# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì™€ í†µì‹ 
async with stdio_client(server_params) as (read, write):
    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±
    async with ClientSession(read, write) as session:
        # ì—°ê²° ì´ˆê¸°í™”
        await session.initialize()

        # MCP ë„êµ¬ ë¡œë“œ
        tools = await load_mcp_tools(session)
        print(tools)

        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = create_react_agent(model, tools)

        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        await astream_graph(agent, {"messages": "ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë– ë‹ˆ?"})

"""
## RAG ë¥¼ êµ¬ì¶•í•œ MCP ì„œë²„ ì‚¬ìš©

- íŒŒì¼: `mcp_server_rag.py`

ì‚¬ì „ì— langchain ìœ¼ë¡œ êµ¬ì¶•í•œ `mcp_server_rag.py` íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

stdio í†µì‹  ë°©ì‹ìœ¼ë¡œ ë„êµ¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ì—¬ê¸°ì„œ ë„êµ¬ëŠ” `retriever` ë„êµ¬ë¥¼ ê°€ì ¸ì˜¤ê²Œ ë˜ë©°, ì´ ë„êµ¬ëŠ” `mcp_server_rag.py` ì—ì„œ ì •ì˜ëœ ë„êµ¬ì…ë‹ˆë‹¤. ì´ íŒŒì¼ì€ ì‚¬ì „ì— ì„œë²„ì—ì„œ ì‹¤í–‰ë˜ì§€ **ì•Šì•„ë„** ë©ë‹ˆë‹¤.
"""

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from utils import astream_graph

# Anthropicì˜ Claude ëª¨ë¸ ì´ˆê¸°í™”
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# RAG ì„œë²„ë¥¼ ìœ„í•œ StdIO ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •
server_params = StdioServerParameters(
    command="./.venv/bin/python",
    args=["./mcp_server_rag.py"],
)

# StdIO í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ì„œë²„ì™€ í†µì‹ 
async with stdio_client(server_params) as (read, write):
    # í´ë¼ì´ì–¸íŠ¸ ì„¸ì…˜ ìƒì„±
    async with ClientSession(read, write) as session:
        # ì—°ê²° ì´ˆê¸°í™”
        await session.initialize()

        # MCP ë„êµ¬ ë¡œë“œ (ì—¬ê¸°ì„œëŠ” retriever ë„êµ¬)
        tools = await load_mcp_tools(session)

        # ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰
        agent = create_react_agent(model, tools)

        # ì—ì´ì „íŠ¸ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        await astream_graph(
            agent, {"messages": "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜"}
        )

"""
## SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ í˜¼í•© ì‚¬ìš©

- íŒŒì¼: `mcp_server_rag.py` ëŠ” StdIO ë°©ì‹ìœ¼ë¡œ í†µì‹ 
- `langchain-dev-docs` ëŠ” SSE ë°©ì‹ìœ¼ë¡œ í†µì‹ 

SSE ë°©ì‹ê³¼ StdIO ë°©ì‹ì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic

# Anthropicì˜ Claude ëª¨ë¸ ì´ˆê¸°í™”
model = ChatAnthropic(
    model_name="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000
)

# 1. ë‹¤ì¤‘ ì„œë²„ MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = MultiServerMCPClient(
    {
        "document-retriever": {
            "command": "./.venv/bin/python",
            # mcp_server_rag.py íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤
            "args": ["./mcp_server_rag.py"],
            # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)
            "transport": "stdio",
        },
        "langchain-dev-docs": {
            # SSE ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”
            "url": "https://teddynote.io/mcp/langchain/sse",
            # SSE(Server-Sent Events) ë°©ì‹ìœ¼ë¡œ í†µì‹ 
            "transport": "sse",
        },
    }
)


# 2. ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ëª…ì‹œì  ì—°ê²° ì´ˆê¸°í™”
await client.__aenter__()

"""
langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

prompt = (
    "You are a smart agent. "
    "Use `retriever` tool to search on AI related documents and answer questions."
    "Use `langchain-dev-docs` tool to search on langchain / langgraph related documents and answer questions."
    "Answer in Korean."
)
agent = create_react_agent(
    model, client.get_tools(), prompt=prompt, checkpointer=MemorySaver()
)

"""
êµ¬ì¶•í•´ ë†“ì€ `mcp_server_rag.py` ì—ì„œ ì •ì˜í•œ `retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

config = RunnableConfig(recursion_limit=30, thread_id=1)
await astream_graph(
    agent,
    {
        "messages": "`retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜"
    },
    config=config,
)

"""
ì´ë²ˆì—ëŠ” `langchain-dev-docs` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

config = RunnableConfig(recursion_limit=30, thread_id=1)
await astream_graph(
    agent,
    {"messages": "langgraph-dev-docs ì°¸ê³ í•´ì„œ self-rag ì˜ ì •ì˜ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜"},
    config=config,
)

"""
`MemorySaver` ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¨ê¸° ê¸°ì–µì„ ìœ ì§€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, multi-turn ëŒ€í™”ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

await astream_graph(
    agent, {"messages": "ì´ì „ì˜ ë‚´ìš©ì„ bullet point ë¡œ ìš”ì•½í•´ì¤˜"}, config=config
)

"""
## LangChain ì— í†µí•©ëœ ë„êµ¬ + MCP ë„êµ¬

ì—¬ê¸°ì„œëŠ” LangChain ì— í†µí•©ëœ ë„êµ¬ë¥¼ ê¸°ì¡´ì˜ MCP ë¡œë§Œ ì´ë£¨ì–´ì§„ ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©ì´ ê°€ëŠ¥í•œì§€ í…ŒìŠ¤íŠ¸ í•©ë‹ˆë‹¤.
"""

from langchain_community.tools.tavily_search import TavilySearchResults

# Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤. (news íƒ€ì…, ìµœê·¼ 3ì¼ ë‚´ ë‰´ìŠ¤)
tavily = TavilySearchResults(max_results=3, topic="news", days=3)

# ê¸°ì¡´ì˜ MCP ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.
tools = client.get_tools() + [tavily]

"""
langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

# ì¬ê·€ ì œí•œ ë° ìŠ¤ë ˆë“œ ì•„ì´ë”” ì„¤ì •
config = RunnableConfig(recursion_limit=30, thread_id=2)

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
prompt = "You are a smart agent with various tools. Answer questions in Korean."

# ì—ì´ì „íŠ¸ ìƒì„±
agent = create_react_agent(model, tools, prompt=prompt, checkpointer=MemorySaver())

"""
ìƒˆë¡­ê²Œ ì¶”ê°€í•œ `tavily` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

await astream_graph(agent, {"messages": "ì˜¤ëŠ˜ ë‰´ìŠ¤ ì°¾ì•„ì¤˜"}, config=config)

"""
`retriever` ë„êµ¬ê°€ ì›í™œí•˜ê²Œ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

await astream_graph(
    agent,
    {
        "messages": "`retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì´ë¦„ì„ ê²€ìƒ‰í•´ì¤˜"
    },
    config=config,
)

"""
## Smithery ì—ì„œ ì œê³µí•˜ëŠ” MCP ì„œë²„

- ë§í¬: https://smithery.ai/
"""

"""
ì‚¬ìš©í•œ ë„êµ¬ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking
  - êµ¬ì¡°í™”ëœ ì‚¬ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì—­ë™ì ì´ê³  ì„±ì°°ì ì¸ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” MCP ì„œë²„
- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander
  - ë‹¤ì–‘í•œ í¸ì§‘ ê¸°ëŠ¥ìœ¼ë¡œ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³  íŒŒì¼ì„ ê´€ë¦¬í•˜ì„¸ìš”. ì½”ë”©, ì…¸ ë° í„°ë¯¸ë„, ì‘ì—… ìë™í™”

**ì°¸ê³ **

- smithery ì—ì„œ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¬ë•Œ, ì•„ë˜ì˜ ì˜ˆì‹œì²˜ëŸ¼ `"transport": "stdio"` ë¡œ ê¼­ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
"""

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic

# LLM ëª¨ë¸ ì´ˆê¸°í™”
model = ChatAnthropic(model="claude-3-7-sonnet-latest", temperature=0, max_tokens=20000)

# 1. í´ë¼ì´ì–¸íŠ¸ ìƒì„±
client = MultiServerMCPClient(
    {
        "server-sequential-thinking": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@smithery-ai/server-sequential-thinking",
                "--key",
                "89a4780a-53b7-4b7b-92e9-a29815f2669b",
            ],
            "transport": "stdio",  # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        },
        "desktop-commander": {
            "command": "npx",
            "args": [
                "-y",
                "@smithery/cli@latest",
                "run",
                "@wonderwhy-er/desktop-commander",
                "--key",
                "89a4780a-53b7-4b7b-92e9-a29815f2669b",
            ],
            "transport": "stdio",  # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹ ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        },
        "document-retriever": {
            "command": "./.venv/bin/python",
            # mcp_server_rag.py íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤
            "args": ["./mcp_server_rag.py"],
            # stdio ë°©ì‹ìœ¼ë¡œ í†µì‹  (í‘œì¤€ ì…ì¶œë ¥ ì‚¬ìš©)
            "transport": "stdio",
        },
    }
)


# 2. ëª…ì‹œì ìœ¼ë¡œ ì—°ê²° ì´ˆê¸°í™”
await client.__aenter__()

"""
langgraph ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig

config = RunnableConfig(recursion_limit=30, thread_id=3)
agent = create_react_agent(model, client.get_tools(), checkpointer=MemorySaver())

"""
`Desktop Commander` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í„°ë¯¸ë„ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

await astream_graph(
    agent,
    {
        "messages": "í˜„ì¬ ê²½ë¡œë¥¼ í¬í•¨í•œ í•˜ìœ„ í´ë” êµ¬ì¡°ë¥¼ tree ë¡œ ê·¸ë ¤ì¤˜. ë‹¨, .venv í´ë”ëŠ” ì œì™¸í•˜ê³  ì¶œë ¥í•´ì¤˜."
    },
    config=config,
)

"""
ì´ë²ˆì—ëŠ” `Sequential Thinking` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµì  ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

await astream_graph(
    agent,
    {
        "messages": (
            "`retriever` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ê³  "
            "`Sequential Thinking` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜."
        )
    },
    config=config,
)



================================================
FILE: mcp_server_local.py
================================================
from mcp.server.fastmcp import FastMCP

# Initialize FastMCP server with configuration
mcp = FastMCP(
    "Weather",  # Name of the MCP server
    instructions="You are a weather assistant that can answer questions about the weather in a given location.",  # Instructions for the LLM on how to use this tool
    host="0.0.0.0",  # Host address (0.0.0.0 allows connections from any IP)
    port=8005,  # Port number for the server
)


@mcp.tool()
async def get_weather(location: str) -> str:
    """
    Get current weather information for the specified location.

    This function simulates a weather service by returning a fixed response.
    In a production environment, this would connect to a real weather API.

    Args:
        location (str): The name of the location (city, region, etc.) to get weather for

    Returns:
        str: A string containing the weather information for the specified location
    """
    # Return a mock weather response
    # In a real implementation, this would call a weather API
    return f"It's always Sunny in {location}"


if __name__ == "__main__":
    # Start the MCP server with stdio transport
    # stdio transport allows the server to communicate with clients
    # through standard input/output streams, making it suitable for
    # local development and testing
    mcp.run(transport="stdio")



================================================
FILE: mcp_server_rag.py
================================================
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from typing import Any

# Load environment variables from .env file (contains API keys)
load_dotenv(override=True)


def create_retriever() -> Any:
    """
    Creates and returns a document retriever based on FAISS vector store.

    This function performs the following steps:
    1. Loads a PDF document(place your PDF file in the data folder)
    2. Splits the document into manageable chunks
    3. Creates embeddings for each chunk
    4. Builds a FAISS vector store from the embeddings
    5. Returns a retriever interface to the vector store

    Returns:
        Any: A retriever object that can be used to query the document database
    """
    # Step 1: Load Documents
    # PyMuPDFLoader is used to extract text from PDF files
    loader = PyMuPDFLoader("data/sample.pdf")
    docs = loader.load()

    # Step 2: Split Documents
    # Recursive splitter divides documents into chunks with some overlap to maintain context
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)

    # Step 3: Create Embeddings
    # OpenAI's text-embedding-3-small model is used to convert text chunks into vector embeddings
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # Step 4: Create Vector Database
    # FAISS is an efficient similarity search library that stores vector embeddings
    # and allows for fast retrieval of similar vectors
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)

    # Step 5: Create Retriever
    # The retriever provides an interface to search the vector database
    # and retrieve documents relevant to a query
    retriever = vectorstore.as_retriever()
    return retriever


# Initialize FastMCP server with configuration
mcp = FastMCP(
    "Retriever",
    instructions="A Retriever that can retrieve information from the database.",
    host="0.0.0.0",
    port=8005,
)


@mcp.tool()
async def retrieve(query: str) -> str:
    """
    Retrieves information from the document database based on the query.

    This function creates a retriever, queries it with the provided input,
    and returns the concatenated content of all retrieved documents.

    Args:
        query (str): The search query to find relevant information

    Returns:
        str: Concatenated text content from all retrieved documents
    """
    # Create a new retriever instance for each query
    # Note: In production, consider caching the retriever for better performance
    retriever = create_retriever()

    # Use the invoke() method to get relevant documents based on the query
    retrieved_docs = retriever.invoke(query)

    # Join all document contents with newlines and return as a single string
    return "\n".join([doc.page_content for doc in retrieved_docs])


if __name__ == "__main__":
    # Run the MCP server with stdio transport for integration with MCP clients
    mcp.run(transport="stdio")



================================================
FILE: mcp_server_remote.py
================================================
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(
    "Weather",  # Name of the MCP server
    instructions="You are a weather assistant that can answer questions about the weather in a given location.",  # Instructions for the LLM on how to use this tool
    host="0.0.0.0",  # Host address (0.0.0.0 allows connections from any IP)
    port=8005,  # Port number for the server
)


@mcp.tool()
async def get_weather(location: str) -> str:
    """
    Get current weather information for the specified location.

    This function simulates a weather service by returning a fixed response.
    In a production environment, this would connect to a real weather API.

    Args:
        location (str): The name of the location (city, region, etc.) to get weather for

    Returns:
        str: A string containing the weather information for the specified location
    """
    # Return a mock weather response
    # In a real implementation, this would call a weather API
    return f"It's always Sunny in {location}"


if __name__ == "__main__":
    # Print a message indicating the server is starting
    print("mcp remote server is running...")

    # Start the MCP server with SSE transport
    # Server-Sent Events (SSE) transport allows the server to communicate with clients
    # over HTTP, making it suitable for remote/distributed deployments
    mcp.run(transport="sse")



================================================
FILE: mcp_server_time.py
================================================
from mcp.server.fastmcp import FastMCP
from datetime import datetime
import pytz
from typing import Optional

# Initialize FastMCP server with configuration
mcp = FastMCP(
    "TimeService",  # Name of the MCP server
    instructions="You are a time assistant that can provide the current time for different timezones.",  # Instructions for the LLM on how to use this tool
    host="0.0.0.0",  # Host address (0.0.0.0 allows connections from any IP)
    port=8005,  # Port number for the server
)


@mcp.tool()
async def get_current_time(timezone: Optional[str] = "Asia/Seoul") -> str:
    """
    Get current time information for the specified timezone.

    This function returns the current system time for the requested timezone.

    Args:
        timezone (str, optional): The timezone to get current time for. Defaults to "Asia/Seoul".

    Returns:
        str: A string containing the current time information for the specified timezone
    """
    try:
        # Get the timezone object
        tz = pytz.timezone(timezone)

        # Get current time in the specified timezone
        current_time = datetime.now(tz)

        # Format the time as a string
        formatted_time = current_time.strftime("%Y-%m-%d %H:%M:%S %Z")

        return f"Current time in {timezone} is: {formatted_time}"
    except pytz.exceptions.UnknownTimeZoneError:
        return f"Error: Unknown timezone '{timezone}'. Please provide a valid timezone."
    except Exception as e:
        return f"Error getting time: {str(e)}"


if __name__ == "__main__":
    # Start the MCP server with stdio transport
    # stdio transport allows the server to communicate with clients
    # through standard input/output streams, making it suitable for
    # local development and testing
    mcp.run(transport="stdio")



================================================
FILE: packages.txt
================================================
curl
gnupg
ca-certificates
nodejs
npm


================================================
FILE: pyproject.toml
================================================
[project]
name = "langgraph-mcp-agents"
version = "0.1.0"
description = "LangGraph Agent with MCP Adapters"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "nest-asyncio>=1.6.0",
    "faiss-cpu>=1.10.0",
    "jupyter>=1.1.1",
    "langchain-anthropic>=0.3.10",
    "langchain-community>=0.3.20",
    "langchain-mcp-adapters>=0.0.7",
    "langchain-openai>=0.3.11",
    "langgraph>=0.3.21",
    "mcp[cli]>=1.6.0",
    "notebook>=7.3.3",
    "pymupdf>=1.25.4",
    "python-dotenv>=1.1.0",
    "streamlit>=1.44.1",
]



================================================
FILE: README_KOR.md
================================================
# LangGraph ì—ì´ì „íŠ¸ + MCP

[![English](https://img.shields.io/badge/Language-English-blue)](README.md) [![Korean](https://img.shields.io/badge/Language-í•œêµ­ì–´-red)](README_KOR.md)

[![GitHub](https://img.shields.io/badge/GitHub-langgraph--mcp--agents-black?logo=github)](https://github.com/teddylee777/langgraph-mcp-agents)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-â‰¥3.12-blue?logo=python&logoColor=white)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-0.1.0-orange)](https://github.com/teddylee777/langgraph-mcp-agents)

![project demo](./assets/project-demo.png)

## í”„ë¡œì íŠ¸ ê°œìš”

![project architecture](./assets/architecture.png)

`LangChain-MCP-Adapters`ëŠ” **LangChain AI**ì—ì„œ ì œê³µí•˜ëŠ” íˆ´í‚·ìœ¼ë¡œ, AI ì—ì´ì „íŠ¸ê°€ Model Context Protocol(MCP)ì„ í†µí•´ ì™¸ë¶€ ë„êµ¬ ë° ë°ì´í„° ì†ŒìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” MCP ë„êµ¬ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ APIì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ReAct ì—ì´ì „íŠ¸ë¥¼ ë°°í¬í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### íŠ¹ì§•

- **Streamlit ì¸í„°í˜ì´ìŠ¤**: MCP ë„êµ¬ê°€ í¬í•¨ëœ LangGraph `ReAct Agent`ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤
- **ë„êµ¬ ê´€ë¦¬**: UIë¥¼ í†µí•´ MCP ë„êµ¬ë¥¼ ì¶”ê°€, ì œê±° ë° êµ¬ì„±(Smithery JSON í˜•ì‹ ì§€ì›). ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¬ì‹œì‘í•˜ì§€ ì•Šê³ ë„ ë™ì ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.
- **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: ì—ì´ì „íŠ¸ ì‘ë‹µê³¼ ë„êµ¬ í˜¸ì¶œì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸
- **ëŒ€í™” ê¸°ë¡**: ì—ì´ì „íŠ¸ì™€ì˜ ëŒ€í™” ì¶”ì  ë° ê´€ë¦¬

## MCP ì•„í‚¤í…ì²˜

MCP(Model Context Protocol)ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

1. **MCP í˜¸ìŠ¤íŠ¸**: Claude Desktop, IDE ë˜ëŠ” LangChain/LangGraphì™€ ê°™ì´ MCPë¥¼ í†µí•´ ë°ì´í„°ì— ì ‘ê·¼í•˜ê³ ì í•˜ëŠ” í”„ë¡œê·¸ë¨.

2. **MCP í´ë¼ì´ì–¸íŠ¸**: ì„œë²„ì™€ 1:1 ì—°ê²°ì„ ìœ ì§€í•˜ëŠ” í”„ë¡œí† ì½œ í´ë¼ì´ì–¸íŠ¸ë¡œ, í˜¸ìŠ¤íŠ¸ì™€ ì„œë²„ ì‚¬ì´ì˜ ì¤‘ê°œì ì—­í• ì„ í•©ë‹ˆë‹¤.

3. **MCP ì„œë²„**: í‘œì¤€í™”ëœ ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡œí† ì½œì„ í†µí•´ íŠ¹ì • ê¸°ëŠ¥ì„ ë…¸ì¶œí•˜ëŠ” ê²½ëŸ‰ í”„ë¡œê·¸ë¨ìœ¼ë¡œ, ì£¼ìš” ë°ì´í„° ì†ŒìŠ¤ ì—­í• ì„ í•©ë‹ˆë‹¤.

## Docker ë¡œ ë¹ ë¥¸ ì‹¤í–‰

ë¡œì»¬ Python í™˜ê²½ì„ ì„¤ì •í•˜ì§€ ì•Šê³ ë„ Dockerë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ í”„ë¡œì íŠ¸ë¥¼ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­(Docker Desktop)

ì•„ë˜ì˜ ë§í¬ì—ì„œ Docker Desktopì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

- [Docker Desktop ì„¤ì¹˜](https://www.docker.com/products/docker-desktop/)

### Docker Composeë¡œ ì‹¤í–‰í•˜ê¸°

1. `dockers` ë””ë ‰í† ë¦¬ë¡œ ì´ë™

```bash
cd dockers
```

2. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— API í‚¤ê°€ í¬í•¨ëœ `.env` íŒŒì¼ ìƒì„±.

```bash
cp .env.example .env
```

ë°œê¸‰ ë°›ì€ API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì…ë ¥í•©ë‹ˆë‹¤.

(ì°¸ê³ ) ëª¨ë“  API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì…ë ¥í•˜ì„¸ìš”.
- `ANTHROPIC_API_KEY`: Anthropic API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° "claude-3-7-sonnet-latest", "claude-3-5-sonnet-latest", "claude-3-haiku-latest" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `OPENAI_API_KEY`: OpenAI API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° "gpt-4o", "gpt-4o-mini" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `LANGSMITH_API_KEY`: LangSmith API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° LangSmith tracingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENAI_API_KEY=your_openai_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_PROJECT=LangGraph-MCP-Agents
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
```

(ì‹ ê·œ ê¸°ëŠ¥) ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ê¸°ëŠ¥ ì‚¬ìš©

ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì‚¬ìš©ì‹œ `USE_LOGIN`ì„ `true`ë¡œ ì„¤ì •í•˜ê³ , `USER_ID`ì™€ `USER_PASSWORD`ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.

```bash
USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin123
```

ë§Œì•½, ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´, `USE_LOGIN`ì„ `false`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
USE_LOGIN=false
```

3. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ë§ëŠ” Docker Compose íŒŒì¼ ì„ íƒ.

**AMD64/x86_64 ì•„í‚¤í…ì²˜(Intel/AMD í”„ë¡œì„¸ì„œ)**

```bash
# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker compose -f docker-compose-KOR.yaml up -d
```

**ARM64 ì•„í‚¤í…ì²˜(Apple Silicon M1/M2/M3/M4)**

```bash
# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker compose -f docker-compose-KOR-mac.yaml up -d
```

4. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8585 ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì ‘ì†

(ì°¸ê³ )
- í¬íŠ¸ë‚˜ ë‹¤ë¥¸ ì„¤ì •ì„ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ê²½ìš°, ë¹Œë“œ ì „ì— í•´ë‹¹ docker-compose-KOR.yaml íŒŒì¼ì„ í¸ì§‘í•˜ì„¸ìš”.

## ì†ŒìŠ¤ì½”ë“œë¡œ ë¶€í„° ì§ì ‘ ì„¤ì¹˜

1. ì´ ì €ì¥ì†Œë¥¼ í´ë¡ í•©ë‹ˆë‹¤

```bash
git clone https://github.com/teddynote-lab/langgraph-mcp-agents.git
cd langgraph-mcp-agents
```

2. ê°€ìƒ í™˜ê²½ì„ ìƒì„±í•˜ê³  uvë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤

```bash
uv venv
uv pip install -r requirements.txt
source .venv/bin/activate  # Windowsì˜ ê²½ìš°: .venv\Scripts\activate
```

3. API í‚¤ê°€ í¬í•¨ëœ `.env` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤(`.env.example` ì—ì„œ ë³µì‚¬)

```bash
cp .env.example .env
```

ë°œê¸‰ ë°›ì€ API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì…ë ¥í•©ë‹ˆë‹¤.

(ì°¸ê³ ) ëª¨ë“  API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì…ë ¥í•˜ì„¸ìš”.
- `ANTHROPIC_API_KEY`: Anthropic API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° "claude-3-7-sonnet-latest", "claude-3-5-sonnet-latest", "claude-3-haiku-latest" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `OPENAI_API_KEY`: OpenAI API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° "gpt-4o", "gpt-4o-mini" ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- `LANGSMITH_API_KEY`: LangSmith API í‚¤ë¥¼ ì…ë ¥í•  ê²½ìš° LangSmith tracingì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
OPENAI_API_KEY=your_openai_api_key(optional)
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_PROJECT=LangGraph-MCP-Agents
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
```

4. (ì‹ ê·œ ê¸°ëŠ¥) ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ ê¸°ëŠ¥ ì‚¬ìš©

ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì‚¬ìš©ì‹œ `USE_LOGIN`ì„ `true`ë¡œ ì„¤ì •í•˜ê³ , `USER_ID`ì™€ `USER_PASSWORD`ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.

```bash
USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin123
```

ë§Œì•½, ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ ì•Šë‹¤ë©´, `USE_LOGIN`ì„ `false`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

```bash
USE_LOGIN=false
```

## ì‚¬ìš©ë²•

1. Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. (í•œêµ­ì–´ ë²„ì „ íŒŒì¼ì€ `app_KOR.py` ì…ë‹ˆë‹¤.)

```bash
streamlit run app_KOR.py
```

2. ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ë¸Œë¼ìš°ì €ì—ì„œ ì‹¤í–‰ë˜ì–´ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

3. ì‚¬ì´ë“œë°”ë¥¼ ì‚¬ìš©í•˜ì—¬ MCP ë„êµ¬ë¥¼ ì¶”ê°€í•˜ê³  êµ¬ì„±í•©ë‹ˆë‹¤

ìœ ìš©í•œ MCP ì„œë²„ë¥¼ ì°¾ìœ¼ë ¤ë©´ [Smithery](https://smithery.ai/)ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”.

ë¨¼ì €, ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

ì˜¤ë¥¸ìª½ì˜ JSON êµ¬ì„±ì—ì„œ COPY ë²„íŠ¼ì„ ëˆ„ë¦…ë‹ˆë‹¤.

![copy from Smithery](./assets/smithery-copy-json.png)

ë³µì‚¬ëœ JSON ë¬¸ìì—´ì„ `Tool JSON` ì„¹ì…˜ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.

<img src="./assets/add-tools.png" alt="tool json" style="width: auto; height: auto;">

`Add Tool` ë²„íŠ¼ì„ ëˆŒëŸ¬ "Registered Tools List" ì„¹ì…˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ, "Apply" ë²„íŠ¼ì„ ëˆŒëŸ¬ ìƒˆë¡œìš´ ë„êµ¬ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ë„ë¡ ë³€ê²½ì‚¬í•­ì„ ì ìš©í•©ë‹ˆë‹¤.

<img src="./assets/apply-tool-configuration.png" alt="tool json" style="width: auto; height: auto;">

4. ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

![check status](./assets/check-status.png)

5. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ì„ í•˜ì—¬ êµ¬ì„±ëœ MCP ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ReAct ì—ì´ì „íŠ¸ì™€ ìƒí˜¸ì‘ìš©í•©ë‹ˆë‹¤.

![project demo](./assets/project-demo.png)

## í•¸ì¦ˆì˜¨ íŠœí† ë¦¬ì–¼

ê°œë°œìê°€ MCPì™€ LangGraphì˜ í†µí•© ì‘ë™ ë°©ì‹ì— ëŒ€í•´ ë” ê¹Šì´ ì•Œì•„ë³´ë ¤ë©´, í¬ê´„ì ì¸ Jupyter ë…¸íŠ¸ë¶ íŠœí† ë¦¬ì–¼ì„ ì œê³µí•©ë‹ˆë‹¤:

- ë§í¬: [MCP-HandsOn-KOR.ipynb](./MCP-HandsOn-KOR.ipynb)

ì´ í•¸ì¦ˆì˜¨ íŠœí† ë¦¬ì–¼ì€ ë‹¤ìŒ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤.

1. **MCP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •** - MCP ì„œë²„ì— ì—°ê²°í•˜ê¸° ìœ„í•œ MultiServerMCPClient êµ¬ì„± ë° ì´ˆê¸°í™” ë°©ë²• í•™ìŠµ
2. **ë¡œì»¬ MCP ì„œë²„ í†µí•©** - SSE ë° Stdio ë©”ì„œë“œë¥¼ í†µí•´ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ MCP ì„œë²„ì— ì—°ê²°
3. **RAG í†µí•©** - ë¬¸ì„œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ìœ„í•´ MCPë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ ë„êµ¬ ì ‘ê·¼
4. **í˜¼í•© ì „ì†¡ ë°©ë²•** - í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ì—ì„œ ë‹¤ì–‘í•œ ì „ì†¡ í”„ë¡œí† ì½œ(SSE ë° Stdio) ê²°í•©
5. **LangChain ë„êµ¬ + MCP** - MCP ë„êµ¬ì™€ í•¨ê»˜ ë„¤ì´í‹°ë¸Œ LangChain ë„êµ¬ í†µí•©

ì´ íŠœí† ë¦¬ì–¼ì€ MCP ë„êµ¬ë¥¼ LangGraph ì—ì´ì „íŠ¸ì— êµ¬ì¶•í•˜ê³  í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ë‹¨ê³„ë³„ ì„¤ëª…ì´ í¬í•¨ëœ ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ë¼ì´ì„ ìŠ¤

MIT License 

## íŠœí† ë¦¬ì–¼ ë¹„ë””ì˜¤ ë³´ê¸°(í•œêµ­ì–´)

[![Tutorial Video Thumbnail](https://img.youtube.com/vi/ISrYHGg2C2c/maxresdefault.jpg)](https://youtu.be/ISrYHGg2C2c?si=eWmKFVUS1BLtPm5U)

## ì°¸ê³  ìë£Œ

- https://github.com/langchain-ai/langchain-mcp-adapters




================================================
FILE: requirements.txt
================================================
faiss-cpu>=1.10.0
jupyter>=1.1.1
langchain-anthropic>=0.3.10
langchain-community>=0.3.20
langchain-mcp-adapters>=0.0.7
langchain-openai>=0.3.11
langgraph>=0.3.21
mcp>=1.6.0
notebook>=7.3.3
pymupdf>=1.25.4
python-dotenv>=1.1.0
streamlit>=1.44.1 
nest-asyncio>=1.6.0


================================================
FILE: utils.py
================================================
from typing import Any, Dict, List, Callable, Optional
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph.state import CompiledStateGraph
import uuid


def random_uuid():
    return str(uuid.uuid4())


async def astream_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: List[str] = [],
    callback: Optional[Callable] = None,
    stream_mode: str = "messages",
    include_subgraphs: bool = False,
) -> Dict[str, Any]:
    """
    LangGraphì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ì§ì ‘ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (Optional[RunnableConfig]): ì‹¤í–‰ ì„¤ì • (ì„ íƒì )
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Optional[Callable], optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": Any} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.
        stream_mode (str, optional): ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ ("messages" ë˜ëŠ” "updates"). ê¸°ë³¸ê°’ì€ "messages"
        include_subgraphs (bool, optional): ì„œë¸Œê·¸ë˜í”„ í¬í•¨ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ False

    Returns:
        Dict[str, Any]: ìµœì¢… ê²°ê³¼ (ì„ íƒì )
    """
    config = config or {}
    final_result = {}

    def format_namespace(namespace):
        return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"

    prev_node = ""

    if stream_mode == "messages":
        async for chunk_msg, metadata in graph.astream(
            inputs, config, stream_mode=stream_mode
        ):
            curr_node = metadata["langgraph_node"]
            final_result = {
                "node": curr_node,
                "content": chunk_msg,
                "metadata": metadata,
            }

            # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if not node_names or curr_node in node_names:
                # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
                if callback:
                    result = callback({"node": curr_node, "content": chunk_msg})
                    if hasattr(result, "__await__"):
                        await result
                # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
                else:
                    # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
                    if curr_node != prev_node:
                        print("\n" + "=" * 50)
                        print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
                        print("- " * 25)

                    # Claude/Anthropic ëª¨ë¸ì˜ í† í° ì²­í¬ ì²˜ë¦¬ - í•­ìƒ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    if hasattr(chunk_msg, "content"):
                        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ content (Anthropic/Claude ìŠ¤íƒ€ì¼)
                        if isinstance(chunk_msg.content, list):
                            for item in chunk_msg.content:
                                if isinstance(item, dict) and "text" in item:
                                    print(item["text"], end="", flush=True)
                        # ë¬¸ìì—´ í˜•íƒœì˜ content
                        elif isinstance(chunk_msg.content, str):
                            print(chunk_msg.content, end="", flush=True)
                    # ê·¸ ì™¸ í˜•íƒœì˜ chunk_msg ì²˜ë¦¬
                    else:
                        print(chunk_msg, end="", flush=True)

                prev_node = curr_node

    elif stream_mode == "updates":
        # ì—ëŸ¬ ìˆ˜ì •: ì–¸íŒ¨í‚¹ ë°©ì‹ ë³€ê²½
        # REACT ì—ì´ì „íŠ¸ ë“± ì¼ë¶€ ê·¸ë˜í”„ì—ì„œëŠ” ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ë§Œ ë°˜í™˜í•¨
        async for chunk in graph.astream(
            inputs, config, stream_mode=stream_mode, subgraphs=include_subgraphs
        ):
            # ë°˜í™˜ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬ ë°©ë²• ë¶„ê¸°
            if isinstance(chunk, tuple) and len(chunk) == 2:
                # ê¸°ì¡´ ì˜ˆìƒ í˜•ì‹: (namespace, chunk_dict)
                namespace, node_chunks = chunk
            else:
                # ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ë§Œ ë°˜í™˜í•˜ëŠ” ê²½ìš° (REACT ì—ì´ì „íŠ¸ ë“±)
                namespace = []  # ë¹ˆ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ë£¨íŠ¸ ê·¸ë˜í”„)
                node_chunks = chunk  # chunk ìì²´ê°€ ë…¸ë“œ ì²­í¬ ë”•ì…”ë„ˆë¦¬

            # ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸í•˜ê³  í•­ëª© ì²˜ë¦¬
            if isinstance(node_chunks, dict):
                for node_name, node_chunk in node_chunks.items():
                    final_result = {
                        "node": node_name,
                        "content": node_chunk,
                        "namespace": namespace,
                    }

                    # node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§
                    if len(node_names) > 0 and node_name not in node_names:
                        continue

                    # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
                    if callback is not None:
                        result = callback({"node": node_name, "content": node_chunk})
                        if hasattr(result, "__await__"):
                            await result
                    # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
                    else:
                        # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥ (messages ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ)
                        if node_name != prev_node:
                            print("\n" + "=" * 50)
                            print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
                            print("- " * 25)

                        # ë…¸ë“œì˜ ì²­í¬ ë°ì´í„° ì¶œë ¥ - í…ìŠ¤íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ì²˜ë¦¬
                        if isinstance(node_chunk, dict):
                            for k, v in node_chunk.items():
                                if isinstance(v, BaseMessage):
                                    # BaseMessageì˜ content ì†ì„±ì´ í…ìŠ¤íŠ¸ë‚˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë¥¼ ì²˜ë¦¬
                                    if hasattr(v, "content"):
                                        if isinstance(v.content, list):
                                            for item in v.content:
                                                if (
                                                    isinstance(item, dict)
                                                    and "text" in item
                                                ):
                                                    print(
                                                        item["text"], end="", flush=True
                                                    )
                                        else:
                                            print(v.content, end="", flush=True)
                                    else:
                                        v.pretty_print()
                                elif isinstance(v, list):
                                    for list_item in v:
                                        if isinstance(list_item, BaseMessage):
                                            if hasattr(list_item, "content"):
                                                if isinstance(list_item.content, list):
                                                    for item in list_item.content:
                                                        if (
                                                            isinstance(item, dict)
                                                            and "text" in item
                                                        ):
                                                            print(
                                                                item["text"],
                                                                end="",
                                                                flush=True,
                                                            )
                                                else:
                                                    print(
                                                        list_item.content,
                                                        end="",
                                                        flush=True,
                                                    )
                                            else:
                                                list_item.pretty_print()
                                        elif (
                                            isinstance(list_item, dict)
                                            and "text" in list_item
                                        ):
                                            print(list_item["text"], end="", flush=True)
                                        else:
                                            print(list_item, end="", flush=True)
                                elif isinstance(v, dict) and "text" in v:
                                    print(v["text"], end="", flush=True)
                                else:
                                    print(v, end="", flush=True)
                        elif node_chunk is not None:
                            if hasattr(node_chunk, "__iter__") and not isinstance(
                                node_chunk, str
                            ):
                                for item in node_chunk:
                                    if isinstance(item, dict) and "text" in item:
                                        print(item["text"], end="", flush=True)
                                    else:
                                        print(item, end="", flush=True)
                            else:
                                print(node_chunk, end="", flush=True)

                        # êµ¬ë¶„ì„ ì„ ì—¬ê¸°ì„œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ (messages ëª¨ë“œì™€ ë™ì¼í•˜ê²Œ)

                    prev_node = node_name
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì „ì²´ ì²­í¬ ì¶œë ¥
                print("\n" + "=" * 50)
                print(f"ğŸ”„ Raw output ğŸ”„")
                print("- " * 25)
                print(node_chunks, end="", flush=True)
                # êµ¬ë¶„ì„ ì„ ì—¬ê¸°ì„œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
                final_result = {"content": node_chunks}

    else:
        raise ValueError(
            f"Invalid stream_mode: {stream_mode}. Must be 'messages' or 'updates'."
        )

    # í•„ìš”ì— ë”°ë¼ ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return final_result


async def ainvoke_graph(
    graph: CompiledStateGraph,
    inputs: dict,
    config: Optional[RunnableConfig] = None,
    node_names: List[str] = [],
    callback: Optional[Callable] = None,
    include_subgraphs: bool = True,
) -> Dict[str, Any]:
    """
    LangGraph ì•±ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        graph (CompiledStateGraph): ì‹¤í–‰í•  ì»´íŒŒì¼ëœ LangGraph ê°ì²´
        inputs (dict): ê·¸ë˜í”„ì— ì „ë‹¬í•  ì…ë ¥ê°’ ë”•ì…”ë„ˆë¦¬
        config (Optional[RunnableConfig]): ì‹¤í–‰ ì„¤ì • (ì„ íƒì )
        node_names (List[str], optional): ì¶œë ¥í•  ë…¸ë“œ ì´ë¦„ ëª©ë¡. ê¸°ë³¸ê°’ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        callback (Optional[Callable], optional): ê° ì²­í¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì½œë°± í•¨ìˆ˜. ê¸°ë³¸ê°’ì€ None
            ì½œë°± í•¨ìˆ˜ëŠ” {"node": str, "content": Any} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ë¥¼ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.
        include_subgraphs (bool, optional): ì„œë¸Œê·¸ë˜í”„ í¬í•¨ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ True

    Returns:
        Dict[str, Any]: ìµœì¢… ê²°ê³¼ (ë§ˆì§€ë§‰ ë…¸ë“œì˜ ì¶œë ¥)
    """
    config = config or {}
    final_result = {}

    def format_namespace(namespace):
        return namespace[-1].split(":")[0] if len(namespace) > 0 else "root graph"

    # subgraphs ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ì„œë¸Œê·¸ë˜í”„ì˜ ì¶œë ¥ë„ í¬í•¨
    async for chunk in graph.astream(
        inputs, config, stream_mode="updates", subgraphs=include_subgraphs
    ):
        # ë°˜í™˜ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬ ë°©ë²• ë¶„ê¸°
        if isinstance(chunk, tuple) and len(chunk) == 2:
            # ê¸°ì¡´ ì˜ˆìƒ í˜•ì‹: (namespace, chunk_dict)
            namespace, node_chunks = chunk
        else:
            # ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ë§Œ ë°˜í™˜í•˜ëŠ” ê²½ìš° (REACT ì—ì´ì „íŠ¸ ë“±)
            namespace = []  # ë¹ˆ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ë£¨íŠ¸ ê·¸ë˜í”„)
            node_chunks = chunk  # chunk ìì²´ê°€ ë…¸ë“œ ì²­í¬ ë”•ì…”ë„ˆë¦¬

        # ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸í•˜ê³  í•­ëª© ì²˜ë¦¬
        if isinstance(node_chunks, dict):
            for node_name, node_chunk in node_chunks.items():
                final_result = {
                    "node": node_name,
                    "content": node_chunk,
                    "namespace": namespace,
                }

                # node_namesê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„í„°ë§
                if node_names and node_name not in node_names:
                    continue

                # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
                if callback is not None:
                    result = callback({"node": node_name, "content": node_chunk})
                    # ì½”ë£¨í‹´ì¸ ê²½ìš° await
                    if hasattr(result, "__await__"):
                        await result
                # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
                else:
                    print("\n" + "=" * 50)
                    formatted_namespace = format_namespace(namespace)
                    if formatted_namespace == "root graph":
                        print(f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m ğŸ”„")
                    else:
                        print(
                            f"ğŸ”„ Node: \033[1;36m{node_name}\033[0m in [\033[1;33m{formatted_namespace}\033[0m] ğŸ”„"
                        )
                    print("- " * 25)

                    # ë…¸ë“œì˜ ì²­í¬ ë°ì´í„° ì¶œë ¥
                    if isinstance(node_chunk, dict):
                        for k, v in node_chunk.items():
                            if isinstance(v, BaseMessage):
                                v.pretty_print()
                            elif isinstance(v, list):
                                for list_item in v:
                                    if isinstance(list_item, BaseMessage):
                                        list_item.pretty_print()
                                    else:
                                        print(list_item)
                            elif isinstance(v, dict):
                                for node_chunk_key, node_chunk_value in v.items():
                                    print(f"{node_chunk_key}:\n{node_chunk_value}")
                            else:
                                print(f"\033[1;32m{k}\033[0m:\n{v}")
                    elif node_chunk is not None:
                        if hasattr(node_chunk, "__iter__") and not isinstance(
                            node_chunk, str
                        ):
                            for item in node_chunk:
                                print(item)
                        else:
                            print(node_chunk)
                    print("=" * 50)
        else:
            # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì „ì²´ ì²­í¬ ì¶œë ¥
            print("\n" + "=" * 50)
            print(f"ğŸ”„ Raw output ğŸ”„")
            print("- " * 25)
            print(node_chunks)
            print("=" * 50)
            final_result = {"content": node_chunks}

    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return final_result



================================================
FILE: .env.example
================================================
ANTHROPIC_API_KEY=sk-ant-api03...
OPENAI_API_KEY=sk-proj-o0gulL2J2a...
LANGSMITH_API_KEY=lsv2_sk_ed22...
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=LangGraph-MCP-Agents

USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin1234


================================================
FILE: .python-version
================================================
3.12




================================================
FILE: dockers/config.json
================================================
{
  "get_current_time": {
    "command": "python",
    "args": [
      "./mcp_server_time.py"
    ],
    "transport": "stdio"
  }
}


================================================
FILE: dockers/docker-compose-KOR-mac.yaml
================================================
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDPLATFORM: ${BUILDPLATFORM:-linux/arm64}
        TARGETPLATFORM: "linux/arm64"
    image: teddylee777/langgraph-mcp-agents:KOR-0.2.1
    platform: "linux/arm64"
    ports:
      - "8585:8585"
    env_file:
      - ./.env
    environment:
      - PYTHONUNBUFFERED=1
      # Mac-specific optimizations
      - NODE_OPTIONS=--max_old_space_size=2048
      # Delegated file system performance for macOS
      - PYTHONMALLOC=malloc
      - USE_LOGIN=${USE_LOGIN:-false}
      - USER_ID=${USER_ID:-}
      - USER_PASSWORD=${USER_PASSWORD:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NODE_OPTIONS=${NODE_OPTIONS:-}
    volumes:
      - ./data:/app/data:cached
      - ./config.json:/app/config.json
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8585/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s



================================================
FILE: dockers/docker-compose-KOR.yaml
================================================
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDPLATFORM: ${BUILDPLATFORM:-linux/amd64}
        TARGETPLATFORM: ${TARGETPLATFORM:-linux/amd64}
    image: teddylee777/langgraph-mcp-agents:KOR-0.2.1
    platform: ${TARGETPLATFORM:-linux/amd64}
    ports:
      - "8585:8585"
    volumes:
      - ./.env:/app/.env:ro
      - ./data:/app/data:rw
      - ./config.json:/app/config.json
    env_file:
      - ./.env
    environment:
      - PYTHONUNBUFFERED=1
      - USE_LOGIN=${USE_LOGIN:-false}
      - USER_ID=${USER_ID:-}
      - USER_PASSWORD=${USER_PASSWORD:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NODE_OPTIONS=${NODE_OPTIONS:-}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8585/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s



================================================
FILE: dockers/docker-compose-mac.yaml
================================================
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDPLATFORM: ${BUILDPLATFORM:-linux/arm64}
        TARGETPLATFORM: "linux/arm64"
    image: teddylee777/langgraph-mcp-agents:0.2.1
    platform: "linux/arm64"
    ports:
      - "8585:8585"
    env_file:
      - ./.env
    environment:
      - PYTHONUNBUFFERED=1
      # Mac-specific optimizations
      - NODE_OPTIONS=--max_old_space_size=2048
      # Delegated file system performance for macOS
      - PYTHONMALLOC=malloc
      - USE_LOGIN=${USE_LOGIN:-false}
      - USER_ID=${USER_ID:-}
      - USER_PASSWORD=${USER_PASSWORD:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NODE_OPTIONS=${NODE_OPTIONS:-}
    volumes:
      - ./data:/app/data:cached
      - ./config.json:/app/config.json
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8585/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s



================================================
FILE: dockers/docker-compose.yaml
================================================
services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        BUILDPLATFORM: ${BUILDPLATFORM:-linux/amd64}
        TARGETPLATFORM: ${TARGETPLATFORM:-linux/amd64}
    image: teddylee777/langgraph-mcp-agents:0.2.1
    platform: ${TARGETPLATFORM:-linux/amd64}
    ports:
      - "8585:8585"
    volumes:
      # Optionally, you can remove this volume if you donâ€™t need the file at runtime
      - ./.env:/app/.env:ro
      - ./data:/app/data:rw
      - ./config.json:/app/config.json
    env_file:
      - ./.env
    environment:
      - PYTHONUNBUFFERED=1
      - USE_LOGIN=${USE_LOGIN:-false}
      - USER_ID=${USER_ID:-}
      - USER_PASSWORD=${USER_PASSWORD:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NODE_OPTIONS=${NODE_OPTIONS:-}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8585/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s



================================================
FILE: dockers/.env.example
================================================
ANTHROPIC_API_KEY=sk-ant-api03...
OPENAI_API_KEY=sk-proj-o0gulL2J2a...
LANGSMITH_API_KEY=lsv2_sk_ed22...
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=LangGraph-MCP-Agents

USE_LOGIN=true
USER_ID=admin
USER_PASSWORD=admin1234

