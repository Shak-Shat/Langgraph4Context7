Directory structure:
└── clevrr-ai-clevrr-computer/
    ├── README.md
    ├── main.py
    ├── requirements.txt
    ├── .env_dev
    ├── examples/
    └── utils/
        ├── agent.py
        ├── contants.py
        ├── prompt.py
        ├── tools.py
        └── __pycache__/

================================================
FILE: README.md
================================================
# Clevrr Computer

Clevrr Computer, inspired by  is an automation agent designed to perform precise and efficient system actions on behalf of the user using the PyAutoGUI library. It can automate keyboard, mouse, and screen interactions while ensuring safety and accuracy in every task.

## Features

- Automate mouse movements, clicks, and keyboard inputs.
- Take screenshots and manage windows.
- Handle errors gracefully and provide feedback.
- Execute tasks with maximum precision to avoid unintentional actions.

## Installation

> [!CAUTION]
> Clevrr Computer is a beta feature. Please be aware that Clevrr Computer poses unique risks that are distinct from standard API features or chat interfaces. These risks are heightened when using Clevrr Computer to interact with the internet. To minimize risks, consider taking precautions such as:
>
> - Use a dedicated virtual machine or container with minimal privileges to prevent direct system attacks or accidents.
> - Avoid giving the model access to sensitive data, such as account login information, to prevent information theft.
> - Limit internet access to an allowlist of domains to reduce exposure to malicious content.
> - Ask a human to confirm decisions that may result in meaningful real-world consequences as well as any tasks requiring affirmative consent, such as accepting cookies, executing financial transactions, or agreeing to terms of service.
>
> In some circumstances, Clevrr Computer will follow commands found in content even if it conflicts with the user's instructions. For example, instructions on webpages or contained in images may override user instructions or cause Clevrr Computer to make mistakes. We suggest taking precautions to isolate Clevrr Computer from sensitive data and actions to avoid risks related to prompt injection.


1. **Clone the repository:**

   ```bash
   git clone https://github.com/Clevrr-AI/Clevrr-Computer.git
   cd Clevrr-Computer
   ```

2. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables:**

   Rename the `.env_dev` file to `.env` and add your API keys and other configurations:

   ```plaintext
   AZURE_OPENAI_API_KEY=<YOUR_AZURE_API_KEY>
   AZURE_OPENAI_ENDPOINT=<YOUR_AZURE_ENDPOINT_URL>
   AZURE_OPENAI_API_VERSION=<YOUR_AZURE_API_VERSION>
   AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=<YOUR_AZURE_DEPLOYMENT_NAME>
   GOOGLE_API_KEY=<YOUR_GEMINI_API_KEY>
   ```

## Usage

1. **Run the Application:**

   You can run the application using the following command:

   ```bash
   python main.py
   ```

   By default, this will use the `gemini` model and enable the floating UI.

2. **Optional Arguments:**

    - **Choose a Model:**
    You can specify which model to use by passing the `--model` argument. Only acceptable args are `gemini` or `openai`.

    ```bash
    python main.py --model openai
    ```

    - **Floating UI:**
    The TKinter UI will be floating and remain on top of the screen by default. You can disable this behavior by passing the `--float-ui` flag as `0`. By default it will be `1`.

    ```bash
    python main.py --float-ui 0
    ```


## Examples

![Demo](./examples/demo.gif)

![Demo 2](./examples/demo_2.gif)

![Example 1](./examples/2.png)

![Example 2](./examples/3.png)

![Example 3](./examples/4.png)

![Example 4](./examples/5.png)


## How it works?
It's a multi-modal AI Agent running at the back with a constant screenshot capturing mechanism to learn what it is seeing on the screen and direct the main action agent to function accordingly, using python' `PyAutoGUI` library to perform actions.
- The agent is given a task to perform and it creates a chain of thought to perform the task.
- It uses the `get_screen_info` tool to get the information about the screen. This tool takes a screenshot of the current screen and uses a grid to mark the true coordinates of the screen. It then uses a Multi-modal LLM to understand the contents of the screen and give answer based on the agent's questions.
- The Chain of thought is then used to perform the task, supported by the `get_screen_info` tool and the `PythonREPLAst` tool, which is design to perform actions using the `PyAutoGUI` library of Python.


## Contributing

Contributions are welcome! Please fork the repository and submit a pull request for any improvements or bug fixes.

## Contact

For any questions or issues, please contact [yurvaj@getclevrr.com](mailto:yurvaj@getclevrr.com).



================================================
FILE: main.py
================================================
from utils.agent import create_clevrr_agent
from utils.prompt import prompt
from utils.contants import *

import time

import pyautogui as pg

pg.PAUSE = 2

import argparse
from tkinter import *


def main():
    # Initialize the argument parser
    parser = argparse.ArgumentParser(description="Launch the application with optional model and UI settings.")
    
    # Add arguments
    parser.add_argument('--model', type=str, default='gemini', choices=['openai', 'gemini'],
                        help="Choose the model to use. Default is 'gemini'. Options: 'openai', 'gemini'.")
    parser.add_argument('--float-ui', type=int, default=1, choices=[0, 1],
                        help="Enable or disable the float UI. Default is 1 (enabled). Pass 0 to disable.")
    
    # Parse the arguments
    args = parser.parse_args()

    # Convert float-ui argument to boolean
    float_ui = bool(args.float_ui)

    # Print out the configurations
    print(f"Using model: {args.model}")
    print(f"Float UI is {'enabled' if float_ui else 'disabled'}")

    # Create the agent executor
    agent_executor = create_clevrr_agent(MODELS[args.model], prompt)

    # Initialize the GUI
    root = Tk()
    root.title("Clevrr Computer")
    
    # Set the window size to fill the whole screen vertically and 20% horizontally
    screen_width = root.winfo_screenwidth()
    screen_height = root.winfo_screenheight()
    window_width = int(screen_width * 0.3)
    window_height = screen_height - 150
    root.geometry(f"{window_width}x{window_height}")

    # Define the send function
    def send():
        # pg.hotkey("alt", "tab")
        user_input = e.get().lower()
        txt.insert(END, f"\nYou -> {user_input}")
        time.sleep(1.5)
        response = agent_executor.invoke({"input": user_input})
        txt.insert(END, f"\nBot -> {response.get('output')}")

        e.delete(0, END)
    
    # Set up the GUI components
    Label(root, bg=BG_COLOR, fg=TEXT_COLOR, text="Welcome to Clevrr Computer", font=FONT_BOLD, pady=10, width=30, height=2).grid(row=0)
    
    txt = Text(root, bg=BG_COLOR, fg=TEXT_COLOR, font=FONT, width=40, height=30)
    txt.grid(row=1, column=0, columnspan=2)
    
    scrollbar = Scrollbar(txt)
    scrollbar.place(relheight=1, relx=0.974)
    
    e = Entry(root, bg="#FCFCFC", fg=TEXT_COLOR, font=FONT, width=30)
    e.grid(row=2, column=0)
    
    Button(root, text="Send", font=FONT_BOLD, bg=BG_GRAY, command=send).grid(row=2, column=1)

    # Set window attributes and start the main loop
    root.attributes('-topmost', float_ui)
    root.mainloop()

if __name__ == "__main__":
    main()



================================================
FILE: requirements.txt
================================================
aiohappyeyeballs==2.3.2
aiohttp==3.10.0
aiosignal==1.3.1
annotated-types==0.7.0
anyio==4.4.0
attrs==23.2.0
blinker==1.8.2
CacheControl==0.14.0
cachetools==5.4.0
certifi==2024.7.4
cffi==1.16.0
charset-normalizer==3.3.2
click==8.1.7
colorama==0.4.6
cryptography==43.0.0
dataclasses-json==0.6.7
distro==1.9.0
et-xmlfile==1.1.0
frozenlist==1.4.1
google-ai-generativelanguage==0.6.6
google-api-core==2.19.1
google-api-python-client==2.139.0
google-auth==2.32.0
google-auth-httplib2==0.2.0
google-cloud-core==2.4.1
google-cloud-firestore==2.17.0
google-cloud-storage==2.18.0
google-crc32c==1.5.0
google-generativeai==0.7.2
google-resumable-media==2.7.1
googleapis-common-protos==1.63.2
greenlet==3.0.3
grpcio==1.65.2
grpcio-status==1.62.2
h11==0.14.0
httpcore==1.0.5
httplib2==0.22.0
httpx==0.27.0
idna==3.7
ipykernel==6.29.4
ipython==8.24.0
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.2.11
langchain-community==0.2.10
langchain-core==0.2.25
langchain-experimental==0.0.63
langchain-google-genai==1.0.8
langchain-openai==0.1.19
langchain-text-splitters==0.2.2
langsmith==0.1.94
MarkupSafe==2.1.5
marshmallow==3.21.3
msgpack==1.0.8
multidict==6.0.5
mypy-extensions==1.0.0
numpy==1.26.4
openai==1.37.1
openpyxl==3.1.5
orjson==3.10.6
packaging==24.1
pandas==2.2.2
prompt-toolkit==3.0.43
proto-plus==1.24.0
protobuf==4.25.4
pyasn1==0.6.0
pyasn1_modules==0.4.0
pycparser==2.22
pydantic==2.8.2
pydantic_core==2.20.1
PyJWT==2.8.0
pyparsing==3.1.2
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytz==2024.1
PyYAML==6.0.1
regex==2024.7.24
requests==2.32.3
rsa==4.9
six==1.16.0
sniffio==1.3.1
SQLAlchemy==2.0.31
tabulate==0.9.0
tenacity==8.5.0
tiktoken==0.7.0
tqdm==4.66.4
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.1
uritemplate==4.1.1
urllib3==2.2.2
Werkzeug==3.0.3
yarl==1.9.4
pyautogui==0.9.54


================================================
FILE: .env_dev
================================================
AZURE_OPENAI_API_KEY  = "<YOUR_AZURE_API_KEY>"
AZURE_OPENAI_ENDPOINT = "<YOUR_AZURE_ENDPOINT_URL>"
AZURE_OPENAI_API_VERSION = "<YOUR_AZURE_API_VERSION>"
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = "<YOUR_AZURE_DEPLOYMENT_NAME>"
GOOGLE_API_KEY = "<YOUR_GEMINI_API_KEY>"
VERSION = "0.9.2" # Do not change this.
LAST_CHANGES = ["Support for Multiple Sheets in files", "Updated to 0.9.2"] # Do not change this.



================================================
FILE: utils/agent.py
================================================
from langchain.agents import AgentExecutor, create_react_agent
from langchain_experimental.tools import PythonAstREPLTool

from utils.contants import MODELS
from utils.tools import get_screen_info

import pyautogui as pg # type: ignore

pg.PAUSE = 2

def create_clevrr_agent(model, prompt):
    print("============================================\nInitialising Clevrr Agent\n============================================")
    df_locals = {}
    df_locals["pg"] = pg
    tools = [PythonAstREPLTool(locals=df_locals), get_screen_info]
    model = MODELS["openai"]
    agent = create_react_agent(model, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True)
    return agent_executor


================================================
FILE: utils/contants.py
================================================
from langchain_openai import AzureChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI

import dotenv
import os

_ = dotenv.load_dotenv()

BG_GRAY = "#5D5FEF"
BG_COLOR = "#F2F2F2"
TEXT_COLOR = "#1C1C1C"

FONT = "Helvetica 14"
FONT_BOLD = "Helvetica 13 bold"

OPENAI = AzureChatOpenAI(
    openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    azure_deployment=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
)

GEMINI = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro-latest",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
)

MODELS = {
    "gemini": GEMINI,
    "openai": OPENAI
}

PREFIX = """
YOU ARE AN EXPERT AUTOMATION AGENT WITH FULL ACCESS TO THE PyAutoGUI LIBRARY in the variable `pg`, SPECIALIZED IN PERFORMING PRECISE AND EFFICIENT SYSTEM ACTIONS ON BEHALF OF THE USER. YOU MUST FOLLOW THE USER'S COMMANDS TO AUTOMATE KEYBOARD, MOUSE, AND SCREEN INTERACTIONS, WHILE ENSURING SAFETY AND ACCURACY IN EVERY TASK. YOU ARE RESPONSIBLE FOR COMPLETING TASKS SWIFTLY, AVOIDING ERRORS, AND HANDLING POTENTIAL EXCEPTIONS GRACEFULLY.

INSTRUCTIONS

- You MUST use the variable `pg` of PyAutoGUI library to perform system actions such as moving the mouse, clicking, typing, taking screenshots, and automating window actions as directed by the user.
- Always EXECUTE tasks with maximum precision to avoid unintentional actions.
- You MUST IMPLEMENT a logical chain of thoughts to approach every task methodically, ensuring the user's commands are carried out on action at a time.
- ONLY perform one action at a time from the chain of thoughts. DO NOT write code to perform all actions all at once.
- After each action, use the `get_screen_info` tool to get the information of the screen, coordinates to click, and plan the next actions to be taken.
- ALWAYS CATCH ERRORS or unexpected situations, and inform the user about potential issues.

FOLLOW this process to AUTOMATE each task effectively:

1. Thought:
    1.1. THOROUGHLY READ the user's request and IDENTIFY the specific system action they want to automate.
    1.2. EVALUATE whether the task is feasible using PyAutoGUI, considering any constraints related to screen size, active windows, or input permissions.

2. Action Input:
    2.0. OPEN the app in the user's request from the Windows search bar by pressing `pg.press('win')\npg.write(<app_name>)`. DO NOT SKIP THIS STEP.
    2.1. INITIATE the appropriate PyAutoGUI functions (e.g., mouse movement, typing, clicking) based on the user's request.
    2.2. MAKE USE of `pyautogui` commands such as `moveTo`, `click`, `write`, `press`, `screenshot`, etc., while confirming coordinates and actions.
    2.3. MAKE USE of `get_screen_info` tool to validate whether the previous step is successfully completed or not.
    2.4. HANDLE task dependencies (e.g., waiting for certain screens, pauses, or timeouts) by using PyAutoGUI's built-in functions like `sleep` or `timeout`.
    2.5. ALWAYS wait for 5 seconds after each action to ensure the system has time to process the action.
    2.6. ONLY perform one action at a time and do not write code to perform all actions at once.

3. VERIFY THE OUTCOME:
    3.0. Call the `get_screen_info` tool after every action and plan the next action accordingly.
    3.1. PROVIDE FEEDBACK to the user, confirming the successful completion of the task.
    3.2. If an error occurs (e.g., the screen changes unexpectedly or coordinates are incorrect), IMPLEMENT error handling and INFORM the user clearly.


OBEY these rules to avoid common pitfalls:
- ALWAYS open the app in the user's request from the task bar by pressing Windows button and searching for that app. DO NOT SKIP this step.
- ALWAYS call the `get_screen_info` tool to verify the previous step has been successfully completed or not. DO NOT SKIP THIS STEP
- NEVER PERFORM DANGEROUS SYSTEM ACTIONS (e.g., force quitting critical applications, deleting system files) unless the user explicitly requests it and you have confirmed their intent.
- DO NOT MAKE ASSUMPTIONS about user intent—always follow their exact request, and if unclear, ASK for clarification.
- AVOID MOVING THE MOUSE OR TYPING without calculating the correct screen coordinates or target window using the `get_screen_info` tool.
- NEVER IGNORE ERRORS—if PyAutoGUI fails to perform an action (e.g., window not found), INFORM the user and PROVIDE an alternative solution.
- DO NOT OVERUSE SYSTEM RESOURCES—ensure that frequent or complex automation tasks are performed efficiently, minimizing system load.

"""
EXAMPLES = """#### Example 1: Move Mouse to Specific Coordinates and Click
User: "Open YouTube in Google Chrome"
Agent:
Thought: User wants to open YouTube on Google Chrome. For this I need to perform the following tasks.\n1. Open Google Chrome, if not already opened.\n2. Search for https://youtube.com/ in a new tab.
Action: get_screen_info
Action Input: Is Google Chrome open?
Yes
Thought: Chrome is Open. Open a new tab and search for Youtube
Action Input:
```python
pg.press('Win')
pg.write('chrome')
pg.press('enter')
pg.hotkey("ctrl", "t") # Open new window
pg.write("https://youtube.com") # Open YouTube
print("I have opened the YouTube page on Google Chrome")
```
Verify: YouTube successfully Opened in a new tab. Report success.


#### Example 2: Type a Message in a Text Editor
User: "Open Notepad and type 'Hello, World!'."
Agent:
Thought: User wants Notepad opened and text typed.
Action: python_repl_ast
Action Input:
```python
pg.press('win')  # Open start menu
pg.write('Notepad')  # Type 'Notepad'
pg.press('enter')  # Open Notepad
pg.write('Hello, World!')  # Type the message
print("I have written 'Hello, World!' in the notepad")
```
VERIFY: Notepad opened, text written. Report completion.
"""

SUFFIX = """
User's input: {input}

You have access to the following tools: {tools}

Carefully use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}], it should only contain the tool name and nothing else
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!


Question: {input}
Thought:{agent_scratchpad}
"""



================================================
FILE: utils/prompt.py
================================================
from langchain_core.prompts import PromptTemplate
from utils.contants import PREFIX, SUFFIX, EXAMPLES

_template = PREFIX + "\n\n" + EXAMPLES + "\n\n"  + SUFFIX
prompt = PromptTemplate(input_variables=['agent_scratchpad', 'tool_names', 'input', 'tools'], template=_template)


================================================
FILE: utils/tools.py
================================================
from langchain.pydantic_v1 import BaseModel, Field
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.tools import tool

from utils.contants import MODELS

from PIL import Image, ImageDraw, ImageFont
import pyautogui as pg
import base64

pg.PAUSE = 2

def get_ruled_screenshot():

    image = pg.screenshot()
    # Get the image dimensions
    width, height = image.size

    # Create a new image for the semi-transparent layer
    overlay = Image.new("RGBA", (width, height), (255, 255, 255, 0))  # Transparent layer
    draw = ImageDraw.Draw(overlay)

    # Set the line color (gray) and line opacity (adjusting the alpha value)
    line_color = (200, 200, 0, 128)  # The last value (128) controls opacity, 0 = fully transparent, 255 = fully opaque

    # Load a font for the labels (you can specify any TTF font you have)
    try:
        font = ImageFont.truetype("arial.ttf", 25)
    except IOError:
        font = ImageFont.load_default()

    # Draw vertical and horizontal lines every 100 pixels and add labels
    for x in range(0, width, 50):
        draw.line([(x, 0), (x, height)], fill=line_color, width=1)
        # Add labels at the top for vertical lines
        if x % 100 == 0:
            draw.text((x + 5, 5), str(x), font=font, fill=(250, 250, 0, 128))
            draw.text((x, height - 25), str(x), font=font, fill=(250, 250, 0, 128))

    for y in range(0, height, 50):
        draw.line([(0, y), (width, y)], fill=line_color, width=1)
        # Add labels on the left for horizontal lines
        if y % 100 == 0:
            draw.text((5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))
            text_width, text_height = 35, 15
            draw.text((width - text_width - 5, y + 5), str(y), font=font, fill=(0, 250, 250, 128))

    # Convert screenshot to RGBA for proper merging
    image = image.convert("RGBA")

    # Merge the overlay (with lines and labels) back onto the original image
    combined = Image.alpha_composite(image.convert("RGBA"), overlay)
    combined.save("screenshot.png")

    from langchain.pydantic_v1 import BaseModel, Field

class ScreenInfo(BaseModel):
    query: str = Field(description="should be a question about the screenshot of the current screen. Should always be in text.")

@tool(args_schema=ScreenInfo)
def get_screen_info(question: str) -> dict:
    """Tool to get the information about the current screen on the basis of the question of the user. The tool will take the screenshot of the screen to understand the contents of the screen and give answer based on the agent's questions. Do not write code to take screenshot."""
    try:
        get_ruled_screenshot()
        with open(f"screenshot.png", "rb") as image:
            image = base64.b64encode(image.read()).decode("utf-8")
            messages = [
                SystemMessage(
                content="""You are a Computer agent that is responsible for answering questions based on the input provided to you. You will have access to the screenshot of the current screen of the user along with a grid marked with true coordinates of the screen. The size of the screen is 1920 x 1080 px.
                        ONLY rely on the coordinates marked in the screen. DO NOT create an assumption of the coordinates. 
                        Here's how you can help:
                        1. Find out coordinates of a specific thing. You have to be super specific about the coordinates. These coordinates will be passed to PyAutoGUI Agent to perform further tasks. Refer the grid line to get the accurate coordinates.
                        2. Give information on the contents of the screen.
                        3. Analyse the screen to give instructions to perform further steps.
                        
                    """
                ),
                HumanMessage(
                    content=[
                        {
                            "type": "text",
                            "text": f"{question}"
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image}"}
                        }
                    ]
                )
            ]
            image_model = MODELS["gemini"]
            response = image_model.invoke(messages)
            return response.content
        
    except Exception as e:
        return {"error": str(e)}



