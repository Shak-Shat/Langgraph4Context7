Directory structure:
└── tools/
    ├── __init__.py
    ├── data_loader.py
    ├── dataframe.py
    ├── eda.py
    ├── h2o.py
    ├── mlflow.py
    └── sql.py

================================================
FILE: ai_data_science_team/tools/__init__.py
================================================



================================================
FILE: ai_data_science_team/tools/data_loader.py
================================================

from langchain.tools import tool
from langgraph.prebuilt import InjectedState

import pandas as pd
import os

from typing import Tuple, List, Dict, Optional, Annotated


@tool(response_format='content_and_artifact')
def load_directory(
    directory_path: str = os.getcwd(),  
    file_type: Optional[str] = None
) -> Tuple[str, Dict]:
    """
    Tool: load_directory
    Description: Loads all recognized tabular files in a directory. 
                 If file_type is specified (e.g., 'csv'), only files 
                 with that extension are loaded.
    
    Parameters:
    ----------
    directory_path : str
        The path to the directory to load. Defaults to the current working directory.

    file_type : str, optional
        The extension of the file type you want to load exclusively 
        (e.g., 'csv', 'xlsx', 'parquet'). If None or not provided, 
        attempts to load all recognized tabular files.
    
    Returns:
    -------
    Tuple[str, Dict]
        A tuple containing a message and a dictionary of data frames.
    """
    print(f"    * Tool: load_directory | {directory_path}")
    
    import os
    import pandas as pd
    
    if directory_path is None:
        return "No directory path provided.", {}
    
    if not os.path.isdir(directory_path):
        return f"Directory not found: {directory_path}", {}

    data_frames = {}

    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        
        # Skip directories
        if os.path.isdir(file_path):
            continue

        # If file_type is specified, only process files that match.
        if file_type:
            # Make sure extension check is case-insensitive
            if not filename.lower().endswith(f".{file_type.lower()}"):
                continue
        
        try:
            # Attempt to auto-detect and load the file
            data_frames[filename] = auto_load_file(file_path).to_dict()
        except Exception as e:
            # If loading fails, record the error message
            data_frames[filename] = f"Error loading file: {e}"

    return (
        f"Returned the following data frames: {list(data_frames.keys())}",
        data_frames
    )


@tool(response_format='content_and_artifact')
def load_file(file_path: str) -> Tuple[str, Dict]:
    """
    Automatically loads a file based on its extension.
    
    Parameters:
    ----------
    file_path : str
        The path to the file to load.
        
    Returns:
    -------
    Tuple[str, Dict]
        A tuple containing a message and a dictionary of the data frame.
    """
    print(f"    * Tool: load_file | {file_path}")
    return f"Returned the following data frame from this file: {file_path}", auto_load_file(file_path).to_dict()


@tool(response_format='content_and_artifact')
def list_directory_contents(
    directory_path: str = os.getcwd(),  
    show_hidden: bool = False
) -> Tuple[List[str], List[Dict]]:
    """
    Tool: list_directory_contents
    Description: Lists all files and folders in the specified directory.
    Args:
        directory_path (str): The path of the directory to list.
        show_hidden (bool): Whether to include hidden files (default: False).
    Returns:
        tuple:
            - content (list[str]): A list of filenames/folders (suitable for display)
            - artifact (list[dict]): A list of dictionaries where each dict includes 
              the keys {"filename": <name>, "type": <'file' or 'directory'>}.
              This structure can be easily converted to a pandas DataFrame.
    """
    print(f"    * Tool: list_directory_contents | {directory_path}")
    import os
    
    if directory_path is None:
        return "No directory path provided.", []
    
    if not os.path.isdir(directory_path):
        return f"Directory not found: {directory_path}", []
    
    items = []
    for item in os.listdir(directory_path):
        # If show_hidden is False, skip items starting with '.'
        if not show_hidden and item.startswith('.'):
            continue
        items.append(item)
    items.reverse()

    # content: just the raw list of item names (files/folders).
    content = items.copy()
    
    content.append(f"Total items: {len(items)}")
    content.append(f"Directory: {directory_path}")

    # artifact: list of dicts with both "filename" and "type" keys.
    artifact = []
    for item in items:
        item_path = os.path.join(directory_path, item)
        artifact.append({
            "filename": item,
            "type": "directory" if os.path.isdir(item_path) else "file"
        })

    return content, artifact



@tool(response_format='content_and_artifact')
def list_directory_recursive(
    directory_path: str = os.getcwd(), 
    show_hidden: bool = False
) -> Tuple[str, List[Dict]]:
    """
    Tool: list_directory_recursive
    Description:
        Recursively lists all files and folders within the specified directory.
        Returns a two-tuple:
          (1) A human-readable tree representation of the directory (content).
          (2) A list of dicts (artifact) that can be easily converted into a DataFrame.

    Args:
        directory_path (str): The path of the directory to list.
        show_hidden (bool): Whether to include hidden files (default: False).

    Returns:
        Tuple[str, List[dict]]:
            content: A multiline string showing the directory tree.
            artifact: A list of dictionaries, each with information about a file or directory.

    Example:
        content, artifact = list_directory_recursive("/path/to/folder", show_hidden=False)
    """
    print(f"    * Tool: list_directory_recursive | {directory_path}")

    # We'll store two things as we recurse:
    # 1) lines for building the "tree" string
    # 2) records in a list of dicts for easy DataFrame creation
    import os
    
    if directory_path is None:
        return "No directory path provided.", {}
    
    if not os.path.isdir(directory_path):
        return f"Directory not found: {directory_path}", {}
    
    lines = []
    records = []

    def recurse(path: str, indent_level: int = 0):
        # List items in the current directory
        try:
            items = os.listdir(path)
        except PermissionError:
            # If we don't have permission to read the directory, just note it.
            lines.append("  " * indent_level + "[Permission Denied]")
            return

        # Sort items for a consistent order (optional)
        items.sort()

        for item in items:
            if not show_hidden and item.startswith('.'):
                continue

            full_path = os.path.join(path, item)
            # Build an indented prefix for the tree
            prefix = "  " * indent_level

            if os.path.isdir(full_path):
                # Directory
                lines.append(f"{prefix}{item}/")
                records.append({
                    "type": "directory",
                    "name": item,
                    "parent_path": path,
                    "absolute_path": full_path
                })
                # Recursively descend
                recurse(full_path, indent_level + 1)
            else:
                # File
                lines.append(f"{prefix}- {item}")
                records.append({
                    "type": "file",
                    "name": item,
                    "parent_path": path,
                    "absolute_path": full_path
                })

    # Kick off recursion
    if os.path.isdir(directory_path):
        # Add the top-level directory to lines/records if you like
        dir_name = os.path.basename(os.path.normpath(directory_path)) or directory_path
        lines.append(f"{dir_name}/")  # Show the root as well
        records.append({
            "type": "directory",
            "name": dir_name,
            "parent_path": os.path.dirname(directory_path),
            "absolute_path": os.path.abspath(directory_path)
        })
        recurse(directory_path, indent_level=1)
    else:
        # If the given path is not a directory, just return a note
        lines.append(f"{directory_path} is not a directory.")
        records.append({
            "type": "error",
            "name": directory_path,
            "parent_path": None,
            "absolute_path": os.path.abspath(directory_path)
        })

    # content: multiline string with the entire tree
    content = "\n".join(lines)
    # artifact: list of dicts, easily converted into a DataFrame
    artifact = records

    return content, artifact


@tool(response_format='content_and_artifact')
def get_file_info(file_path: str) -> Tuple[str, List[Dict]]:
    """
    Tool: get_file_info
    Description: Retrieves metadata (size, modification time, etc.) about a file.
                 Returns a tuple (content, artifact):
                   - content (str): A textual summary of the file info.
                   - artifact (List[Dict]): A list with a single dictionary of file metadata.
                                            Useful for direct conversion into a DataFrame.
    Args:
        file_path (str): The path of the file to inspect.
    Returns:
        Tuple[str, List[dict]]:
            content: Summary text
            artifact: A list[dict] of file metadata
    Example:
        content, artifact = get_file_info("/path/to/mydata.csv")
    """
    print(f"    * Tool: get_file_info | {file_path}")
    
    # Ensure the file exists
    import os
    import time

    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"{file_path} is not a valid file.")

    file_stats = os.stat(file_path)

    # Construct the data dictionary
    file_data = {
        "file_name": os.path.basename(file_path),
        "size_bytes": file_stats.st_size,
        "modification_time": time.ctime(file_stats.st_mtime),
        "absolute_path": os.path.abspath(file_path),
    }

    # Create a user-friendly summary (content)
    content_str = (
        f"File Name: {file_data['file_name']}\n"
        f"Size (bytes): {file_data['size_bytes']}\n"
        f"Last Modified: {file_data['modification_time']}\n"
        f"Absolute Path: {file_data['absolute_path']}"
    )

    # Artifact should be a list of dict(s) to easily convert to DataFrame
    artifact = [file_data]

    return content_str, artifact


@tool(response_format='content_and_artifact')
def search_files_by_pattern(
    directory_path: str = os.getcwd(),  
    pattern: str = "*.csv", 
    recursive: bool = False
) -> Tuple[str, List[Dict]]:
    """
    Tool: search_files_by_pattern
    Description:
        Searches for files (optionally in subdirectories) that match a given
        wildcard pattern (e.g. "*.csv", "*.xlsx", etc.), returning a tuple:
          (1) content (str): A multiline summary of the matched files.
          (2) artifact (List[Dict]): A list of dicts with file path info.

    Args:
        directory_path (str): Directory path to start searching from.
        pattern (str): A wildcard pattern, e.g. "*.csv". Default is "*.csv".
        recursive (bool): Whether to search in subdirectories. Default is False.

    Returns:
        Tuple[str, List[Dict]]:
            content: A user-friendly string showing matched file paths.
            artifact: A list of dictionaries, each representing a matched file.

    Example:
        content, artifact = search_files_by_pattern("/path/to/folder", "*.csv", recursive=True)
    """
    print(f"    * Tool: search_files_by_pattern | {directory_path}")
    
    import os
    import fnmatch

    matched_files = []
    if recursive:
        for root, dirs, files in os.walk(directory_path):
            for filename in files:
                if fnmatch.fnmatch(filename, pattern):
                    matched_files.append(os.path.join(root, filename))
    else:
        # Non-recursive
        for filename in os.listdir(directory_path):
            full_path = os.path.join(directory_path, filename)
            if os.path.isfile(full_path) and fnmatch.fnmatch(filename, pattern):
                matched_files.append(full_path)

    # Create a human-readable summary (content)
    if matched_files:
        lines = [f"Found {len(matched_files)} file(s) matching '{pattern}':"]
        for f in matched_files:
            lines.append(f" - {f}")
        content = "\n".join(lines)
    else:
        content = f"No files found matching '{pattern}'."

    # Create artifact as a list of dicts for DataFrame conversion
    artifact = [{"file_path": path} for path in matched_files]

    return content, artifact


# Loaders

def auto_load_file(file_path: str) -> pd.DataFrame:
    """
    Auto loads a file based on its extension.
    
    Parameters:
    ----------
    file_path : str
        The path to the file to load.
    
    Returns:
    -------
    pd.DataFrame
    """
    import pandas as pd
    try:
        ext = file_path.split(".")[-1].lower()
        if ext == "csv":
            return load_csv(file_path)
        elif ext in ["xlsx", "xls"]:
            return load_excel(file_path)
        elif ext == "json":
            return load_json(file_path)
        elif ext == "parquet":
            return load_parquet(file_path)
        elif ext == "pkl":
            return load_pickle(file_path)
        else:
            return f"Unsupported file extension: {ext}"
    except Exception as e:
        return f"Error loading file: {e}"

def load_csv(file_path: str) -> pd.DataFrame:
    """
    Tool: load_csv
    Description: Loads a CSV file into a pandas DataFrame.
    Args:
      file_path (str): Path to the CSV file.
    Returns:
      pd.DataFrame
    """
    import pandas as pd
    return pd.read_csv(file_path)

def load_excel(file_path: str, sheet_name=None) -> pd.DataFrame:
    """
    Tool: load_excel
    Description: Loads an Excel file into a pandas DataFrame.
    """
    import pandas as pd
    return pd.read_excel(file_path, sheet_name=sheet_name)

def load_json(file_path: str) -> pd.DataFrame:
    """
    Tool: load_json
    Description: Loads a JSON file or NDJSON into a pandas DataFrame.
    """
    import pandas as pd
    # For simple JSON arrays
    return pd.read_json(file_path, orient="records", lines=False)

def load_parquet(file_path: str) -> pd.DataFrame:
    """
    Tool: load_parquet
    Description: Loads a Parquet file into a pandas DataFrame.
    """
    import pandas as pd
    return pd.read_parquet(file_path)

def load_pickle(file_path: str) -> pd.DataFrame:
    """
    Tool: load_pickle
    Description: Loads a Pickle file into a pandas DataFrame.
    """
    import pandas as pd
    return pd.read_pickle(file_path)



================================================
FILE: ai_data_science_team/tools/dataframe.py
================================================
import io
import pandas as pd
from typing import Union, List, Dict

def get_dataframe_summary(
    dataframes: Union[pd.DataFrame, List[pd.DataFrame], Dict[str, pd.DataFrame]],
    n_sample: int = 30,
    skip_stats: bool = False,
) -> List[str]:
    """
    Generate a summary for one or more DataFrames. Accepts a single DataFrame, a list of DataFrames,
    or a dictionary mapping names to DataFrames.

    Parameters
    ----------
    dataframes : pandas.DataFrame or list of pandas.DataFrame or dict of (str -> pandas.DataFrame)
        - Single DataFrame: produce a single summary (returned within a one-element list).
        - List of DataFrames: produce a summary for each DataFrame, using index-based names.
        - Dictionary of DataFrames: produce a summary for each DataFrame, using dictionary keys as names.
    n_sample : int, default 30
        Number of rows to display in the "Data (first 30 rows)" section.
    skip_stats : bool, default False
        If True, skip the descriptive statistics and DataFrame info sections.
        
    Example:
    --------
    ``` python
    import pandas as pd
    from sklearn.datasets import load_iris
    data = load_iris(as_frame=True)
    dataframes = {
        "iris": data.frame,
        "iris_target": data.target,
    }
    summaries = get_dataframe_summary(dataframes)
    print(summaries[0])
    ```

    Returns
    -------
    list of str
        A list of summaries, one for each provided DataFrame. Each summary includes:
        - Shape of the DataFrame (rows, columns)
        - Column data types
        - Missing value percentage
        - Unique value counts
        - First 30 rows
        - Descriptive statistics
        - DataFrame info output
    """

    summaries = []

    # --- Dictionary Case ---
    if isinstance(dataframes, dict):
        for dataset_name, df in dataframes.items():
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    # --- Single DataFrame Case ---
    elif isinstance(dataframes, pd.DataFrame):
        summaries.append(_summarize_dataframe(dataframes, "Single_Dataset", n_sample, skip_stats))

    # --- List of DataFrames Case ---
    elif isinstance(dataframes, list):
        for idx, df in enumerate(dataframes):
            dataset_name = f"Dataset_{idx}"
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    else:
        raise TypeError(
            "Input must be a single DataFrame, a list of DataFrames, or a dictionary of DataFrames."
        )

    return summaries


def _summarize_dataframe(
    df: pd.DataFrame, 
    dataset_name: str, 
    n_sample=30, 
    skip_stats=False
) -> str:
    """Generate a summary string for a single DataFrame."""
    # 1. Convert dictionary-type cells to strings
    #    This prevents unhashable dict errors during df.nunique().
    df = df.apply(lambda col: col.map(lambda x: str(x) if isinstance(x, dict) else x))
    
    # 2. Capture df.info() output
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_text = buffer.getvalue()

    # 3. Calculate missing value stats
    missing_stats = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)
    missing_summary = "\n".join([f"{col}: {val:.2f}%" for col, val in missing_stats.items()])

    # 4. Get column data types
    column_types = "\n".join([f"{col}: {dtype}" for col, dtype in df.dtypes.items()])

    # 5. Get unique value counts
    unique_counts = df.nunique()  # Will no longer fail on unhashable dict
    unique_counts_summary = "\n".join([f"{col}: {count}" for col, count in unique_counts.items()])

    # 6. Generate the summary text
    if not skip_stats:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Missing Value Percentage:
        {missing_summary}

        Unique Value Counts:
        {unique_counts_summary}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}

        Data Description:
        {df.describe().to_string()}

        Data Info:
        {info_text}
        """
    else:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}
        """
        
    return summary_text.strip()





================================================
FILE: ai_data_science_team/tools/eda.py
================================================
from typing import Annotated, Dict, Tuple, Union

import os
import tempfile

from langchain.tools import tool

from langgraph.prebuilt import InjectedState

from ai_data_science_team.tools.dataframe import get_dataframe_summary


@tool(response_format="content")
def explain_data(
    data_raw: Annotated[dict, InjectedState("data_raw")],
    n_sample: int = 30,
    skip_stats: bool = False,
):
    """
    Tool: explain_data
    Description:
        Provides an extensive, narrative summary of a DataFrame including its shape, column types,
        missing value percentages, unique counts, sample rows, and (if not skipped) descriptive stats/info.

    Parameters:
        data_raw (dict): Raw data.
        n_sample (int, default=30): Number of rows to display.
        skip_stats (bool, default=False): If True, omit descriptive stats/info.

    LLM Guidance:
        Use when a detailed, human-readable explanation is needed—i.e., a full overview is preferred over a concise numerical summary.

    Returns:
        str: Detailed DataFrame summary.
    """
    print("    * Tool: explain_data")
    import pandas as pd

    result = get_dataframe_summary(
        pd.DataFrame(data_raw), n_sample=n_sample, skip_stats=skip_stats
    )

    return result


@tool(response_format="content_and_artifact")
def describe_dataset(
    data_raw: Annotated[dict, InjectedState("data_raw")],
) -> Tuple[str, Dict]:
    """
    Tool: describe_dataset
    Description:
        Compute and return summary statistics for the dataset using pandas' describe() method.
        The tool provides both a textual summary and a structured artifact (a dictionary) for further processing.

    Parameters:
    -----------
    data_raw : dict
        The raw data in dictionary format.

    LLM Selection Guidance:
    ------------------------
    Use this tool when:
      - The request emphasizes numerical descriptive statistics (e.g., count, mean, std, min, quartiles, max).
      - The user needs a concise statistical snapshot rather than a detailed narrative.
      - Both a brief text explanation and a structured data artifact (for downstream tasks) are required.

    Returns:
    -------
    Tuple[str, Dict]:
        - content: A textual summary indicating that summary statistics have been computed.
        - artifact: A dictionary (derived from DataFrame.describe()) containing detailed statistical measures.
    """
    print("    * Tool: describe_dataset")
    import pandas as pd

    df = pd.DataFrame(data_raw)
    description_df = df.describe(include="all")
    content = "Summary statistics computed using pandas describe()."
    artifact = {"describe_df": description_df.to_dict()}
    return content, artifact


@tool(response_format="content_and_artifact")
def visualize_missing(
    data_raw: Annotated[dict, InjectedState("data_raw")], n_sample: int = None
) -> Tuple[str, Dict]:
    """
    Tool: visualize_missing
    Description:
        Missing value analysis using the missingno library. Generates a matrix plot, bar plot, and heatmap plot.

    Parameters:
    -----------
    data_raw : dict
        The raw data in dictionary format.
    n_sample : int, optional (default: None)
        The number of rows to sample from the dataset if it is large.

    Returns:
    -------
    Tuple[str, Dict]:
        content: A message describing the generated plots.
        artifact: A dict with keys 'matrix_plot', 'bar_plot', and 'heatmap_plot' each containing the
                  corresponding base64 encoded PNG image.
    """
    print("    * Tool: visualize_missing")

    try:
        import missingno as msno  # Ensure missingno is installed
    except ImportError:
        raise ImportError(
            "Please install the 'missingno' package to use this tool. pip install missingno"
        )

    import pandas as pd
    import base64
    from io import BytesIO
    import matplotlib.pyplot as plt

    # Create the DataFrame and sample if n_sample is provided.
    df = pd.DataFrame(data_raw)
    if n_sample is not None:
        df = df.sample(n=n_sample, random_state=42)

    # Dictionary to store the base64 encoded images for each plot.
    encoded_plots = {}

    # Define a helper function to create a plot, save it, and encode it.
    def create_and_encode_plot(plot_func, plot_name: str):
        plt.figure(figsize=(8, 6))
        # Call the missingno plotting function.
        plot_func(df)
        plt.tight_layout()
        buf = BytesIO()
        plt.savefig(buf, format="png")
        plt.close()
        buf.seek(0)
        return base64.b64encode(buf.getvalue()).decode("utf-8")

    # Create and encode the matrix plot.
    encoded_plots["matrix_plot"] = create_and_encode_plot(msno.matrix, "matrix")

    # Create and encode the bar plot.
    encoded_plots["bar_plot"] = create_and_encode_plot(msno.bar, "bar")

    # Create and encode the heatmap plot.
    encoded_plots["heatmap_plot"] = create_and_encode_plot(msno.heatmap, "heatmap")

    content = (
        "Missing data visualizations (matrix, bar, and heatmap) have been generated."
    )
    artifact = encoded_plots
    return content, artifact


@tool(response_format="content_and_artifact")
def generate_correlation_funnel(
    data_raw: Annotated[dict, InjectedState("data_raw")],
    target: str,
    target_bin_index: Union[int, str] = -1,
    corr_method: str = "pearson",
    n_bins: int = 4,
    thresh_infreq: float = 0.01,
    name_infreq: str = "-OTHER",
) -> Tuple[str, Dict]:
    """
    Tool: generate_correlation_funnel
    Description:
        Correlation analysis using the correlation funnel method. The tool binarizes the data and computes correlation versus a target column.

    Parameters:
    ----------
    target : str
        The base target column name (e.g., 'Member_Status'). The tool will look for columns that begin
        with this string followed by '__' (e.g., 'Member_Status__Gold', 'Member_Status__Platinum').
    target_bin_index : int or str, default -1
        If an integer, selects the target level by position from the matching columns.
        If a string (e.g., "Yes"), attempts to match to the suffix of a column name
        (i.e., 'target__Yes').
    corr_method : str
        The correlation method ('pearson', 'kendall', or 'spearman'). Default is 'pearson'.
    n_bins : int
        The number of bins to use for binarization. Default is 4.
    thresh_infreq : float
        The threshold for infrequent levels. Default is 0.01.
    name_infreq : str
        The name to use for infrequent levels. Default is '-OTHER'.
    """
    print("    * Tool: generate_correlation_funnel")
    try:
        import pytimetk as tk
    except ImportError:
        raise ImportError(
            "Please install the 'pytimetk' package to use this tool. pip install pytimetk"
        )
    import pandas as pd
    import base64
    from io import BytesIO
    import matplotlib.pyplot as plt
    import json
    import plotly.io as pio

    # Convert the raw injected state into a DataFrame.
    df = pd.DataFrame(data_raw)

    # Apply the binarization method.
    df_binarized = df.binarize(
        n_bins=n_bins,
        thresh_infreq=thresh_infreq,
        name_infreq=name_infreq,
        one_hot=True,
    )

    # Determine the full target column name.
    # Look for all columns that start with "target__"
    matching_columns = [
        col for col in df_binarized.columns if col.startswith(f"{target}__")
    ]
    if not matching_columns:
        # If no matching columns are found, warn and use the provided target as-is.
        full_target = target
    else:
        # Determine the full target based on target_bin_index.
        if isinstance(target_bin_index, str):
            # Build the candidate column name
            candidate = f"{target}__{target_bin_index}"
            if candidate in matching_columns:
                full_target = candidate
            else:
                # If no matching candidate is found, default to the last matching column.
                full_target = matching_columns[-1]
        else:
            # target_bin_index is an integer.
            try:
                full_target = matching_columns[target_bin_index]
            except IndexError:
                # If index is out of bounds, use the last matching column.
                full_target = matching_columns[-1]

    # Compute correlation funnel using the full target column name.
    df_correlated = df_binarized.correlate(target=full_target, method=corr_method)

    # Attempt to generate a static plot.
    encoded = None
    try:
        # Here we assume that your DataFrame has a method plot_correlation_funnel.
        fig = df_correlated.plot_correlation_funnel(engine="plotnine", height=600)
        buf = BytesIO()
        # Use the appropriate save method for your figure object.
        fig.save(buf, format="png")
        plt.close()
        buf.seek(0)
        encoded = base64.b64encode(buf.getvalue()).decode("utf-8")
    except Exception as e:
        encoded = {"error": str(e)}

    # Attempt to generate a Plotly plot.
    fig_dict = None
    try:
        fig = df_correlated.plot_correlation_funnel(engine="plotly", base_size=14)

        fig_json = pio.to_json(fig)
        fig_dict = json.loads(fig_json)
    except Exception as e:
        fig_dict = {"error": str(e)}

    content = (
        f"Correlation funnel computed using method '{corr_method}' for target level '{full_target}'. "
        f"Base target was '{target}' with target_bin_index '{target_bin_index}'."
    )
    artifact = {
        "correlation_data": df_correlated.to_dict(orient="list"),
        "plot_image": encoded,
        "plotly_figure": fig_dict,
    }
    return content, artifact


@tool(response_format="content_and_artifact")
def generate_sweetviz_report(
    data_raw: Annotated[dict, InjectedState("data_raw")],
    target: str = None,
    report_name: str = "sweetviz_report.html",
    report_directory: str = None,  # <-- Default to None
    open_browser: bool = False,
) -> Tuple[str, Dict]:
    """
    Tool: generate_sweetviz_report
    Description:
        Make an Exploratory Data Analysis (EDA) report using the Sweetviz library.

    Parameters:
    -----------
    data_raw : dict
        The raw data injected as a dictionary (converted from a DataFrame).
    target : str, optional
        The target feature to analyze. Default is None.
    report_name : str, optional
        The file name to save the Sweetviz HTML report. Default is "sweetviz_report.html".
    report_directory : str, optional
        The directory where the report should be saved.
        If None, a temporary directory is created and used.
    open_browser : bool, optional
        Whether to open the report in a web browser. Default is False.

    Returns:
    --------
    Tuple[str, Dict]:
        content: A summary message describing the generated report.
        artifact: A dictionary with the report file path and optionally the report's HTML content.
    """
    print("    * Tool: generate_sweetviz_report")

    # Import sweetviz
    try:
        import sweetviz as sv
    except ImportError:
        raise ImportError(
            "Please install the 'sweetviz' package to use this tool. Run: pip install sweetviz"
        )

    import pandas as pd

    # Convert injected raw data to a DataFrame.
    df = pd.DataFrame(data_raw)

    # If no directory is specified, use a temporary directory.
    if not report_directory:
        report_directory = tempfile.mkdtemp()
        print(f"    * Using temporary directory: {report_directory}")
    else:
        # Ensure user-specified directory exists.
        if not os.path.exists(report_directory):
            os.makedirs(report_directory)

    # Create the Sweetviz report.
    report = sv.analyze(df, target_feat=target)

    # Determine the full path for the report.
    full_report_path = os.path.join(report_directory, report_name)

    # Save the report to the specified HTML file.
    report.show_html(
        filepath=full_report_path,
        open_browser=open_browser,
    )

    # Optionally, read the HTML content (if desired to pass along in the artifact).
    try:
        with open(full_report_path, "r", encoding="utf-8") as f:
            html_content = f.read()
    except Exception:
        html_content = None

    content = (
        f"Sweetviz EDA report generated and saved as '{os.path.abspath(full_report_path)}'. "
        f"{'This was saved in a temporary directory.' if 'tmp' in report_directory else ''}"
    )
    artifact = {
        "report_file": os.path.abspath(full_report_path),
        "report_html": html_content,
    }
    return content, artifact


@tool(response_format="content_and_artifact")
def generate_dtale_report(
    data_raw: Annotated[dict, InjectedState("data_raw")],
    host: str = "localhost",
    port: int = 40000,
    open_browser: bool = False,
) -> Tuple[str, Dict]:
    """
    Tool: generate_dtale_report
    Description:
        Creates an interactive data exploration report using the dtale library.

    Parameters:
    -----------
    data_raw : dict
        The raw data in dictionary format.
    host : str, optional
        The host IP address to serve the dtale app. Default is "localhost".
    port : int, optional
        The port number to serve the dtale app. Default is 40000.
    open_browser : bool, optional
        Whether to open the report in a web browser. Default is False.

    Returns:
    --------
    Tuple[str, Dict]:
        content: A summary message describing the dtale report.
        artifact: A dictionary containing the URL of the dtale report.
    """
    print("    * Tool: generate_dtale_report")

    try:
        import dtale
    except ImportError:
        raise ImportError(
            "Please install the 'dtale' package to use this tool. Run: pip install dtale"
        )

    import pandas as pd

    df = pd.DataFrame(data_raw)

    # Create the dtale report
    d = dtale.show(df, host=host, port=port, open_browser=open_browser)

    content = f"Dtale report generated and available at: {d.main_url()}"
    artifact = {"dtale_url": d.main_url()}

    return content, artifact



================================================
FILE: ai_data_science_team/tools/h2o.py
================================================

from typing import Optional, Dict, Any, Union, List
from langchain.tools import tool


@tool("train_h2o_automl", return_direct=True, response_format='content_and_artifact')
def train_h2o_automl(
    data_raw: List[Dict[str, Any]],
    target: str = "Churn",
    max_runtime_secs: int = 30,
    exclude_algos: List[str] = None,
    balance_classes: bool = True,
    nfolds: int = 5,
    seed: int = 42,
    max_models: int = 20,
    stopping_metric: str = "logloss",
    stopping_tolerance: float = 0.001,
    stopping_rounds: int = 3,
    sort_metric: str = "AUC",
    model_directory: Optional[str] = None,
    log_path: Optional[str] = None,
    enable_mlflow: bool = False,               
    mlflow_tracking_uri: Optional[str] = None, 
    mlflow_experiment_name: str = "H2O AutoML",
    run_name: str = None,
    **kwargs
) -> str:
    """
    A tool to train an H2O AutoML model on the provided data.
    Optionally logs results to MLflow if `enable_mlflow=True`. 

    Parameters
    ----------
    data_raw : List[Dict[str, Any]]
        Row-wise data (like df.to_dict(orient="records")).
    target : str, default "Churn"
        The target column name.
    max_runtime_secs : int, default 30
    exclude_algos : List[str], optional
        e.g., ["DeepLearning"]. If not provided, defaults to ["DeepLearning"].
    balance_classes : bool, default True
    nfolds : int, default 5
    seed : int, default 42
    max_models : int, default 20
    stopping_metric : str, default "logloss"
    stopping_tolerance : float, default 0.001
    stopping_rounds : int, default 3
    sort_metric : str, default "AUC"
    model_directory : str or None
        Directory path to save the best model. If None, won't save unless log_path is set.
    log_path : str or None
        Fallback path if model_directory is None. If both are None, model won't be saved.
    enable_mlflow : bool, default False
        Whether to enable MLflow logging. If False, skip MLflow entirely.
    mlflow_tracking_uri : str or None
        If provided, sets MLflow tracking URI at runtime.
    mlflow_experiment_name : str
        Name of the MLflow experiment (created if doesn't exist).
    run_name : str, default "h2o_automl_run"
        A custom name for the MLflow run.
    **kwargs : dict
        Additional keyword arguments to pass to H2OAutoML().

    Returns
    -------
    str (JSON)
        {
          "leaderboard": dict of entire leaderboard,
          "best_model_id": str,
          "model_path": str or None,
          "model_results": {
             "model_flavor": "H2O AutoML",
             "model_path": str or None,
             "best_model_id": str,
             "metrics": dict
          },
          "mlflow_run_id": Optional[str]
        }
    """

    import h2o
    from h2o.automl import H2OAutoML
    import pandas as pd
    import json

    # Optional MLflow usage
    if enable_mlflow:
        import mlflow
        if mlflow_tracking_uri:
            mlflow.set_tracking_uri(mlflow_tracking_uri)
        mlflow.set_experiment(mlflow_experiment_name)
        run_context = mlflow.start_run(run_name=run_name)
    else:
        # Dummy context manager to skip MLflow if not enabled
        from contextlib import nullcontext
        run_context = nullcontext()

    exclude_algos = exclude_algos or ["DeepLearning"]  # default if not provided

    # Convert data to DataFrame
    df = pd.DataFrame(data_raw)

    with run_context as run:
        # If using MLflow, track run ID
        run_id = None
        if enable_mlflow and run is not None:
            run_id = run.info.run_id
            import mlflow
            # Log user-specified parameters
            mlflow.log_params({
                "target": target,
                "max_runtime_secs": max_runtime_secs,
                "exclude_algos": str(exclude_algos),
                "balance_classes": balance_classes,
                "nfolds": nfolds,
                "seed": seed,
                "max_models": max_models,
                "stopping_metric": stopping_metric,
                "stopping_tolerance": stopping_tolerance,
                "stopping_rounds": stopping_rounds,
                "sort_metric": sort_metric,
                "model_directory": model_directory,
                "log_path": log_path,
                **kwargs
            })

        # Initialize H2O
        h2o.init()

        # Create H2OFrame
        data_h2o = h2o.H2OFrame(df)

        # Setup AutoML
        aml = H2OAutoML(
            max_runtime_secs=max_runtime_secs,
            exclude_algos=exclude_algos,
            balance_classes=balance_classes,
            nfolds=nfolds,
            seed=seed,
            max_models=max_models,
            stopping_metric=stopping_metric,
            stopping_tolerance=stopping_tolerance,
            stopping_rounds=stopping_rounds,
            sort_metric=sort_metric,
            **kwargs
        )

        # Train
        x = [col for col in data_h2o.columns if col != target]
        aml.train(x=x, y=target, training_frame=data_h2o)

        # Save model if we have a directory/log path
        if model_directory is None and log_path is None:
            model_path = None
        else:
            path_to_save = model_directory if model_directory else log_path
            model_path = h2o.save_model(model=aml.leader, path=path_to_save, force=True)

        # Leaderboard (DataFrame -> dict)
        leaderboard_df = aml.leaderboard.as_data_frame()
        leaderboard_dict = leaderboard_df.to_dict()

        # Gather top-model metrics from the first row
        top_row = leaderboard_df.iloc[0].to_dict()  # includes model_id, etc.
        # Optionally remove model_id from metrics
        top_metrics = {k: v for k, v in top_row.items() if k.lower() != "model_id"}

        # Construct model_results
        model_results = {
            "model_flavor": "H2O AutoML",
            "model_path": model_path,
            "best_model_id": aml.leader.model_id,
            "metrics": top_metrics  # all metrics from the top row
        }

        # If using MLflow, log the top metrics
        if enable_mlflow and run is not None:
            for metric_name, metric_value in top_metrics.items():
                # Only log floats/ints as metrics
                if isinstance(metric_value, (int, float)):
                    mlflow.log_metric(metric_name, metric_value)

            # Log artifact if we saved the model
            if model_path is not None:
                mlflow.log_artifact(model_path, artifact_path="h2o_best_model")

        # Build the output
        output = {
            "leaderboard": leaderboard_dict,
            "best_model_id": aml.leader.model_id,
            "model_path": model_path,
            "model_results": model_results,
            "mlflow_run_id": run_id
        }

    return json.dumps(output, indent=2)


H2O_AUTOML_DOCUMENTATION = """
Title: H2O AutoML: Automatic Machine Learning
Source: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html

AutoML Interface
The H2O AutoML interface is designed to have as few parameters as possible so that all the user needs to do is point to their dataset, identify the response column and optionally specify a time constraint or limit on the number of total models trained. Below are the parameters that can be set by the user in the R and Python interfaces. See the Web UI via H2O Wave section below for information on how to use the H2O Wave web interface for AutoML.

In both the R and Python API, AutoML uses the same data-related arguments, x, y, training_frame, validation_frame, as the other H2O algorithms. Most of the time, all you'll need to do is specify the data arguments. You can then configure values for max_runtime_secs and/or max_models to set explicit time or number-of-model limits on your run.

Required Parameters
Required Data Parameters
y: This argument is the name (or index) of the response column.

training_frame: Specifies the training set.

Required Stopping Parameters
One of the following stopping strategies (time or number-of-model based) must be specified. When both options are set, then the AutoML run will stop as soon as it hits one of either When both options are set, then the AutoML run will stop as soon as it hits either of these limits.

max_runtime_secs: This argument specifies the maximum time that the AutoML process will run for. The default is 0 (no limit), but dynamically sets to 1 hour if none of max_runtime_secs and max_models are specified by the user.

max_models: Specify the maximum number of models to build in an AutoML run, excluding the Stacked Ensemble models. Defaults to NULL/None. Always set this parameter to ensure AutoML reproducibility: all models are then trained until convergence and none is constrained by a time budget.

Optional Parameters
Optional Data Parameters
x: A list/vector of predictor column names or indexes. This argument only needs to be specified if the user wants to exclude columns from the set of predictors. If all columns (other than the response) should be used in prediction, then this does not need to be set.

validation_frame: This argument is ignored unless nfolds == 0, in which a validation frame can be specified and used for early stopping of individual models and early stopping of the grid searches (unless max_models or max_runtime_secs overrides metric-based early stopping). By default and when nfolds > 1, cross-validation metrics will be used for early stopping and thus validation_frame will be ignored.

leaderboard_frame: This argument allows the user to specify a particular data frame to use to score and rank models on the leaderboard. This frame will not be used for anything besides leaderboard scoring. If a leaderboard frame is not specified by the user, then the leaderboard will use cross-validation metrics instead, or if cross-validation is turned off by setting nfolds = 0, then a leaderboard frame will be generated automatically from the training frame.

blending_frame: Specifies a frame to be used for computing the predictions that serve as the training frame for the Stacked Ensemble models metalearner. If provided, all Stacked Ensembles produced by AutoML will be trained using Blending (a.k.a. Holdout Stacking) instead of the default Stacking method based on cross-validation.

fold_column: Specifies a column with cross-validation fold index assignment per observation. This is used to override the default, randomized, 5-fold cross-validation scheme for individual models in the AutoML run.

weights_column: Specifies a column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative weights are not allowed.

Optional Miscellaneous Parameters
nfolds: Specify a value >= 2 for the number of folds for k-fold cross-validation of the models in the AutoML run or specify “-1” to let AutoML choose if k-fold cross-validation or blending mode should be used. Blending mode will use part of training_frame (if no blending_frame is provided) to train Stacked Ensembles. Use 0 to disable cross-validation; this will also disable Stacked Ensembles (thus decreasing the overall best model performance). This value defaults to “-1”.

balance_classes: Specify whether to oversample the minority classes to balance the class distribution. This option is not enabled by default and can increase the data frame size. This option is only applicable for classification. If the oversampled size of the dataset exceeds the maximum size calculated using the max_after_balance_size parameter, then the majority classes will be undersampled to satisfy the size limit.

class_sampling_factors: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance. Note that this requires balance_classes set to True.

max_after_balance_size: Specify the maximum relative size of the training data after balancing class counts (balance_classes must be enabled). Defaults to 5.0. (The value can be less than 1.0).

max_runtime_secs_per_model: Specify the max amount of time dedicated to the training of each individual model in the AutoML run. Defaults to 0 (disabled). Note that models constrained by a time budget are not guaranteed reproducible.

stopping_metric: Specify the metric to use for early stopping. Defaults to AUTO. The available options are:

- AUTO: This defaults to logloss for classification and deviance for regression.
- deviance (mean residual deviance)
- logloss
- MSE
- RMSE
- MAE
- RMSLE
- AUC (area under the ROC curve)
- AUCPR (area under the Precision-Recall curve)
- lift_top_group
- misclassification
- mean_per_class_error

stopping_tolerance: This option specifies the relative tolerance for the metric-based stopping criterion to stop a grid search and the training of individual models within the AutoML run. This value defaults to 0.001 if the dataset is at least 1 million rows; otherwise it defaults to a bigger value determined by the size of the dataset and the non-NA-rate. In that case, the value is computed as 1/sqrt(nrows * non-NA-rate).

stopping_rounds: This argument is used to stop model training when the stopping metric (e.g. AUC) doesn't improve for this specified number of training rounds, based on a simple moving average. In the context of AutoML, this controls early stopping both within the random grid searches as well as the individual models. Defaults to 3 and must be an non-negative integer. To disable early stopping altogether, set this to 0.

sort_metric: Specifies the metric used to sort the Leaderboard by at the end of an AutoML run. Available options include:

- AUTO: This defaults to AUC for binary classification, mean_per_class_error for multinomial classification, and deviance for regression.
- deviance (mean residual deviance)
- logloss
- MSE
- RMSE
- MAE
- RMSLE
- AUC (area under the ROC curve)
- AUCPR (area under the Precision-Recall curve)
- mean_per_class_error

seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility under certain conditions. H2O Deep Learning models are not reproducible by default for performance reasons, so if the user requires reproducibility, then exclude_algos must contain "DeepLearning". In addition max_models must be used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.

project_name: Character string to identify an AutoML project. Defaults to NULL/None, which means a project name will be auto-generated based on the training frame ID. More models can be trained and added to an existing AutoML project by specifying the same project name in multiple calls to the AutoML function (as long as the same training frame is used in subsequent runs).

exclude_algos: A list/vector of character strings naming the algorithms to skip during the model-building phase. An example use is exclude_algos = ["GLM", "DeepLearning", "DRF"] in Python or exclude_algos = c("GLM", "DeepLearning", "DRF") in R. Defaults to None/NULL, which means that all appropriate H2O algorithms will be used if the search stopping criteria allows and if the include_algos option is not specified. This option is mutually exclusive with include_algos. See include_algos below for the list of available options.

include_algos: A list/vector of character strings naming the algorithms to include during the model-building phase. An example use is include_algos = ["GLM", "DeepLearning", "DRF"] in Python or include_algos = c("GLM", "DeepLearning", "DRF") in R. Defaults to None/NULL, which means that all appropriate H2O algorithms will be used if the search stopping criteria allows and if no algorithms are specified in exclude_algos. This option is mutually exclusive with exclude_algos. The available algorithms are:

- DRF (This includes both the Distributed Random Forest (DRF) and Extremely Randomized Trees (XRT) models. Refer to the Extremely Randomized Trees section in the DRF chapter and the histogram_type parameter description for more information.)
- GLM (Generalized Linear Model with regularization)
- XGBoost (XGBoost GBM)
- GBM (H2O GBM)
- DeepLearning (Fully-connected multi-layer artificial neural network)
- StackedEnsemble (Stacked Ensembles, includes an ensemble of all the base models and ensembles using subsets of the base models)

modeling_plan: The list of modeling steps to be used by the AutoML engine. (They may not all get executed, depending on other constraints.)

preprocessing: The list of preprocessing steps to run. Only ["target_encoding"] is currently supported. There is more information about how Target Encoding is automatically applied here. Experimental.

exploitation_ratio: Specify the budget ratio (between 0 and 1) dedicated to the exploitation (vs exploration) phase. By default, the exploitation phase is disabled (exploitation_ratio=0) as this is still experimental; to activate it, it is recommended to try a ratio around 0.1. Note that the current exploitation phase only tries to fine-tune the best XGBoost and the best GBM found during exploration. Experimental.

monotone_constraints: A mapping that represents monotonic constraints. Use +1 to enforce an increasing constraint and -1 to specify a decreasing constraint.

keep_cross_validation_predictions: Specify whether to keep the predictions of the cross-validation predictions. This needs to be set to TRUE if running the same AutoML object for repeated runs because CV predictions are required to build additional Stacked Ensemble models in AutoML. This option defaults to FALSE.

keep_cross_validation_models: Specify whether to keep the cross-validated models. Keeping cross-validation models may consume significantly more memory in the H2O cluster. This option defaults to FALSE.

keep_cross_validation_fold_assignment: Enable this option to preserve the cross-validation fold assignment. Defaults to FALSE.

verbosity: (Optional: Python and R only) The verbosity of the backend messages printed during training. Must be one of "debug", "info", "warn". Defaults to NULL/None (client logging disabled).

export_checkpoints_dir: Specify a directory to which generated models will automatically be exported.

Notes
Validation Options
If the user turns off cross-validation by setting nfolds == 0, then cross-validation metrics will not be available to populate the leaderboard. In this case, we need to make sure there is a holdout frame (i.e. the “leaderboard frame”) to score the models on so that we can generate model performance metrics for the leaderboard. Without cross-validation, we will also require a validation frame to be used for early stopping on the models. Therefore, if either of these frames are not provided by the user, they will be automatically partitioned from the training data. If either frame is missing, 10% of the training data will be used to create a missing frame (if both are missing then a total of 20% of the training data will be used to create a 10% validation and 10% leaderboard frame).

XGBoost Memory Requirements
XGBoost, which is included in H2O as a third party library, requires its own memory outside the H2O (Java) cluster. When running AutoML with XGBoost (it is included by default), be sure you allow H2O no more than 2/3 of the total available RAM. Example: If you have 60G RAM, use h2o.init(max_mem_size = "40G"), leaving 20G for XGBoost.

Scikit-learn Compatibility
H2OAutoML can interact with the h2o.sklearn module. The h2o.sklearn module exposes 2 wrappers for H2OAutoML (H2OAutoMLClassifier and H2OAutoMLRegressor), which expose the standard API familiar to sklearn users: fit, predict, fit_predict, score, get_params, and set_params. It accepts various formats as input data (H2OFrame, numpy array, pandas Dataframe) which allows them to be combined with pure sklearn components in pipelines. For an example using H2OAutoML with the h2o.sklearn module, click here.

Explainability
AutoML objects are fully supported though the H2O Model Explainability interface. A large number of multi-model comparison and single model (AutoML leader) plots can be generated automatically with a single call to h2o.explain(). We invite you to learn more at page linked above.

Code Examples

Training
Here’s an example showing basic usage of the h2o.automl() function in R and the H2OAutoML class in Python. For demonstration purposes only, we explicitly specify the x argument, even though on this dataset, that’s not required. With this dataset, the set of predictors is all columns other than the response. Like other H2O algorithms, the default value of x is “all columns, excluding y”, so that will produce the same result.

``` python
import h2o
from h2o.automl import H2OAutoML

# Start the H2O cluster (locally)
h2o.init()

# Import a sample binary outcome train/test set into H2O
train = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/higgs/higgs_train_10k.csv")
test = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/higgs/higgs_test_5k.csv")

# Identify predictors and response
x = train.columns
y = "response"
x.remove(y)

# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

# Run AutoML for 20 base models
aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)

# View the AutoML Leaderboard
lb = aml.leaderboard
lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)

# model_id                                                  auc    logloss    mean_per_class_error      rmse       mse
# ---------------------------------------------------  --------  ---------  ----------------------  --------  --------
# StackedEnsemble_AllModels_AutoML_20181212_105540     0.789801   0.551109                0.333174  0.43211   0.186719
# StackedEnsemble_BestOfFamily_AutoML_20181212_105540  0.788425   0.552145                0.323192  0.432625  0.187165
# XGBoost_1_AutoML_20181212_105540                     0.784651   0.55753                 0.325471  0.434949  0.189181
# XGBoost_grid_1_AutoML_20181212_105540_model_4        0.783523   0.557854                0.318819  0.435249  0.189441
# XGBoost_grid_1_AutoML_20181212_105540_model_3        0.783004   0.559613                0.325081  0.435708  0.189841
# XGBoost_2_AutoML_20181212_105540                     0.78136    0.55888                 0.347074  0.435907  0.190015
# XGBoost_3_AutoML_20181212_105540                     0.780847   0.559589                0.330739  0.43613   0.190209
# GBM_5_AutoML_20181212_105540                         0.780837   0.559903                0.340848  0.436191  0.190263
# GBM_2_AutoML_20181212_105540                         0.780036   0.559806                0.339926  0.436415  0.190458
# GBM_1_AutoML_20181212_105540                         0.779827   0.560857                0.335096  0.436616  0.190633
# GBM_3_AutoML_20181212_105540                         0.778669   0.56179                 0.325538  0.437189  0.191134
# XGBoost_grid_1_AutoML_20181212_105540_model_2        0.774411   0.575017                0.322811  0.4427    0.195984
# GBM_4_AutoML_20181212_105540                         0.771426   0.569712                0.33742   0.44107   0.194543
# GBM_grid_1_AutoML_20181212_105540_model_1            0.769752   0.572583                0.344331  0.442452  0.195764
# GBM_grid_1_AutoML_20181212_105540_model_2            0.754366   0.918567                0.355855  0.496638  0.246649
# DRF_1_AutoML_20181212_105540                         0.742892   0.595883                0.355403  0.452774  0.205004
# XRT_1_AutoML_20181212_105540                         0.742091   0.599346                0.356583  0.453117  0.205315
# DeepLearning_grid_1_AutoML_20181212_105540_model_2   0.741795   0.601497                0.368291  0.454904  0.206937
# XGBoost_grid_1_AutoML_20181212_105540_model_1        0.693554   0.620702                0.40588   0.465791  0.216961
# DeepLearning_1_AutoML_20181212_105540                0.69137    0.637954                0.409351  0.47178   0.222576
# DeepLearning_grid_1_AutoML_20181212_105540_model_1   0.690084   0.661794                0.418469  0.476635  0.227181
# GLM_grid_1_AutoML_20181212_105540_model_1            0.682648   0.63852                 0.397234  0.472683  0.223429
#
# [22 rows x 6 columns]

# The leader model is stored here
aml.leader
```

Prediction
Using the predict() function with AutoML generates predictions on the leader model from the run. The order of the rows in the results is the same as the order in which the data was loaded, even if some rows fail (for example, due to missing values or unseen factor levels).

``` python
# To generate predictions on a test set, you can make predictions
# directly on the `H2OAutoML` object or on the leader model
# object directly
preds = aml.predict(test)

# or:
preds = aml.leader.predict(test)
```

AutoML Output

Leaderboard
The AutoML object includes a “leaderboard” of models that were trained in the process, including the 5-fold cross-validated model performance (by default). The number of folds used in the model evaluation process can be adjusted using the nfolds parameter. If you would like to score the models on a specific dataset, you can specify the leaderboard_frame argument in the AutoML run, and then the leaderboard will show scores on that dataset instead.

The models are ranked by a default metric based on the problem type (the second column of the leaderboard). In binary classification problems, that metric is AUC, and in multiclass classification problems, the metric is mean per-class error. In regression problems, the default sort metric is RMSE. Some additional metrics are also provided, for convenience.

To help users assess the complexity of AutoML models, the h2o.get_leaderboard function has been been expanded by allowing an extra_columns parameter. This parameter allows you to specify which (if any) optional columns should be added to the leaderboard. This defaults to None. Allowed options include:

- training_time_ms: A column providing the training time of each model in milliseconds. (Note that this doesn't include the training of cross validation models.)

- predict_time_per_row_ms: A column providing the average prediction time by the model for a single row.

- ALL: Adds columns for both training_time_ms and predict_time_per_row_ms.

``` python
# Get leaderboard with all possible columns
lb = h2o.automl.get_leaderboard(aml, extra_columns = "ALL")
lb
```

Examine Models
To examine the trained models more closely, you can interact with the models, either by model ID, or a convenience function which can grab the best model of each model type (ranked by the default metric, or a metric of your choosing).

``` python
# Get the best model using the metric
m = aml.leader
# this is equivalent to
m = aml.get_best_model()

# Get the best model using a non-default metric
m = aml.get_best_model(criterion="logloss")

# Get the best XGBoost model using default sort metric
xgb = aml.get_best_model(algorithm="xgboost")

# Get the best XGBoost model, ranked by logloss
xgb = aml.get_best_model(algorithm="xgboost", criterion="logloss")
```

Get a specific model by model ID:

``` python
# Get a specific model by model ID
m = h2o.get_model("StackedEnsemble_BestOfFamily_AutoML_20191213_174603")
```

Once you have retreived the model in R or Python, you can inspect the model parameters as follows:

``` python
# View the parameters for the XGBoost model selected above
xgb.params.keys()

# Inspect individual parameter values
xgb.params['ntrees']
```

AutoML Log
When using Python or R clients, you can also access meta information with the following AutoML object properties:

- event_log: an H2OFrame with selected AutoML backend events generated during training.

- training_info: a dictionary exposing data that could be useful for post-analysis (e.g. various timings). If you want training and prediction times for each model, it's easier to explore that data in the extended leaderboard using the h2o.get_leaderboard() function.

``` python
# Get AutoML event log
log = aml.event_log

# Get training timing info
info = aml.training_info
```

Experimental Features

Preprocessing
As of H2O 3.32.0.1, AutoML now has a preprocessing option with minimal support for automated Target Encoding of high cardinality categorical variables. The only currently supported option is preprocessing = ["target_encoding"]: we automatically tune a Target Encoder model and apply it to columns that meet certain cardinality requirements for the tree-based algorithms (XGBoost, H2O GBM and Random Forest). 

FAQ

1. Which models are trained in the AutoML process?

The current version of AutoML trains and cross-validates the following algorithms: three pre-specified XGBoost GBM (Gradient Boosting Machine) models, a fixed grid of GLMs, a default Random Forest (DRF), five pre-specified H2O GBMs, a near-default Deep Neural Net, an Extremely Randomized Forest (XRT), a random grid of XGBoost GBMs, a random grid of H2O GBMs, and a random grid of Deep Neural Nets. In some cases, there will not be enough time to complete all the algorithms, so some may be missing from the leaderboard. In other cases, the grids will stop early, and if there's time left, the top two random grids will be restarted to train more models. AutoML trains multiple Stacked Ensemble models throughout the process (more info about the ensembles below).

Particular algorithms (or groups of algorithms) can be switched off using the exclude_algos argument. This is useful if you already have some idea of the algorithms that will do well on your dataset, though sometimes this can lead to a loss of performance because having more diversity among the set of models generally increases the performance of the Stacked Ensembles. As a first step you could leave all the algorithms on, and examine their performance characteristics (e.g. prediction speed) to get a sense of what might be practically useful in your specific use-case, and then turn off algorithms that are not interesting or useful to you. We recommend using the H2O Model Explainability interface to explore and further evaluate your AutoML models, which can inform your choice of model (if you have other goals beyond simply maximizing model accuracy).

A list of the hyperparameters searched over for each algorithm in the AutoML process is included in the appendix below. More details about the hyperparameter ranges for the models in addition to the hard-coded models will be added to the appendix at a later date.

AutoML trains several Stacked Ensemble models during the run (unless ensembles are turned off using exclude_algos). We have subdivided the model training in AutoML into “model groups” with different priority levels. After each group is completed, and at the very end of the AutoML process, we train (at most) two additional Stacked Ensembles with the existing models. There are currently two types of Stacked Ensembles: one which includes all the base models (“All Models”), and one comprised only of the best model from each algorithm family (“Best of Family”). The Best of Family ensembles are more optimized for production use since it only contains six (or fewer) base models. It should be relatively fast to use in production (to generate predictions on new data) without much degradation in model performance when compared to the final “All Models” ensemble, for example. This may be useful if you want the model performance boost from ensembling without the added time or complexity of a large ensemble. You can also inspect some of the earlier “All Models” Stacked Ensembles that have fewer models as an alternative to the Best of Family ensembles. The metalearner used in all ensembles is a variant of the default Stacked Ensemble metalearner: a non-negative GLM with regularization (Lasso or Elastic net, chosen by CV) to encourage more sparse ensembles. The metalearner also uses a logit transform (on the base learner CV preds) for classification tasks before training.

For information about how previous versions of AutoML were different than the current one, there's a brief description here.

2. How do I save AutoML runs?

Rather than saving an AutoML object itself, currently, the best thing to do is to save the models you want to keep, individually. A utility for saving all of the models at once, along with a way to save the AutoML object (with leaderboard), will be added in a future release.

3. Can we make use of GPUs with AutoML?

XGBoost models in AutoML can make use of GPUs. Keep in mind that the following requirements must be met:

- NVIDIA GPUs (GPU Cloud, DGX Station, DGX-1, or DGX-2)
- CUDA 8

You can monitor your GPU utilization via the nvidia-smi command. Refer to https://developer.nvidia.com/nvidia-system-management-interface for more information.

4. Why don't I see XGBoost models?

AutoML includes XGBoost GBMs (Gradient Boosting Machines) among its set of algorithms. This feature is currently provided with the following restrictions:

- XGBoost is not currently available on Windows machines. Follow here: https://github.com/h2oai/h2o-3/issues/7139 for updates.

- XGBoost is used only if it is available globally and if it hasn't been explicitly disabled. You can check if XGBoost is available by using the h2o.xgboost.available() in R or h2o.estimators.xgboost.H2OXGBoostEstimator.available() in Python.

5. Why doesn't AutoML use all the time that it's given?

If you're using 3.34.0.1 or later, AutoML should use all the time that it's given using max_runtime_secs. However, if you're using an earlier version, then early stopping was enabled by default and you can stop early. With early stopping, AutoML will stop once there's no longer “enough” incremental improvement. The user can tweak the early stopping paramters to be more or less sensitive. Set stopping_rounds higher if you want to slow down early stopping and let AutoML train more models before it stops.

6. Does AutoML support MOJOs?

AutoML will always produce a model which has a MOJO. Though it depends on the run, you are most likely to get a Stacked Ensemble. While all models are importable, only individual models are exportable.

7. Why doesn't AutoML use all the time that it's given?

If you're using 3.34.0.1 or later, AutoML should use all the time that it's given using max_runtime_secs. However, if you're using an earlier version, then early stopping was enabled by default and you can stop early. With early stopping, AutoML will stop once there's no longer “enough” incremental improvement. The user can tweak the early stopping paramters to be more or less sensitive. Set stopping_rounds higher if you want to slow down early stopping and let AutoML train more models before it stops.

8. What is the history of H2O AutoML?

The H2O AutoML algorithm was first released in H2O 3.12.0.1 on June 6, 2017 by Erin LeDell, and is based on research from her PhD thesis. New features and performance improvements have been made in every major version of H2O since the initial release.

"""

# @tool("get_h2o_leaderboard", return_direct=True)
# def get_h2o_leaderboard(h2o_agent) -> str:
#     """
#     Retrieve the current H2O AutoML leaderboard from the given H2OMLAgent.

#     Parameters
#     ----------
#     h2o_agent : H2OMLAgent
#         An instance of your H2OMLAgent that has run H2O AutoML.

#     Returns
#     -------
#     str
#         A stringified JSON of the H2O AutoML leaderboard. 
#         (Use JSON or CSV format as desired for your tooling.)
#     """
#     leaderboard_df = h2o_agent.get_leaderboard()
#     if leaderboard_df is None:
#         return "No leaderboard found. Make sure the agent has been run."
#     return leaderboard_df.to_json(orient="records")


# @tool("get_h2o_best_model_id", return_direct=True)
# def get_h2o_best_model_id(h2o_agent) -> str:
#     """
#     Retrieve the best model ID from the H2OMLAgent.

#     Parameters
#     ----------
#     h2o_agent : H2OMLAgent

#     Returns
#     -------
#     str
#         The best model identifier (e.g., "StackedEnsemble_BestOfFamily_...").
#     """
#     best_id = h2o_agent.get_best_model_id()
#     return best_id if best_id else "No best model found."


# @tool("get_h2o_best_model_path", return_direct=True)
# def get_h2o_best_model_path(h2o_agent) -> str:
#     """
#     Retrieve the file path of the best model saved by the H2OMLAgent.

#     Parameters
#     ----------
#     h2o_agent : H2OMLAgent

#     Returns
#     -------
#     str
#         The path where the best model is saved, or 'None' if not saved.
#     """
#     path = h2o_agent.get_model_path()
#     return path if path else "No model path found."


# @tool("predict_with_h2o_model", return_direct=True)
# def predict_with_h2o_model(
#     h2o_agent, 
#     data: List[Dict[str, Any]], 
#     model_id_or_path: Optional[str] = None
# ) -> str:
#     """
#     Predict on new data using a model from H2O. You can specify either:
#       - model_id_or_path as a model ID (if it's in the H2O cluster), or
#       - a local file path to a saved H2O model (if you have it).

#     Parameters
#     ----------
#     h2o_agent : H2OMLAgent
#         Instance of H2OMLAgent, to facilitate H2O usage.
#     data : List[Dict[str, Any]]
#         The data to predict on, in a list-of-rows (dictionary) format.
#     model_id_or_path : str, optional
#         Either the H2O model ID or the local path where the model is saved.

#     Returns
#     -------
#     str
#         A stringified JSON of the prediction results.
#     """
#     import h2o
#     import pandas as pd

#     # Convert the data to a pandas DataFrame
#     df = pd.DataFrame(data)
#     # Convert to H2OFrame
#     h2o_frame = h2o.H2OFrame(df)

#     # If model_id_or_path is an H2O model ID in the cluster:
#     try:
#         # Attempt to load it as a model ID from the cluster
#         model = h2o.get_model(model_id_or_path)
#     except Exception:
#         model = None

#     # If that fails, assume it's a local path
#     if model is None and model_id_or_path is not None:
#         model = h2o.load_model(model_id_or_path)

#     if model is None:
#         # As a fallback, default to the best model in the agent if no ID/path was given
#         best_id = h2o_agent.get_best_model_id()
#         if best_id:
#             model = h2o.get_model(best_id)
#         else:
#             return "No valid model_id_or_path found, and agent has no best model."

#     preds = model.predict(h2o_frame).as_data_frame()
#     return preds.to_json(orient="records")




================================================
FILE: ai_data_science_team/tools/mlflow.py
================================================


from typing import Optional, Dict, Any, Union, List, Annotated
from langgraph.prebuilt import InjectedState
from langchain.tools import tool


@tool(response_format='content_and_artifact')
def mlflow_search_experiments(
    filter_string: Optional[str] = None,
    tracking_uri: str | None = None,
    registry_uri: str | None = None
) -> str:
    """
    Search and list existing MLflow experiments.
    
    Parameters
    ----------
    filter_string : str, optional
        Filter query string (e.g., "name = 'my_experiment'"), defaults to
        searching for all experiments. 
        
    tracking_uri: str, optional
        Address of local or remote tracking server. 
        If not provided, defaults
        to the service set by mlflow.tracking.set_tracking_uri. See Where Runs Get Recorded <../tracking.html#where-runs-get-recorded>_ for more info.
    registry_uri: str, optional
        Address of local or remote model registry
        server. If not provided,
        defaults to the service set by mlflow.tracking.set_registry_uri. If no such service was set, defaults to the tracking uri of the client.

    Returns
    -------
    tuple
        - JSON-serialized list of experiment metadata (ID, name, etc.).
        - DataFrame of experiment metadata.
    """
    print("    * Tool: mlflow_search_experiments")
    from mlflow.tracking import MlflowClient
    import pandas as pd

    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=registry_uri)
    experiments = client.search_experiments(filter_string=filter_string)
    # Convert to a dictionary in a list
    experiments_data = [
        dict(e)
        for e in experiments
    ]
    # Convert to a DataFrame
    experiments_df = pd.DataFrame(experiments_data)
    # Convert timestamps to datetime objects
    experiments_df["last_update_time"] = pd.to_datetime(experiments_df["last_update_time"], unit="ms")
    experiments_df["creation_time"] = pd.to_datetime(experiments_df["creation_time"], unit="ms")
    
    return (experiments_df.to_dict(), experiments_df.to_dict())


@tool(response_format='content_and_artifact')
def mlflow_search_runs(
    experiment_ids: Optional[Union[List[str], List[int], str, int]] = None,
    filter_string: Optional[str] = None,
    tracking_uri: str | None = None,
    registry_uri: str | None = None
) -> str:
    """
    Search runs within one or more MLflow experiments, optionally filtering by a filter_string.

    Parameters
    ----------
    experiment_ids : list or str or int, optional
        One or more Experiment IDs.
    filter_string : str, optional
        MLflow filter expression, e.g. "metrics.rmse < 1.0".
    tracking_uri: str, optional
        Address of local or remote tracking server. 
        If not provided, defaults
        to the service set by mlflow.tracking.set_tracking_uri. See Where Runs Get Recorded <../tracking.html#where-runs-get-recorded>_ for more info.
    registry_uri: str, optional
        Address of local or remote model registry
        server. If not provided,
        defaults to the service set by mlflow.tracking.set_registry_uri. If no such service was set, defaults to the tracking uri of the client.

    Returns
    -------
    str
        JSON-formatted list of runs that match the query.
    """
    print("    * Tool: mlflow_search_runs")
    from mlflow.tracking import MlflowClient
    import pandas as pd
    
    client = MlflowClient(
        tracking_uri=tracking_uri, 
        registry_uri=registry_uri
    )
    
    if experiment_ids is None:
        experiment_ids = []
    if isinstance(experiment_ids, (str, int)):
        experiment_ids = [experiment_ids]
    
    runs = client.search_runs(
        experiment_ids=experiment_ids,
        filter_string=filter_string
    )
    
    # If no runs are found, return an empty DataFrame
    if not runs:
        return "No runs found.", pd.DataFrame()
    
    # Extract relevant information
    data = []
    for run in runs:
        run_info = {
            "run_id": run.info.run_id,
            "run_name": run.info.run_name,
            "status": run.info.status,
            "start_time": pd.to_datetime(run.info.start_time, unit="ms"),
            "end_time": pd.to_datetime(run.info.end_time, unit="ms"),
            "experiment_id": run.info.experiment_id,
            "user_id": run.info.user_id
        }

        # Flatten metrics, parameters, and tags
        run_info.update(run.data.metrics)
        run_info.update({f"param_{k}": v for k, v in run.data.params.items()})
        run_info.update({f"tag_{k}": v for k, v in run.data.tags.items()})

        data.append(run_info)

    # Convert to DataFrame
    df = pd.DataFrame(data)
    
    return (df.iloc[:,0:15].to_dict(), df.to_dict())



@tool(response_format='content')
def mlflow_create_experiment(experiment_name: str) -> str:
    """
    Create a new MLflow experiment by name.

    Parameters
    ----------
    experiment_name : str
        The name of the experiment to create.

    Returns
    -------
    str
        The experiment ID or an error message if creation failed.
    """
    print("    * Tool: mlflow_create_experiment")
    from mlflow.tracking import MlflowClient

    client = MlflowClient()
    exp_id = client.create_experiment(experiment_name)
    return f"Experiment created with ID: {exp_id}, name: {experiment_name}"




@tool(response_format='content_and_artifact')
def mlflow_predict_from_run_id(
    run_id: str, 
    data_raw: Annotated[dict, InjectedState("data_raw")],
    tracking_uri: Optional[str] = None
) -> tuple:
    """
    Predict using an MLflow model (PyFunc) directly from a given run ID.
    
    Parameters
    ----------
    run_id : str
        The ID of the MLflow run that logged the model.
    data_raw : dict
        The incoming data as a dictionary.
    tracking_uri : str, optional
        Address of local or remote tracking server.
    
    Returns
    -------
    tuple
        (user_facing_message, artifact_dict)
    """
    print("    * Tool: mlflow_predict_from_run_id")
    import mlflow
    import mlflow.pyfunc
    import pandas as pd

    # 1. Check if data is loaded
    if not data_raw:
        return "No data provided for prediction. Please use `data_raw` parameter inside of `invoke_agent()` or `ainvoke_agent()`.", {}
    df = pd.DataFrame(data_raw)

    # 2. Prepare model URI
    model_uri = f"runs:/{run_id}/model"

    # 3. Load or cache the MLflow model
    model = mlflow.pyfunc.load_model(model_uri)

    # 4. Make predictions
    try:
        preds = model.predict(df)
    except Exception as e:
        return f"Error during inference: {str(e)}", {}

    # 5. Convert predictions to a user-friendly summary + artifact
    if isinstance(preds, pd.DataFrame):
        sample_json = preds.head().to_json(orient='records')
        artifact_dict = preds.to_dict(orient='records')  # entire DF
        message = f"Predictions returned. Sample: {sample_json}"
    elif hasattr(preds, "to_json"):
        # e.g., pd.Series
        sample_json = preds[:5].to_json(orient='records')
        artifact_dict = preds.to_dict()
        message = f"Predictions returned. Sample: {sample_json}"
    elif hasattr(preds, "tolist"):
        # e.g., a NumPy array
        preds_list = preds.tolist()
        artifact_dict = {"predictions": preds_list}
        message = f"Predictions returned. First 5: {preds_list[:5]}"
    else:
        # fallback
        preds_str = str(preds)
        artifact_dict = {"predictions": preds_str}
        message = f"Predictions returned (unrecognized type). Example: {preds_str[:100]}..."

    return (message, artifact_dict)


# MLflow tool to launch gui for mlflow
@tool(response_format='content')
def mlflow_launch_ui(
    port: int = 5000,
    host: str = "localhost",
    tracking_uri: Optional[str] = None
) -> str:
    """
    Launch the MLflow UI.

    Parameters
    ----------
    port : int, optional
        The port on which to run the UI.
    host : str, optional
        The host address to bind the UI to.
    tracking_uri : str, optional
        Address of local or remote tracking server.

    Returns
    -------
    str
        Confirmation message.
    """
    print("    * Tool: mlflow_launch_ui")
    import subprocess
    
    # Try binding to the user-specified port first
    allocated_port = _find_free_port(start_port=port, host=host)

    cmd = ["mlflow", "ui", "--host", host, "--port", str(allocated_port)]
    if tracking_uri:
        cmd.extend(["--backend-store-uri", tracking_uri])
    
    process = subprocess.Popen(cmd)
    return (f"MLflow UI launched at http://{host}:{allocated_port}. "
            f"(PID: {process.pid})")

def _find_free_port(start_port: int, host: str) -> int:
    """
    Find a free port >= start_port on the specified host.
    If the start_port is free, returns start_port, else tries subsequent ports.
    """
    import socket
    for port_candidate in range(start_port, start_port + 1000):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            try:
                sock.bind((host, port_candidate))
            except OSError:
                # Port is in use, try the next one
                continue
            # If bind succeeds, it's free
            return port_candidate
    
    raise OSError("No available ports found in the range "
                  f"{start_port}-{start_port + 999}")


@tool(response_format='content')
def mlflow_stop_ui(port: int = 5000) -> str:
    """
    Kill any process currently listening on the given MLflow UI port.
    Requires `pip install psutil`.
    
    Parameters
    ----------
    port : int, optional
        The port on which the UI is running.
    """
    print("    * Tool: mlflow_stop_ui")
    import psutil
    
    # Gather system-wide inet connections
    for conn in psutil.net_connections(kind="inet"):
        # Check if this connection has a local address (laddr) and if
        # the port matches the one we're trying to free
        if conn.laddr and conn.laddr.port == port:
            # Some connections may not have an associated PID
            if conn.pid is not None:
                try:
                    p = psutil.Process(conn.pid)
                    p_name = p.name()  # optional: get process name for clarity
                    p.kill()           # forcibly terminate the process
                    return (
                        f"Killed process {conn.pid} ({p_name}) listening on port {port}."
                    )
                except psutil.NoSuchProcess:
                    return (
                        "Process was already terminated before we could kill it."
                    )
    return f"No process found listening on port {port}."


@tool(response_format='content_and_artifact')
def mlflow_list_artifacts(
    run_id: str,
    path: Optional[str] = None,
    tracking_uri: Optional[str] = None
) -> tuple:
    """
    List artifacts under a given MLflow run.

    Parameters
    ----------
    run_id : str
        The ID of the run whose artifacts to list.
    path : str, optional
        Path within the run's artifact directory to list. Defaults to the root.
    tracking_uri : str, optional
        Custom tracking server URI.

    Returns
    -------
    tuple
        (summary_message, artifact_listing)
    """
    print("    * Tool: mlflow_list_artifacts")
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient(tracking_uri=tracking_uri)
    # If path is None, list the root folder
    artifact_list = client.list_artifacts(run_id, path or "")
    
    # Convert to a more user-friendly structure
    artifacts_data = []
    for artifact in artifact_list:
        artifacts_data.append({
            "path": artifact.path,
            "is_dir": artifact.is_dir,
            "file_size": artifact.file_size
        })
    
    return (
        f"Found {len(artifacts_data)} artifacts.",
        artifacts_data
    )


@tool(response_format='content_and_artifact')
def mlflow_download_artifacts(
    run_id: str,
    path: Optional[str] = None,
    dst_path: Optional[str] = "./downloaded_artifacts",
    tracking_uri: Optional[str] = None
) -> tuple:
    """
    Download artifacts from MLflow to a local directory.

    Parameters
    ----------
    run_id : str
        The ID of the run whose artifacts to download.
    path : str, optional
        Path within the run's artifact directory to download. Defaults to the root.
    dst_path : str, optional
        Local destination path to store artifacts.
    tracking_uri : str, optional
        MLflow tracking server URI.

    Returns
    -------
    tuple
        (summary_message, artifact_dict)
    """
    print("    * Tool: mlflow_download_artifacts")
    from mlflow.tracking import MlflowClient
    import os
    
    client = MlflowClient(tracking_uri=tracking_uri)
    local_path = client.download_artifacts(run_id, path or "", dst_path)
    
    # Build a recursive listing of what was downloaded
    downloaded_files = []
    for root, dirs, files in os.walk(local_path):
        for f in files:
            downloaded_files.append(os.path.join(root, f))
    
    message = (
        f"Artifacts for run_id='{run_id}' have been downloaded to: {local_path}. "
        f"Total files: {len(downloaded_files)}."
    )
    
    return (
        message,
        {"downloaded_files": downloaded_files}
    )


@tool(response_format='content_and_artifact')
def mlflow_list_registered_models(
    max_results: int = 100,
    tracking_uri: Optional[str] = None,
    registry_uri: Optional[str] = None
) -> tuple:
    """
    List all registered models in MLflow's model registry.

    Parameters
    ----------
    max_results : int, optional
        Maximum number of models to return.
    tracking_uri : str, optional
    registry_uri : str, optional

    Returns
    -------
    tuple
        (summary_message, model_list)
    """
    print("    * Tool: mlflow_list_registered_models")
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=registry_uri)
    # The list_registered_models() call can be paginated; for simplicity, we just pass max_results
    models = client.list_registered_models(max_results=max_results)
    
    models_data = []
    for m in models:
        models_data.append({
            "name": m.name,
            "latest_versions": [
                {
                    "version": v.version,
                    "run_id": v.run_id,
                    "current_stage": v.current_stage,
                }
                for v in m.latest_versions
            ]
        })
    
    return (
        f"Found {len(models_data)} registered models.",
        models_data
    )


@tool(response_format='content_and_artifact')
def mlflow_search_registered_models(
    filter_string: Optional[str] = None,
    order_by: Optional[List[str]] = None,
    max_results: int = 100,
    tracking_uri: Optional[str] = None,
    registry_uri: Optional[str] = None
) -> tuple:
    """
    Search registered models in MLflow's registry using optional filters.

    Parameters
    ----------
    filter_string : str, optional
        e.g. "name LIKE 'my_model%'" or "tags.stage = 'production'". 
    order_by : list, optional
        e.g. ["name ASC"] or ["timestamp DESC"].
    max_results : int, optional
        Max number of results.
    tracking_uri : str, optional
    registry_uri : str, optional

    Returns
    -------
    tuple
        (summary_message, model_dict_list)
    """
    print("    * Tool: mlflow_search_registered_models")
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=registry_uri)
    models = client.search_registered_models(
        filter_string=filter_string,
        order_by=order_by,
        max_results=max_results
    )
    
    models_data = []
    for m in models:
        models_data.append({
            "name": m.name,
            "description": m.description,
            "creation_timestamp": m.creation_timestamp,
            "last_updated_timestamp": m.last_updated_timestamp,
            "latest_versions": [
                {
                    "version": v.version,
                    "run_id": v.run_id,
                    "current_stage": v.current_stage
                }
                for v in m.latest_versions
            ]
        })
    
    return (
        f"Found {len(models_data)} models matching filter={filter_string}.",
        models_data
    )


@tool(response_format='content_and_artifact')
def mlflow_get_model_version_details(
    name: str,
    version: str,
    tracking_uri: Optional[str] = None,
    registry_uri: Optional[str] = None
) -> tuple:
    """
    Retrieve details about a specific model version in the MLflow registry.

    Parameters
    ----------
    name : str
        Name of the registered model.
    version : str
        Version number of that model.
    tracking_uri : str, optional
    registry_uri : str, optional

    Returns
    -------
    tuple
        (summary_message, version_data_dict)
    """
    print("    * Tool: mlflow_get_model_version_details")
    from mlflow.tracking import MlflowClient
    
    client = MlflowClient(tracking_uri=tracking_uri, registry_uri=registry_uri)
    version_details = client.get_model_version(name, version)
    
    data = {
        "name": version_details.name,
        "version": version_details.version,
        "run_id": version_details.run_id,
        "creation_timestamp": version_details.creation_timestamp,
        "current_stage": version_details.current_stage,
        "description": version_details.description,
        "status": version_details.status
    }
    
    return (
        f"Model version details retrieved for {name} v{version}",
        data
    )


# @tool
# def get_or_create_experiment(experiment_name):
#     """
#     Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.

#     This function checks if an experiment with the given name exists within MLflow.
#     If it does, the function returns its ID. If not, it creates a new experiment
#     with the provided name and returns its ID.

#     Parameters:
#     - experiment_name (str): Name of the MLflow experiment.

#     Returns:
#     - str: ID of the existing or newly created MLflow experiment.
#     """
#     import mlflow
#     if experiment := mlflow.get_experiment_by_name(experiment_name):
#         return experiment.experiment_id
#     else:
#         return mlflow.create_experiment(experiment_name)



# @tool("mlflow_set_tracking_uri", return_direct=True)
# def mlflow_set_tracking_uri(tracking_uri: str) -> str:
#     """
#     Set or change the MLflow tracking URI.

#     Parameters
#     ----------
#     tracking_uri : str
#         The URI/path where MLflow logs & metrics are stored.

#     Returns
#     -------
#     str
#         Confirmation message.
#     """
#     import mlflow
#     mlflow.set_tracking_uri(tracking_uri)
#     return f"MLflow tracking URI set to: {tracking_uri}"


# @tool("mlflow_list_experiments", return_direct=True)
# def mlflow_list_experiments() -> str:
#     """
#     List existing MLflow experiments.

#     Returns
#     -------
#     str
#         JSON-serialized list of experiment metadata (ID, name, etc.).
#     """
#     from mlflow.tracking import MlflowClient
#     import json

#     client = MlflowClient()
#     experiments = client.list_experiments()
#     # Convert to a JSON-like structure
#     experiments_data = [
#         dict(experiment_id=e.experiment_id, name=e.name, artifact_location=e.artifact_location)
#         for e in experiments
#     ]
    
#     return json.dumps(experiments_data)


# @tool("mlflow_create_experiment", return_direct=True)
# def mlflow_create_experiment(experiment_name: str) -> str:
#     """
#     Create a new MLflow experiment by name.

#     Parameters
#     ----------
#     experiment_name : str
#         The name of the experiment to create.

#     Returns
#     -------
#     str
#         The experiment ID or an error message if creation failed.
#     """
#     from mlflow.tracking import MlflowClient

#     client = MlflowClient()
#     exp_id = client.create_experiment(experiment_name)
#     return f"Experiment created with ID: {exp_id}"


# @tool("mlflow_set_experiment", return_direct=True)
# def mlflow_set_experiment(experiment_name: str) -> str:
#     """
#     Set or create an MLflow experiment for subsequent logging.

#     Parameters
#     ----------
#     experiment_name : str
#         The name of the experiment to set.

#     Returns
#     -------
#     str
#         Confirmation of the chosen experiment name.
#     """
#     import mlflow
#     mlflow.set_experiment(experiment_name)
#     return f"Active MLflow experiment set to: {experiment_name}"


# @tool("mlflow_start_run", return_direct=True)
# def mlflow_start_run(run_name: Optional[str] = None) -> str:
#     """
#     Start a new MLflow run under the current experiment.

#     Parameters
#     ----------
#     run_name : str, optional
#         Optional run name.

#     Returns
#     -------
#     str
#         The run_id of the newly started MLflow run.
#     """
#     import mlflow
#     with mlflow.start_run(run_name=run_name) as run:
#         run_id = run.info.run_id
#     return f"MLflow run started with run_id: {run_id}"


# @tool("mlflow_log_params", return_direct=True)
# def mlflow_log_params(params: Dict[str, Any]) -> str:
#     """
#     Log a batch of parameters to the current MLflow run.

#     Parameters
#     ----------
#     params : dict
#         A dictionary of parameter name -> parameter value.

#     Returns
#     -------
#     str
#         Confirmation message.
#     """
#     import mlflow
#     mlflow.log_params(params)
#     return f"Logged parameters: {params}"


# @tool("mlflow_log_metrics", return_direct=True)
# def mlflow_log_metrics(metrics: Dict[str, float], step: Optional[int] = None) -> str:
#     """
#     Log a dictionary of metrics to the current MLflow run.

#     Parameters
#     ----------
#     metrics : dict
#         Metric name -> numeric value.
#     step : int, optional
#         The training step or iteration number.

#     Returns
#     -------
#     str
#         Confirmation message.
#     """
#     import mlflow
#     mlflow.log_metrics(metrics, step=step)
#     return f"Logged metrics: {metrics} at step {step}"


# @tool("mlflow_log_artifact", return_direct=True)
# def mlflow_log_artifact(artifact_path: str, artifact_folder_name: Optional[str] = None) -> str:
#     """
#     Log a local file or directory as an MLflow artifact.

#     Parameters
#     ----------
#     artifact_path : str
#         The local path to the file/directory to be logged.
#     artifact_folder_name : str, optional
#         Subfolder within the run's artifact directory.

#     Returns
#     -------
#     str
#         Confirmation message.
#     """
#     import mlflow
#     if artifact_folder_name:
#         mlflow.log_artifact(artifact_path, artifact_folder_name)
#         return f"Artifact logged from {artifact_path} into folder '{artifact_folder_name}'"
#     else:
#         mlflow.log_artifact(artifact_path)
#         return f"Artifact logged from {artifact_path}"


# @tool("mlflow_log_model", return_direct=True)
# def mlflow_log_model(model_path: str, registered_model_name: Optional[str] = None) -> str:
#     """
#     Log a model artifact (e.g., an H2O-saved model directory) to MLflow.

#     Parameters
#     ----------
#     model_path : str
#         The local filesystem path containing the model artifacts.
#     registered_model_name : str, optional
#         If provided, will also attempt to register the model under this name.

#     Returns
#     -------
#     str
#         Confirmation message with any relevant registration details.
#     """
#     import mlflow
#     if registered_model_name:
#         mlflow.pyfunc.log_model(
#             artifact_path="model",
#             python_model=None,  # if you have a pyfunc wrapper, specify it
#             registered_model_name=registered_model_name,
#             code_path=None,
#             conda_env=None,
#             model_path=model_path  # for certain model flavors, or use flavors
#         )
#         return f"Model logged and registered under '{registered_model_name}' from path {model_path}"
#     else:
#         # Simple log as generic artifact
#         mlflow.pyfunc.log_model(
#             artifact_path="model",
#             python_model=None,
#             code_path=None,
#             conda_env=None,
#             model_path=model_path
#         )
#         return f"Model logged (no registration) from path {model_path}"


# @tool("mlflow_end_run", return_direct=True)
# def mlflow_end_run() -> str:
#     """
#     End the current MLflow run (if one is active).

#     Returns
#     -------
#     str
#         Confirmation message.
#     """
#     import mlflow
#     mlflow.end_run()
#     return "MLflow run ended."


# @tool("mlflow_search_runs", return_direct=True)
# def mlflow_search_runs(
#     experiment_names_or_ids: Optional[Union[List[str], List[int], str, int]] = None,
#     filter_string: Optional[str] = None
# ) -> str:
#     """
#     Search runs within one or more MLflow experiments, optionally filtering by a filter_string.

#     Parameters
#     ----------
#     experiment_names_or_ids : list or str or int, optional
#         Experiment IDs or names.
#     filter_string : str, optional
#         MLflow filter expression, e.g. "metrics.rmse < 1.0".

#     Returns
#     -------
#     str
#         JSON-formatted list of runs that match the query.
#     """
#     import mlflow
#     import json
#     if experiment_names_or_ids is None:
#         experiment_names_or_ids = []
#     if isinstance(experiment_names_or_ids, (str, int)):
#         experiment_names_or_ids = [experiment_names_or_ids]

#     df = mlflow.search_runs(
#         experiment_names=experiment_names_or_ids if all(isinstance(e, str) for e in experiment_names_or_ids) else None,
#         experiment_ids=experiment_names_or_ids if all(isinstance(e, int) for e in experiment_names_or_ids) else None,
#         filter_string=filter_string
#     )
#     return df.to_json(orient="records")


# @tool("mlflow_get_run", return_direct=True)
# def mlflow_get_run(run_id: str) -> str:
#     """
#     Retrieve details (params, metrics, etc.) for a specific MLflow run by ID.

#     Parameters
#     ----------
#     run_id : str
#         The ID of the MLflow run to retrieve.

#     Returns
#     -------
#     str
#         JSON-formatted data containing run info, params, and metrics.
#     """
#     from mlflow.tracking import MlflowClient
#     import json

#     client = MlflowClient()
#     run = client.get_run(run_id)
#     data = {
#         "run_id": run.info.run_id,
#         "experiment_id": run.info.experiment_id,
#         "status": run.info.status,
#         "start_time": run.info.start_time,
#         "end_time": run.info.end_time,
#         "artifact_uri": run.info.artifact_uri,
#         "params": run.data.params,
#         "metrics": run.data.metrics,
#         "tags": run.data.tags
#     }
#     return json.dumps(data)


# @tool("mlflow_load_model", return_direct=True)
# def mlflow_load_model(model_uri: str) -> str:
#     """
#     Load an MLflow-model (PyFunc flavor or other) into memory, returning a handle reference.
#     For demonstration, we store the loaded model globally in a registry dict.

#     Parameters
#     ----------
#     model_uri : str
#         The URI of the model to load, e.g. "runs:/<RUN_ID>/model" or "models:/MyModel/Production".

#     Returns
#     -------
#     str
#         A reference key identifying the loaded model (for subsequent predictions), 
#         or a direct message if you prefer to store it differently.
#     """
#     import mlflow.pyfunc
#     from uuid import uuid4

#     # For demonstration, create a global registry:
#     global _LOADED_MODELS
#     if "_LOADED_MODELS" not in globals():
#         _LOADED_MODELS = {}

#     loaded_model = mlflow.pyfunc.load_model(model_uri)
#     model_key = f"model_{uuid4().hex}"
#     _LOADED_MODELS[model_key] = loaded_model

#     return f"Model loaded with reference key: {model_key}"


# @tool("mlflow_predict", return_direct=True)
# def mlflow_predict(model_key: str, data: List[Dict[str, Any]]) -> str:
#     """
#     Predict using a previously loaded MLflow model (PyFunc), identified by its reference key.

#     Parameters
#     ----------
#     model_key : str
#         The reference key for the loaded model (returned by mlflow_load_model).
#     data : List[Dict[str, Any]]
#         The data rows for which predictions should be made.

#     Returns
#     -------
#     str
#         JSON-formatted prediction results.
#     """
#     import pandas as pd
#     import json

#     global _LOADED_MODELS
#     if model_key not in _LOADED_MODELS:
#         return f"No model found for key: {model_key}"

#     model = _LOADED_MODELS[model_key]
#     df = pd.DataFrame(data)
#     preds = model.predict(df)
#     # Convert to JSON (DataFrame or Series)
#     if hasattr(preds, "to_json"):
#         return preds.to_json(orient="records")
#     else:
#         # If preds is just a numpy array or list
#         return json.dumps(preds.tolist())




================================================
FILE: ai_data_science_team/tools/sql.py
================================================

import pandas as pd
import sqlalchemy as sql
from sqlalchemy import inspect


def get_database_metadata(connection, n_samples=10) -> dict:
    """
    Collects metadata and sample data from a database, with safe identifier quoting and
    basic dialect-aware row limiting. Prevents issues with spaces/reserved words in identifiers.
    
    Parameters
    ----------
    connection : Union[sql.engine.base.Connection, sql.engine.base.Engine]
        An active SQLAlchemy connection or engine.
    n_samples : int
        Number of sample values to retrieve for each column.

    Returns
    -------
    dict
        A dictionary with database metadata, including some sample data from each column.
    """
    is_engine = isinstance(connection, sql.engine.base.Engine)
    conn = connection.connect() if is_engine else connection

    metadata = {
        "dialect": None,
        "driver": None,
        "connection_url": None,
        "schemas": [],
    }

    try:
        sql_engine = conn.engine
        dialect_name = sql_engine.dialect.name.lower()

        metadata["dialect"] = sql_engine.dialect.name
        metadata["driver"] = sql_engine.driver
        metadata["connection_url"] = str(sql_engine.url)

        inspector = inspect(sql_engine)
        preparer = inspector.bind.dialect.identifier_preparer

        # For each schema
        for schema_name in inspector.get_schema_names():
            schema_obj = {
                "schema_name": schema_name,
                "tables": []
            }

            tables = inspector.get_table_names(schema=schema_name)
            for table_name in tables:
                table_info = {
                    "table_name": table_name,
                    "columns": [],
                    "primary_key": [],
                    "foreign_keys": [],
                    "indexes": []
                }
                # Get columns
                columns = inspector.get_columns(table_name, schema=schema_name)
                for col in columns:
                    col_name = col["name"]
                    col_type = str(col["type"])
                    table_name_quoted = f"{preparer.quote_identifier(schema_name)}.{preparer.quote_identifier(table_name)}"
                    col_name_quoted = preparer.quote_identifier(col_name)

                    # Build query for sample data
                    query = build_query(col_name_quoted, table_name_quoted, n_samples, dialect_name)

                    # Retrieve sample data
                    try:
                        df = pd.read_sql(query, conn)
                        samples = df[col_name].head(n_samples).tolist()
                    except Exception as e:
                        samples = [f"Error retrieving data: {str(e)}"]

                    table_info["columns"].append({
                        "name": col_name,
                        "type": col_type,
                        "sample_values": samples
                    })

                # Primary keys
                pk_constraint = inspector.get_pk_constraint(table_name, schema=schema_name)
                table_info["primary_key"] = pk_constraint.get("constrained_columns", [])

                # Foreign keys
                fks = inspector.get_foreign_keys(table_name, schema=schema_name)
                table_info["foreign_keys"] = [
                    {
                        "local_cols": fk["constrained_columns"],
                        "referred_table": fk["referred_table"],
                        "referred_cols": fk["referred_columns"]
                    }
                    for fk in fks
                ]

                # Indexes
                idxs = inspector.get_indexes(table_name, schema=schema_name)
                table_info["indexes"] = idxs

                schema_obj["tables"].append(table_info)
            
            metadata["schemas"].append(schema_obj)
    
    finally:
        if is_engine:
            conn.close()

    return metadata

def build_query(col_name_quoted: str, table_name_quoted: str, n: int, dialect_name: str) -> str:
    # Example: expand your build_query to handle random sampling if possible
    if "postgres" in dialect_name:
        return f"SELECT {col_name_quoted} FROM {table_name_quoted} ORDER BY RANDOM() LIMIT {n}"
    if "mysql" in dialect_name:
        return f"SELECT {col_name_quoted} FROM {table_name_quoted} ORDER BY RAND() LIMIT {n}"
    if "sqlite" in dialect_name:
        return f"SELECT {col_name_quoted} FROM {table_name_quoted} ORDER BY RANDOM() LIMIT {n}"
    if "mssql" in dialect_name:
        return f"SELECT TOP {n} {col_name_quoted} FROM {table_name_quoted} ORDER BY NEWID()"
    # Oracle or fallback
    return f"SELECT {col_name_quoted} FROM {table_name_quoted} WHERE ROWNUM <= {n}"



