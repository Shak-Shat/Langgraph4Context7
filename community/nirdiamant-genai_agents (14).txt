Directory structure:
└── news_tldr_langgraph.ipynb

================================================
FILE: all_agents_tutorials/news_tldr_langgraph.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# News TL;DR using Langgraph (Too Long Didn't Read)

## Overview
This project demonstrates the creation of a news summarization agent uses large language models (LLMs) for decision making and summarization as well as a news API calls. The integration of LangGraph to coordinate sequential and cyclical processes, open-ai to choose and condense articles, newsAPI to retrieve relevant article metadata, and BeautifulSoup for web scraping allows for the generation of relevant current event article TL;DRs from a single query.

## Motivation
Although LLMs demonstrate excellent conversational and educational ability, they lack access to knowledge of current events. This project allow users to ask about a news topic they are interested and receive a TL;DR of relevant articles. The goal is to allow users to conveniently follow their interest and stay current with their connection to world events.

## Key Components
1. **LangGraph**: Orchestrates the overall workflow, managing the flow of data between different stages of the process.
2. **GPT-4o-mini (via LangChain)**: Generates search terms, selects relevant articles, parses html, provides article summaries
3. **NewsAPI**: Retrieves article metadata from keyword search
4. **BeautifulSoup**: Retrieves html from page
5. **Asyncio**: Allows separate LLM calls to be made concurrently for speed efficiency.

## Method
The news research follows these high-level steps:

1. **NewsAPI Parameter Creation (LLM 1)**: Given a user query, the model generates a formatted parameter dict for the news search.

2. **Article Metadata Retrieval**: An API call to NewsAPI retrieves relevant article metadata.

3. **Article Text Retrieval**: Beautiful Soup scrapes the full article text from the urls to ensure validity.

4. **Conditional Logic**: Conditional logic either: repeats 1-3 if article threshold not reached, proceeds to step 5, end with no articles found.

5. **Relevant Article Selection (LLM 2)**: The model selects urls from the most relevant n-articles for the user query based on the short synopsis provided by the API.

6. **Generate TL;DR (LLM 3+)**: A summarized set of bullet points for each article is generated concurrently with Asyncio.

This workflow is managed by LangGraph to make sure that the appropriate prompt is fed to the each LLM call.

## Conclusion
This news TL;DR agent highlights the utility of coordinating successive LLM generations in order to
achieve a higher level goal.

Although the current implementation only retrieves bulleted summaries, it could be elaborated to start
a dialogue with the user that could allow them to ask questions about the article and get 
more information or to collectively generate a coherent opinion.
"""

"""
## Setup and Imports

Install and import necessary libraries
"""

!pip install langgraph -q
!pip install langchain-openai -q
!pip install langchain-core -q
!pip install pydantic -q
!pip install python-dotenv -q
!pip install newsapi-python -q
!pip install beautifulsoup4 -q
!pip install ipython -q
!pip install nest_asyncio -q

import os
from typing import TypedDict, Annotated, List
from langgraph.graph import Graph, END
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables.graph import MermaidDrawMethod
from datetime import datetime
import re

from getpass import getpass
from dotenv import load_dotenv

from newsapi import NewsApiClient
import requests
from bs4 import BeautifulSoup

from IPython.display import display, Image as IPImage
import asyncio

"""
## Get an NewsAPI Key
* create a free developer account at https://newsapi.org/
* 100 requests per day
* articles between 1 day and 1 month old
"""

"""
## Setup LLM Model
* create an account and register a credit card at https://platform.openai.com/chat-completions
* create an API key
"""

"""
## Create Your Environmental Variables (Optional)
Create a file named `.env` in the same directory as this notebook with the following
```
OPENAI_API_KEY = 'your-api-key'
NEWSAPI_KEY = 'your-api-key'
```

If you skip this step, you will be asked to input all API keys once each time you start this notebook.
"""

"""
## Initialize Model and Environmental Variables

If you're not running a local model with Ollama, the next cell will ask for your OPENAI_API_KEY and
securely add it as an environmental variable. It will not persist in this notebook.
"""

# check for .env file
if os.path.exists("../.env"):
    load_dotenv()
else:
    # ask for API keys
    os.environ["NEWSAPI_KEY"] = getpass("Enter your News API key: ")
    os.environ["OPENAI_API_KEY"] = getpass("Enter your OpenAI API key: ")

# sets the OpenAI model to use and initialize model
model = "gpt-4o-mini"
llm = ChatOpenAI(model=model,)

newsapi_key = os.getenv("NEWSAPI_KEY")
if newsapi_key:
        print("NEWSAPI_KEY successfully loaded from .env.")
# Output:
#   NEWSAPI_KEY successfully loaded from .env.


"""
## Test APIs
"""

llm.invoke("Why is the sky blue?").content
# Output:
#   "The sky appears blue primarily due to a phenomenon known as Rayleigh scattering. This occurs when sunlight enters the Earth's atmosphere and interacts with air molecules. \n\nSunlight, or white light, is made up of different colors, each with varying wavelengths. Blue light has a shorter wavelength than other colors, such as red or yellow. When sunlight passes through the atmosphere, the shorter wavelengths (blue and violet) are scattered in all directions by the gases and particles in the air. \n\nAlthough violet light is scattered even more than blue light, our eyes are more sensitive to blue light, and some of the violet light is absorbed by the ozone layer. As a result, we perceive the sky as blue during the day. \n\nAt sunrise and sunset, the sun's light has to pass through more of the Earth's atmosphere, which scatters the shorter blue wavelengths out of our line of sight, allowing the longer wavelengths (reds and oranges) to dominate the sky's colors during those times."

newsapi = NewsApiClient(api_key=os.getenv('NEWSAPI_KEY'))

query = 'ai news of the day'

all_articles = newsapi.get_everything(q=query,
                                      sources='google-news,bbc-news,techcrunch',
                                      domains='techcrunch.com, bbc.co.uk',
                                      language='en',
                                      sort_by='relevancy',)


all_articles['articles'][0]

"""
## Define Data Structures

Define the GraphState class. Each user query will be added to a new instance of this class, which will be passed
through the LangGraph structure while collect outputs from each step. When it reaches the END node, it's final
result will be returned to the user.
"""

class GraphState(TypedDict):
    news_query: Annotated[str, "Input query to extract news search parameters from."]
    num_searches_remaining: Annotated[int, "Number of articles to search for."]
    newsapi_params: Annotated[dict, "Structured argument for the News API."]
    past_searches: Annotated[List[dict], "List of search params already used."]
    articles_metadata: Annotated[list[dict], "Article metadata response from the News API"]
    scraped_urls: Annotated[List[str], "List of urls already scraped."]
    num_articles_tldr: Annotated[int, "Number of articles to create TL;DR for."]
    potential_articles: Annotated[List[dict[str, str, str]], "Article with full text to consider summarizing."]
    tldr_articles: Annotated[List[dict[str, str, str]], "Selected article TL;DRs."]
    formatted_results: Annotated[str, "Formatted results to display."]

"""
## Define NewsAPI argument data structure with Pydantic
* the model will create a formatted dictionary of params for the NewsAPI call
* the NewsApiParams class inherits from the Pydantic BaseModel
* Langchain will parse and feed paramd descriptions to the LLM
"""

class NewsApiParams(BaseModel):
    q: str = Field(description="1-3 concise keyword search terms that are not too specific")
    sources: str =Field(description="comma-separated list of sources from: 'abc-news,abc-news-au,associated-press,australian-financial-review,axios,bbc-news,bbc-sport,bloomberg,business-insider,cbc-news,cbs-news,cnn,financial-post,fortune'")
    from_param: str = Field(description="date in format 'YYYY-MM-DD' Two days ago minimum. Extend up to 30 days on second and subsequent requests.")
    to: str = Field(description="date in format 'YYYY-MM-DD' today's date unless specified")
    language: str = Field(description="language of articles 'en' unless specified one of ['ar', 'de', 'en', 'es', 'fr', 'he', 'it', 'nl', 'no', 'pt', 'ru', 'se', 'ud', 'zh']")
    sort_by: str = Field(description="sort by 'relevancy', 'popularity', or 'publishedAt'")

"""
## Define Graph Functions

Define the functions (nodes) that will be used in the LangGraph workflow.
"""

def generate_newsapi_params(state: GraphState) -> GraphState:
    """Based on the query, generate News API params."""
    # initialize parser to define the structure of the response
    parser = JsonOutputParser(pydantic_object=NewsApiParams)

    # retrieve today's date
    today_date = datetime.now().strftime("%Y-%m-%d")

    # retrieve list of past search params
    past_searches = state["past_searches"]

    # retrieve number of searches remaining
    num_searches_remaining = state["num_searches_remaining"]

    # retrieve the user's query
    news_query = state["news_query"]

    template = """
    Today is {today_date}.

    Create a param dict for the News API based on the user query:
    {query}

    These searches have already been made. Loosen the search terms to get more results.
    {past_searches}
    
    Following these formatting instructions:
    {format_instructions}

    Including this one, you have {num_searches_remaining} searches remaining.
    If this is your last search, use all news sources and a 30 days search range.
    """

    # create a prompt template to merge the query, today's date, and the format instructions
    prompt_template = PromptTemplate(
        template=template,
        variables={"today": today_date, "query": news_query, "past_searches": past_searches, "num_searches_remaining": num_searches_remaining},
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )

    # create prompt chain template
    chain = prompt_template | llm | parser

    # invoke the chain with the news api query
    result = chain.invoke({"query": news_query, "today_date": today_date, "past_searches": past_searches, "num_searches_remaining": num_searches_remaining})

    # update the state
    state["newsapi_params"] = result

    return state

def retrieve_articles_metadata(state: GraphState) -> GraphState:
    """Using the NewsAPI params, perform api call."""
    # parameters generated for the News API
    newsapi_params = state["newsapi_params"]

    # decrement the number of searches remaining
    state['num_searches_remaining'] -= 1

    try:
        # create a NewsApiClient object
        newsapi = NewsApiClient(api_key=os.getenv('NEWSAPI_KEY'))
        
        # retreive the metadata of the new articles
        articles = newsapi.get_everything(**newsapi_params)

        # append this search term to the past searches to avoid duplicates
        state['past_searches'].append(newsapi_params)

        # load urls that have already been returned and scraped
        scraped_urls = state["scraped_urls"]

        # filter out articles that have already been scraped
        new_articles = []
        for article in articles['articles']:
            if article['url'] not in scraped_urls and len(state['potential_articles']) + len(new_articles) < 10:
                new_articles.append(article)

        # reassign new articles to the state
        state["articles_metadata"] = new_articles

    # handle exceptions
    except Exception as e:
        print(f"Error: {e}")

    return state

def retrieve_articles_text(state: GraphState) -> GraphState:
    """Web scrapes to retrieve article text."""
    # load retrieved article metadata
    articles_metadata = state["articles_metadata"]
    # Add headers to simulate a browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36'
    }

    # create list to store valid article dicts
    potential_articles = []

    # iterate over the urls
    for article in articles_metadata:
        # extract the url
        url = article['url']

        # use beautiful soup to extract the article content
        response = requests.get(url, headers=headers)
        
        # check if the request was successful
        if response.status_code == 200:
            # parse the HTML content
            soup = BeautifulSoup(response.content, 'html.parser')

            # find the article content
            text = soup.get_text(strip=True)

            # append article dict to list
            potential_articles.append({"title": article["title"], "url": url, "description": article["description"], "text": text})

            # append the url to the processed urls
            state["scraped_urls"].append(url)

    # append the processed articles to the state
    state["potential_articles"].extend(potential_articles)

    return state

def select_top_urls(state: GraphState) -> GraphState:
    """Based on the article synoses, choose the top-n articles to summarize."""
    news_query = state["news_query"]
    num_articles_tldr = state["num_articles_tldr"]
    
    # load all processed articles with full text but no summaries
    potential_articles = state["potential_articles"]

    # format the metadata
    formatted_metadata = "\n".join([f"{article['url']}\n{article['description']}\n" for article in potential_articles])

    prompt = f"""
    Based on the user news query:
    {news_query}

    Reply with a list of strings of up to {num_articles_tldr} relevant urls.
    Don't add any urls that are not relevant or aren't listed specifically.
    {formatted_metadata}
    """
    result = llm.invoke(prompt).content

    # use regex to extract the urls as a list
    url_pattern = r'(https?://[^\s",]+)'

    # Find all URLs in the text
    urls = re.findall(url_pattern, result)

    # add the selected article metadata to the state
    tldr_articles = [article for article in potential_articles if article['url'] in urls]

    # tldr_articles = [article for article in potential_articles if article['url'] in urls]
    state["tldr_articles"] = tldr_articles

    return state

async def summarize_articles_parallel(state: GraphState) -> GraphState:
    """Summarize the articles based on full text."""
    tldr_articles = state["tldr_articles"]

    # prompt = """
    # Summarize the article text in a bulleted tl;dr. Each line should start with a hyphen -
    # {article_text}
    # """

    prompt = """
    Create a * bulleted summarizing tldr for the article:
    {text}
      
    Be sure to follow the following format exaxtly with nothing else:
    {title}
    {url}
    * tl;dr bulleted summary
    * use bullet points for each sentence
    """

    # iterate over the selected articles and collect summaries synchronously
    for i in range(len(tldr_articles)):
        text = tldr_articles[i]["text"]
        title = tldr_articles[i]["title"]
        url = tldr_articles[i]["url"]
        # invoke the llm synchronously
        result = llm.invoke(prompt.format(title=title, url=url, text=text))
        tldr_articles[i]["summary"] = result.content

    state["tldr_articles"] = tldr_articles

    return state

def format_results(state: GraphState) -> GraphState:
    """Format the results for display."""
    # load a list of past search queries
    q = [newsapi_params["q"] for newsapi_params in state["past_searches"]]
    formatted_results = f"Here are the top {len(state['tldr_articles'])} articles based on search terms:\n{', '.join(q)}\n\n"

    # load the summarized articles
    tldr_articles = state["tldr_articles"]

    # format article tl;dr summaries
    tldr_articles = "\n\n".join([f"{article['summary']}" for article in tldr_articles])

    # concatenate summaries to the formatted results
    formatted_results += tldr_articles

    state["formatted_results"] = formatted_results

    return state

"""
## Set Up LangGraph Workflow
"""

"""
Set up decision logic to try to retrieve `num_searches_remaining` articles, while limiting attempts to 5.
"""

def articles_text_decision(state: GraphState) -> str:
    """Check results of retrieve_articles_text to determine next step."""
    if state["num_searches_remaining"] == 0:
        # if no articles with text were found return END
        if len(state["potential_articles"]) == 0:
            state["formatted_results"] = "No articles with text found."
            return "END"
        # if some articles were found, move on to selecting the top urls
        else:
            return "select_top_urls"
    else:
        # if the number of articles found is less than the number of articles to summarize, continue searching
        if len(state["potential_articles"]) < state["num_articles_tldr"]:
            return "generate_newsapi_params"
        # otherwise move on to selecting the top urls
        else:
            return "select_top_urls"
        

"""
Define the LangGraph workflow by adding nodes and edges.
"""

workflow = Graph()

workflow.set_entry_point("generate_newsapi_params")

workflow.add_node("generate_newsapi_params", generate_newsapi_params)
workflow.add_node("retrieve_articles_metadata", retrieve_articles_metadata)
workflow.add_node("retrieve_articles_text", retrieve_articles_text)
workflow.add_node("select_top_urls", select_top_urls)
workflow.add_node("summarize_articles_parallel", summarize_articles_parallel)
workflow.add_node("format_results", format_results)
# workflow.add_node("add_commentary", add_commentary)

workflow.add_edge("generate_newsapi_params", "retrieve_articles_metadata")
workflow.add_edge("retrieve_articles_metadata", "retrieve_articles_text")
# # if the number of articles with parseable text is less than number requested, then search for more articles
workflow.add_conditional_edges(
    "retrieve_articles_text",
    articles_text_decision,
    {
        "generate_newsapi_params": "generate_newsapi_params",
        "select_top_urls": "select_top_urls",
        "END": END
    }
    )
workflow.add_edge("select_top_urls", "summarize_articles_parallel")
workflow.add_conditional_edges(
    "summarize_articles_parallel",
    lambda state: "format_results" if len(state["tldr_articles"]) > 0 else "END",
    {
        "format_results": "format_results",
        "END": END
    }
    )
workflow.add_edge("format_results", END)

app = workflow.compile()


"""
## Display Graph Structure
"""

display(
    IPImage(
        app.get_graph().draw_mermaid_png(
            draw_method=MermaidDrawMethod.API,
        )
    )
)
# Output:
#   <IPython.core.display.Image object>

"""
## Run Workflow Function

Define a function to run the workflow and display results.
"""

async def run_workflow(query: str, num_searches_remaining: int = 10, num_articles_tldr: int = 3):
    """Run the LangGraph workflow and display results."""
    initial_state = {
        "news_query": query,
        "num_searches_remaining": num_searches_remaining,
        "newsapi_params": {},
        "past_searches": [],
        "articles_metadata": [],
        "scraped_urls": [],
        "num_articles_tldr": num_articles_tldr,
        "potential_articles": [],
        "tldr_articles": [],
        "formatted_results": "No articles with text found."
    }
    try:
        result = await app.ainvoke(initial_state)
        
        return result["formatted_results"]
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
    

"""
## Execute Workflow

Run the workflow with a sample query.
"""

query = "what are the top genai news of today?"
print(await run_workflow(query, num_articles_tldr=3))
# Output:
#   Here are the top 2 articles based on search terms:

#   genai news

#   

#   NIQ Releases 2025 CMO Outlook Report  

#   https://financialpost.com/pmn/business-wire-news-releases-pmn/niq-releases-2025-cmo-outlook-report  

#   * NIQ's annual CMO Outlook report highlights evolving priorities for senior marketing leaders.  

#   * The report emphasizes the role of AI, marketing measurement tools, and collaboration in driving growth for 2025.  

#   * Economic challenges, such as rising costs and potential downturns, are affecting consumer spending patterns.  

#   * Despite economic headwinds, 78% of marketers remain optimistic about their future position.  

#   * Over half (56%) of marketers still view marketing as key for immediate sales, shifting focus towards long-term brand building.  

#   * AI is increasingly being integrated into marketing strategies, with 72% utilizing it for content generation.  

#   * Data-driven insights are crucial, with 81% of marketers relying on them for performance monitoring.  

#   * The CMO Outlook Index shows slight improvement in marketing health, particularly in Europe.  

#   * Marketers plan to enhance collaboration across departments to maximize AI potential.  

#   * The report is based on a survey of nearly 600 senior marketing leaders from 18 countries.

#   

#   FPT Leverages AI to Optimize Legacy Systems for Enterprises  

#   https://financialpost.com/pmn/business-wire-news-releases-pmn/fpt-leverages-ai-to-optimize-legacy-systems-for-enterprises  

#   * FPT Corporation emphasizes the need for legacy system modernization at the FPT Techday 2024 event.  

#   * Many of FPT's over 1,000 global clients still rely on outdated legacy systems that require significant maintenance.  

#   * These legacy systems are costly, prone to errors, and hinder business agility in a rapidly changing tech landscape.  

#   * FPT offers end-to-end services for legacy system management, including maintenance and cloud services.  

#   * AI is central to FPT's strategy for modernizing legacy systems, enhancing efficiency and accuracy.  

#   * The company utilizes tools like EMT, xMainframe, and CodeVista to facilitate modernization and onboarding.  

#   * xMainframe reduces project onboarding time by 30% while maintaining 90% accuracy.  

#   * CodeVista has generated 1.5 million lines of code, saving approximately 6,000 man-months in development time.  

#   * FPT aims to help businesses navigate legacy system challenges and align with market demands for future success.  

#   * FPT Corporation is a leading technology provider based in Vietnam, with a focus on sustainable growth and innovative solutions.  



