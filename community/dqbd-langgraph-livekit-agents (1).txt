Directory structure:
└── example/
    ├── agent.py
    └── pipeline.py

================================================
FILE: python/example/agent.py
================================================
import logging
from typing import Literal
from typing_extensions import Annotated, TypedDict, Optional

from langgraph.constants import TAG_NOSTREAM
from langgraph.graph import StateGraph, add_messages
from langgraph.types import Command
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.types import interrupt
from langgraph.types import StreamWriter
from langgraph_livekit_agents.types import TypedLivekit

logger = logging.getLogger(__name__)


class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
    title: Optional[str]
    content: Optional[str]


async def human(state: AgentState, writer: StreamWriter) -> AgentState:
    livekit = TypedLivekit(writer)
    livekit.say("This is a human node")

    title, title_msgs = interrupt("What is the title of the article?")
    content, content_msgs = interrupt("What is the content of the article?")

    logger.info(f"human: {title} {content}")
    return {"title": title, "content": content, "messages": title_msgs + content_msgs}


async def weather(state: AgentState) -> AgentState:
    response = await ChatOpenAI(model="gpt-4o-mini").ainvoke(
        [HumanMessage(content="Tell me a random weather fact")]
    )

    logger.info(f"weather: {response}")
    return {"messages": response}


async def other(state: AgentState) -> AgentState:
    response = await ChatOpenAI(model="gpt-4o-mini").ainvoke(
        [HumanMessage(content=state["messages"][-1].content)]
    )

    logger.info(f"other: {response}")
    return {"messages": response}


async def supervisor(
    state: AgentState, writer: StreamWriter
) -> Command[Literal["weather", "other"]]:
    livekit = TypedLivekit(writer)

    class RouterOutput(TypedDict):
        next_step: Annotated[
            Literal["weather", "other"], ..., "Classify the user request"
        ]

    response = await (
        ChatOpenAI(model="gpt-4o-mini")
        .with_structured_output(RouterOutput)
        .with_config(tags=[TAG_NOSTREAM])
    ).ainvoke([HumanMessage(content=state["messages"][-1].content)])

    # Send a flush event to send directly to TTS
    livekit.flush()
    logger.info(f"supervisor: {response}")

    if response["next_step"] == "weather":
        return Command(goto="weather")
    else:
        return Command(goto="other")


builder = StateGraph(AgentState)
builder.add_node(human)
builder.add_node(supervisor)
builder.add_node(weather)
builder.add_node(other)
builder.set_entry_point("human")
builder.add_edge("human", "supervisor")

graph = builder.compile()



================================================
FILE: python/example/pipeline.py
================================================
import logging
from uuid import uuid4, uuid5, UUID
from dotenv import load_dotenv
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    WorkerOptions,
    cli,
    pipeline,
)
from livekit.plugins import openai, deepgram, silero
from langgraph_livekit_agents import LangGraphAdapter
from langgraph.pregel.remote import RemoteGraph

load_dotenv(dotenv_path=".env")
logger = logging.getLogger("voice-agent")


def get_thread_id(sid: str | None) -> str:
    NAMESPACE = UUID("41010b5d-5447-4df5-baf2-97d69f2e9d06")
    if sid is not None:
        return str(uuid5(NAMESPACE, sid))
    return str(uuid4())


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(ctx: JobContext):
    logger.info(f"connecting to room {ctx.room.name}")
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    participant = await ctx.wait_for_participant()
    thread_id = get_thread_id(participant.sid)

    logger.info(
        f"starting voice assistant for participant {participant.identity} (thread ID: {thread_id})"
    )

    graph = RemoteGraph("agent", url="http://localhost:2024")
    agent = pipeline.VoicePipelineAgent(
        vad=ctx.proc.userdata["vad"],
        stt=deepgram.STT(),
        llm=LangGraphAdapter(graph, config={"configurable": {"thread_id": thread_id}}),
        tts=openai.TTS(),
    )

    agent.start(ctx.room, participant)


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
        ),
    )


