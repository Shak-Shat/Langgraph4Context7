Directory structure:
└── botextractai-ai-langgraph-multi-agent/
    ├── readme.md
    ├── main.py
    ├── requirements.txt
    └── .github/
        ├── timestamp
        └── workflows/
            └── main.yml

================================================
FILE: readme.md
================================================
# Multi AI agent system for report writing with LangGraph

This multi-agent example uses OpenAI's ChatGPT 4 model and the LangChain Tavily search tool to write a report about the latest inflation figures in the European Union.

Unlike web search tools, the LangChain Tavily search tool delivers actual search results, rather than just links or raw web (HTML) pages.

This example uses a total of 5 LangGraph agents (nodes). LangGraph is a library for building stateful, multi-agent applications with Large Language Models (LLMs), built on top of (and intended to be used with) LangChain. LangGraph describes and orchestrates agent control flows.

- LangGraph is an extension of LangChain that supports graphs
- Single and multi-agent flows are described and represented as graphs
- LangGraph allows for extremely controlled "flows"
- Built-in persistence allows for "human in the loop" workflows

In this example, each of the 5 agents plays a different role:

1. "plan"
2. "research_plan"
3. "generate"
4. "reflect"
5. "research_critique"

The workflow of this example starts with the "plan" agent (1). This agent defines the task along with any relevant notes or instructions.

The "reseach_plan" agent (2) is then charged with providing information that can be used to write the report. This agent generates a list of search queries that gather relevant information using the Taviliy search tool. It does generate a maximum of 3 queries.

The "generate" agent (3) then writes the actual report. If this agent is called for the first time, then it writes the initial draft to be passed on to the "reflect" agent (4). If this agent is called in later iterations ("revisions"), it also incorporates the feedback provided by the following two agents (4 and 5). The number of revisions can be set in the `main.py` script with the `max_revisions` setting. The default value is 2, which means that the workflow just does 1 full revision loop (including agents 4 and 5).

After the "generate" agent (3), the workflow checks, if the maximum number of revisions (`max_revisions`), has been reached. If yes, then the answer from the "generate" agent (3) is considered final and the workflow gets terminated. If not, then the answer from the "generate" agent (3) is passed on to the "reflect" agent (4) for a loop that includes agents 4 and 5.

The "reflect" agent (4) then makes suggestions on how that the current report can be improved.

The "research_critique" agent (5) then provides additional search queries for the Tavily search tool to collect additional relevant information. It does generate a maximum of 3 queries.

After this, the "generate" agent (3) gets called again to write an improved version of the report that incorporates the additional suggestions and information.

You need an OpenAI API key for this example. [Get your OpenAI API key here](https://platform.openai.com/login). You can insert your OpenAI API key in the `main.py` script, or you can supply your OpenAI API key either via the `.env` file, or through an environment variable called `OPENAI_API_KEY`. If you don't want to use an OpenAI model, then you can also use other models, including local models.

You also need a free Tavily API key for this example. [Get your free Tavily API key here](https://app.tavily.com/sign-in). You can insert your Tavily API key in the `main.py` script, or you can supply your Tavily API key either via the `.env` file, or through an environment variable called `TAVILY_API_KEY`.

| >>>>> The final answer will look similar to this example: <<<<< |
| --------------------------------------------------------------- |

## Report on the Latest Inflation Figures in the European Union

### 1. Introduction

Inflation represents the rate at which the general level of prices for goods and services rises, eroding purchasing power. In the context of the European Union (EU), monitoring inflation is crucial due to its direct impact on citizens' cost of living, monetary policy, and overall economic stability.

### 2. Methodology

This report utilizes data primarily sourced from Eurostat and the European Central Bank (ECB). Inflation rates are calculated using the Harmonised Index of Consumer Prices (HICP), which provides a consistent method for comparing inflation rates across EU member states.

### 3. Current Inflation Trends in the European Union

As of November 2023, the inflation rate in the EU stood at 3.6%. This represents a slight decrease from the peak rates observed in late 2022 but remains above the ECB's target of 2%. Graphical trends indicate a gradual stabilization of prices, although certain categories remain volatile.

### 4. Analysis by Country

- **Germany**: Reported a moderate inflation rate of 3.2%, reflecting robust monetary policies.
- **France**: Inflation here is slightly higher at 3.8%, influenced by rising food prices.
- **Italy and Spain**: These countries show varied rates of 4.1% and 3.9% respectively, driven by differences in energy costs and domestic demand.
- **Hungary**: Stands out with a significantly higher rate of 9.6%, attributed to external economic pressures and fiscal policies.

### 5. Sectoral Analysis

- **Energy**: Despite global reductions in energy prices, this sector shows elevated inflation levels due to previous supply constraints.
- **Food**: High inflation persists in this sector, impacted by adverse weather conditions and global supply chain disruptions.
- **Services**: Inflation in services has been relatively stable, reflecting slower recovery in consumer demand post-pandemic.

### 6. Impact of Inflation

The sustained higher inflation rates have eroded consumer purchasing power, particularly affecting lower-income households. In response, the ECB has adjusted interest rates to temper inflation, influencing borrowing costs and economic growth.

### 7. Comparative Analysis

When compared to major economies like the US (inflation at 4.1%) and the UK (inflation at 4.5%), the EU's inflation rate is relatively lower, reflecting tighter monetary controls and differing impacts from global economic conditions.

### 8. Future Outlook

Economic forecasts suggest a gradual decrease in inflation rates within the EU, approaching the target of 2% by late 2025. This outlook assumes continued economic recovery and stability in energy prices. However, geopolitical tensions and supply chain issues pose potential risks.

### 9. Conclusion

The EU faces a complex inflation landscape, influenced by both internal policies and external factors. Policymakers must continue to balance growth with inflation control, adapting strategies as global economic conditions evolve.

### 10. Appendices and References

- **Appendix A**: Inflation Rate Charts by Country
- **Appendix B**: Sectoral Inflation Analysis
- **References**: A comprehensive list of data sources from Eurostat, ECB, and other economic research bodies.

This structured analysis provides a detailed overview of the current inflation dynamics within the EU, offering insights that can guide economic decision-making and policy formulation.



================================================
FILE: main.py
================================================
import os
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, END
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.pydantic_v1 import BaseModel
from langchain_openai import ChatOpenAI
from tavily import TavilyClient
from typing import TypedDict, List

os.environ['OPENAI_API_KEY'] = "REPLACE_THIS_WITH_YOUR_OPENAI_API_KEY"
os.environ['TAVILY_API_KEY'] = "REPLACE_THIS_WITH_YOUR_TAVILY_API_KEY"

# Set the conversation memory type
memory = SqliteSaver.from_conn_string(":memory:")

# Data structure of the agent state information
class AgentState(TypedDict):
    task: str
    plan: str
    draft: str
    critique: str
    content: List[str]
    revision_number: int
    max_revisions: int

model = ChatOpenAI(model="gpt-4-turbo", temperature=0.3)

PLAN_PROMPT = """You are an expert writer tasked with writing a report. \
Write a report for the user provided topic. Give an outline of the report along with any relevant notes \
or instructions for the sections."""

WRITER_PROMPT = """You are a report assistant tasked with writing excellent reports.\
Generate the best report possible for the user's request and the initial outline. \
If the user provides critique, respond with a revised version of your previous attempts. \
Use all the information below as needed: 
------ 
{content}"""

REFLECTION_PROMPT = """You are a critic reviewing a report. \
Generate critique and recommendations for the user's submission. \
Provide detailed recommendations, including requests for length, depth, style, etc."""

RESEARCH_PLAN_PROMPT = """You are a researcher charged with providing information that can \
be used when writing the following report. Generate a list of search queries that will gather \
any relevant information. Only generate 3 queries max."""

RESEARCH_CRITIQUE_PROMPT = """You are a researcher charged with providing information that can \
be used when making any requested revisions (as outlined below). \
Generate a list of search queries that will gather any relevant information. Only generate 3 queries max."""

# Queries list for the Tavily search tool
class Queries(BaseModel):
    queries: List[str]

tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def plan_node(state: AgentState):
    messages = [
        SystemMessage(content=PLAN_PROMPT),
        HumanMessage(content=state['task'])
    ]
    response = model.invoke(messages)
    return {"plan": response.content}

def research_plan_node(state: AgentState):
    queries = model.with_structured_output(Queries).invoke([
        SystemMessage(content=RESEARCH_PLAN_PROMPT),
        HumanMessage(content=state['task'])
    ])
    content = state['content'] or []
    for q in queries.queries:
        response = tavily.search(query=q, max_results=2)
        for r in response['results']:
            content.append(r['content'])
    return {"content": content}

def generation_node(state: AgentState):
    content = "\n\n".join(state['content'] or [])
    user_message = HumanMessage(
        content=f"{state['task']}\n\nHere is my plan:\n\n{state['plan']}")
    messages = [
        SystemMessage(
            content=WRITER_PROMPT.format(content=content)
        ),
        user_message
        ]
    response = model.invoke(messages)
    return {
        "draft": response.content,
        "revision_number": state.get("revision_number", 1) + 1
    }

def reflection_node(state: AgentState):
    messages = [
        SystemMessage(content=REFLECTION_PROMPT),
        HumanMessage(content=state['draft'])
    ]
    response = model.invoke(messages)
    return {"critique": response.content}

def research_critique_node(state: AgentState):
    queries = model.with_structured_output(Queries).invoke([
        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),
        HumanMessage(content=state['critique'])
    ])
    content = state['content'] or []
    for q in queries.queries:
        response = tavily.search(query=q, max_results=2)
        for r in response['results']:
            content.append(r['content'])
    return {"content": content}

def should_continue(state):
    if state["revision_number"] > state["max_revisions"]:
        return END
    return "reflect"

# Initialise the graph with the agent state
builder = StateGraph(AgentState)

# Add all the nodes (agents)
builder.add_node("planner", plan_node)
builder.add_node("generate", generation_node)
builder.add_node("reflect", reflection_node)
builder.add_node("research_plan", research_plan_node)
builder.add_node("research_critique", research_critique_node)

# Set the starting agent
builder.set_entry_point("planner")

# Set the conditional edge
# This decides, whether to do another refinement loop, or to end
builder.add_conditional_edges(
    "generate", 
    should_continue, 
    {END: END, "reflect": "reflect"}
)

# Agent workflow ("generate" is already covered by the conditional edge)
builder.add_edge("planner", "research_plan")
builder.add_edge("research_plan", "generate")

builder.add_edge("reflect", "research_critique")
builder.add_edge("research_critique", "generate")

# Compile with the chosen memory type
graph = builder.compile(checkpointer=memory)

# Run it!
thread = {"configurable": {"thread_id": "1"}}
for s in graph.stream({
    'task': "Write a report about the latest inflation figures in the European Union.",
    "max_revisions": 2,
    "revision_number": 1
}, thread):
    print(s)



================================================
FILE: requirements.txt
================================================
[Non-text file]


================================================
FILE: .github/timestamp
================================================
2025-05-10T13:50:42Z



================================================
FILE: .github/workflows/main.yml
================================================
name: Timestamp
on:
  push:
    branches:
      - master
  schedule:
    - cron: "45 13 * * *"
jobs:
  auto_commit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
      - name: Modify timestamp file
        run: |
          d=`date '+%Y-%m-%dT%H:%M:%SZ'`
          echo $d > .github/timestamp
      - name: Commit changes
        run: |
          git config --local user.email "${{ secrets.USEREMAIL }}"
          git config --local user.name "${{ secrets.USERNAME }}"
          git commit -a -m "Timestamp"
      - name: Push Back
        uses: ad-m/github-push-action@master
        with:
          force: true
          directory: "."
          github_token: ${{ secrets.GITHUB_TOKEN }}


