Directory structure:
â””â”€â”€ langchain-ai-social-media-agent/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ FEATURES.md
    â”œâ”€â”€ jest.config.js
    â”œâ”€â”€ jest.setup.cjs
    â”œâ”€â”€ langgraph.json
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ package.json
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ tsconfig.json
    â”œâ”€â”€ uv.lock
    â”œâ”€â”€ .codespellignore
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .editorconfig
    â”œâ”€â”€ .env.full.example
    â”œâ”€â”€ .env.quickstart.example
    â”œâ”€â”€ .eslintrc.cjs
    â”œâ”€â”€ memory-v2/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ langgraph.json
    â”‚   â”œâ”€â”€ Makefile
    â”‚   â”œâ”€â”€ poetry.lock
    â”‚   â”œâ”€â”€ pyproject.toml
    â”‚   â””â”€â”€ memory_v2/
    â”‚       â”œâ”€â”€ graph.py
    â”‚       â””â”€â”€ state.py
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ backfill.ts
    â”‚   â”œâ”€â”€ checkLanggraphPaths.js
    â”‚   â”œâ”€â”€ delete-run-thread.ts
    â”‚   â”œâ”€â”€ generate-post.ts
    â”‚   â”œâ”€â”€ get-all-used-links.ts
    â”‚   â”œâ”€â”€ get-scheduled-runs.ts
    â”‚   â”œâ”€â”€ reinterrupt.ts
    â”‚   â”œâ”€â”€ crons/
    â”‚   â”‚   â”œâ”€â”€ create-cron.ts
    â”‚   â”‚   â”œâ”€â”€ delete-cron.ts
    â”‚   â”‚   â””â”€â”€ list-crons.ts
    â”‚   â””â”€â”€ repurposer/
    â”‚       â”œâ”€â”€ create-cron.ts
    â”‚       â””â”€â”€ ingest.ts
    â”œâ”€â”€ slack-messaging/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ .gitignore
    â”‚   â””â”€â”€ src/
    â”‚       â””â”€â”€ langgraph_slack/
    â”‚           â”œâ”€â”€ __init__.py
    â”‚           â”œâ”€â”€ __main__.py
    â”‚           â”œâ”€â”€ auth.py
    â”‚           â”œâ”€â”€ config.py
    â”‚           â””â”€â”€ server.py
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â”œâ”€â”€ should-exclude.ts
    â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”œâ”€â”€ utils.ts
    â”‚   â”‚   â”œâ”€â”€ curate-data/
    â”‚   â”‚   â”‚   â”œâ”€â”€ constants.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ loaders/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ai-news-blog.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ latent-space.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ reddit.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ twitter.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ github/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ langchain.ts
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ trending.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ai-news-blog.int.test.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ github.int.test.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ latent-space.int.test.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ reddit.int.test.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ twitter.int.test.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ extract-ai-newsletter-content.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ format-data.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-posts-subgraph.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingest-data.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate-bulk-tweets.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-github-wrapper.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-reddit-wrapper.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ tweets/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ group-tweets-by-content.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ prompts.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ re-group-tweets.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ reflect-tweet-groups.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ tests/
    â”‚   â”‚   â”‚   â”‚           â”œâ”€â”€ group-by-content.int.test.ts
    â”‚   â”‚   â”‚   â”‚           â”œâ”€â”€ re-group-reflect.int.test.ts
    â”‚   â”‚   â”‚   â”‚           â””â”€â”€ validate-bulk-tweets.int.test.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ tests/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ e2e.int.test.ts
    â”‚   â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚   â”‚       â”œâ”€â”€ created-at-after.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ get-unique-array.ts
    â”‚   â”‚   â”‚       â””â”€â”€ stores/
    â”‚   â”‚   â”‚           â”œâ”€â”€ github-repos.ts
    â”‚   â”‚   â”‚           â”œâ”€â”€ latent-space-links.ts
    â”‚   â”‚   â”‚           â”œâ”€â”€ reddit-post-ids.ts
    â”‚   â”‚   â”‚           â””â”€â”€ twitter.ts
    â”‚   â”‚   â”œâ”€â”€ curated-post-interrupt/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â””â”€â”€ types.ts
    â”‚   â”‚   â”œâ”€â”€ find-images/
    â”‚   â”‚   â”‚   â”œâ”€â”€ find-images-graph.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ screenshot.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ find-images.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ re-rank-images.ts
    â”‚   â”‚   â”‚       â””â”€â”€ validate-images.ts
    â”‚   â”‚   â”œâ”€â”€ generate-post/
    â”‚   â”‚   â”‚   â”œâ”€â”€ constants.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ generate-post-graph.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ generate-post-state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth-socials.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ condense-post.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rewrite-with-split-url.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-post/
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.ts
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ utils.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ generate-report/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ prompts.ts
    â”‚   â”‚   â”‚   â””â”€â”€ prompts/
    â”‚   â”‚   â”‚       â”œâ”€â”€ examples.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚       â””â”€â”€ prompts.langchain.ts
    â”‚   â”‚   â”œâ”€â”€ generate-report/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ utils.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ extract-key-details.ts
    â”‚   â”‚   â”‚       â””â”€â”€ generate-report.ts
    â”‚   â”‚   â”œâ”€â”€ generate-thread/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ utils.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ generate-thread-plan.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ generate-thread-posts.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ rewrite-thread.ts
    â”‚   â”‚   â”‚       â”œâ”€â”€ schedule-thread.ts
    â”‚   â”‚   â”‚       â””â”€â”€ human-node/
    â”‚   â”‚   â”‚           â””â”€â”€ index.ts
    â”‚   â”‚   â”œâ”€â”€ ingest-data/
    â”‚   â”‚   â”‚   â”œâ”€â”€ ingest-data-graph.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ ingest-data-state.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ ingest-slack.ts
    â”‚   â”‚   â”‚       â””â”€â”€ ingest-twitter.ts
    â”‚   â”‚   â”œâ”€â”€ ingest-repurposed-data/
    â”‚   â”‚   â”‚   â”œâ”€â”€ constants.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ extract.ts
    â”‚   â”‚   â”‚       â””â”€â”€ ingest-slack.ts
    â”‚   â”‚   â”œâ”€â”€ reflection/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â””â”€â”€ prompts.ts
    â”‚   â”‚   â”œâ”€â”€ repurposer/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ utils.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-campaign-plan.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-posts.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ start-interrupt-graph.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ validate-images.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ extract-content/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ get-url-contents.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ index.ts
    â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚       â”œâ”€â”€ graph.int.test.ts
    â”‚   â”‚   â”‚       â””â”€â”€ images.test.ts
    â”‚   â”‚   â”œâ”€â”€ repurposer-post-interrupt/
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â””â”€â”€ nodes/
    â”‚   â”‚   â”‚       â”œâ”€â”€ rewrite-posts.ts
    â”‚   â”‚   â”‚       â””â”€â”€ human-node/
    â”‚   â”‚   â”‚           â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚           â”œâ”€â”€ router.ts
    â”‚   â”‚   â”‚           â””â”€â”€ utils.ts
    â”‚   â”‚   â”œâ”€â”€ shared/
    â”‚   â”‚   â”‚   â”œâ”€â”€ shared-state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ auth/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ linkedin.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ twitter.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ route-response.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ update-scheduled-date.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-content.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-general.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-github.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-luma.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ verify-youtube.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ youtube.utils.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ generate-post/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ human-node.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ rewrite-post.ts
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ schedule-post.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ stores/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ post-subject-urls.ts
    â”‚   â”‚   â”‚   â””â”€â”€ youtube/
    â”‚   â”‚   â”‚       â””â”€â”€ video-summary.ts
    â”‚   â”‚   â”œâ”€â”€ supervisor/
    â”‚   â”‚   â”‚   â”œâ”€â”€ supervisor-graph.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ supervisor-state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ determine-post-type.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generate-posts.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ group-reports.ts
    â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚       â””â”€â”€ e2e.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ upload-post/
    â”‚   â”‚   â”‚   â””â”€â”€ index.ts
    â”‚   â”‚   â”œâ”€â”€ verify-links/
    â”‚   â”‚   â”‚   â”œâ”€â”€ verify-links-graph.ts
    â”‚   â”‚   â”‚   â””â”€â”€ verify-links-state.ts
    â”‚   â”‚   â”œâ”€â”€ verify-reddit-post/
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ utils.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ verify-reddit-post-graph.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ verify-reddit-post-state.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodes/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ get-external-urls.ts
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ get-post.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ validate-reddit-post.ts
    â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚       â”œâ”€â”€ e2e.int.test.ts
    â”‚   â”‚   â”‚       â””â”€â”€ data/
    â”‚   â”‚   â”‚           â””â”€â”€ inputs-outputs.ts
    â”‚   â”‚   â””â”€â”€ verify-tweet/
    â”‚   â”‚       â”œâ”€â”€ verify-tweet-graph.ts
    â”‚   â”‚       â”œâ”€â”€ verify-tweet-state.ts
    â”‚   â”‚       â””â”€â”€ nodes/
    â”‚   â”‚           â”œâ”€â”€ get-tweet-content.ts
    â”‚   â”‚           â”œâ”€â”€ validate-tweet.ts
    â”‚   â”‚           â””â”€â”€ tests/
    â”‚   â”‚               â””â”€â”€ get-tweet-content.int.test.ts
    â”‚   â”œâ”€â”€ clients/
    â”‚   â”‚   â”œâ”€â”€ auth-server.ts
    â”‚   â”‚   â”œâ”€â”€ linkedin.ts
    â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”œâ”€â”€ reddit/
    â”‚   â”‚   â”‚   â”œâ”€â”€ client.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ get-user-less-token.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ snoowrap.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â””â”€â”€ tests/
    â”‚   â”‚   â”‚       â””â”€â”€ reddit.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ slack/
    â”‚   â”‚   â”‚   â”œâ”€â”€ client.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts
    â”‚   â”‚   â”‚   â””â”€â”€ utils.ts
    â”‚   â”‚   â””â”€â”€ twitter/
    â”‚   â”‚       â”œâ”€â”€ client.ts
    â”‚   â”‚       â”œâ”€â”€ SETUP.md
    â”‚   â”‚       â”œâ”€â”€ types.ts
    â”‚   â”‚       â”œâ”€â”€ utils.ts
    â”‚   â”‚       â””â”€â”€ tests/
    â”‚   â”‚           â”œâ”€â”€ arcade.int.test.ts
    â”‚   â”‚           â””â”€â”€ twitter.int.test.ts
    â”‚   â”œâ”€â”€ evals/
    â”‚   â”‚   â”œâ”€â”€ e2e/
    â”‚   â”‚   â”‚   â”œâ”€â”€ e2e.int.test.ts
    â”‚   â”‚   â”‚   â””â”€â”€ inputs.ts
    â”‚   â”‚   â”œâ”€â”€ general/
    â”‚   â”‚   â”‚   â””â”€â”€ index.ts
    â”‚   â”‚   â”œâ”€â”€ github/
    â”‚   â”‚   â”‚   â””â”€â”€ index.ts
    â”‚   â”‚   â”œâ”€â”€ twitter/
    â”‚   â”‚   â”‚   â””â”€â”€ index.ts
    â”‚   â”‚   â”œâ”€â”€ validate-images/
    â”‚   â”‚   â”‚   â”œâ”€â”€ inputs.ts
    â”‚   â”‚   â”‚   â””â”€â”€ validate-images.int.test.ts
    â”‚   â”‚   â””â”€â”€ youtube/
    â”‚   â”‚       â””â”€â”€ index.ts
    â”‚   â”œâ”€â”€ tests/
    â”‚   â”‚   â”œâ”€â”€ agent.test.ts
    â”‚   â”‚   â”œâ”€â”€ expected.ts
    â”‚   â”‚   â”œâ”€â”€ github.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ graph.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ linkedin.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ scrape-general-content.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ slack.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ states.ts
    â”‚   â”‚   â”œâ”€â”€ utils.int.test.ts
    â”‚   â”‚   â”œâ”€â”€ youtube.int.test.ts
    â”‚   â”‚   â””â”€â”€ data/
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ create-dir.ts
    â”‚       â”œâ”€â”€ date.ts
    â”‚       â”œâ”€â”€ delay-run.ts
    â”‚       â”œâ”€â”€ firecrawl.ts
    â”‚       â”œâ”€â”€ github-repo-contents.ts
    â”‚       â”œâ”€â”€ image-message.ts
    â”‚       â”œâ”€â”€ reflections.ts
    â”‚       â”œâ”€â”€ screenshot.ts
    â”‚       â”œâ”€â”€ supabase.ts
    â”‚       â””â”€â”€ schedule-date/
    â”‚           â”œâ”€â”€ constants.ts
    â”‚           â”œâ”€â”€ helpers.ts
    â”‚           â”œâ”€â”€ index.ts
    â”‚           â”œâ”€â”€ types.ts
    â”‚           â””â”€â”€ tests/
    â”‚               â””â”€â”€ schedule-date.test.ts
    â”œâ”€â”€ static/
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â”œâ”€â”€ ci.yml
            â”œâ”€â”€ integration-tests.yml
            â””â”€â”€ unit-tests.yml

================================================
FILE: README.md
================================================
# Social Media Agent

This repository contains an 'agent' which can take in a URL, and generate a Twitter & LinkedIn post based on the content of the URL. It uses a human-in-the-loop (HITL) flow to handle authentication with different social media platforms, and to allow the user to make changes, or accept/reject the generated post.

![Screenshot of the social media agent flow](./static/agent_flow.png)

## Table of contents

- [Quickstart](#quickstart)
  - [Environment variables](#set-environment-variables)
  - [LangGraph Server](#start-the-langgraph-server)
- [Full setup](#advanced-setup)
  - [Environment variables](#set-environment-variables-1)
  - [Authentication](#setup-authentication)
  - [Supabase](#setup-supabase)
  - [Slack](#setup-slack)
  - [GitHub](#setup-github)
- [Usage](#usage)
  - [Generate Post](#generate-post)
  - [Setup Crons](#setup-crons)
  - [Prebuilt Scripts](#prebuilt-scripts)
- [Setup Agent Inbox](#setup-agent-inbox)
  - [Using the deployed inbox](#using-the-deployed-inbox)
  - [Using the local inbox](#using-the-local-inbox)
- [Customization](#customization)
  - [Prompts](#prompts)
  - [Post Style](#post-style)

# Quickstart

> [!TIP]
> ðŸŽ¥ For a visual guide, check out our [step-by-step video tutorial](https://youtu.be/TmTl5FMgkCQ) that walks you through the account setup process and project configuration.

This quickstart covers how to setup the Social Media Agent in a basic setup mode. This is the quickest way to get up and running, however it will lack some of the features of the full setup mode. See [here](#advanced-setup) for the full setup guide.

<details>
<summary>Running in basic setup mode will lack the following features:</summary>

- Parsing content from GitHub, Twitter or YouTube URLs
- Ingesting data from Slack, or sending updates to Slack
- Image selection & uploads

</details>

To get started, you'll need the following API keys/software:

- [Anthropic API](https://console.anthropic.com/) - General LLM
- [LangSmith](https://smith.langchain.com/) - LangSmith API key required to run the LangGraph server locally (free)
- [FireCrawl API](https://www.firecrawl.dev/) - Web scraping. New users get 500 credits for free
- [Arcade](https://www.arcade.dev/) - Easy authentication for reading & writing to social media platforms

## Setup Instructions

### Clone the repository:

```bash
git clone https://github.com/langchain-ai/social-media-agent.git
```

```bash
cd social-media-agent
```

### Install dependencies:

```bash
yarn install
```

### Set environment variables.

Copy the values of the quickstart `.env.quickstart.example` to `.env`, then add the values:

```bash
cp .env.quickstart.example .env
```

Once done, ensure you have the following environment variables set:

```bash
# For LangSmith tracing (optional)
LANGSMITH_API_KEY=
LANGSMITH_TRACING_V2=true

# For LLM generations
ANTHROPIC_API_KEY=

# For web scraping
FIRECRAWL_API_KEY=

# Arcade API key - used for fetching Tweets, and scheduling LinkedIn/Twitter posts
ARCADE_API_KEY=
```

If you plan to post to LinkedIn as an organization (rather than as yourself), you'll also need to set:

```bash
# Get the organization ID from the URL of the company page when you're logged in as an admin.
# For example, if the URL is `https://www.linkedin.com/company/12345678/admin/dashboard/`, the organization ID would be `12345678`.
POST_TO_LINKEDIN_ORGANIZATION=true
LINKEDIN_ORGANIZATION_ID=
```

### Install LangGraph CLI

```bash
pip install langgraph-cli
```

Then run the following command to check the CLI is installed:

```bash
langgraph --version
```

Click [here](https://langchain-ai.github.io/langgraph/cloud/reference/cli/) to read the full download instructions for the LangGraph CLI.

### Start the LangGraph server:

To start the LangGraph server, run this script:

```bash
yarn langgraph:in_mem:up
```

Under the hood, this will execute the following command:

```bash
npx @langchain/langgraph-cli dev --port 54367
```

> [!NOTE]
> The first time running this command (or if a new version of `@langchain/langgraph-cli` has been released), it will ask you to accept an install for the CLI. Enter `y` to accept.

Once the server is ready, you can execute the following command to generate a post:

```bash
yarn generate_post
```

You may also modify this script to pass different URLs to generate posts for other content.

This will kick off a new run to generate a post on a [LangChain blog post](https://blog.langchain.dev/customers-appfolio/).

To view the output, either inspect it in LangSmith, or use Agent Inbox.

> [!TIP]
> Follow these steps to setup & configure the Agent Inbox: [Setup Agent Inbox Guide](#setup-agent-inbox)

# Advanced Setup

![Screenshot of the social media agent graph](./static/graph_screenshot.png)

To use all of the features of the Social Media Agent, you'll need the following:

- [Anthropic API](https://console.anthropic.com/) - General LLM
- [Google Vertex AI](https://cloud.google.com/vertex-ai) - For dealing with YouTube video content
- [LangSmith](https://smith.langchain.com/) - LangSmith API key required to run the LangGraph server locally (free)
- [FireCrawl API](https://www.firecrawl.dev/) - Web scraping
- [Arcade](https://www.arcade.dev) - Social media authentication and scheduling
- [Twitter Developer Account](https://developer.twitter.com/en/portal/dashboard) - For uploading media to Twitter
- [LinkedIn Developer Account](https://developer.linkedin.com/) - Posting to LinkedIn
- [GitHub API](https://github.com/settings/personal-access-tokens) - Reading GitHub content
- [Supabase](https://supabase.com/) - Storing images
- [Slack Developer Account](https://api.slack.com/apps) (optional) - ingesting data from a Slack channel

## Setup Instructions

### Clone the repository:

```bash
git clone https://github.com/langchain-ai/social-media-agent.git
```

```bash
cd social-media-agent
```

### Install dependencies:

```bash
yarn install
```

### Set environment variables.

Copy the values of the full env example file `.env.full.example` to `.env`, then update the values as needed.

```bash
cp .env.full.example .env
```

### Setup authentication

The agent needs your authorization to read and write to social media platforms. There are two ways to authorize the agent:

1. Use Arcade (quickest to set up)
2. Use your own Twitter and LinkedIn developer accounts

You can use either method, but not both.

#### Arcade setup

Create an Arcade account [here](https://www.arcade.dev). After you register, [get an Arcade API key](https://docs.arcade.dev/home/quickstart?lang=typescript). Set this value as `ARCADE_API_KEY` in your `.env` file.

Then, you will need to set these environment variables in your `.env` file:

- `TWITTER_USER_ID` - The ID/email of the Twitter account you want to use to post to Twitter.
- `LINKEDIN_USER_ID` - The ID/email of the LinkedIn account you want to use to post to LinkedIn.

Make sure you have the `USE_ARCADE_AUTH` environment variable set to `true` to have the graph use Arcade authentication.

If you plan to post to LinkedIn as an organization (rather than as yourself), you'll also need to set:

```bash
# Get the organization ID from the URL of the company page when you're logged in as an admin.
# For example, if the URL is `https://www.linkedin.com/company/12345678/admin/dashboard/`, the organization ID would be `12345678`.
LINKEDIN_ORGANIZATION_ID=
POST_TO_LINKEDIN_ORGANIZATION=true
```

> [!NOTE]
> If you want to upload media to Twitter, you will still need to set up your own Twitter developer account (below) in addition to using Arcade.
>
> If you are only planning to read/write text posts on Twitter, you can use Arcade without any additional setup.

#### Twitter app setup

You'll need to follow these instructions if you plan on uploading media to Twitter, and/or you are not using Arcade for authorization.

1. Create a Twitter developer account
2. Create a new app and give it a name.
3. Copy the `API Key`, `API Key Secret` and `Bearer Token` and set them as `TWITTER_API_KEY`, `TWITTER_API_KEY_SECRET`, and `TWITTER_BEARER_TOKEN` in your `.env` file.
4. After saving, visit the App Dashboard. Find the `User authentication settings` section, and click the `Set up` button. This is how you will authorize users to use the Twitter API on their behalf.
5. Set the following fields:

- `App permissions`: `Read and write`
- `Type of App`: `Web App, Automated App or Bot`
- `App info`:
  - `Callback URI/Redirect URL`: `http://localhost:3000/auth/twitter/callback`
  - `Website URL`: Your website URL

6. Save. You'll then be given a `Client ID` and `Client Secret`. Set these as `TWITTER_CLIENT_ID` and `TWITTER_CLIENT_SECRET` in your `.env` file.

Once done, run the `yarn start:auth` command to run the Twitter OAuth server. Open [http://localhost:3000](http://localhost:3000) in your browser, and click `Login with Twitter`.

After authorizing your account with the app, navigate to your terminal where you'll see a JSON object logged. Copy the `token` and `tokenSecret` values and set them as `TWITTER_USER_TOKEN` and `TWITTER_USER_TOKEN_SECRET` in your `.env` file.

#### LinkedIn app setup

You'll need to follow these instructions if you plan on posting to LinkedIn and are not using Arcade for authorization.

1. Create a new LinkedIn developer account, and app [here](https://developer.linkedin.com/)
2. After creating your app, navigate to the `Auth` tab, and add a new authorized redirect URL for OAuth 2.0. Set it to `http://localhost:3000/auth/linkedin/callback`
3. Go to the `Products` tab and enable the `Share on LinkedIn` and `Sign In with LinkedIn using OpenID Connect` products.

<details>
<summary>If you plan on posting from company pages, you'll need to do the following:</summary>

1. If you plan on posting from company pages, you'll also need to enable the `Advertising API` product. Furthermore, ensure your personal account has at least one one of the following roles with the company page:

- `ADMINISTRATOR`
- `DIRECT_SPONSORED_CONTENT_POSTER`
- `RECRUITING_POSTER`

2. Next, ensure your company page has verified the app. You can create a verification link on the `Settings` tab of your app, then click the `Verify` button on the company page card.
3. Once requesting access, you'll need to fill out a form for verification. Once submitted, you should receive an email stating you've been granted developer access which will give you the proper permission to test out the API until it's been approved.
4. Inside the [authorization server file (./src/clients/auth-server.ts)](./src/clients/auth-server.ts), ensure the `w_organization_social` scope is enabled inside the scopes string in the `/auth/linkedin` route. Once done, the scopes string should look like this: `openid profile email w_member_social w_organization_social`
5. Get the organization ID from the URL of the company page when you're logged in as an admin and set it as the `LINKEDIN_ORGANIZATION_ID` environment variable. For example, if the URL is `https://www.linkedin.com/company/12345678/admin/dashboard/`, the organization ID would be `12345678`.

> [!NOTE]
> If you plan on only posting from the company account, you can set the `POST_TO_LINKEDIN_ORGANIZATION` to `"true"` in your `.env` file. If you want to choose dynamically, you can set this to `true`/`false` in the configurable fields (`postToLinkedInOrganization`) when invoking the `generate_post` graph.
>
> This value will take precedence over the `POST_TO_LINKEDIN_ORGANIZATION` environment variable.

</details>

4. Save the following environment variables in your `.env` file:

- `LINKEDIN_CLIENT_ID`
- `LINKEDIN_CLIENT_SECRET`

5. Run the `yarn start:auth` command to run the LinkedIn OAuth server. Open [http://localhost:3000](http://localhost:3000) in your browser, and click `Login with LinkedIn`.
6. After logging in, copy the `access_token` and `sub` values from the objects logged to the terminal. Set these values as `LINKEDIN_ACCESS_TOKEN` (`access_token`) and `LINKEDIN_PERSON_URN` (`sub`) in your `.env` file.

</details>

### Setup Supabase

Supabase is required for storing images found/generated by the agent. This step is not required for running the agent in basic setup mode.

To setup Supabase, create an account and a new project.

Set the `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY` environment variables to the values provided by Supabase.

Create a new storage bucket called `images`. Make sure the bucket is set to public to the image URLs are accessible. Also ensure the max upload size is set to at least 5MB inside the global project settings, and the bucket specific settings.

### Setup Slack

Slack integration is optional, but recommended if you intend on using the `ingest_data` agent, or having updates sent to Slack.

This agent can be used in a cron job to fetch messages from a Slack channel, and call the `generate_post` graph for each message. We use this flow internally at LangChain to enable having a single Slack channel for submitting relevant URLs to the agent, which are then turned into posts once daily.

To configure the Slack integration, create a new Slack app and install it into your desired Slack workspace.

Once installed, ensure it has access to the channel you want to ingest messages from. Additionally, if you want it to send update messages to Slack, it will need write permissions to the workspace.

Finally, make sure the `SLACK_BOT_TOKEN` environment variable is set in your `.env` file. Then, when you create a cron (see the [Setup Crons](#setup-crons) section), you'll only have to pass in the channel ID to ingest messages from.

To enable sending updates to Slack, add a `SLACK_CHANNEL_ID` environment variable to your `.env` file with the channel ID you want to send updates to (at LangChain, we have one channel for sending content links, and a separate one for sending update messages).

### Setup GitHub

The GitHub API token is required to fetch details about GitHub repository URLs submitted to the agent. This is not required if you do not plan on sending GitHub URLs to the agent.

To get a GitHub API token, create a new fine grained token with the `Public Repositories (read-only)` scope at a minimum. If you intend on using this agent for private GitHub repositories, you'll need to give the token access to those repositories as well.

Ensure this is set as `GITHUB_TOKEN` in your `.env` file.

# Usage

## Generate Post

Once all the setup steps have been completed, start your graph server by running:

```bash
yarn langgraph:in_mem:up
```

> [!NOTE]
> The first time running this command (or if a new version of `@langchain/langgraph-cli` has been released), it will ask you to accept an install for the CLI. Enter `y` to accept.

Once the server is ready, you can execute the following command to generate a post:

(before doing this, you should edit the file so that the text only mode is set to false: `[TEXT_ONLY_MODE]: false` if using advanced setup mode)

```bash
yarn generate_post
```

This will kick off a new run to generate a post on a [LangChain blog post](https://blog.langchain.dev/customers-appfolio/).

To view the output, either inspect it in LangSmith, or use [the Agent Inbox](#setup-agent-inbox).

You may also modify this script to pass different URLs to generate posts for other content.

## Setup Crons

The Agent Inbox is most powerful when used with cron jobs. Doing this allows you to send links to generate posts on to a Slack channel, and have the cron job check for these links asynchronously. With this setup, you can send links to Slack, and have the inbox handle the rest while you sleep.

We have a series of scripts to help with this, which you can find in the [`scripts/crons`](./scripts/crons) directory. In this section, we'll explain how to quickly create a cron for ingesting links to your graph from Slack.

### Slack Setup

Before we get started, ensure you have the proper Slack integration setup, as described in the [Setup Slack](#setup-slack) section. Then, edit the [`create-cron.ts`](./scripts/crons/create-cron.ts) file, and set the `slackChannelId` field to the channel ID of the channel you want to ingest links from.

After editing, run the following command to create the cron:

```bash
yarn cron:create
```

This will create a new cron job that will ingest links from Slack into your graph, once daily.

## Prebuilt Scripts

For more information on all of the prebuilt scripts, see the [`scripts/README.md`](./scripts/README.md) file.

# Setup Agent Inbox

The Agent Inbox is the easiest way to view interrupted events, and manage accepting, responding, or other allowed actions. To view your events in the inbox, you can either add your graph to the deployed version of the Agent Inbox, or clone & run the Agent Inbox locally.

## Using the deployed inbox

The Agent Inbox is setup in a way that allows for any graph --local or deployed-- to be added & accessed via the UI.

To add your local graph to the inbox, visit the deployed site here: [dev.agentinbox.ai](https://dev.agentinbox.ai/).

If it's your first time vising the site, you'll immediately be prompted to add a new graph. Fill out the form with the following values:

- Graph ID: `generate_post`
- Graph API URL: `http://localhost:54367`
- Name: (optional) `Generate Post (local)`

After saving, you should be able to view your graph in the inbox. If you do this after invoking your graph (and waiting for the thread to interrupt), it will automatically fetch the interrupted event.

## Using the local inbox

To run the Agent Inbox locally, follow the setup instructions [here](https://github.com/langchain-ai/agent-inbox/blob/main/README.md).

Once the web server is running, open your browser and visit [http://localhost:3000](http://localhost:3000). This will then prompt you to add your graph to the inbox.

Fill out the form with the following values:

- Graph ID: `generate_post`
- Graph API URL: `http://localhost:54367`
- Name: (optional) `Generate Post (local)`

## Using the Agent Inbox with a graph deployed on LangGraph Platform

The Agent Inbox can also be used with graph's deployed in production on LangGraph Platform. To use these graphs, the setup steps are the exact same, with the only difference being the graph API URL should be the URL of the deployed graph, and you're required to set a LangSmith API key. This is required to fetch & invoke the deployed graph. LangSmith API keys are stored in your browser's local storage, and never stored on the server.

# Customization

## Prompts

This agent is setup to generate posts for LangChain, using LangChain products as context. To use the agent for your own use case, you should update the following prompts/prompt sections inside the [`prompts`](./src/agents/generate-post/prompts/index.ts) folder:

- `BUSINESS_CONTEXT` - Context to be used when checking whether or not content is relevant to your business/use case.
- `TWEET_EXAMPLES` ([`prompts/examples.ts`](./src/agents/generate-post/prompts/examples.ts)) - A list of examples of posts you'd like the agent to use as a guide when generating the final post.
- `POST_STRUCTURE_INSTRUCTIONS` - A set of structure instructions for the agent to follow when generating the final post.
- `POST_CONTENT_RULES` - A set of general writing style/content guidelines for the agent to follow when generating a post.

The prompt for the marketing report is located in the [`generate-post/nodes/generate-report/prompts.ts`](./src/agents/generate-post/nodes/generate-report/prompts.ts) file. You likely don't need to update this, as it's already structured to be general.

## Post Style

There are two main prompts to modify to change the style of the posts.

1. Post structure instructions (`POST_STRUCTURE_INSTRUCTIONS`). These are the instructions the LLM will follow for how to structure the post generations. This should _not_ be where you specify tone, or writing style. This prompt is used to set the structure each post should follow. By default, it's prompted to include three parts: `Header`, `Body`, `Call to action`. When experimenting with this prompt this, try removing it completely, instead relying on the few-shot examples (`TWEET_EXAMPLES`) and the content rules (`POST_CONTENT_RULES`).
2. Few-shot examples (`TWEET_EXAMPLES`). These are the examples given to the LLM of which it's prompted to use as examples for style, content, tone and structure. This is arguably one of the most important parts of the prompt. Currently, these are set to a handful of Tweets by popular AI focused Twitter accounts. You should _definitely_ update these if you want to generate non-AI focused Tweets, instead with examples of Tweets/posts on your target content.
3. "Business context" (`BUSINESS_CONTEXT`). This prompt is used widely throughout the agent to provide context into your main goal of the social media agent. For us at LangChain, this prompt is used to describe the different LangChain products and services. The default prompt is focused on AI content, but should be updated/edited to match your use case. This prompt is used in verifying content is relevant for you, generating marketing reports, and generating tweets.



================================================
FILE: FEATURES.md
================================================
# Detailed Feature List

This README contains a list of small features/logic flows which play a small, yet significate role in how the agents in the Social Media Agent work.

Each feature is nested under the graph it belongs to. If it does not belong to a graph, it will be under the [**Shared**](#shared) section.

## Main Key

- [**Generate Post**](#generate-post)
- [**Shared**](#shared)

## Generate Post

### Key

- [Used URLs](#used-urls)

### Used URLs

Once a run reaches the `humanNode`, all of the URLs inside the `relevantLinks` and `links` state fields will be stored in the LangGraph store. This is then referenced after the `verifyLinksSubGraph` executes, to see if any of the URLs (`relevantLinks` and `links`) have already been used in previous posts.

If _any_ of the URLs exist in the store, it will route the graph to the `END` node, and not generate a post.

This is implemented to ensure duplicated content is not generated.

#### Skipping Saved URLs check

If you are okay with duplicated content being generated, you can set the `SKIP_USED_URLS_CHECK` environment variable to `true`, or pass the `skipUsedUrlsCheck` configurable field ([variable](src/agents/generate-post/constants.ts#L103)) to the graph.

This will prevent the graph from saving any URLs, or reading URLs, ensuring the graph will never prevent a link from being used due to the URLs already being used in previous posts.

```typescript
import { Client } from "@langchain/langgraph-sdk";
import { SKIP_USED_URLS_CHECK } from "src/agents/generate-post/constants";

const client = new Client({
  apiUrl: process.env.LANGGRAPH_API_URL,
});

const { thread_id } = await client.threads.create();
const res = await client.runs.create(thread_id, "generate_post", {
  input: {
    links: ["https://www.example.com"],
  },
  config: {
    configurable: {
      // Pass this to the graph to skip used URLs check, or set the environment variable
      [SKIP_USED_URLS_CHECK]: true,
    },
  },
});
```

### Skip Content Verification

The `generate_post` graph by default always attempts to "verify" the content from the link provided is relevant to your [business context](src/agents/generate-post/prompts/index.ts#L60). If you want to bypass this step, and assume all input links are relevant, you can set the `SKIP_CONTENT_RELEVANCY_CHECK` environment variable to `true`, or pass the `skipContentRelevancyCheck` configurable field ([variable](src/agents/generate-post/constants.ts#L101)) to the graph.

```typescript
import { Client } from "@langchain/langgraph-sdk";
import { SKIP_CONTENT_RELEVANCY_CHECK } from "src/agents/generate-post/constants";

const client = new Client({
  apiUrl: process.env.LANGGRAPH_API_URL,
});

const { thread_id } = await client.threads.create();
const res = await client.runs.create(thread_id, "generate_post", {
  input: {
    links: ["https://www.example.com"],
  },
  config: {
    configurable: {
      // Pass this to the graph to skip content verification, or set the environment variable
      [SKIP_CONTENT_RELEVANCY_CHECK]: true,
    },
  },
});
```

## Shared

### Key

- [Exclude URLs](#exclude-urls)

### Exclude URLs

Inside each of the verify links subgraphs (`verify-general`, `verify-github`, `verify-youtube`, `verify-tweet`) there is a check (typically named `shouldExclude<type>Content`) which checks if the URL which was passed should be excluded, and thus, a post should _not_ be generated.

Each of these util functions can be found inside the [`should-exclude.ts`](src/agents/should-exclude.ts) file.

They will first check if the `USE_LANGCHAIN_PROMPTS` env variable is set to `true`. If it is not, then they will return `false` and the graph will continue as normal. If it is set to `true`, it will then check the URL against a list/single string to see if it should be excluded. This is because the exclusion logic for these functions is specific to LangChain, and we do not want it running (by default) for non-LangChain users.

If the input matches the exclusion string(s), it will return `true` to not generate a post for that URL.



================================================
FILE: jest.config.js
================================================
export default {
  preset: "ts-jest/presets/default-esm",
  moduleNameMapper: {
    "^(\\.{1,2}/.*)\\.js$": "$1",
  },
  transform: {
    "^.+\\.tsx?$": [
      "ts-jest",
      {
        useESM: true,
      },
    ],
  },
  extensionsToTreatAsEsm: [".ts"],
  setupFiles: ["dotenv/config", "./jest.setup.cjs"],
  passWithNoTests: true,
  testTimeout: 20_000,
};



================================================
FILE: jest.setup.cjs
================================================
// const timezoneMock = require("timezone-mock");
require("dotenv").config();

// Mock the timezone to 'America/Los_Angeles'
// timezoneMock.register("US/Pacific"); // Alternatively, use 'America/Los_Angeles'

// Optional: Log the current timezone to verify
// console.log(
//   "Current Timezone:",
//   Intl.DateTimeFormat().resolvedOptions().timeZone,
// );

// If you have any global configurations or mocks, add them here
// For example, you can set up global variables, mock APIs, etc.



================================================
FILE: langgraph.json
================================================
{
  "node_version": "20",
  "graphs": {
    "ingest_data": "./src/agents/ingest-data/ingest-data-graph.ts:graph",
    "generate_post": "./src/agents/generate-post/generate-post-graph.ts:generatePostGraph",
    "upload_post": "./src/agents/upload-post/index.ts:uploadPostGraph",
    "reflection": "./src/agents/reflection/index.ts:reflectionGraph",
    "generate_thread": "./src/agents/generate-thread/index.ts:generateThreadGraph",
    "curate_data": "./src/agents/curate-data/index.ts:curateDataGraph",
    "verify_reddit_post": "./src/agents/verify-reddit-post/verify-reddit-post-graph.ts:verifyRedditPostGraph",
    "verify_tweet": "./src/agents/verify-tweet/verify-tweet-graph.ts:verifyTweetGraph",
    "supervisor": "./src/agents/supervisor/supervisor-graph.ts:supervisorGraph",
    "generate_report": "./src/agents/generate-report/index.ts:generateReportGraph",
    "repurposer": "./src/agents/repurposer/index.ts:repurposerGraph",
    "curated_post_interrupt": "./src/agents/curated-post-interrupt/index.ts:curatedPostInterruptGraph",
    "ingest_repurposed_data": "./src/agents/ingest-repurposed-data/index.ts:graph",
    "repurposer_post_interrupt": "./src/agents/repurposer-post-interrupt/index.ts:repurposerPostInterruptGraph"
  },
  "env": ".env",
  "dependencies": ["."],
  "dockerfile_lines": ["RUN npx -y playwright@1.49.1 install --with-deps"]
}



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 LangChain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: package.json
================================================
{
  "name": "example-graph",
  "version": "0.0.1",
  "description": "A starter template for creating a LangGraph workflow.",
  "packageManager": "yarn@1.22.22",
  "main": "my_app/graph.ts",
  "author": "Your Name",
  "license": "MIT",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "yarn langgraph:in_mem:up",
    "build": "tsc",
    "clean": "rm -rf dist",
    "test": "cross-env TZ=America/Los_Angeles node --experimental-vm-modules node_modules/jest/bin/jest.js --testPathPattern=\\.test\\.ts$ --testPathIgnorePatterns=\\.int\\.test\\.ts$",
    "test:int": "cross-env TZ=America/Los_Angeles node --experimental-vm-modules node_modules/jest/bin/jest.js --testPathPattern=\\.int\\.test\\.ts$",
    "test:single": "cross-env TZ=America/Los_Angeles NODE_OPTIONS=--experimental-vm-modules yarn run jest --config jest.config.js --testTimeout 100000",
    "format": "prettier --write .",
    "lint": "eslint src",
    "lint:fix": "eslint src --fix",
    "format:check": "prettier --check .",
    "lint:langgraph-json": "node scripts/checkLanggraphPaths.js",
    "lint:all": "yarn lint & yarn lint:langgraph-json",
    "test:all": "yarn test && yarn test:int && yarn lint:langgraph",
    "start:auth": "yarn tsx src/clients/auth-server.ts",
    "generate_post": "yarn tsx scripts/generate-post.ts",
    "cron:create": "yarn tsx scripts/crons/create-cron.ts",
    "cron:delete": "yarn tsx scripts/crons/delete-cron.ts",
    "cron:list": "yarn tsx scripts/crons/list-crons.ts",
    "graph:backfill": "yarn tsx scripts/backfill.ts",
    "get:scheduled_runs": "yarn tsx scripts/get-scheduled-runs.ts",
    "get:used_links": "yarn tsx scripts/get-all-used-links.ts",
    "graph:delete:run_thread": "yarn tsx scripts/delete-run-thread.ts",
    "langgraph:up": "langgraph up --watch --port 54367",
    "langgraph:in_mem:up": "npx @langchain/langgraph-cli dev --port 54367"
  },
  "dependencies": {
    "@arcadeai/arcadejs": "^1.0.0",
    "@googleapis/youtube": "^20.0.0",
    "@langchain/anthropic": "^0.3.16",
    "@langchain/community": "^0.3.39",
    "@langchain/core": "^0.3.43",
    "@langchain/google-vertexai-web": "^0.1.2",
    "@langchain/langgraph": "^0.2.62",
    "@langchain/langgraph-sdk": "0.0.62",
    "@langchain/openai": "^0.3.17",
    "@mendable/firecrawl-js": "^1.10.1",
    "@octokit/rest": "^21.0.2",
    "@slack/web-api": "^7.7.0",
    "@supabase/supabase-js": "^2.47.10",
    "@types/snoowrap": "^1.19.0",
    "cheerio": "^1.0.0",
    "date-fns": "^4.1.0",
    "date-fns-tz": "^3.2.0",
    "dotenv": "^16.4.7",
    "express": "^4.21.2",
    "express-session": "^1.18.1",
    "file-type": "^19.6.0",
    "google-auth-library": "^9.15.0",
    "langsmith": "0.2.15-rc.6",
    "moment": "^2.30.1",
    "passport": "^0.7.0",
    "passport-twitter": "^1.0.4",
    "playwright": "^1.49.1",
    "snoowrap": "^1.23.0",
    "twitter-api-v2": "^1.18.2",
    "uuid": "^11.0.4",
    "xml2js": "^0.6.2",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3.1.0",
    "@eslint/js": "^9.9.1",
    "@jest/globals": "^29.7.0",
    "@tsconfig/recommended": "^1.0.7",
    "@types/express": "^5.0.0",
    "@types/express-session": "^1.18.1",
    "@types/jest": "^29.5.0",
    "@types/node": "^22.10.6",
    "@types/passport": "^1.0.17",
    "@types/passport-twitter": "^1.0.40",
    "@types/xml2js": "^0.4.14",
    "@typescript-eslint/eslint-plugin": "^5.59.8",
    "@typescript-eslint/parser": "^5.59.8",
    "cross-env": "^7.0.3",
    "eslint": "^8.41.0",
    "eslint-config-prettier": "^8.8.0",
    "eslint-plugin-import": "^2.27.5",
    "eslint-plugin-no-instanceof": "^1.0.1",
    "eslint-plugin-prettier": "^4.2.1",
    "jest": "^29.7.0",
    "prettier": "^3.3.3",
    "timezone-mock": "^1.3.6",
    "ts-jest": "^29.1.0",
    "tsx": "^4.19.2",
    "typescript": "^5.3.3"
  }
}



================================================
FILE: pyproject.toml
================================================
[project]
name = "langgraph-slack"
version = "0.0.1"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "fastapi>=0.115.6",
    "langchain>=0.3.20",
    "langchain-openai>=0.3.7",
    "langgraph-sdk>=0.1.48",
    "langmem>=0.0.15",
    "python-dotenv>=1.0.1",
    "slack-bolt>=1.22.0",
    "uvicorn>=0.34.0",
    "langgraph-prebuilt>=0.1.2",
    "aiohttp>=3.11.13",
]

[project.packages]
find = { where = ["src"] }

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"


[dependency-groups]
dev = [
    "ruff>=0.8.4",
    "langgraph-api>=0.0.28",
    "langgraph-cli>=0.1.75",
]



================================================
FILE: tsconfig.json
================================================
{
  "extends": "@tsconfig/recommended",
  "compilerOptions": {
    "target": "ES2021",
    "lib": ["ES2021", "ES2022.Object", "DOM"],
    "module": "NodeNext",
    "moduleResolution": "nodenext",
    "esModuleInterop": true,
    "noImplicitReturns": true,
    "declaration": true,
    "noFallthroughCasesInSwitch": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "useDefineForClassFields": true,
    "strictPropertyInitialization": false,
    "allowJs": true,
    "strict": true,
    "strictFunctionTypes": false,
    "outDir": "dist",
    "types": ["jest", "node"],
    "resolveJsonModule": true
  },
  "include": ["**/*.ts", "**/*.js", "jest.setup.cjs"],
  "exclude": ["node_modules", "dist"]
}



================================================
FILE: uv.lock
================================================
version = 1
requires-python = ">=3.11"
resolution-markers = [
    "python_full_version < '3.12.4'",
    "python_full_version >= '3.12.4'",
]

[[package]]
name = "aiohappyeyeballs"
version = "2.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/0c/458958007041f4b4de2d307e6b75d9e7554dad0baf26fe7a48b741aac126/aiohappyeyeballs-2.5.0.tar.gz", hash = "sha256:18fde6204a76deeabc97c48bdd01d5801cfda5d6b9c8bbeb1aaaee9d648ca191", size = 22494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1b/9a/e4886864ce06e1579bd428208127fbdc0d62049c751e4e9e3b509c0059dc/aiohappyeyeballs-2.5.0-py3-none-any.whl", hash = "sha256:0850b580748c7071db98bffff6d4c94028d0d3035acc20fd721a0ce7e8cac35d", size = 15128 },
]

[[package]]
name = "aiohttp"
version = "3.11.13"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiohappyeyeballs" },
    { name = "aiosignal" },
    { name = "attrs" },
    { name = "frozenlist" },
    { name = "multidict" },
    { name = "propcache" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b3/3f/c4a667d184c69667b8f16e0704127efc5f1e60577df429382b4d95fd381e/aiohttp-3.11.13.tar.gz", hash = "sha256:8ce789231404ca8fff7f693cdce398abf6d90fd5dae2b1847477196c243b1fbb", size = 7674284 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3b/93/8e012ae31ff1bda5d43565d6f9e0bad325ba6f3f2d78f298bd39645be8a3/aiohttp-3.11.13-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:6b35aab22419ba45f8fc290d0010898de7a6ad131e468ffa3922b1b0b24e9d2e", size = 709013 },
    { url = "https://files.pythonhosted.org/packages/d8/be/fc7c436678ffe547d038319add8e44fd5e33090158752e5c480aed51a8d0/aiohttp-3.11.13-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:f81cba651db8795f688c589dd11a4fbb834f2e59bbf9bb50908be36e416dc760", size = 468896 },
    { url = "https://files.pythonhosted.org/packages/d9/1c/56906111ac9d4dab4baab43c89d35d5de1dbb38085150257895005b08bef/aiohttp-3.11.13-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:f55d0f242c2d1fcdf802c8fabcff25a9d85550a4cf3a9cf5f2a6b5742c992839", size = 455968 },
    { url = "https://files.pythonhosted.org/packages/ba/16/229d36ed27c2bb350320364efb56f906af194616cc15fc5d87f3ef21dbef/aiohttp-3.11.13-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c4bea08a6aad9195ac9b1be6b0c7e8a702a9cec57ce6b713698b4a5afa9c2e33", size = 1686082 },
    { url = "https://files.pythonhosted.org/packages/3a/44/78fd174509c56028672e5dfef886569cfa1fced0c5fd5c4480426db19ac9/aiohttp-3.11.13-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c6070bcf2173a7146bb9e4735b3c62b2accba459a6eae44deea0eb23e0035a23", size = 1744056 },
    { url = "https://files.pythonhosted.org/packages/a3/11/325145c6dce8124b5caadbf763e908f2779c14bb0bc5868744d1e5cb9cb7/aiohttp-3.11.13-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:718d5deb678bc4b9d575bfe83a59270861417da071ab44542d0fcb6faa686636", size = 1785810 },
    { url = "https://files.pythonhosted.org/packages/95/de/faba18a0af09969e10eb89fdbd4cb968bea95e75449a7fa944d4de7d1d2f/aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f6b2c5b4a4d22b8fb2c92ac98e0747f5f195e8e9448bfb7404cd77e7bfa243f", size = 1675540 },
    { url = "https://files.pythonhosted.org/packages/ea/53/0437c46e960b79ae3b1ff74c1ec12f04bf4f425bd349c8807acb38aae3d7/aiohttp-3.11.13-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:747ec46290107a490d21fe1ff4183bef8022b848cf9516970cb31de6d9460088", size = 1620210 },
    { url = "https://files.pythonhosted.org/packages/04/2f/31769ed8e29cc22baaa4005bd2749a7fd0f61ad0f86024d38dff8e394cf6/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:01816f07c9cc9d80f858615b1365f8319d6a5fd079cd668cc58e15aafbc76a54", size = 1654399 },
    { url = "https://files.pythonhosted.org/packages/b0/24/acb24571815b9a86a8261577c920fd84f819178c02a75b05b1a0d7ab83fb/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:a08ad95fcbd595803e0c4280671d808eb170a64ca3f2980dd38e7a72ed8d1fea", size = 1660424 },
    { url = "https://files.pythonhosted.org/packages/91/45/30ca0c3ba5bbf7592eee7489eae30437736f7ff912eaa04cfdcf74edca8c/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:c97be90d70f7db3aa041d720bfb95f4869d6063fcdf2bb8333764d97e319b7d0", size = 1650415 },
    { url = "https://files.pythonhosted.org/packages/86/8d/4d887df5e732cc70349243c2c9784911979e7bd71c06f9e7717b8a896f75/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:ab915a57c65f7a29353c8014ac4be685c8e4a19e792a79fe133a8e101111438e", size = 1733292 },
    { url = "https://files.pythonhosted.org/packages/40/c9/bd950dac0a4c84d44d8da8d6e0f9c9511d45e02cf908a4e1fca591f46a25/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:35cda4e07f5e058a723436c4d2b7ba2124ab4e0aa49e6325aed5896507a8a42e", size = 1755536 },
    { url = "https://files.pythonhosted.org/packages/32/04/aafeda6b4ed3693a44bb89eae002ebaa74f88b2265a7e68f8a31c33330f5/aiohttp-3.11.13-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:af55314407714fe77a68a9ccaab90fdb5deb57342585fd4a3a8102b6d4370080", size = 1693126 },
    { url = "https://files.pythonhosted.org/packages/a1/4f/67729187e884b0f002a0317d2cc7962a5a0416cadc95ea88ba92477290d9/aiohttp-3.11.13-cp311-cp311-win32.whl", hash = "sha256:42d689a5c0a0c357018993e471893e939f555e302313d5c61dfc566c2cad6185", size = 416800 },
    { url = "https://files.pythonhosted.org/packages/29/23/d98d491ca073ee92cc6a741be97b6b097fb06dacc5f95c0c9350787db549/aiohttp-3.11.13-cp311-cp311-win_amd64.whl", hash = "sha256:b73a2b139782a07658fbf170fe4bcdf70fc597fae5ffe75e5b67674c27434a9f", size = 442891 },
    { url = "https://files.pythonhosted.org/packages/9a/a9/6657664a55f78db8767e396cc9723782ed3311eb57704b0a5dacfa731916/aiohttp-3.11.13-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:2eabb269dc3852537d57589b36d7f7362e57d1ece308842ef44d9830d2dc3c90", size = 705054 },
    { url = "https://files.pythonhosted.org/packages/3b/06/f7df1fe062d16422f70af5065b76264f40b382605cf7477fa70553a9c9c1/aiohttp-3.11.13-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7b77ee42addbb1c36d35aca55e8cc6d0958f8419e458bb70888d8c69a4ca833d", size = 464440 },
    { url = "https://files.pythonhosted.org/packages/22/3a/8773ea866735754004d9f79e501fe988bdd56cfac7fdecbc8de17fc093eb/aiohttp-3.11.13-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:55789e93c5ed71832e7fac868167276beadf9877b85697020c46e9a75471f55f", size = 456394 },
    { url = "https://files.pythonhosted.org/packages/7f/61/8e2f2af2327e8e475a2b0890f15ef0bbfd117e321cce1e1ed210df81bbac/aiohttp-3.11.13-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c929f9a7249a11e4aa5c157091cfad7f49cc6b13f4eecf9b747104befd9f56f2", size = 1682752 },
    { url = "https://files.pythonhosted.org/packages/24/ed/84fce816bc8da39aa3f6c1196fe26e47065fea882b1a67a808282029c079/aiohttp-3.11.13-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d33851d85537bbf0f6291ddc97926a754c8f041af759e0aa0230fe939168852b", size = 1737375 },
    { url = "https://files.pythonhosted.org/packages/d9/de/35a5ba9e3d21ebfda1ebbe66f6cc5cbb4d3ff9bd6a03e5e8a788954f8f27/aiohttp-3.11.13-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9229d8613bd8401182868fe95688f7581673e1c18ff78855671a4b8284f47bcb", size = 1793660 },
    { url = "https://files.pythonhosted.org/packages/ff/fe/0f650a8c7c72c8a07edf8ab164786f936668acd71786dd5885fc4b1ca563/aiohttp-3.11.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:669dd33f028e54fe4c96576f406ebb242ba534dd3a981ce009961bf49960f117", size = 1692233 },
    { url = "https://files.pythonhosted.org/packages/a8/20/185378b3483f968c6303aafe1e33b0da0d902db40731b2b2b2680a631131/aiohttp-3.11.13-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:7c1b20a1ace54af7db1f95af85da530fe97407d9063b7aaf9ce6a32f44730778", size = 1619708 },
    { url = "https://files.pythonhosted.org/packages/a4/f9/d9c181750980b17e1e13e522d7e82a8d08d3d28a2249f99207ef5d8d738f/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:5724cc77f4e648362ebbb49bdecb9e2b86d9b172c68a295263fa072e679ee69d", size = 1641802 },
    { url = "https://files.pythonhosted.org/packages/50/c7/1cb46b72b1788710343b6e59eaab9642bd2422f2d87ede18b1996e0aed8f/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:aa36c35e94ecdb478246dd60db12aba57cfcd0abcad43c927a8876f25734d496", size = 1684678 },
    { url = "https://files.pythonhosted.org/packages/71/87/89b979391de840c5d7c34e78e1148cc731b8aafa84b6a51d02f44b4c66e2/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:9b5b37c863ad5b0892cc7a4ceb1e435e5e6acd3f2f8d3e11fa56f08d3c67b820", size = 1646921 },
    { url = "https://files.pythonhosted.org/packages/a7/db/a463700ac85b72f8cf68093e988538faaf4e865e3150aa165cf80ee29d6e/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:e06cf4852ce8c4442a59bae5a3ea01162b8fcb49ab438d8548b8dc79375dad8a", size = 1702493 },
    { url = "https://files.pythonhosted.org/packages/b8/32/1084e65da3adfb08c7e1b3e94f3e4ded8bd707dee265a412bc377b7cd000/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:5194143927e494616e335d074e77a5dac7cd353a04755330c9adc984ac5a628e", size = 1735004 },
    { url = "https://files.pythonhosted.org/packages/a0/bb/a634cbdd97ce5d05c2054a9a35bfc32792d7e4f69d600ad7e820571d095b/aiohttp-3.11.13-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:afcb6b275c2d2ba5d8418bf30a9654fa978b4f819c2e8db6311b3525c86fe637", size = 1694964 },
    { url = "https://files.pythonhosted.org/packages/fd/cf/7d29db4e5c28ec316e5d2ac9ac9df0e2e278e9ea910e5c4205b9b64c2c42/aiohttp-3.11.13-cp312-cp312-win32.whl", hash = "sha256:7104d5b3943c6351d1ad7027d90bdd0ea002903e9f610735ac99df3b81f102ee", size = 411746 },
    { url = "https://files.pythonhosted.org/packages/65/a9/13e69ad4fd62104ebd94617f9f2be58231b50bb1e6bac114f024303ac23b/aiohttp-3.11.13-cp312-cp312-win_amd64.whl", hash = "sha256:47dc018b1b220c48089b5b9382fbab94db35bef2fa192995be22cbad3c5730c8", size = 438078 },
    { url = "https://files.pythonhosted.org/packages/87/dc/7d58d33cec693f1ddf407d4ab975445f5cb507af95600f137b81683a18d8/aiohttp-3.11.13-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:9862d077b9ffa015dbe3ce6c081bdf35135948cb89116e26667dd183550833d1", size = 698372 },
    { url = "https://files.pythonhosted.org/packages/84/e7/5d88514c9e24fbc8dd6117350a8ec4a9314f4adae6e89fe32e3e639b0c37/aiohttp-3.11.13-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:fbfef0666ae9e07abfa2c54c212ac18a1f63e13e0760a769f70b5717742f3ece", size = 461057 },
    { url = "https://files.pythonhosted.org/packages/96/1a/8143c48a929fa00c6324f85660cb0f47a55ed9385f0c1b72d4b8043acf8e/aiohttp-3.11.13-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:93a1f7d857c4fcf7cabb1178058182c789b30d85de379e04f64c15b7e88d66fb", size = 453340 },
    { url = "https://files.pythonhosted.org/packages/2f/1c/b8010e4d65c5860d62681088e5376f3c0a940c5e3ca8989cae36ce8c3ea8/aiohttp-3.11.13-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ba40b7ae0f81c7029583a338853f6607b6d83a341a3dcde8bed1ea58a3af1df9", size = 1665561 },
    { url = "https://files.pythonhosted.org/packages/19/ed/a68c3ab2f92fdc17dfc2096117d1cfaa7f7bdded2a57bacbf767b104165b/aiohttp-3.11.13-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b5b95787335c483cd5f29577f42bbe027a412c5431f2f80a749c80d040f7ca9f", size = 1718335 },
    { url = "https://files.pythonhosted.org/packages/27/4f/3a0b6160ce663b8ebdb65d1eedff60900cd7108838c914d25952fe2b909f/aiohttp-3.11.13-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a7d474c5c1f0b9405c1565fafdc4429fa7d986ccbec7ce55bc6a330f36409cad", size = 1775522 },
    { url = "https://files.pythonhosted.org/packages/0b/58/9da09291e19696c452e7224c1ce8c6d23a291fe8cd5c6b247b51bcda07db/aiohttp-3.11.13-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1e83fb1991e9d8982b3b36aea1e7ad27ea0ce18c14d054c7a404d68b0319eebb", size = 1677566 },
    { url = "https://files.pythonhosted.org/packages/3d/18/6184f2bf8bbe397acbbbaa449937d61c20a6b85765f48e5eddc6d84957fe/aiohttp-3.11.13-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4586a68730bd2f2b04a83e83f79d271d8ed13763f64b75920f18a3a677b9a7f0", size = 1603590 },
    { url = "https://files.pythonhosted.org/packages/04/94/91e0d1ca0793012ccd927e835540aa38cca98bdce2389256ab813ebd64a3/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9fe4eb0e7f50cdb99b26250d9328faef30b1175a5dbcfd6d0578d18456bac567", size = 1618688 },
    { url = "https://files.pythonhosted.org/packages/71/85/d13c3ea2e48a10b43668305d4903838834c3d4112e5229177fbcc23a56cd/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:2a8a6bc19818ac3e5596310ace5aa50d918e1ebdcc204dc96e2f4d505d51740c", size = 1658053 },
    { url = "https://files.pythonhosted.org/packages/12/6a/3242a35100de23c1e8d9e05e8605e10f34268dee91b00d9d1e278c58eb80/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:7f27eec42f6c3c1df09cfc1f6786308f8b525b8efaaf6d6bd76c1f52c6511f6a", size = 1616917 },
    { url = "https://files.pythonhosted.org/packages/f5/b3/3f99b6f0a9a79590a7ba5655dbde8408c685aa462247378c977603464d0a/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:2a4a13dfbb23977a51853b419141cd0a9b9573ab8d3a1455c6e63561387b52ff", size = 1685872 },
    { url = "https://files.pythonhosted.org/packages/8a/2e/99672181751f280a85e24fcb9a2c2469e8b1a0de1746b7b5c45d1eb9a999/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:02876bf2f69b062584965507b07bc06903c2dc93c57a554b64e012d636952654", size = 1715719 },
    { url = "https://files.pythonhosted.org/packages/7a/cd/68030356eb9a7d57b3e2823c8a852709d437abb0fbff41a61ebc351b7625/aiohttp-3.11.13-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:b992778d95b60a21c4d8d4a5f15aaab2bd3c3e16466a72d7f9bfd86e8cea0d4b", size = 1673166 },
    { url = "https://files.pythonhosted.org/packages/03/61/425397a9a2839c609d09fdb53d940472f316a2dbeaa77a35b2628dae6284/aiohttp-3.11.13-cp313-cp313-win32.whl", hash = "sha256:507ab05d90586dacb4f26a001c3abf912eb719d05635cbfad930bdbeb469b36c", size = 410615 },
    { url = "https://files.pythonhosted.org/packages/9c/54/ebb815bc0fe057d8e7a11c086c479e972e827082f39aeebc6019dd4f0862/aiohttp-3.11.13-cp313-cp313-win_amd64.whl", hash = "sha256:5ceb81a4db2decdfa087381b5fc5847aa448244f973e5da232610304e199e7b2", size = 436452 },
]

[[package]]
name = "aiosignal"
version = "1.3.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "frozenlist" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ba/b5/6d55e80f6d8a08ce22b982eafa278d823b541c925f11ee774b0b9c43473d/aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54", size = 19424 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5", size = 7597 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anthropic"
version = "0.49.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/86/e3/a88c8494ce4d1a88252b9e053607e885f9b14d0a32273d47b727cbee4228/anthropic-0.49.0.tar.gz", hash = "sha256:c09e885b0f674b9119b4f296d8508907f6cff0009bc20d5cf6b35936c40b4398", size = 210016 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/74/5d90ad14d55fbe3f9c474fdcb6e34b4bed99e3be8efac98734a5ddce88c1/anthropic-0.49.0-py3-none-any.whl", hash = "sha256:bbc17ad4e7094988d2fa86b87753ded8dce12498f4b85fe5810f208f454a8375", size = 243368 },
]

[[package]]
name = "anyio"
version = "4.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f6/40/318e58f669b1a9e00f5c4453910682e2d9dd594334539c7b7817dabb765f/anyio-4.7.0.tar.gz", hash = "sha256:2f834749c602966b7d456a7567cafcb309f96482b5081d14ac93ccd457f9dd48", size = 177076 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a0/7a/4daaf3b6c08ad7ceffea4634ec206faeff697526421c20f07628c7372156/anyio-4.7.0-py3-none-any.whl", hash = "sha256:ea60c3723ab42ba6fff7e8ccb0488c898ec538ff4df1f1d5e642c3601d07e352", size = 93052 },
]

[[package]]
name = "attrs"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/49/7c/fdf464bcc51d23881d110abd74b512a42b3d5d376a55a831b44c603ae17f/attrs-25.1.0.tar.gz", hash = "sha256:1c97078a80c814273a76b2a298a932eb681c87415c11dee0a6921de7f1b02c3e", size = 810562 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fc/30/d4986a882011f9df997a55e6becd864812ccfcd821d64aac8570ee39f719/attrs-25.1.0-py3-none-any.whl", hash = "sha256:c75a69e28a550a7e93789579c22aa26b0f5b83b75dc4e08fe092980051e1090a", size = 63152 },
]

[[package]]
name = "certifi"
version = "2024.12.14"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/0f/bd/1d41ee578ce09523c81a15426705dd20969f5abf006d1afe8aeff0dd776a/certifi-2024.12.14.tar.gz", hash = "sha256:b650d30f370c2b724812bee08008be0c4163b163ddaec3f2546c1caf65f191db", size = 166010 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a5/32/8f6669fc4798494966bf446c8c4a162e0b5d893dff088afddf76414f70e1/certifi-2024.12.14-py3-none-any.whl", hash = "sha256:1275f7a45be9464efc1173084eaa30f866fe2e47d389406136d332ed4967ec56", size = 164927 },
]

[[package]]
name = "cffi"
version = "1.17.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pycparser" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fc/97/c783634659c2920c3fc70419e3af40972dbaf758daa229a7d6ea6135c90d/cffi-1.17.1.tar.gz", hash = "sha256:1c39c6016c32bc48dd54561950ebd6836e1670f2ae46128f67cf49e789c52824", size = 516621 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/f4/927e3a8899e52a27fa57a48607ff7dc91a9ebe97399b357b85a0c7892e00/cffi-1.17.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:a45e3c6913c5b87b3ff120dcdc03f6131fa0065027d0ed7ee6190736a74cd401", size = 182264 },
    { url = "https://files.pythonhosted.org/packages/6c/f5/6c3a8efe5f503175aaddcbea6ad0d2c96dad6f5abb205750d1b3df44ef29/cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:30c5e0cb5ae493c04c8b42916e52ca38079f1b235c2f8ae5f4527b963c401caf", size = 178651 },
    { url = "https://files.pythonhosted.org/packages/94/dd/a3f0118e688d1b1a57553da23b16bdade96d2f9bcda4d32e7d2838047ff7/cffi-1.17.1-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f75c7ab1f9e4aca5414ed4d8e5c0e303a34f4421f8a0d47a4d019ceff0ab6af4", size = 445259 },
    { url = "https://files.pythonhosted.org/packages/2e/ea/70ce63780f096e16ce8588efe039d3c4f91deb1dc01e9c73a287939c79a6/cffi-1.17.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a1ed2dd2972641495a3ec98445e09766f077aee98a1c896dcb4ad0d303628e41", size = 469200 },
    { url = "https://files.pythonhosted.org/packages/1c/a0/a4fa9f4f781bda074c3ddd57a572b060fa0df7655d2a4247bbe277200146/cffi-1.17.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:46bf43160c1a35f7ec506d254e5c890f3c03648a4dbac12d624e4490a7046cd1", size = 477235 },
    { url = "https://files.pythonhosted.org/packages/62/12/ce8710b5b8affbcdd5c6e367217c242524ad17a02fe5beec3ee339f69f85/cffi-1.17.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a24ed04c8ffd54b0729c07cee15a81d964e6fee0e3d4d342a27b020d22959dc6", size = 459721 },
    { url = "https://files.pythonhosted.org/packages/ff/6b/d45873c5e0242196f042d555526f92aa9e0c32355a1be1ff8c27f077fd37/cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:610faea79c43e44c71e1ec53a554553fa22321b65fae24889706c0a84d4ad86d", size = 467242 },
    { url = "https://files.pythonhosted.org/packages/1a/52/d9a0e523a572fbccf2955f5abe883cfa8bcc570d7faeee06336fbd50c9fc/cffi-1.17.1-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:a9b15d491f3ad5d692e11f6b71f7857e7835eb677955c00cc0aefcd0669adaf6", size = 477999 },
    { url = "https://files.pythonhosted.org/packages/44/74/f2a2460684a1a2d00ca799ad880d54652841a780c4c97b87754f660c7603/cffi-1.17.1-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:de2ea4b5833625383e464549fec1bc395c1bdeeb5f25c4a3a82b5a8c756ec22f", size = 454242 },
    { url = "https://files.pythonhosted.org/packages/f8/4a/34599cac7dfcd888ff54e801afe06a19c17787dfd94495ab0c8d35fe99fb/cffi-1.17.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:fc48c783f9c87e60831201f2cce7f3b2e4846bf4d8728eabe54d60700b318a0b", size = 478604 },
    { url = "https://files.pythonhosted.org/packages/34/33/e1b8a1ba29025adbdcda5fb3a36f94c03d771c1b7b12f726ff7fef2ebe36/cffi-1.17.1-cp311-cp311-win32.whl", hash = "sha256:85a950a4ac9c359340d5963966e3e0a94a676bd6245a4b55bc43949eee26a655", size = 171727 },
    { url = "https://files.pythonhosted.org/packages/3d/97/50228be003bb2802627d28ec0627837ac0bf35c90cf769812056f235b2d1/cffi-1.17.1-cp311-cp311-win_amd64.whl", hash = "sha256:caaf0640ef5f5517f49bc275eca1406b0ffa6aa184892812030f04c2abf589a0", size = 181400 },
    { url = "https://files.pythonhosted.org/packages/5a/84/e94227139ee5fb4d600a7a4927f322e1d4aea6fdc50bd3fca8493caba23f/cffi-1.17.1-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:805b4371bf7197c329fcb3ead37e710d1bca9da5d583f5073b799d5c5bd1eee4", size = 183178 },
    { url = "https://files.pythonhosted.org/packages/da/ee/fb72c2b48656111c4ef27f0f91da355e130a923473bf5ee75c5643d00cca/cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:733e99bc2df47476e3848417c5a4540522f234dfd4ef3ab7fafdf555b082ec0c", size = 178840 },
    { url = "https://files.pythonhosted.org/packages/cc/b6/db007700f67d151abadf508cbfd6a1884f57eab90b1bb985c4c8c02b0f28/cffi-1.17.1-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1257bdabf294dceb59f5e70c64a3e2f462c30c7ad68092d01bbbfb1c16b1ba36", size = 454803 },
    { url = "https://files.pythonhosted.org/packages/1a/df/f8d151540d8c200eb1c6fba8cd0dfd40904f1b0682ea705c36e6c2e97ab3/cffi-1.17.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:da95af8214998d77a98cc14e3a3bd00aa191526343078b530ceb0bd710fb48a5", size = 478850 },
    { url = "https://files.pythonhosted.org/packages/28/c0/b31116332a547fd2677ae5b78a2ef662dfc8023d67f41b2a83f7c2aa78b1/cffi-1.17.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d63afe322132c194cf832bfec0dc69a99fb9bb6bbd550f161a49e9e855cc78ff", size = 485729 },
    { url = "https://files.pythonhosted.org/packages/91/2b/9a1ddfa5c7f13cab007a2c9cc295b70fbbda7cb10a286aa6810338e60ea1/cffi-1.17.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f79fc4fc25f1c8698ff97788206bb3c2598949bfe0fef03d299eb1b5356ada99", size = 471256 },
    { url = "https://files.pythonhosted.org/packages/b2/d5/da47df7004cb17e4955df6a43d14b3b4ae77737dff8bf7f8f333196717bf/cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b62ce867176a75d03a665bad002af8e6d54644fad99a3c70905c543130e39d93", size = 479424 },
    { url = "https://files.pythonhosted.org/packages/0b/ac/2a28bcf513e93a219c8a4e8e125534f4f6db03e3179ba1c45e949b76212c/cffi-1.17.1-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:386c8bf53c502fff58903061338ce4f4950cbdcb23e2902d86c0f722b786bbe3", size = 484568 },
    { url = "https://files.pythonhosted.org/packages/d4/38/ca8a4f639065f14ae0f1d9751e70447a261f1a30fa7547a828ae08142465/cffi-1.17.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:4ceb10419a9adf4460ea14cfd6bc43d08701f0835e979bf821052f1805850fe8", size = 488736 },
    { url = "https://files.pythonhosted.org/packages/86/c5/28b2d6f799ec0bdecf44dced2ec5ed43e0eb63097b0f58c293583b406582/cffi-1.17.1-cp312-cp312-win32.whl", hash = "sha256:a08d7e755f8ed21095a310a693525137cfe756ce62d066e53f502a83dc550f65", size = 172448 },
    { url = "https://files.pythonhosted.org/packages/50/b9/db34c4755a7bd1cb2d1603ac3863f22bcecbd1ba29e5ee841a4bc510b294/cffi-1.17.1-cp312-cp312-win_amd64.whl", hash = "sha256:51392eae71afec0d0c8fb1a53b204dbb3bcabcb3c9b807eedf3e1e6ccf2de903", size = 181976 },
    { url = "https://files.pythonhosted.org/packages/8d/f8/dd6c246b148639254dad4d6803eb6a54e8c85c6e11ec9df2cffa87571dbe/cffi-1.17.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:f3a2b4222ce6b60e2e8b337bb9596923045681d71e5a082783484d845390938e", size = 182989 },
    { url = "https://files.pythonhosted.org/packages/8b/f1/672d303ddf17c24fc83afd712316fda78dc6fce1cd53011b839483e1ecc8/cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:0984a4925a435b1da406122d4d7968dd861c1385afe3b45ba82b750f229811e2", size = 178802 },
    { url = "https://files.pythonhosted.org/packages/0e/2d/eab2e858a91fdff70533cab61dcff4a1f55ec60425832ddfdc9cd36bc8af/cffi-1.17.1-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d01b12eeeb4427d3110de311e1774046ad344f5b1a7403101878976ecd7a10f3", size = 454792 },
    { url = "https://files.pythonhosted.org/packages/75/b2/fbaec7c4455c604e29388d55599b99ebcc250a60050610fadde58932b7ee/cffi-1.17.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:706510fe141c86a69c8ddc029c7910003a17353970cff3b904ff0686a5927683", size = 478893 },
    { url = "https://files.pythonhosted.org/packages/4f/b7/6e4a2162178bf1935c336d4da8a9352cccab4d3a5d7914065490f08c0690/cffi-1.17.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:de55b766c7aa2e2a3092c51e0483d700341182f08e67c63630d5b6f200bb28e5", size = 485810 },
    { url = "https://files.pythonhosted.org/packages/c7/8a/1d0e4a9c26e54746dc08c2c6c037889124d4f59dffd853a659fa545f1b40/cffi-1.17.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c59d6e989d07460165cc5ad3c61f9fd8f1b4796eacbd81cee78957842b834af4", size = 471200 },
    { url = "https://files.pythonhosted.org/packages/26/9f/1aab65a6c0db35f43c4d1b4f580e8df53914310afc10ae0397d29d697af4/cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd398dbc6773384a17fe0d3e7eeb8d1a21c2200473ee6806bb5e6a8e62bb73dd", size = 479447 },
    { url = "https://files.pythonhosted.org/packages/5f/e4/fb8b3dd8dc0e98edf1135ff067ae070bb32ef9d509d6cb0f538cd6f7483f/cffi-1.17.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:3edc8d958eb099c634dace3c7e16560ae474aa3803a5df240542b305d14e14ed", size = 484358 },
    { url = "https://files.pythonhosted.org/packages/f1/47/d7145bf2dc04684935d57d67dff9d6d795b2ba2796806bb109864be3a151/cffi-1.17.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:72e72408cad3d5419375fc87d289076ee319835bdfa2caad331e377589aebba9", size = 488469 },
    { url = "https://files.pythonhosted.org/packages/bf/ee/f94057fa6426481d663b88637a9a10e859e492c73d0384514a17d78ee205/cffi-1.17.1-cp313-cp313-win32.whl", hash = "sha256:e03eab0a8677fa80d646b5ddece1cbeaf556c313dcfac435ba11f107ba117b5d", size = 172475 },
    { url = "https://files.pythonhosted.org/packages/7c/fc/6a8cb64e5f0324877d503c854da15d76c1e50eb722e320b15345c4d0c6de/cffi-1.17.1-cp313-cp313-win_amd64.whl", hash = "sha256:f6a16c31041f09ead72d69f583767292f750d24913dadacf5756b966aacb3f1a", size = 182009 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/16/b0/572805e227f01586461c80e0fd25d65a2115599cc9dad142fee4b747c357/charset_normalizer-3.4.1.tar.gz", hash = "sha256:44251f18cd68a75b56585dd00dae26183e102cd5e0f9f1466e6df5da2ed64ea3", size = 123188 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/72/80/41ef5d5a7935d2d3a773e3eaebf0a9350542f2cab4eac59a7a4741fbbbbe/charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:8bfa33f4f2672964266e940dd22a195989ba31669bd84629f05fab3ef4e2d125", size = 194995 },
    { url = "https://files.pythonhosted.org/packages/7a/28/0b9fefa7b8b080ec492110af6d88aa3dea91c464b17d53474b6e9ba5d2c5/charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:28bf57629c75e810b6ae989f03c0828d64d6b26a5e205535585f96093e405ed1", size = 139471 },
    { url = "https://files.pythonhosted.org/packages/71/64/d24ab1a997efb06402e3fc07317e94da358e2585165930d9d59ad45fcae2/charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f08ff5e948271dc7e18a35641d2f11a4cd8dfd5634f55228b691e62b37125eb3", size = 149831 },
    { url = "https://files.pythonhosted.org/packages/37/ed/be39e5258e198655240db5e19e0b11379163ad7070962d6b0c87ed2c4d39/charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:234ac59ea147c59ee4da87a0c0f098e9c8d169f4dc2a159ef720f1a61bbe27cd", size = 142335 },
    { url = "https://files.pythonhosted.org/packages/88/83/489e9504711fa05d8dde1574996408026bdbdbd938f23be67deebb5eca92/charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd4ec41f914fa74ad1b8304bbc634b3de73d2a0889bd32076342a573e0779e00", size = 143862 },
    { url = "https://files.pythonhosted.org/packages/c6/c7/32da20821cf387b759ad24627a9aca289d2822de929b8a41b6241767b461/charset_normalizer-3.4.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:eea6ee1db730b3483adf394ea72f808b6e18cf3cb6454b4d86e04fa8c4327a12", size = 145673 },
    { url = "https://files.pythonhosted.org/packages/68/85/f4288e96039abdd5aeb5c546fa20a37b50da71b5cf01e75e87f16cd43304/charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:c96836c97b1238e9c9e3fe90844c947d5afbf4f4c92762679acfe19927d81d77", size = 140211 },
    { url = "https://files.pythonhosted.org/packages/28/a3/a42e70d03cbdabc18997baf4f0227c73591a08041c149e710045c281f97b/charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:4d86f7aff21ee58f26dcf5ae81a9addbd914115cdebcbb2217e4f0ed8982e146", size = 148039 },
    { url = "https://files.pythonhosted.org/packages/85/e4/65699e8ab3014ecbe6f5c71d1a55d810fb716bbfd74f6283d5c2aa87febf/charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:09b5e6733cbd160dcc09589227187e242a30a49ca5cefa5a7edd3f9d19ed53fd", size = 151939 },
    { url = "https://files.pythonhosted.org/packages/b1/82/8e9fe624cc5374193de6860aba3ea8070f584c8565ee77c168ec13274bd2/charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:5777ee0881f9499ed0f71cc82cf873d9a0ca8af166dfa0af8ec4e675b7df48e6", size = 149075 },
    { url = "https://files.pythonhosted.org/packages/3d/7b/82865ba54c765560c8433f65e8acb9217cb839a9e32b42af4aa8e945870f/charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:237bdbe6159cff53b4f24f397d43c6336c6b0b42affbe857970cefbb620911c8", size = 144340 },
    { url = "https://files.pythonhosted.org/packages/b5/b6/9674a4b7d4d99a0d2df9b215da766ee682718f88055751e1e5e753c82db0/charset_normalizer-3.4.1-cp311-cp311-win32.whl", hash = "sha256:8417cb1f36cc0bc7eaba8ccb0e04d55f0ee52df06df3ad55259b9a323555fc8b", size = 95205 },
    { url = "https://files.pythonhosted.org/packages/1e/ab/45b180e175de4402dcf7547e4fb617283bae54ce35c27930a6f35b6bef15/charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl", hash = "sha256:d7f50a1f8c450f3925cb367d011448c39239bb3eb4117c36a6d354794de4ce76", size = 102441 },
    { url = "https://files.pythonhosted.org/packages/0a/9a/dd1e1cdceb841925b7798369a09279bd1cf183cef0f9ddf15a3a6502ee45/charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:73d94b58ec7fecbc7366247d3b0b10a21681004153238750bb67bd9012414545", size = 196105 },
    { url = "https://files.pythonhosted.org/packages/d3/8c/90bfabf8c4809ecb648f39794cf2a84ff2e7d2a6cf159fe68d9a26160467/charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dad3e487649f498dd991eeb901125411559b22e8d7ab25d3aeb1af367df5efd7", size = 140404 },
    { url = "https://files.pythonhosted.org/packages/ad/8f/e410d57c721945ea3b4f1a04b74f70ce8fa800d393d72899f0a40526401f/charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c30197aa96e8eed02200a83fba2657b4c3acd0f0aa4bdc9f6c1af8e8962e0757", size = 150423 },
    { url = "https://files.pythonhosted.org/packages/f0/b8/e6825e25deb691ff98cf5c9072ee0605dc2acfca98af70c2d1b1bc75190d/charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2369eea1ee4a7610a860d88f268eb39b95cb588acd7235e02fd5a5601773d4fa", size = 143184 },
    { url = "https://files.pythonhosted.org/packages/3e/a2/513f6cbe752421f16d969e32f3583762bfd583848b763913ddab8d9bfd4f/charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bc2722592d8998c870fa4e290c2eec2c1569b87fe58618e67d38b4665dfa680d", size = 145268 },
    { url = "https://files.pythonhosted.org/packages/74/94/8a5277664f27c3c438546f3eb53b33f5b19568eb7424736bdc440a88a31f/charset_normalizer-3.4.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ffc9202a29ab3920fa812879e95a9e78b2465fd10be7fcbd042899695d75e616", size = 147601 },
    { url = "https://files.pythonhosted.org/packages/7c/5f/6d352c51ee763623a98e31194823518e09bfa48be2a7e8383cf691bbb3d0/charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:804a4d582ba6e5b747c625bf1255e6b1507465494a40a2130978bda7b932c90b", size = 141098 },
    { url = "https://files.pythonhosted.org/packages/78/d4/f5704cb629ba5ab16d1d3d741396aec6dc3ca2b67757c45b0599bb010478/charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:0f55e69f030f7163dffe9fd0752b32f070566451afe180f99dbeeb81f511ad8d", size = 149520 },
    { url = "https://files.pythonhosted.org/packages/c5/96/64120b1d02b81785f222b976c0fb79a35875457fa9bb40827678e54d1bc8/charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:c4c3e6da02df6fa1410a7680bd3f63d4f710232d3139089536310d027950696a", size = 152852 },
    { url = "https://files.pythonhosted.org/packages/84/c9/98e3732278a99f47d487fd3468bc60b882920cef29d1fa6ca460a1fdf4e6/charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:5df196eb874dae23dcfb968c83d4f8fdccb333330fe1fc278ac5ceeb101003a9", size = 150488 },
    { url = "https://files.pythonhosted.org/packages/13/0e/9c8d4cb99c98c1007cc11eda969ebfe837bbbd0acdb4736d228ccaabcd22/charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:e358e64305fe12299a08e08978f51fc21fac060dcfcddd95453eabe5b93ed0e1", size = 146192 },
    { url = "https://files.pythonhosted.org/packages/b2/21/2b6b5b860781a0b49427309cb8670785aa543fb2178de875b87b9cc97746/charset_normalizer-3.4.1-cp312-cp312-win32.whl", hash = "sha256:9b23ca7ef998bc739bf6ffc077c2116917eabcc901f88da1b9856b210ef63f35", size = 95550 },
    { url = "https://files.pythonhosted.org/packages/21/5b/1b390b03b1d16c7e382b561c5329f83cc06623916aab983e8ab9239c7d5c/charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl", hash = "sha256:6ff8a4a60c227ad87030d76e99cd1698345d4491638dfa6673027c48b3cd395f", size = 102785 },
    { url = "https://files.pythonhosted.org/packages/38/94/ce8e6f63d18049672c76d07d119304e1e2d7c6098f0841b51c666e9f44a0/charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:aabfa34badd18f1da5ec1bc2715cadc8dca465868a4e73a0173466b688f29dda", size = 195698 },
    { url = "https://files.pythonhosted.org/packages/24/2e/dfdd9770664aae179a96561cc6952ff08f9a8cd09a908f259a9dfa063568/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:22e14b5d70560b8dd51ec22863f370d1e595ac3d024cb8ad7d308b4cd95f8313", size = 140162 },
    { url = "https://files.pythonhosted.org/packages/24/4e/f646b9093cff8fc86f2d60af2de4dc17c759de9d554f130b140ea4738ca6/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8436c508b408b82d87dc5f62496973a1805cd46727c34440b0d29d8a2f50a6c9", size = 150263 },
    { url = "https://files.pythonhosted.org/packages/5e/67/2937f8d548c3ef6e2f9aab0f6e21001056f692d43282b165e7c56023e6dd/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d074908e1aecee37a7635990b2c6d504cd4766c7bc9fc86d63f9c09af3fa11b", size = 142966 },
    { url = "https://files.pythonhosted.org/packages/52/ed/b7f4f07de100bdb95c1756d3a4d17b90c1a3c53715c1a476f8738058e0fa/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:955f8851919303c92343d2f66165294848d57e9bba6cf6e3625485a70a038d11", size = 144992 },
    { url = "https://files.pythonhosted.org/packages/96/2c/d49710a6dbcd3776265f4c923bb73ebe83933dfbaa841c5da850fe0fd20b/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:44ecbf16649486d4aebafeaa7ec4c9fed8b88101f4dd612dcaf65d5e815f837f", size = 147162 },
    { url = "https://files.pythonhosted.org/packages/b4/41/35ff1f9a6bd380303dea55e44c4933b4cc3c4850988927d4082ada230273/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0924e81d3d5e70f8126529951dac65c1010cdf117bb75eb02dd12339b57749dd", size = 140972 },
    { url = "https://files.pythonhosted.org/packages/fb/43/c6a0b685fe6910d08ba971f62cd9c3e862a85770395ba5d9cad4fede33ab/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:2967f74ad52c3b98de4c3b32e1a44e32975e008a9cd2a8cc8966d6a5218c5cb2", size = 149095 },
    { url = "https://files.pythonhosted.org/packages/4c/ff/a9a504662452e2d2878512115638966e75633519ec11f25fca3d2049a94a/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c75cb2a3e389853835e84a2d8fb2b81a10645b503eca9bcb98df6b5a43eb8886", size = 152668 },
    { url = "https://files.pythonhosted.org/packages/6c/71/189996b6d9a4b932564701628af5cee6716733e9165af1d5e1b285c530ed/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:09b26ae6b1abf0d27570633b2b078a2a20419c99d66fb2823173d73f188ce601", size = 150073 },
    { url = "https://files.pythonhosted.org/packages/e4/93/946a86ce20790e11312c87c75ba68d5f6ad2208cfb52b2d6a2c32840d922/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa88b843d6e211393a37219e6a1c1df99d35e8fd90446f1118f4216e307e48cd", size = 145732 },
    { url = "https://files.pythonhosted.org/packages/cd/e5/131d2fb1b0dddafc37be4f3a2fa79aa4c037368be9423061dccadfd90091/charset_normalizer-3.4.1-cp313-cp313-win32.whl", hash = "sha256:eb8178fe3dba6450a3e024e95ac49ed3400e506fd4e9e5c32d30adda88cbd407", size = 95391 },
    { url = "https://files.pythonhosted.org/packages/27/f2/4f9a69cc7712b9b5ad8fdb87039fd89abba997ad5cbe690d1835d40405b0/charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:b1ac5992a838106edb89654e0aebfc24f5848ae2547d22c2c3f66454daa11971", size = 102702 },
    { url = "https://files.pythonhosted.org/packages/0e/f6/65ecc6878a89bb1c23a086ea335ad4bf21a588990c3f535a227b9eea9108/charset_normalizer-3.4.1-py3-none-any.whl", hash = "sha256:d98b1668f06378c6dbefec3b92299716b931cd4e6061f3c875a71ced1780ab85", size = 49767 },
]

[[package]]
name = "click"
version = "8.1.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "platform_system == 'Windows'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/96/d3/f04c7bfcf5c1862a2a5b845c6b2b360488cf47af55dfa79c98f6a6bf98b5/click-8.1.7.tar.gz", hash = "sha256:ca9853ad459e787e2192211578cc907e7594e294c7ccc834310722b41b9ca6de", size = 336121 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl", hash = "sha256:ae74fb96c20a0277a1d615f1e4d73c8414f5a98db8b799a7931d1582f3390c28", size = 97941 },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "cryptography"
version = "43.0.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cffi", marker = "platform_python_implementation != 'PyPy'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0d/05/07b55d1fa21ac18c3a8c79f764e2514e6f6a9698f1be44994f5adf0d29db/cryptography-43.0.3.tar.gz", hash = "sha256:315b9001266a492a6ff443b61238f956b214dbec9910a081ba5b6646a055a805", size = 686989 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1f/f3/01fdf26701a26f4b4dbc337a26883ad5bccaa6f1bbbdd29cd89e22f18a1c/cryptography-43.0.3-cp37-abi3-macosx_10_9_universal2.whl", hash = "sha256:bf7a1932ac4176486eab36a19ed4c0492da5d97123f1406cf15e41b05e787d2e", size = 6225303 },
    { url = "https://files.pythonhosted.org/packages/a3/01/4896f3d1b392025d4fcbecf40fdea92d3df8662123f6835d0af828d148fd/cryptography-43.0.3-cp37-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:63efa177ff54aec6e1c0aefaa1a241232dcd37413835a9b674b6e3f0ae2bfd3e", size = 3760905 },
    { url = "https://files.pythonhosted.org/packages/0a/be/f9a1f673f0ed4b7f6c643164e513dbad28dd4f2dcdf5715004f172ef24b6/cryptography-43.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7e1ce50266f4f70bf41a2c6dc4358afadae90e2a1e5342d3c08883df1675374f", size = 3977271 },
    { url = "https://files.pythonhosted.org/packages/4e/49/80c3a7b5514d1b416d7350830e8c422a4d667b6d9b16a9392ebfd4a5388a/cryptography-43.0.3-cp37-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:443c4a81bb10daed9a8f334365fe52542771f25aedaf889fd323a853ce7377d6", size = 3746606 },
    { url = "https://files.pythonhosted.org/packages/0e/16/a28ddf78ac6e7e3f25ebcef69ab15c2c6be5ff9743dd0709a69a4f968472/cryptography-43.0.3-cp37-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:74f57f24754fe349223792466a709f8e0c093205ff0dca557af51072ff47ab18", size = 3986484 },
    { url = "https://files.pythonhosted.org/packages/01/f5/69ae8da70c19864a32b0315049866c4d411cce423ec169993d0434218762/cryptography-43.0.3-cp37-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:9762ea51a8fc2a88b70cf2995e5675b38d93bf36bd67d91721c309df184f49bd", size = 3852131 },
    { url = "https://files.pythonhosted.org/packages/fd/db/e74911d95c040f9afd3612b1f732e52b3e517cb80de8bf183be0b7d413c6/cryptography-43.0.3-cp37-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:81ef806b1fef6b06dcebad789f988d3b37ccaee225695cf3e07648eee0fc6b73", size = 4075647 },
    { url = "https://files.pythonhosted.org/packages/56/48/7b6b190f1462818b324e674fa20d1d5ef3e24f2328675b9b16189cbf0b3c/cryptography-43.0.3-cp37-abi3-win32.whl", hash = "sha256:cbeb489927bd7af4aa98d4b261af9a5bc025bd87f0e3547e11584be9e9427be2", size = 2623873 },
    { url = "https://files.pythonhosted.org/packages/eb/b1/0ebff61a004f7f89e7b65ca95f2f2375679d43d0290672f7713ee3162aff/cryptography-43.0.3-cp37-abi3-win_amd64.whl", hash = "sha256:f46304d6f0c6ab8e52770addfa2fc41e6629495548862279641972b6215451cd", size = 3068039 },
    { url = "https://files.pythonhosted.org/packages/30/d5/c8b32c047e2e81dd172138f772e81d852c51f0f2ad2ae8a24f1122e9e9a7/cryptography-43.0.3-cp39-abi3-macosx_10_9_universal2.whl", hash = "sha256:8ac43ae87929a5982f5948ceda07001ee5e83227fd69cf55b109144938d96984", size = 6222984 },
    { url = "https://files.pythonhosted.org/packages/2f/78/55356eb9075d0be6e81b59f45c7b48df87f76a20e73893872170471f3ee8/cryptography-43.0.3-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:846da004a5804145a5f441b8530b4bf35afbf7da70f82409f151695b127213d5", size = 3762968 },
    { url = "https://files.pythonhosted.org/packages/2a/2c/488776a3dc843f95f86d2f957ca0fc3407d0242b50bede7fad1e339be03f/cryptography-43.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0f996e7268af62598f2fc1204afa98a3b5712313a55c4c9d434aef49cadc91d4", size = 3977754 },
    { url = "https://files.pythonhosted.org/packages/7c/04/2345ca92f7a22f601a9c62961741ef7dd0127c39f7310dffa0041c80f16f/cryptography-43.0.3-cp39-abi3-manylinux_2_28_aarch64.whl", hash = "sha256:f7b178f11ed3664fd0e995a47ed2b5ff0a12d893e41dd0494f406d1cf555cab7", size = 3749458 },
    { url = "https://files.pythonhosted.org/packages/ac/25/e715fa0bc24ac2114ed69da33adf451a38abb6f3f24ec207908112e9ba53/cryptography-43.0.3-cp39-abi3-manylinux_2_28_x86_64.whl", hash = "sha256:c2e6fc39c4ab499049df3bdf567f768a723a5e8464816e8f009f121a5a9f4405", size = 3988220 },
    { url = "https://files.pythonhosted.org/packages/21/ce/b9c9ff56c7164d8e2edfb6c9305045fbc0df4508ccfdb13ee66eb8c95b0e/cryptography-43.0.3-cp39-abi3-musllinux_1_2_aarch64.whl", hash = "sha256:e1be4655c7ef6e1bbe6b5d0403526601323420bcf414598955968c9ef3eb7d16", size = 3853898 },
    { url = "https://files.pythonhosted.org/packages/2a/33/b3682992ab2e9476b9c81fff22f02c8b0a1e6e1d49ee1750a67d85fd7ed2/cryptography-43.0.3-cp39-abi3-musllinux_1_2_x86_64.whl", hash = "sha256:df6b6c6d742395dd77a23ea3728ab62f98379eff8fb61be2744d4679ab678f73", size = 4076592 },
    { url = "https://files.pythonhosted.org/packages/81/1e/ffcc41b3cebd64ca90b28fd58141c5f68c83d48563c88333ab660e002cd3/cryptography-43.0.3-cp39-abi3-win32.whl", hash = "sha256:d56e96520b1020449bbace2b78b603442e7e378a9b3bd68de65c782db1507995", size = 2623145 },
    { url = "https://files.pythonhosted.org/packages/87/5c/3dab83cc4aba1f4b0e733e3f0c3e7d4386440d660ba5b1e3ff995feb734d/cryptography-43.0.3-cp39-abi3-win_amd64.whl", hash = "sha256:0c580952eef9bf68c4747774cde7ec1d85a6e61de97281f2dba83c7d2c806362", size = 3068026 },
]

[[package]]
name = "distro"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fc/f8/98eea607f65de6527f8a2e8885fc8015d3e6f5775df186e443e0964a11c3/distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed", size = 60722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2", size = 20277 },
]

[[package]]
name = "dydantic"
version = "0.0.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
]
sdist = { url = "https://files.pythonhosted.org/packages/08/c5/2d097e5a4816b15186c1ae06c5cfe3c332e69a0f3556dc6cee2d370acf2a/dydantic-0.0.8.tar.gz", hash = "sha256:14a31d4cdfce314ce3e69e8f8c7c46cbc26ce3ce4485de0832260386c612942f", size = 8115 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/7c/a1b120141a300853d82291faf0ba1a95133fa390e4b7d773647b69c8c0f4/dydantic-0.0.8-py3-none-any.whl", hash = "sha256:cd0a991f523bd8632699872f1c0c4278415dd04783e36adec5428defa0afb721", size = 8637 },
]

[[package]]
name = "fastapi"
version = "0.115.6"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "starlette" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/93/72/d83b98cd106541e8f5e5bfab8ef2974ab45a62e8a6c5b5e6940f26d2ed4b/fastapi-0.115.6.tar.gz", hash = "sha256:9ec46f7addc14ea472958a96aae5b5de65f39721a46aaf5705c480d9a8b76654", size = 301336 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/52/b3/7e4df40e585df024fac2f80d1a2d579c854ac37109675db2b0cc22c0bb9e/fastapi-0.115.6-py3-none-any.whl", hash = "sha256:e9240b29e36fa8f4bb7290316988e90c381e5092e0cbe84e7818cc3713bcf305", size = 94843 },
]

[[package]]
name = "frozenlist"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8f/ed/0f4cec13a93c02c47ec32d81d11c0c1efbadf4a471e3f3ce7cad366cbbd3/frozenlist-1.5.0.tar.gz", hash = "sha256:81d5af29e61b9c8348e876d442253723928dce6433e0e76cd925cd83f1b4b817", size = 39930 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/79/43/0bed28bf5eb1c9e4301003b74453b8e7aa85fb293b31dde352aac528dafc/frozenlist-1.5.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:fd74520371c3c4175142d02a976aee0b4cb4a7cc912a60586ffd8d5929979b30", size = 94987 },
    { url = "https://files.pythonhosted.org/packages/bb/bf/b74e38f09a246e8abbe1e90eb65787ed745ccab6eaa58b9c9308e052323d/frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:2f3f7a0fbc219fb4455264cae4d9f01ad41ae6ee8524500f381de64ffaa077d5", size = 54584 },
    { url = "https://files.pythonhosted.org/packages/2c/31/ab01375682f14f7613a1ade30149f684c84f9b8823a4391ed950c8285656/frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:f47c9c9028f55a04ac254346e92977bf0f166c483c74b4232bee19a6697e4778", size = 52499 },
    { url = "https://files.pythonhosted.org/packages/98/a8/d0ac0b9276e1404f58fec3ab6e90a4f76b778a49373ccaf6a563f100dfbc/frozenlist-1.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0996c66760924da6e88922756d99b47512a71cfd45215f3570bf1e0b694c206a", size = 276357 },
    { url = "https://files.pythonhosted.org/packages/ad/c9/c7761084fa822f07dac38ac29f841d4587570dd211e2262544aa0b791d21/frozenlist-1.5.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a2fe128eb4edeabe11896cb6af88fca5346059f6c8d807e3b910069f39157869", size = 287516 },
    { url = "https://files.pythonhosted.org/packages/a1/ff/cd7479e703c39df7bdab431798cef89dc75010d8aa0ca2514c5b9321db27/frozenlist-1.5.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1a8ea951bbb6cacd492e3948b8da8c502a3f814f5d20935aae74b5df2b19cf3d", size = 283131 },
    { url = "https://files.pythonhosted.org/packages/59/a0/370941beb47d237eca4fbf27e4e91389fd68699e6f4b0ebcc95da463835b/frozenlist-1.5.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:de537c11e4aa01d37db0d403b57bd6f0546e71a82347a97c6a9f0dcc532b3a45", size = 261320 },
    { url = "https://files.pythonhosted.org/packages/b8/5f/c10123e8d64867bc9b4f2f510a32042a306ff5fcd7e2e09e5ae5100ee333/frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c2623347b933fcb9095841f1cc5d4ff0b278addd743e0e966cb3d460278840d", size = 274877 },
    { url = "https://files.pythonhosted.org/packages/fa/79/38c505601ae29d4348f21706c5d89755ceded02a745016ba2f58bd5f1ea6/frozenlist-1.5.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:cee6798eaf8b1416ef6909b06f7dc04b60755206bddc599f52232606e18179d3", size = 269592 },
    { url = "https://files.pythonhosted.org/packages/19/e2/39f3a53191b8204ba9f0bb574b926b73dd2efba2a2b9d2d730517e8f7622/frozenlist-1.5.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:f5f9da7f5dbc00a604fe74aa02ae7c98bcede8a3b8b9666f9f86fc13993bc71a", size = 265934 },
    { url = "https://files.pythonhosted.org/packages/d5/c9/3075eb7f7f3a91f1a6b00284af4de0a65a9ae47084930916f5528144c9dd/frozenlist-1.5.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:90646abbc7a5d5c7c19461d2e3eeb76eb0b204919e6ece342feb6032c9325ae9", size = 283859 },
    { url = "https://files.pythonhosted.org/packages/05/f5/549f44d314c29408b962fa2b0e69a1a67c59379fb143b92a0a065ffd1f0f/frozenlist-1.5.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:bdac3c7d9b705d253b2ce370fde941836a5f8b3c5c2b8fd70940a3ea3af7f4f2", size = 287560 },
    { url = "https://files.pythonhosted.org/packages/9d/f8/cb09b3c24a3eac02c4c07a9558e11e9e244fb02bf62c85ac2106d1eb0c0b/frozenlist-1.5.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:03d33c2ddbc1816237a67f66336616416e2bbb6beb306e5f890f2eb22b959cdf", size = 277150 },
    { url = "https://files.pythonhosted.org/packages/37/48/38c2db3f54d1501e692d6fe058f45b6ad1b358d82cd19436efab80cfc965/frozenlist-1.5.0-cp311-cp311-win32.whl", hash = "sha256:237f6b23ee0f44066219dae14c70ae38a63f0440ce6750f868ee08775073f942", size = 45244 },
    { url = "https://files.pythonhosted.org/packages/ca/8c/2ddffeb8b60a4bce3b196c32fcc30d8830d4615e7b492ec2071da801b8ad/frozenlist-1.5.0-cp311-cp311-win_amd64.whl", hash = "sha256:0cc974cc93d32c42e7b0f6cf242a6bd941c57c61b618e78b6c0a96cb72788c1d", size = 51634 },
    { url = "https://files.pythonhosted.org/packages/79/73/fa6d1a96ab7fd6e6d1c3500700963eab46813847f01ef0ccbaa726181dd5/frozenlist-1.5.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:31115ba75889723431aa9a4e77d5f398f5cf976eea3bdf61749731f62d4a4a21", size = 94026 },
    { url = "https://files.pythonhosted.org/packages/ab/04/ea8bf62c8868b8eada363f20ff1b647cf2e93377a7b284d36062d21d81d1/frozenlist-1.5.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7437601c4d89d070eac8323f121fcf25f88674627505334654fd027b091db09d", size = 54150 },
    { url = "https://files.pythonhosted.org/packages/d0/9a/8e479b482a6f2070b26bda572c5e6889bb3ba48977e81beea35b5ae13ece/frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:7948140d9f8ece1745be806f2bfdf390127cf1a763b925c4a805c603df5e697e", size = 51927 },
    { url = "https://files.pythonhosted.org/packages/e3/12/2aad87deb08a4e7ccfb33600871bbe8f0e08cb6d8224371387f3303654d7/frozenlist-1.5.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:feeb64bc9bcc6b45c6311c9e9b99406660a9c05ca8a5b30d14a78555088b0b3a", size = 282647 },
    { url = "https://files.pythonhosted.org/packages/77/f2/07f06b05d8a427ea0060a9cef6e63405ea9e0d761846b95ef3fb3be57111/frozenlist-1.5.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:683173d371daad49cffb8309779e886e59c2f369430ad28fe715f66d08d4ab1a", size = 289052 },
    { url = "https://files.pythonhosted.org/packages/bd/9f/8bf45a2f1cd4aa401acd271b077989c9267ae8463e7c8b1eb0d3f561b65e/frozenlist-1.5.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7d57d8f702221405a9d9b40f9da8ac2e4a1a8b5285aac6100f3393675f0a85ee", size = 291719 },
    { url = "https://files.pythonhosted.org/packages/41/d1/1f20fd05a6c42d3868709b7604c9f15538a29e4f734c694c6bcfc3d3b935/frozenlist-1.5.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:30c72000fbcc35b129cb09956836c7d7abf78ab5416595e4857d1cae8d6251a6", size = 267433 },
    { url = "https://files.pythonhosted.org/packages/af/f2/64b73a9bb86f5a89fb55450e97cd5c1f84a862d4ff90d9fd1a73ab0f64a5/frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:000a77d6034fbad9b6bb880f7ec073027908f1b40254b5d6f26210d2dab1240e", size = 283591 },
    { url = "https://files.pythonhosted.org/packages/29/e2/ffbb1fae55a791fd6c2938dd9ea779509c977435ba3940b9f2e8dc9d5316/frozenlist-1.5.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:5d7f5a50342475962eb18b740f3beecc685a15b52c91f7d975257e13e029eca9", size = 273249 },
    { url = "https://files.pythonhosted.org/packages/2e/6e/008136a30798bb63618a114b9321b5971172a5abddff44a100c7edc5ad4f/frozenlist-1.5.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:87f724d055eb4785d9be84e9ebf0f24e392ddfad00b3fe036e43f489fafc9039", size = 271075 },
    { url = "https://files.pythonhosted.org/packages/ae/f0/4e71e54a026b06724cec9b6c54f0b13a4e9e298cc8db0f82ec70e151f5ce/frozenlist-1.5.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:6e9080bb2fb195a046e5177f10d9d82b8a204c0736a97a153c2466127de87784", size = 285398 },
    { url = "https://files.pythonhosted.org/packages/4d/36/70ec246851478b1c0b59f11ef8ade9c482ff447c1363c2bd5fad45098b12/frozenlist-1.5.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9b93d7aaa36c966fa42efcaf716e6b3900438632a626fb09c049f6a2f09fc631", size = 294445 },
    { url = "https://files.pythonhosted.org/packages/37/e0/47f87544055b3349b633a03c4d94b405956cf2437f4ab46d0928b74b7526/frozenlist-1.5.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:52ef692a4bc60a6dd57f507429636c2af8b6046db8b31b18dac02cbc8f507f7f", size = 280569 },
    { url = "https://files.pythonhosted.org/packages/f9/7c/490133c160fb6b84ed374c266f42800e33b50c3bbab1652764e6e1fc498a/frozenlist-1.5.0-cp312-cp312-win32.whl", hash = "sha256:29d94c256679247b33a3dc96cce0f93cbc69c23bf75ff715919332fdbb6a32b8", size = 44721 },
    { url = "https://files.pythonhosted.org/packages/b1/56/4e45136ffc6bdbfa68c29ca56ef53783ef4c2fd395f7cbf99a2624aa9aaa/frozenlist-1.5.0-cp312-cp312-win_amd64.whl", hash = "sha256:8969190d709e7c48ea386db202d708eb94bdb29207a1f269bab1196ce0dcca1f", size = 51329 },
    { url = "https://files.pythonhosted.org/packages/da/3b/915f0bca8a7ea04483622e84a9bd90033bab54bdf485479556c74fd5eaf5/frozenlist-1.5.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:7a1a048f9215c90973402e26c01d1cff8a209e1f1b53f72b95c13db61b00f953", size = 91538 },
    { url = "https://files.pythonhosted.org/packages/c7/d1/a7c98aad7e44afe5306a2b068434a5830f1470675f0e715abb86eb15f15b/frozenlist-1.5.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dd47a5181ce5fcb463b5d9e17ecfdb02b678cca31280639255ce9d0e5aa67af0", size = 52849 },
    { url = "https://files.pythonhosted.org/packages/3a/c8/76f23bf9ab15d5f760eb48701909645f686f9c64fbb8982674c241fbef14/frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1431d60b36d15cda188ea222033eec8e0eab488f39a272461f2e6d9e1a8e63c2", size = 50583 },
    { url = "https://files.pythonhosted.org/packages/1f/22/462a3dd093d11df623179d7754a3b3269de3b42de2808cddef50ee0f4f48/frozenlist-1.5.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6482a5851f5d72767fbd0e507e80737f9c8646ae7fd303def99bfe813f76cf7f", size = 265636 },
    { url = "https://files.pythonhosted.org/packages/80/cf/e075e407fc2ae7328155a1cd7e22f932773c8073c1fc78016607d19cc3e5/frozenlist-1.5.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:44c49271a937625619e862baacbd037a7ef86dd1ee215afc298a417ff3270608", size = 270214 },
    { url = "https://files.pythonhosted.org/packages/a1/58/0642d061d5de779f39c50cbb00df49682832923f3d2ebfb0fedf02d05f7f/frozenlist-1.5.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:12f78f98c2f1c2429d42e6a485f433722b0061d5c0b0139efa64f396efb5886b", size = 273905 },
    { url = "https://files.pythonhosted.org/packages/ab/66/3fe0f5f8f2add5b4ab7aa4e199f767fd3b55da26e3ca4ce2cc36698e50c4/frozenlist-1.5.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ce3aa154c452d2467487765e3adc730a8c153af77ad84096bc19ce19a2400840", size = 250542 },
    { url = "https://files.pythonhosted.org/packages/f6/b8/260791bde9198c87a465224e0e2bb62c4e716f5d198fc3a1dacc4895dbd1/frozenlist-1.5.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9b7dc0c4338e6b8b091e8faf0db3168a37101943e687f373dce00959583f7439", size = 267026 },
    { url = "https://files.pythonhosted.org/packages/2e/a4/3d24f88c527f08f8d44ade24eaee83b2627793fa62fa07cbb7ff7a2f7d42/frozenlist-1.5.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:45e0896250900b5aa25180f9aec243e84e92ac84bd4a74d9ad4138ef3f5c97de", size = 257690 },
    { url = "https://files.pythonhosted.org/packages/de/9a/d311d660420b2beeff3459b6626f2ab4fb236d07afbdac034a4371fe696e/frozenlist-1.5.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:561eb1c9579d495fddb6da8959fd2a1fca2c6d060d4113f5844b433fc02f2641", size = 253893 },
    { url = "https://files.pythonhosted.org/packages/c6/23/e491aadc25b56eabd0f18c53bb19f3cdc6de30b2129ee0bc39cd387cd560/frozenlist-1.5.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:df6e2f325bfee1f49f81aaac97d2aa757c7646534a06f8f577ce184afe2f0a9e", size = 267006 },
    { url = "https://files.pythonhosted.org/packages/08/c4/ab918ce636a35fb974d13d666dcbe03969592aeca6c3ab3835acff01f79c/frozenlist-1.5.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:140228863501b44b809fb39ec56b5d4071f4d0aa6d216c19cbb08b8c5a7eadb9", size = 276157 },
    { url = "https://files.pythonhosted.org/packages/c0/29/3b7a0bbbbe5a34833ba26f686aabfe982924adbdcafdc294a7a129c31688/frozenlist-1.5.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:7707a25d6a77f5d27ea7dc7d1fc608aa0a478193823f88511ef5e6b8a48f9d03", size = 264642 },
    { url = "https://files.pythonhosted.org/packages/ab/42/0595b3dbffc2e82d7fe658c12d5a5bafcd7516c6bf2d1d1feb5387caa9c1/frozenlist-1.5.0-cp313-cp313-win32.whl", hash = "sha256:31a9ac2b38ab9b5a8933b693db4939764ad3f299fcaa931a3e605bc3460e693c", size = 44914 },
    { url = "https://files.pythonhosted.org/packages/17/c4/b7db1206a3fea44bf3b838ca61deb6f74424a8a5db1dd53ecb21da669be6/frozenlist-1.5.0-cp313-cp313-win_amd64.whl", hash = "sha256:11aabdd62b8b9c4b84081a3c246506d1cddd2dd93ff0ad53ede5defec7886b28", size = 51167 },
    { url = "https://files.pythonhosted.org/packages/c6/c8/a5be5b7550c10858fcf9b0ea054baccab474da77d37f1e828ce043a3a5d4/frozenlist-1.5.0-py3-none-any.whl", hash = "sha256:d994863bba198a4a518b467bb971c56e1db3f180a25c6cf7bb1949c267f748c3", size = 11901 },
]

[[package]]
name = "greenlet"
version = "3.1.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/2f/ff/df5fede753cc10f6a5be0931204ea30c35fa2f2ea7a35b25bdaf4fe40e46/greenlet-3.1.1.tar.gz", hash = "sha256:4ce3ac6cdb6adf7946475d7ef31777c26d94bccc377e070a7986bd2d5c515467", size = 186022 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/28/62/1c2665558618553c42922ed47a4e6d6527e2fa3516a8256c2f431c5d0441/greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl", hash = "sha256:e4d333e558953648ca09d64f13e6d8f0523fa705f51cae3f03b5983489958c70", size = 272479 },
    { url = "https://files.pythonhosted.org/packages/76/9d/421e2d5f07285b6e4e3a676b016ca781f63cfe4a0cd8eaecf3fd6f7a71ae/greenlet-3.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:09fc016b73c94e98e29af67ab7b9a879c307c6731a2c9da0db5a7d9b7edd1159", size = 640404 },
    { url = "https://files.pythonhosted.org/packages/e5/de/6e05f5c59262a584e502dd3d261bbdd2c97ab5416cc9c0b91ea38932a901/greenlet-3.1.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d5e975ca70269d66d17dd995dafc06f1b06e8cb1ec1e9ed54c1d1e4a7c4cf26e", size = 652813 },
    { url = "https://files.pythonhosted.org/packages/49/93/d5f93c84241acdea15a8fd329362c2c71c79e1a507c3f142a5d67ea435ae/greenlet-3.1.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3b2813dc3de8c1ee3f924e4d4227999285fd335d1bcc0d2be6dc3f1f6a318ec1", size = 648517 },
    { url = "https://files.pythonhosted.org/packages/15/85/72f77fc02d00470c86a5c982b8daafdf65d38aefbbe441cebff3bf7037fc/greenlet-3.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e347b3bfcf985a05e8c0b7d462ba6f15b1ee1c909e2dcad795e49e91b152c383", size = 647831 },
    { url = "https://files.pythonhosted.org/packages/f7/4b/1c9695aa24f808e156c8f4813f685d975ca73c000c2a5056c514c64980f6/greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9e8f8c9cb53cdac7ba9793c276acd90168f416b9ce36799b9b885790f8ad6c0a", size = 602413 },
    { url = "https://files.pythonhosted.org/packages/76/70/ad6e5b31ef330f03b12559d19fda2606a522d3849cde46b24f223d6d1619/greenlet-3.1.1-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:62ee94988d6b4722ce0028644418d93a52429e977d742ca2ccbe1c4f4a792511", size = 1129619 },
    { url = "https://files.pythonhosted.org/packages/f4/fb/201e1b932e584066e0f0658b538e73c459b34d44b4bd4034f682423bc801/greenlet-3.1.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:1776fd7f989fc6b8d8c8cb8da1f6b82c5814957264d1f6cf818d475ec2bf6395", size = 1155198 },
    { url = "https://files.pythonhosted.org/packages/12/da/b9ed5e310bb8b89661b80cbcd4db5a067903bbcd7fc854923f5ebb4144f0/greenlet-3.1.1-cp311-cp311-win_amd64.whl", hash = "sha256:48ca08c771c268a768087b408658e216133aecd835c0ded47ce955381105ba39", size = 298930 },
    { url = "https://files.pythonhosted.org/packages/7d/ec/bad1ac26764d26aa1353216fcbfa4670050f66d445448aafa227f8b16e80/greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl", hash = "sha256:4afe7ea89de619adc868e087b4d2359282058479d7cfb94970adf4b55284574d", size = 274260 },
    { url = "https://files.pythonhosted.org/packages/66/d4/c8c04958870f482459ab5956c2942c4ec35cac7fe245527f1039837c17a9/greenlet-3.1.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f406b22b7c9a9b4f8aa9d2ab13d6ae0ac3e85c9a809bd590ad53fed2bf70dc79", size = 649064 },
    { url = "https://files.pythonhosted.org/packages/51/41/467b12a8c7c1303d20abcca145db2be4e6cd50a951fa30af48b6ec607581/greenlet-3.1.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c3a701fe5a9695b238503ce5bbe8218e03c3bcccf7e204e455e7462d770268aa", size = 663420 },
    { url = "https://files.pythonhosted.org/packages/27/8f/2a93cd9b1e7107d5c7b3b7816eeadcac2ebcaf6d6513df9abaf0334777f6/greenlet-3.1.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2846930c65b47d70b9d178e89c7e1a69c95c1f68ea5aa0a58646b7a96df12441", size = 658035 },
    { url = "https://files.pythonhosted.org/packages/57/5c/7c6f50cb12be092e1dccb2599be5a942c3416dbcfb76efcf54b3f8be4d8d/greenlet-3.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:99cfaa2110534e2cf3ba31a7abcac9d328d1d9f1b95beede58294a60348fba36", size = 660105 },
    { url = "https://files.pythonhosted.org/packages/f1/66/033e58a50fd9ec9df00a8671c74f1f3a320564c6415a4ed82a1c651654ba/greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:1443279c19fca463fc33e65ef2a935a5b09bb90f978beab37729e1c3c6c25fe9", size = 613077 },
    { url = "https://files.pythonhosted.org/packages/19/c5/36384a06f748044d06bdd8776e231fadf92fc896bd12cb1c9f5a1bda9578/greenlet-3.1.1-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:b7cede291382a78f7bb5f04a529cb18e068dd29e0fb27376074b6d0317bf4dd0", size = 1135975 },
    { url = "https://files.pythonhosted.org/packages/38/f9/c0a0eb61bdf808d23266ecf1d63309f0e1471f284300ce6dac0ae1231881/greenlet-3.1.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:23f20bb60ae298d7d8656c6ec6db134bca379ecefadb0b19ce6f19d1f232a942", size = 1163955 },
    { url = "https://files.pythonhosted.org/packages/43/21/a5d9df1d21514883333fc86584c07c2b49ba7c602e670b174bd73cfc9c7f/greenlet-3.1.1-cp312-cp312-win_amd64.whl", hash = "sha256:7124e16b4c55d417577c2077be379514321916d5790fa287c9ed6f23bd2ffd01", size = 299655 },
    { url = "https://files.pythonhosted.org/packages/f3/57/0db4940cd7bb461365ca8d6fd53e68254c9dbbcc2b452e69d0d41f10a85e/greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl", hash = "sha256:05175c27cb459dcfc05d026c4232f9de8913ed006d42713cb8a5137bd49375f1", size = 272990 },
    { url = "https://files.pythonhosted.org/packages/1c/ec/423d113c9f74e5e402e175b157203e9102feeb7088cee844d735b28ef963/greenlet-3.1.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:935e943ec47c4afab8965954bf49bfa639c05d4ccf9ef6e924188f762145c0ff", size = 649175 },
    { url = "https://files.pythonhosted.org/packages/a9/46/ddbd2db9ff209186b7b7c621d1432e2f21714adc988703dbdd0e65155c77/greenlet-3.1.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:667a9706c970cb552ede35aee17339a18e8f2a87a51fba2ed39ceeeb1004798a", size = 663425 },
    { url = "https://files.pythonhosted.org/packages/bc/f9/9c82d6b2b04aa37e38e74f0c429aece5eeb02bab6e3b98e7db89b23d94c6/greenlet-3.1.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b8a678974d1f3aa55f6cc34dc480169d58f2e6d8958895d68845fa4ab566509e", size = 657736 },
    { url = "https://files.pythonhosted.org/packages/d9/42/b87bc2a81e3a62c3de2b0d550bf91a86939442b7ff85abb94eec3fc0e6aa/greenlet-3.1.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:efc0f674aa41b92da8c49e0346318c6075d734994c3c4e4430b1c3f853e498e4", size = 660347 },
    { url = "https://files.pythonhosted.org/packages/37/fa/71599c3fd06336cdc3eac52e6871cfebab4d9d70674a9a9e7a482c318e99/greenlet-3.1.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0153404a4bb921f0ff1abeb5ce8a5131da56b953eda6e14b88dc6bbc04d2049e", size = 615583 },
    { url = "https://files.pythonhosted.org/packages/4e/96/e9ef85de031703ee7a4483489b40cf307f93c1824a02e903106f2ea315fe/greenlet-3.1.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:275f72decf9932639c1c6dd1013a1bc266438eb32710016a1c742df5da6e60a1", size = 1133039 },
    { url = "https://files.pythonhosted.org/packages/87/76/b2b6362accd69f2d1889db61a18c94bc743e961e3cab344c2effaa4b4a25/greenlet-3.1.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:c4aab7f6381f38a4b42f269057aee279ab0fc7bf2e929e3d4abfae97b682a12c", size = 1160716 },
    { url = "https://files.pythonhosted.org/packages/1f/1b/54336d876186920e185066d8c3024ad55f21d7cc3683c856127ddb7b13ce/greenlet-3.1.1-cp313-cp313-win_amd64.whl", hash = "sha256:b42703b1cf69f2aa1df7d1030b9d77d3e584a70755674d60e710f0af570f3761", size = 299490 },
    { url = "https://files.pythonhosted.org/packages/5f/17/bea55bf36990e1638a2af5ba10c1640273ef20f627962cf97107f1e5d637/greenlet-3.1.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f1695e76146579f8c06c1509c7ce4dfe0706f49c6831a817ac04eebb2fd02011", size = 643731 },
    { url = "https://files.pythonhosted.org/packages/78/d2/aa3d2157f9ab742a08e0fd8f77d4699f37c22adfbfeb0c610a186b5f75e0/greenlet-3.1.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:7876452af029456b3f3549b696bb36a06db7c90747740c5302f74a9e9fa14b13", size = 649304 },
    { url = "https://files.pythonhosted.org/packages/f1/8e/d0aeffe69e53ccff5a28fa86f07ad1d2d2d6537a9506229431a2a02e2f15/greenlet-3.1.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4ead44c85f8ab905852d3de8d86f6f8baf77109f9da589cb4fa142bd3b57b475", size = 646537 },
    { url = "https://files.pythonhosted.org/packages/05/79/e15408220bbb989469c8871062c97c6c9136770657ba779711b90870d867/greenlet-3.1.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8320f64b777d00dd7ccdade271eaf0cad6636343293a25074cc5566160e4de7b", size = 642506 },
    { url = "https://files.pythonhosted.org/packages/18/87/470e01a940307796f1d25f8167b551a968540fbe0551c0ebb853cb527dd6/greenlet-3.1.1-cp313-cp313t-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:6510bf84a6b643dabba74d3049ead221257603a253d0a9873f55f6a59a65f822", size = 602753 },
    { url = "https://files.pythonhosted.org/packages/e2/72/576815ba674eddc3c25028238f74d7b8068902b3968cbe456771b166455e/greenlet-3.1.1-cp313-cp313t-musllinux_1_1_aarch64.whl", hash = "sha256:04b013dc07c96f83134b1e99888e7a79979f1a247e2a9f59697fa14b5862ed01", size = 1122731 },
    { url = "https://files.pythonhosted.org/packages/ac/38/08cc303ddddc4b3d7c628c3039a61a3aae36c241ed01393d00c2fd663473/greenlet-3.1.1-cp313-cp313t-musllinux_1_1_x86_64.whl", hash = "sha256:411f015496fec93c1c8cd4e5238da364e1da7a124bcb293f085bf2860c32c6f6", size = 1142112 },
]

[[package]]
name = "h11"
version = "0.14.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f5/38/3af3d3633a34a3316095b39c8e8fb4853a28a536e55d347bd8d8e9a14b03/h11-0.14.0.tar.gz", hash = "sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d", size = 100418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl", hash = "sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761", size = 58259 },
]

[[package]]
name = "httpcore"
version = "1.0.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6a/41/d7d0a89eb493922c37d343b607bc1b5da7f5be7e383740b4753ad8943e90/httpcore-1.0.7.tar.gz", hash = "sha256:8551cb62a169ec7162ac7be8d4817d561f60e08eaa485234898414bb5a8a0b4c", size = 85196 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl", hash = "sha256:a3fff8f43dc260d5bd363d9f9cf1830fa3a458b332856f34282de498ed420edd", size = 78551 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "jiter"
version = "0.8.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f8/70/90bc7bd3932e651486861df5c8ffea4ca7c77d28e8532ddefe2abc561a53/jiter-0.8.2.tar.gz", hash = "sha256:cd73d3e740666d0e639f678adb176fad25c1bcbdae88d8d7b857e1783bb4212d", size = 163007 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/b0/c1a7caa7f9dc5f1f6cfa08722867790fe2d3645d6e7170ca280e6e52d163/jiter-0.8.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:2dd61c5afc88a4fda7d8b2cf03ae5947c6ac7516d32b7a15bf4b49569a5c076b", size = 303666 },
    { url = "https://files.pythonhosted.org/packages/f5/97/0468bc9eeae43079aaa5feb9267964e496bf13133d469cfdc135498f8dd0/jiter-0.8.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:a6c710d657c8d1d2adbbb5c0b0c6bfcec28fd35bd6b5f016395f9ac43e878a15", size = 311934 },
    { url = "https://files.pythonhosted.org/packages/e5/69/64058e18263d9a5f1e10f90c436853616d5f047d997c37c7b2df11b085ec/jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a9584de0cd306072635fe4b89742bf26feae858a0683b399ad0c2509011b9dc0", size = 335506 },
    { url = "https://files.pythonhosted.org/packages/9d/14/b747f9a77b8c0542141d77ca1e2a7523e854754af2c339ac89a8b66527d6/jiter-0.8.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5a90a923338531b7970abb063cfc087eebae6ef8ec8139762007188f6bc69a9f", size = 355849 },
    { url = "https://files.pythonhosted.org/packages/53/e2/98a08161db7cc9d0e39bc385415890928ff09709034982f48eccfca40733/jiter-0.8.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d21974d246ed0181558087cd9f76e84e8321091ebfb3a93d4c341479a736f099", size = 381700 },
    { url = "https://files.pythonhosted.org/packages/7a/38/1674672954d35bce3b1c9af99d5849f9256ac8f5b672e020ac7821581206/jiter-0.8.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:32475a42b2ea7b344069dc1e81445cfc00b9d0e3ca837f0523072432332e9f74", size = 389710 },
    { url = "https://files.pythonhosted.org/packages/f8/9b/92f9da9a9e107d019bcf883cd9125fa1690079f323f5a9d5c6986eeec3c0/jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8b9931fd36ee513c26b5bf08c940b0ac875de175341cbdd4fa3be109f0492586", size = 345553 },
    { url = "https://files.pythonhosted.org/packages/44/a6/6d030003394e9659cd0d7136bbeabd82e869849ceccddc34d40abbbbb269/jiter-0.8.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:ce0820f4a3a59ddced7fce696d86a096d5cc48d32a4183483a17671a61edfddc", size = 376388 },
    { url = "https://files.pythonhosted.org/packages/ad/8d/87b09e648e4aca5f9af89e3ab3cfb93db2d1e633b2f2931ede8dabd9b19a/jiter-0.8.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:8ffc86ae5e3e6a93765d49d1ab47b6075a9c978a2b3b80f0f32628f39caa0c88", size = 511226 },
    { url = "https://files.pythonhosted.org/packages/77/95/8008ebe4cdc82eac1c97864a8042ca7e383ed67e0ec17bfd03797045c727/jiter-0.8.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:5127dc1abd809431172bc3fbe8168d6b90556a30bb10acd5ded41c3cfd6f43b6", size = 504134 },
    { url = "https://files.pythonhosted.org/packages/26/0d/3056a74de13e8b2562e4d526de6dac2f65d91ace63a8234deb9284a1d24d/jiter-0.8.2-cp311-cp311-win32.whl", hash = "sha256:66227a2c7b575720c1871c8800d3a0122bb8ee94edb43a5685aa9aceb2782d44", size = 203103 },
    { url = "https://files.pythonhosted.org/packages/4e/1e/7f96b798f356e531ffc0f53dd2f37185fac60fae4d6c612bbbd4639b90aa/jiter-0.8.2-cp311-cp311-win_amd64.whl", hash = "sha256:cde031d8413842a1e7501e9129b8e676e62a657f8ec8166e18a70d94d4682855", size = 206717 },
    { url = "https://files.pythonhosted.org/packages/a1/17/c8747af8ea4e045f57d6cfd6fc180752cab9bc3de0e8a0c9ca4e8af333b1/jiter-0.8.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:e6ec2be506e7d6f9527dae9ff4b7f54e68ea44a0ef6b098256ddf895218a2f8f", size = 302027 },
    { url = "https://files.pythonhosted.org/packages/3c/c1/6da849640cd35a41e91085723b76acc818d4b7d92b0b6e5111736ce1dd10/jiter-0.8.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:76e324da7b5da060287c54f2fabd3db5f76468006c811831f051942bf68c9d44", size = 310326 },
    { url = "https://files.pythonhosted.org/packages/06/99/a2bf660d8ccffee9ad7ed46b4f860d2108a148d0ea36043fd16f4dc37e94/jiter-0.8.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:180a8aea058f7535d1c84183c0362c710f4750bef66630c05f40c93c2b152a0f", size = 334242 },
    { url = "https://files.pythonhosted.org/packages/a7/5f/cea1c17864828731f11427b9d1ab7f24764dbd9aaf4648a7f851164d2718/jiter-0.8.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60", size = 356654 },
    { url = "https://files.pythonhosted.org/packages/e9/13/62774b7e5e7f5d5043efe1d0f94ead66e6d0f894ae010adb56b3f788de71/jiter-0.8.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ecff0dc14f409599bbcafa7e470c00b80f17abc14d1405d38ab02e4b42e55b57", size = 379967 },
    { url = "https://files.pythonhosted.org/packages/ec/fb/096b34c553bb0bd3f2289d5013dcad6074948b8d55212aa13a10d44c5326/jiter-0.8.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ffd9fee7d0775ebaba131f7ca2e2d83839a62ad65e8e02fe2bd8fc975cedeb9e", size = 389252 },
    { url = "https://files.pythonhosted.org/packages/17/61/beea645c0bf398ced8b199e377b61eb999d8e46e053bb285c91c3d3eaab0/jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:14601dcac4889e0a1c75ccf6a0e4baf70dbc75041e51bcf8d0e9274519df6887", size = 345490 },
    { url = "https://files.pythonhosted.org/packages/d5/df/834aa17ad5dcc3cf0118821da0a0cf1589ea7db9832589278553640366bc/jiter-0.8.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:92249669925bc1c54fcd2ec73f70f2c1d6a817928480ee1c65af5f6b81cdf12d", size = 376991 },
    { url = "https://files.pythonhosted.org/packages/67/80/87d140399d382fb4ea5b3d56e7ecaa4efdca17cd7411ff904c1517855314/jiter-0.8.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:e725edd0929fa79f8349ab4ec7f81c714df51dc4e991539a578e5018fa4a7152", size = 510822 },
    { url = "https://files.pythonhosted.org/packages/5c/37/3394bb47bac1ad2cb0465601f86828a0518d07828a650722e55268cdb7e6/jiter-0.8.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:bf55846c7b7a680eebaf9c3c48d630e1bf51bdf76c68a5f654b8524335b0ad29", size = 503730 },
    { url = "https://files.pythonhosted.org/packages/f9/e2/253fc1fa59103bb4e3aa0665d6ceb1818df1cd7bf3eb492c4dad229b1cd4/jiter-0.8.2-cp312-cp312-win32.whl", hash = "sha256:7efe4853ecd3d6110301665a5178b9856be7e2a9485f49d91aa4d737ad2ae49e", size = 203375 },
    { url = "https://files.pythonhosted.org/packages/41/69/6d4bbe66b3b3b4507e47aa1dd5d075919ad242b4b1115b3f80eecd443687/jiter-0.8.2-cp312-cp312-win_amd64.whl", hash = "sha256:83c0efd80b29695058d0fd2fa8a556490dbce9804eac3e281f373bbc99045f6c", size = 204740 },
    { url = "https://files.pythonhosted.org/packages/6c/b0/bfa1f6f2c956b948802ef5a021281978bf53b7a6ca54bb126fd88a5d014e/jiter-0.8.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:ca1f08b8e43dc3bd0594c992fb1fd2f7ce87f7bf0d44358198d6da8034afdf84", size = 301190 },
    { url = "https://files.pythonhosted.org/packages/a4/8f/396ddb4e292b5ea57e45ade5dc48229556b9044bad29a3b4b2dddeaedd52/jiter-0.8.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5672a86d55416ccd214c778efccf3266b84f87b89063b582167d803246354be4", size = 309334 },
    { url = "https://files.pythonhosted.org/packages/7f/68/805978f2f446fa6362ba0cc2e4489b945695940656edd844e110a61c98f8/jiter-0.8.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:58dc9bc9767a1101f4e5e22db1b652161a225874d66f0e5cb8e2c7d1c438b587", size = 333918 },
    { url = "https://files.pythonhosted.org/packages/b3/99/0f71f7be667c33403fa9706e5b50583ae5106d96fab997fa7e2f38ee8347/jiter-0.8.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:37b2998606d6dadbb5ccda959a33d6a5e853252d921fec1792fc902351bb4e2c", size = 356057 },
    { url = "https://files.pythonhosted.org/packages/8d/50/a82796e421a22b699ee4d2ce527e5bcb29471a2351cbdc931819d941a167/jiter-0.8.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4ab9a87f3784eb0e098f84a32670cfe4a79cb6512fd8f42ae3d0709f06405d18", size = 379790 },
    { url = "https://files.pythonhosted.org/packages/3c/31/10fb012b00f6d83342ca9e2c9618869ab449f1aa78c8f1b2193a6b49647c/jiter-0.8.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:79aec8172b9e3c6d05fd4b219d5de1ac616bd8da934107325a6c0d0e866a21b6", size = 388285 },
    { url = "https://files.pythonhosted.org/packages/c8/81/f15ebf7de57be488aa22944bf4274962aca8092e4f7817f92ffa50d3ee46/jiter-0.8.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:711e408732d4e9a0208008e5892c2966b485c783cd2d9a681f3eb147cf36c7ef", size = 344764 },
    { url = "https://files.pythonhosted.org/packages/b3/e8/0cae550d72b48829ba653eb348cdc25f3f06f8a62363723702ec18e7be9c/jiter-0.8.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:653cf462db4e8c41995e33d865965e79641ef45369d8a11f54cd30888b7e6ff1", size = 376620 },
    { url = "https://files.pythonhosted.org/packages/b8/50/e5478ff9d82534a944c03b63bc217c5f37019d4a34d288db0f079b13c10b/jiter-0.8.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:9c63eaef32b7bebac8ebebf4dabebdbc6769a09c127294db6babee38e9f405b9", size = 510402 },
    { url = "https://files.pythonhosted.org/packages/8e/1e/3de48bbebbc8f7025bd454cedc8c62378c0e32dd483dece5f4a814a5cb55/jiter-0.8.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:eb21aaa9a200d0a80dacc7a81038d2e476ffe473ffdd9c91eb745d623561de05", size = 503018 },
    { url = "https://files.pythonhosted.org/packages/d5/cd/d5a5501d72a11fe3e5fd65c78c884e5164eefe80077680533919be22d3a3/jiter-0.8.2-cp313-cp313-win32.whl", hash = "sha256:789361ed945d8d42850f919342a8665d2dc79e7e44ca1c97cc786966a21f627a", size = 203190 },
    { url = "https://files.pythonhosted.org/packages/51/bf/e5ca301245ba951447e3ad677a02a64a8845b185de2603dabd83e1e4b9c6/jiter-0.8.2-cp313-cp313-win_amd64.whl", hash = "sha256:ab7f43235d71e03b941c1630f4b6e3055d46b6cb8728a17663eaac9d8e83a865", size = 203551 },
    { url = "https://files.pythonhosted.org/packages/2f/3c/71a491952c37b87d127790dd7a0b1ebea0514c6b6ad30085b16bbe00aee6/jiter-0.8.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:b426f72cd77da3fec300ed3bc990895e2dd6b49e3bfe6c438592a3ba660e41ca", size = 308347 },
    { url = "https://files.pythonhosted.org/packages/a0/4c/c02408042e6a7605ec063daed138e07b982fdb98467deaaf1c90950cf2c6/jiter-0.8.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b2dd880785088ff2ad21ffee205e58a8c1ddabc63612444ae41e5e4b321b39c0", size = 342875 },
    { url = "https://files.pythonhosted.org/packages/91/61/c80ef80ed8a0a21158e289ef70dac01e351d929a1c30cb0f49be60772547/jiter-0.8.2-cp313-cp313t-win_amd64.whl", hash = "sha256:3ac9f578c46f22405ff7f8b1f5848fb753cc4b8377fbec8470a7dc3997ca7566", size = 202374 },
]

[[package]]
name = "jsonpatch"
version = "1.33"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "jsonpointer" },
]
sdist = { url = "https://files.pythonhosted.org/packages/42/78/18813351fe5d63acad16aec57f94ec2b70a09e53ca98145589e185423873/jsonpatch-1.33.tar.gz", hash = "sha256:9fcd4009c41e6d12348b4a0ff2563ba56a2923a7dfee731d004e212e1ee5030c", size = 21699 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl", hash = "sha256:0ae28c0cd062bbd8b8ecc26d7d164fbbea9652a1a3693f3b956c1eae5145dade", size = 12898 },
]

[[package]]
name = "jsonpointer"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6a/0a/eebeb1fa92507ea94016a2a790b93c2ae41a7e18778f85471dc54475ed25/jsonpointer-3.0.0.tar.gz", hash = "sha256:2b2d729f2091522d61c3b31f82e11870f60b68f43fbc705cb76bf4b832af59ef", size = 9114 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/71/92/5e77f98553e9e75130c78900d000368476aed74276eb8ae8796f65f00918/jsonpointer-3.0.0-py2.py3-none-any.whl", hash = "sha256:13e088adc14fca8b6aa8177c044e12701e6ad4b28ff10e65f2267a90109c9942", size = 7595 },
]

[[package]]
name = "jsonschema-rs"
version = "0.20.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f0/ff/904d1eee4e56ea3870bea0dfc5fa9b6ca683f7f6f7ddb2bfbc5d883b6e56/jsonschema_rs-0.20.0.tar.gz", hash = "sha256:f76d52b7755d184844f1bfedc209c5107b5be3b6b2ae8531db4a0e563b8317ae", size = 1345005 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/fe/c3/6a981d26222b443543fd034f4fabf880e5ca56756a14850c355c8cb2e225/jsonschema_rs-0.20.0-cp311-cp311-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:2f76153d57c8c3778829e14867dd6479eaba17f56e2ba42da82494663200b37a", size = 3672455 },
    { url = "https://files.pythonhosted.org/packages/47/10/fd6cf2dbe05f9eeb43aacbc3602b9c2b1c57a5db0641c22ad6f279057755/jsonschema_rs-0.20.0-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:1a2b24871ade985580a3f10cfde2a52673823163813fe311abffb30e0aeb4181", size = 1889540 },
    { url = "https://files.pythonhosted.org/packages/c5/22/21800a92ac5746e33b08022d50a92a890b3483b022ecc05434d61cbf1d22/jsonschema_rs-0.20.0-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:6bf5afaa6e80aa59fae06ca92efb1f8e29b2679f8f8884585272f7c100f04c69", size = 1981680 },
    { url = "https://files.pythonhosted.org/packages/53/66/616e37dfc30eddefae6272c2c86a2f727224af8400dc7fa3630e88fdf1c1/jsonschema_rs-0.20.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:42d180b318653184eb69d9641dfded6b2d59c88db8fb82012d3d20e1582af5cd", size = 1967856 },
    { url = "https://files.pythonhosted.org/packages/9d/9a/4aa337ea5575089e68a412f993da00ec3fde7b6d0a198a554aac9c8302d1/jsonschema_rs-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7aa67bf6338fbce3a2a3f9f408413aad7a72593ed73c846bc2667a20b747327f", size = 1993230 },
    { url = "https://files.pythonhosted.org/packages/ce/fd/c94139510b3b34398dcd7881c30b9f24ca5c08b4c2d0595b5a97ab4ab077/jsonschema_rs-0.20.0-cp311-none-win32.whl", hash = "sha256:55c255ca6ee44177b7b5c773a8edb25af34be0d6a454e602be92cc183d6dcbe7", size = 1667528 },
    { url = "https://files.pythonhosted.org/packages/37/92/d64f14b6269fc8b7786213d17766306589d399005315636e067737e691ab/jsonschema_rs-0.20.0-cp311-none-win_amd64.whl", hash = "sha256:3b6effefdef590854522517b84081214b46a8fba9a5d60c538c6f1ff819609c5", size = 1811925 },
    { url = "https://files.pythonhosted.org/packages/84/59/f26f89680a988dd062cdb08f185c928e8ab6a8eb9f5bf7c9933087140d30/jsonschema_rs-0.20.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:8b9d044ecdef9a15e350a35e950c9797c804f185b547bcb56bb91fef9bf31827", size = 3672264 },
    { url = "https://files.pythonhosted.org/packages/9c/2b/aff741df7165fb8f415e91d6581bdab041f13acd2077855f8701062f021f/jsonschema_rs-0.20.0-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:d43b6a65c159f7aed82dca846f93a4c5ffd013ff0c87a5668a645a81792c8001", size = 1888847 },
    { url = "https://files.pythonhosted.org/packages/05/a0/6a0a32fc67d93c2b9381fa63f70b85953a031bfb90759de9eef4144e3775/jsonschema_rs-0.20.0-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.whl", hash = "sha256:9a6500934be21fe64bd65d9e75a67941b675091b8e2154df0a04f5b57073b8bf", size = 1981341 },
    { url = "https://files.pythonhosted.org/packages/70/15/05871d851569a442b191512fead61ba7cdfe272ab109855eb6f9ec095b86/jsonschema_rs-0.20.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:41e42c80eed641a848ee38039291e6ca90e61de1a0b695e706894b175f45c741", size = 1967053 },
    { url = "https://files.pythonhosted.org/packages/03/dc/edacc64a574490dc4349e7a206382eff0050a36e0b46ce2486421bc7d221/jsonschema_rs-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c4e0945cb1ff4d183e855ae975839d648df24604d71827951f57a8b7edabb8ca", size = 1993242 },
    { url = "https://files.pythonhosted.org/packages/bf/ee/1f17ddd64526fbec08db217dfb8e42475d2728be40caee7ebc1159a2a270/jsonschema_rs-0.20.0-cp312-none-win32.whl", hash = "sha256:00016bfdc132aa8c728fe012e9ed2b5c9d0d3dd5c37368990d58288e121a6d79", size = 1667287 },
    { url = "https://files.pythonhosted.org/packages/bd/1e/9b7cdd83164be814cbe62e6103d1c11ef285ee6eb9ecb62e292eeca2bb20/jsonschema_rs-0.20.0-cp312-none-win_amd64.whl", hash = "sha256:2c5abcf6185a4eea7185aadb132f92f6bc1d808d3debe775f0e4608baba8501e", size = 1812100 },
]

[[package]]
name = "langchain"
version = "0.3.20"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
    { name = "langchain-text-splitters" },
    { name = "langsmith" },
    { name = "pydantic" },
    { name = "pyyaml" },
    { name = "requests" },
    { name = "sqlalchemy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2a/b0/5121cdd19cf99e684043f4eae528c893f56bd25e7711d4de89f27832a5f3/langchain-0.3.20.tar.gz", hash = "sha256:edcc3241703e1f6557ef5a5c35cd56f9ccc25ff12e38b4829c66d94971737a93", size = 10225276 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b5/d4/afe8174838bdd3baba5d6a19e9f3af4c54c5db1ab4d66ef0b650c6157919/langchain-0.3.20-py3-none-any.whl", hash = "sha256:273287f8e61ffdf7e811cf8799e6a71e9381325b8625fd6618900faba79cfdd0", size = 1011577 },
]

[[package]]
name = "langchain-anthropic"
version = "0.3.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anthropic" },
    { name = "langchain-core" },
    { name = "pydantic" },
]
sdist = { url = "https://files.pythonhosted.org/packages/be/0a/7ccb79c41575b04266fc4def50f41d0a4689361421d82a14350d9d5e783e/langchain_anthropic-0.3.9.tar.gz", hash = "sha256:e8012d7986ad1d8412df6914c56f3c0d2797f231766a03bb1ad22cc7023e6e1d", size = 42205 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b9/27/258565b4a487fca7db363ea95765e6f1f00c23baa83dc4ec19a009213658/langchain_anthropic-0.3.9-py3-none-any.whl", hash = "sha256:adbbfaf3ce9798d46fb43d6fc01105630238f375dc6043d35d0aafab61fdbb71", size = 24414 },
]

[[package]]
name = "langchain-core"
version = "0.3.41"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "jsonpatch" },
    { name = "langsmith" },
    { name = "packaging" },
    { name = "pydantic" },
    { name = "pyyaml" },
    { name = "tenacity" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2b/0a/aa5167a1a46094024b8fe50917e37f1df5bcd0034adb25452e121dae60e6/langchain_core-0.3.41.tar.gz", hash = "sha256:d3ee9f3616ebbe7943470ade23d4a04e1729b1512c0ec55a4a07bd2ac64dedb4", size = 528826 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bc/a6/551de93e02b1ef4ec031f6e1c0ff31a70790096c1e7066168a7693e4efe5/langchain_core-0.3.41-py3-none-any.whl", hash = "sha256:1a27cca5333bae7597de4004fb634b5f3e71667a3da6493b94ce83bcf15a23bd", size = 415149 },
]

[[package]]
name = "langchain-openai"
version = "0.3.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
    { name = "openai" },
    { name = "tiktoken" },
]
sdist = { url = "https://files.pythonhosted.org/packages/8e/3c/08add067e46409d3e881933155f546edb08644e5e4e2360ff22c6a2104a8/langchain_openai-0.3.7.tar.gz", hash = "sha256:b8b51a3aaa1cc3bda060651ea41145f7728219e8a7150b5404fb1e8446de9cef", size = 256488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/36/0e/816c5293eda67600d374bb8484a9adab873c9096489f6f91634581919f35/langchain_openai-0.3.7-py3-none-any.whl", hash = "sha256:0aefc7bdf8e7398d41e09c4313cace816df6438f2aa93d34f79523487310f0da", size = 55254 },
]

[[package]]
name = "langchain-text-splitters"
version = "0.3.6"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0d/33/89912a07c63e4e818f9b0c8d52e4f9d600c97beca8a91db8c9dae6a1b28f/langchain_text_splitters-0.3.6.tar.gz", hash = "sha256:c537972f4b7c07451df431353a538019ad9dadff7a1073ea363946cea97e1bee", size = 40545 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4c/f8/6b82af988e65af9697f6a2f25373fb173fd32d48b62772a8773c5184c870/langchain_text_splitters-0.3.6-py3-none-any.whl", hash = "sha256:e5d7b850f6c14259ea930be4a964a65fa95d9df7e1dbdd8bad8416db72292f4e", size = 31197 },
]

[[package]]
name = "langgraph"
version = "0.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
    { name = "langgraph-checkpoint" },
    { name = "langgraph-prebuilt" },
    { name = "langgraph-sdk" },
]
sdist = { url = "https://files.pythonhosted.org/packages/4e/fa/b1ecc95a2464bc7dbe5e67fbd21096013829119899c33236090b98c75508/langgraph-0.3.5.tar.gz", hash = "sha256:7c0d8e61aa02578b41036c9f7a599ccba2562d269f66ef76bacbba47a99a7eca", size = 114020 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/5f/1e1d9173b5c41eff54f88d9f4ee82c38eb4928120ab6a21a68a78d1c499e/langgraph-0.3.5-py3-none-any.whl", hash = "sha256:be313ec300633c857873ea3e44aece4dd7d0b11f131d385108b359d377a85bf7", size = 131527 },
]

[[package]]
name = "langgraph-api"
version = "0.0.28"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cryptography" },
    { name = "httpx" },
    { name = "jsonschema-rs" },
    { name = "langchain-core" },
    { name = "langgraph" },
    { name = "langgraph-checkpoint" },
    { name = "langgraph-sdk" },
    { name = "langsmith" },
    { name = "orjson" },
    { name = "pyjwt" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "structlog" },
    { name = "tenacity" },
    { name = "uvicorn" },
    { name = "watchfiles" },
]
sdist = { url = "https://files.pythonhosted.org/packages/da/1b/4efd61a8ef39a32c5e12584cf7366a34436aa3fe0e4d36d59e3c3a077e39/langgraph_api-0.0.28.tar.gz", hash = "sha256:d040833276681a5b00f27eb85d72b0dadc7a324e951b9b35fb542abf0d3e9eda", size = 183944 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/ee/ebf38d33b7a93f8e7e9a85d41c4ad8804f028b5a1d481a3fe7751f2a554f/langgraph_api-0.0.28-py3-none-any.whl", hash = "sha256:06f5784e83c6d8fcd62701d279de6f8c837f7ffe260b2d945d2893e0323e4679", size = 212004 },
]

[[package]]
name = "langgraph-checkpoint"
version = "2.0.17"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
    { name = "msgpack" },
]
sdist = { url = "https://files.pythonhosted.org/packages/90/92/f0d6c3e2b2e131c687a9ec87c6e1a430287c430160038e8dfaa4d0db9aab/langgraph_checkpoint-2.0.17.tar.gz", hash = "sha256:255c249f03369c41252f888bc1e1e481bf4fdecf6b3854a39e4935dc34152bc0", size = 34932 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4d/61/35faa34145ddaffc70eb851b85561c96bbad4718ea6f34ef3c717e748c15/langgraph_checkpoint-2.0.17-py3-none-any.whl", hash = "sha256:7da9cd9af41bda5074afef0dcdbd6fa9a050f68beed9d7f80593a253412bf020", size = 39103 },
]

[[package]]
name = "langgraph-cli"
version = "0.1.75"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e1/a6/6d42c432de17a6d7df2851b9e7d43eeeabc4d4e169c5d478f621276d3984/langgraph_cli-0.1.75.tar.gz", hash = "sha256:4b490a9a3f8b8b910422e0d84d054066902de45d5df0f23b18d4130aaac9ac06", size = 29241 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ab/3b/b120926bd4ba153a2626b1986bb3ac7c1439cf201e91786e8d8931295320/langgraph_cli-0.1.75-py3-none-any.whl", hash = "sha256:edf381264982aef8d9279706c813535c3becf4233620501a7240025b790e55c5", size = 31710 },
]

[[package]]
name = "langgraph-prebuilt"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain-core" },
    { name = "langgraph-checkpoint" },
]
sdist = { url = "https://files.pythonhosted.org/packages/99/68/e1e692dbaeb4e9159b60a585fbfc26fbf073b3bb061caa2ff3153f85121a/langgraph_prebuilt-0.1.2.tar.gz", hash = "sha256:cfa7e54006d45e8f3d034ee88fa1d457c381bf6a2a0de0e64c5d3a776659e6d0", size = 23310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/73/2c/2fd70d557b7343f766f79dc8184b391f3417fc85b34dd04439cdd12dc2e1/langgraph_prebuilt-0.1.2-py3-none-any.whl", hash = "sha256:32028c4c4370576748e6c2e075cab1e13b5e3f2c196a390d71cacfb455212311", size = 24684 },
]

[[package]]
name = "langgraph-sdk"
version = "0.1.55"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "httpx" },
    { name = "orjson" },
]
sdist = { url = "https://files.pythonhosted.org/packages/7a/6c/8286151a21124dc0189b57495541c2e3cace317056f60feb04076b438f82/langgraph_sdk-0.1.55.tar.gz", hash = "sha256:89a0240157a27822cc4edd1c9e72bc852e20f5c71165a4c9b91eeffa11fd6a6b", size = 42690 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4e/64/4b75f4b57f0c8f39bdb43aa74b1d2edcdb604b5baa58465ccc54b8b906c5/langgraph_sdk-0.1.55-py3-none-any.whl", hash = "sha256:266e92a558eb738da1ef04c29fbfc2157cd3a977b80905d9509a2cb79331f8fc", size = 45785 },
]

[[package]]
name = "langgraph-slack"
version = "0.0.1"
source = { editable = "." }
dependencies = [
    { name = "aiohttp" },
    { name = "fastapi" },
    { name = "langchain" },
    { name = "langchain-openai" },
    { name = "langgraph-prebuilt" },
    { name = "langgraph-sdk" },
    { name = "langmem" },
    { name = "python-dotenv" },
    { name = "slack-bolt" },
    { name = "uvicorn" },
]

[package.dev-dependencies]
dev = [
    { name = "langgraph-api" },
    { name = "langgraph-cli" },
    { name = "ruff" },
]

[package.metadata]
requires-dist = [
    { name = "aiohttp", specifier = ">=3.11.13" },
    { name = "fastapi", specifier = ">=0.115.6" },
    { name = "langchain", specifier = ">=0.3.20" },
    { name = "langchain-openai", specifier = ">=0.3.7" },
    { name = "langgraph-prebuilt", specifier = ">=0.1.2" },
    { name = "langgraph-sdk", specifier = ">=0.1.48" },
    { name = "langmem", specifier = ">=0.0.15" },
    { name = "python-dotenv", specifier = ">=1.0.1" },
    { name = "slack-bolt", specifier = ">=1.22.0" },
    { name = "uvicorn", specifier = ">=0.34.0" },
]

[package.metadata.requires-dev]
dev = [
    { name = "langgraph-api", specifier = ">=0.0.28" },
    { name = "langgraph-cli", specifier = ">=0.1.75" },
    { name = "ruff", specifier = ">=0.8.4" },
]

[[package]]
name = "langmem"
version = "0.0.15"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "langchain" },
    { name = "langchain-anthropic" },
    { name = "langchain-openai" },
    { name = "langgraph" },
    { name = "langgraph-checkpoint" },
    { name = "langsmith" },
    { name = "trustcall" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0f/f5/ba8a7481e42f7dcd8218196826163bb407d93bdc539952eff7d3ce446cee/langmem-0.0.15.tar.gz", hash = "sha256:3d613f2248179da6f23673990ebcd1ed29f82856d631143919da65edb11924cb", size = 797245 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/05/96d17440eb75e6514e5285379d74bd1dfe4637a88a7ab9611019c4ef4960/langmem-0.0.15-py3-none-any.whl", hash = "sha256:929eecc84d7f18c2e9c77253a0dee608ee34a84b7b3f5f7a67a6958e3459f2c8", size = 55539 },
]

[[package]]
name = "langsmith"
version = "0.3.12"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "httpx" },
    { name = "orjson", marker = "platform_python_implementation != 'PyPy'" },
    { name = "packaging" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "requests-toolbelt" },
    { name = "zstandard" },
]
sdist = { url = "https://files.pythonhosted.org/packages/48/32/0ef5ad579ae096f40fc108b6920e742267a0e9c07d778c1d381586616715/langsmith-0.3.12.tar.gz", hash = "sha256:045b49d0401d0e985d025ff0cf69743ab9a429e309ce5d533eab3c774d004bc2", size = 324149 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9a/92/9702c45974c4dbea978f8af1cfb077677b96f98df12b1638be8eff5ae5ff/langsmith-0.3.12-py3-none-any.whl", hash = "sha256:cf7926bd12d56adbd74a294ebbfc5a34c413172bfbdcd763175cc472b45afbea", size = 335663 },
]

[[package]]
name = "msgpack"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cb/d0/7555686ae7ff5731205df1012ede15dd9d927f6227ea151e901c7406af4f/msgpack-1.1.0.tar.gz", hash = "sha256:dd432ccc2c72b914e4cb77afce64aab761c1137cc698be3984eee260bcb2896e", size = 167260 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/5e/a4c7154ba65d93be91f2f1e55f90e76c5f91ccadc7efc4341e6f04c8647f/msgpack-1.1.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3d364a55082fb2a7416f6c63ae383fbd903adb5a6cf78c5b96cc6316dc1cedc7", size = 150803 },
    { url = "https://files.pythonhosted.org/packages/60/c2/687684164698f1d51c41778c838d854965dd284a4b9d3a44beba9265c931/msgpack-1.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:79ec007767b9b56860e0372085f8504db5d06bd6a327a335449508bbee9648fa", size = 84343 },
    { url = "https://files.pythonhosted.org/packages/42/ae/d3adea9bb4a1342763556078b5765e666f8fdf242e00f3f6657380920972/msgpack-1.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6ad622bf7756d5a497d5b6836e7fc3752e2dd6f4c648e24b1803f6048596f701", size = 81408 },
    { url = "https://files.pythonhosted.org/packages/dc/17/6313325a6ff40ce9c3207293aee3ba50104aed6c2c1559d20d09e5c1ff54/msgpack-1.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8e59bca908d9ca0de3dc8684f21ebf9a690fe47b6be93236eb40b99af28b6ea6", size = 396096 },
    { url = "https://files.pythonhosted.org/packages/a8/a1/ad7b84b91ab5a324e707f4c9761633e357820b011a01e34ce658c1dda7cc/msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5e1da8f11a3dd397f0a32c76165cf0c4eb95b31013a94f6ecc0b280c05c91b59", size = 403671 },
    { url = "https://files.pythonhosted.org/packages/bb/0b/fd5b7c0b308bbf1831df0ca04ec76fe2f5bf6319833646b0a4bd5e9dc76d/msgpack-1.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:452aff037287acb1d70a804ffd022b21fa2bb7c46bee884dbc864cc9024128a0", size = 387414 },
    { url = "https://files.pythonhosted.org/packages/f0/03/ff8233b7c6e9929a1f5da3c7860eccd847e2523ca2de0d8ef4878d354cfa/msgpack-1.1.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:8da4bf6d54ceed70e8861f833f83ce0814a2b72102e890cbdfe4b34764cdd66e", size = 383759 },
    { url = "https://files.pythonhosted.org/packages/1f/1b/eb82e1fed5a16dddd9bc75f0854b6e2fe86c0259c4353666d7fab37d39f4/msgpack-1.1.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:41c991beebf175faf352fb940bf2af9ad1fb77fd25f38d9142053914947cdbf6", size = 394405 },
    { url = "https://files.pythonhosted.org/packages/90/2e/962c6004e373d54ecf33d695fb1402f99b51832631e37c49273cc564ffc5/msgpack-1.1.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:a52a1f3a5af7ba1c9ace055b659189f6c669cf3657095b50f9602af3a3ba0fe5", size = 396041 },
    { url = "https://files.pythonhosted.org/packages/f8/20/6e03342f629474414860c48aeffcc2f7f50ddaf351d95f20c3f1c67399a8/msgpack-1.1.0-cp311-cp311-win32.whl", hash = "sha256:58638690ebd0a06427c5fe1a227bb6b8b9fdc2bd07701bec13c2335c82131a88", size = 68538 },
    { url = "https://files.pythonhosted.org/packages/aa/c4/5a582fc9a87991a3e6f6800e9bb2f3c82972912235eb9539954f3e9997c7/msgpack-1.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:fd2906780f25c8ed5d7b323379f6138524ba793428db5d0e9d226d3fa6aa1788", size = 74871 },
    { url = "https://files.pythonhosted.org/packages/e1/d6/716b7ca1dbde63290d2973d22bbef1b5032ca634c3ff4384a958ec3f093a/msgpack-1.1.0-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:d46cf9e3705ea9485687aa4001a76e44748b609d260af21c4ceea7f2212a501d", size = 152421 },
    { url = "https://files.pythonhosted.org/packages/70/da/5312b067f6773429cec2f8f08b021c06af416bba340c912c2ec778539ed6/msgpack-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:5dbad74103df937e1325cc4bfeaf57713be0b4f15e1c2da43ccdd836393e2ea2", size = 85277 },
    { url = "https://files.pythonhosted.org/packages/28/51/da7f3ae4462e8bb98af0d5bdf2707f1b8c65a0d4f496e46b6afb06cbc286/msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:58dfc47f8b102da61e8949708b3eafc3504509a5728f8b4ddef84bd9e16ad420", size = 82222 },
    { url = "https://files.pythonhosted.org/packages/33/af/dc95c4b2a49cff17ce47611ca9ba218198806cad7796c0b01d1e332c86bb/msgpack-1.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4676e5be1b472909b2ee6356ff425ebedf5142427842aa06b4dfd5117d1ca8a2", size = 392971 },
    { url = "https://files.pythonhosted.org/packages/f1/54/65af8de681fa8255402c80eda2a501ba467921d5a7a028c9c22a2c2eedb5/msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:17fb65dd0bec285907f68b15734a993ad3fc94332b5bb21b0435846228de1f39", size = 401403 },
    { url = "https://files.pythonhosted.org/packages/97/8c/e333690777bd33919ab7024269dc3c41c76ef5137b211d776fbb404bfead/msgpack-1.1.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a51abd48c6d8ac89e0cfd4fe177c61481aca2d5e7ba42044fd218cfd8ea9899f", size = 385356 },
    { url = "https://files.pythonhosted.org/packages/57/52/406795ba478dc1c890559dd4e89280fa86506608a28ccf3a72fbf45df9f5/msgpack-1.1.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:2137773500afa5494a61b1208619e3871f75f27b03bcfca7b3a7023284140247", size = 383028 },
    { url = "https://files.pythonhosted.org/packages/e7/69/053b6549bf90a3acadcd8232eae03e2fefc87f066a5b9fbb37e2e608859f/msgpack-1.1.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:398b713459fea610861c8a7b62a6fec1882759f308ae0795b5413ff6a160cf3c", size = 391100 },
    { url = "https://files.pythonhosted.org/packages/23/f0/d4101d4da054f04274995ddc4086c2715d9b93111eb9ed49686c0f7ccc8a/msgpack-1.1.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:06f5fd2f6bb2a7914922d935d3b8bb4a7fff3a9a91cfce6d06c13bc42bec975b", size = 394254 },
    { url = "https://files.pythonhosted.org/packages/1c/12/cf07458f35d0d775ff3a2dc5559fa2e1fcd06c46f1ef510e594ebefdca01/msgpack-1.1.0-cp312-cp312-win32.whl", hash = "sha256:ad33e8400e4ec17ba782f7b9cf868977d867ed784a1f5f2ab46e7ba53b6e1e1b", size = 69085 },
    { url = "https://files.pythonhosted.org/packages/73/80/2708a4641f7d553a63bc934a3eb7214806b5b39d200133ca7f7afb0a53e8/msgpack-1.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:115a7af8ee9e8cddc10f87636767857e7e3717b7a2e97379dc2054712693e90f", size = 75347 },
    { url = "https://files.pythonhosted.org/packages/c8/b0/380f5f639543a4ac413e969109978feb1f3c66e931068f91ab6ab0f8be00/msgpack-1.1.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:071603e2f0771c45ad9bc65719291c568d4edf120b44eb36324dcb02a13bfddf", size = 151142 },
    { url = "https://files.pythonhosted.org/packages/c8/ee/be57e9702400a6cb2606883d55b05784fada898dfc7fd12608ab1fdb054e/msgpack-1.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:0f92a83b84e7c0749e3f12821949d79485971f087604178026085f60ce109330", size = 84523 },
    { url = "https://files.pythonhosted.org/packages/7e/3a/2919f63acca3c119565449681ad08a2f84b2171ddfcff1dba6959db2cceb/msgpack-1.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:4a1964df7b81285d00a84da4e70cb1383f2e665e0f1f2a7027e683956d04b734", size = 81556 },
    { url = "https://files.pythonhosted.org/packages/7c/43/a11113d9e5c1498c145a8925768ea2d5fce7cbab15c99cda655aa09947ed/msgpack-1.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:59caf6a4ed0d164055ccff8fe31eddc0ebc07cf7326a2aaa0dbf7a4001cd823e", size = 392105 },
    { url = "https://files.pythonhosted.org/packages/2d/7b/2c1d74ca6c94f70a1add74a8393a0138172207dc5de6fc6269483519d048/msgpack-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0907e1a7119b337971a689153665764adc34e89175f9a34793307d9def08e6ca", size = 399979 },
    { url = "https://files.pythonhosted.org/packages/82/8c/cf64ae518c7b8efc763ca1f1348a96f0e37150061e777a8ea5430b413a74/msgpack-1.1.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:65553c9b6da8166e819a6aa90ad15288599b340f91d18f60b2061f402b9a4915", size = 383816 },
    { url = "https://files.pythonhosted.org/packages/69/86/a847ef7a0f5ef3fa94ae20f52a4cacf596a4e4a010197fbcc27744eb9a83/msgpack-1.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:7a946a8992941fea80ed4beae6bff74ffd7ee129a90b4dd5cf9c476a30e9708d", size = 380973 },
    { url = "https://files.pythonhosted.org/packages/aa/90/c74cf6e1126faa93185d3b830ee97246ecc4fe12cf9d2d31318ee4246994/msgpack-1.1.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:4b51405e36e075193bc051315dbf29168d6141ae2500ba8cd80a522964e31434", size = 387435 },
    { url = "https://files.pythonhosted.org/packages/7a/40/631c238f1f338eb09f4acb0f34ab5862c4e9d7eda11c1b685471a4c5ea37/msgpack-1.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:b4c01941fd2ff87c2a934ee6055bda4ed353a7846b8d4f341c428109e9fcde8c", size = 399082 },
    { url = "https://files.pythonhosted.org/packages/e9/1b/fa8a952be252a1555ed39f97c06778e3aeb9123aa4cccc0fd2acd0b4e315/msgpack-1.1.0-cp313-cp313-win32.whl", hash = "sha256:7c9a35ce2c2573bada929e0b7b3576de647b0defbd25f5139dcdaba0ae35a4cc", size = 69037 },
    { url = "https://files.pythonhosted.org/packages/b6/bc/8bd826dd03e022153bfa1766dcdec4976d6c818865ed54223d71f07862b3/msgpack-1.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:bce7d9e614a04d0883af0b3d4d501171fbfca038f12c77fa838d9f198147a23f", size = 75140 },
]

[[package]]
name = "multidict"
version = "6.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/be/504b89a5e9ca731cd47487e91c469064f8ae5af93b7259758dcfc2b9c848/multidict-6.1.0.tar.gz", hash = "sha256:22ae2ebf9b0c69d206c003e2f6a914ea33f0a932d4aa16f236afc049d9958f4a", size = 64002 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/93/13/df3505a46d0cd08428e4c8169a196131d1b0c4b515c3649829258843dde6/multidict-6.1.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3efe2c2cb5763f2f1b275ad2bf7a287d3f7ebbef35648a9726e3b69284a4f3d6", size = 48570 },
    { url = "https://files.pythonhosted.org/packages/f0/e1/a215908bfae1343cdb72f805366592bdd60487b4232d039c437fe8f5013d/multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:c7053d3b0353a8b9de430a4f4b4268ac9a4fb3481af37dfe49825bf45ca24156", size = 29316 },
    { url = "https://files.pythonhosted.org/packages/70/0f/6dc70ddf5d442702ed74f298d69977f904960b82368532c88e854b79f72b/multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:27e5fc84ccef8dfaabb09d82b7d179c7cf1a3fbc8a966f8274fcb4ab2eb4cadb", size = 29640 },
    { url = "https://files.pythonhosted.org/packages/d8/6d/9c87b73a13d1cdea30b321ef4b3824449866bd7f7127eceed066ccb9b9ff/multidict-6.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0e2b90b43e696f25c62656389d32236e049568b39320e2735d51f08fd362761b", size = 131067 },
    { url = "https://files.pythonhosted.org/packages/cc/1e/1b34154fef373371fd6c65125b3d42ff5f56c7ccc6bfff91b9b3c60ae9e0/multidict-6.1.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d83a047959d38a7ff552ff94be767b7fd79b831ad1cd9920662db05fec24fe72", size = 138507 },
    { url = "https://files.pythonhosted.org/packages/fb/e0/0bc6b2bac6e461822b5f575eae85da6aae76d0e2a79b6665d6206b8e2e48/multidict-6.1.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d1a9dd711d0877a1ece3d2e4fea11a8e75741ca21954c919406b44e7cf971304", size = 133905 },
    { url = "https://files.pythonhosted.org/packages/ba/af/73d13b918071ff9b2205fcf773d316e0f8fefb4ec65354bbcf0b10908cc6/multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ec2abea24d98246b94913b76a125e855eb5c434f7c46546046372fe60f666351", size = 129004 },
    { url = "https://files.pythonhosted.org/packages/74/21/23960627b00ed39643302d81bcda44c9444ebcdc04ee5bedd0757513f259/multidict-6.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4867cafcbc6585e4b678876c489b9273b13e9fff9f6d6d66add5e15d11d926cb", size = 121308 },
    { url = "https://files.pythonhosted.org/packages/8b/5c/cf282263ffce4a596ed0bb2aa1a1dddfe1996d6a62d08842a8d4b33dca13/multidict-6.1.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:5b48204e8d955c47c55b72779802b219a39acc3ee3d0116d5080c388970b76e3", size = 132608 },
    { url = "https://files.pythonhosted.org/packages/d7/3e/97e778c041c72063f42b290888daff008d3ab1427f5b09b714f5a8eff294/multidict-6.1.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:d8fff389528cad1618fb4b26b95550327495462cd745d879a8c7c2115248e399", size = 127029 },
    { url = "https://files.pythonhosted.org/packages/47/ac/3efb7bfe2f3aefcf8d103e9a7162572f01936155ab2f7ebcc7c255a23212/multidict-6.1.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:a7a9541cd308eed5e30318430a9c74d2132e9a8cb46b901326272d780bf2d423", size = 137594 },
    { url = "https://files.pythonhosted.org/packages/42/9b/6c6e9e8dc4f915fc90a9b7798c44a30773dea2995fdcb619870e705afe2b/multidict-6.1.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:da1758c76f50c39a2efd5e9859ce7d776317eb1dd34317c8152ac9251fc574a3", size = 134556 },
    { url = "https://files.pythonhosted.org/packages/1d/10/8e881743b26aaf718379a14ac58572a240e8293a1c9d68e1418fb11c0f90/multidict-6.1.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:c943a53e9186688b45b323602298ab727d8865d8c9ee0b17f8d62d14b56f0753", size = 130993 },
    { url = "https://files.pythonhosted.org/packages/45/84/3eb91b4b557442802d058a7579e864b329968c8d0ea57d907e7023c677f2/multidict-6.1.0-cp311-cp311-win32.whl", hash = "sha256:90f8717cb649eea3504091e640a1b8568faad18bd4b9fcd692853a04475a4b80", size = 26405 },
    { url = "https://files.pythonhosted.org/packages/9f/0b/ad879847ecbf6d27e90a6eabb7eff6b62c129eefe617ea45eae7c1f0aead/multidict-6.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:82176036e65644a6cc5bd619f65f6f19781e8ec2e5330f51aa9ada7504cc1926", size = 28795 },
    { url = "https://files.pythonhosted.org/packages/fd/16/92057c74ba3b96d5e211b553895cd6dc7cc4d1e43d9ab8fafc727681ef71/multidict-6.1.0-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:b04772ed465fa3cc947db808fa306d79b43e896beb677a56fb2347ca1a49c1fa", size = 48713 },
    { url = "https://files.pythonhosted.org/packages/94/3d/37d1b8893ae79716179540b89fc6a0ee56b4a65fcc0d63535c6f5d96f217/multidict-6.1.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:6180c0ae073bddeb5a97a38c03f30c233e0a4d39cd86166251617d1bbd0af436", size = 29516 },
    { url = "https://files.pythonhosted.org/packages/a2/12/adb6b3200c363062f805275b4c1e656be2b3681aada66c80129932ff0bae/multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:071120490b47aa997cca00666923a83f02c7fbb44f71cf7f136df753f7fa8761", size = 29557 },
    { url = "https://files.pythonhosted.org/packages/47/e9/604bb05e6e5bce1e6a5cf80a474e0f072e80d8ac105f1b994a53e0b28c42/multidict-6.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50b3a2710631848991d0bf7de077502e8994c804bb805aeb2925a981de58ec2e", size = 130170 },
    { url = "https://files.pythonhosted.org/packages/7e/13/9efa50801785eccbf7086b3c83b71a4fb501a4d43549c2f2f80b8787d69f/multidict-6.1.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b58c621844d55e71c1b7f7c498ce5aa6985d743a1a59034c57a905b3f153c1ef", size = 134836 },
    { url = "https://files.pythonhosted.org/packages/bf/0f/93808b765192780d117814a6dfcc2e75de6dcc610009ad408b8814dca3ba/multidict-6.1.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55b6d90641869892caa9ca42ff913f7ff1c5ece06474fbd32fb2cf6834726c95", size = 133475 },
    { url = "https://files.pythonhosted.org/packages/d3/c8/529101d7176fe7dfe1d99604e48d69c5dfdcadb4f06561f465c8ef12b4df/multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b820514bfc0b98a30e3d85462084779900347e4d49267f747ff54060cc33925", size = 131049 },
    { url = "https://files.pythonhosted.org/packages/ca/0c/fc85b439014d5a58063e19c3a158a889deec399d47b5269a0f3b6a2e28bc/multidict-6.1.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:10a9b09aba0c5b48c53761b7c720aaaf7cf236d5fe394cd399c7ba662d5f9966", size = 120370 },
    { url = "https://files.pythonhosted.org/packages/db/46/d4416eb20176492d2258fbd47b4abe729ff3b6e9c829ea4236f93c865089/multidict-6.1.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:1e16bf3e5fc9f44632affb159d30a437bfe286ce9e02754759be5536b169b305", size = 125178 },
    { url = "https://files.pythonhosted.org/packages/5b/46/73697ad7ec521df7de5531a32780bbfd908ded0643cbe457f981a701457c/multidict-6.1.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:76f364861c3bfc98cbbcbd402d83454ed9e01a5224bb3a28bf70002a230f73e2", size = 119567 },
    { url = "https://files.pythonhosted.org/packages/cd/ed/51f060e2cb0e7635329fa6ff930aa5cffa17f4c7f5c6c3ddc3500708e2f2/multidict-6.1.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:820c661588bd01a0aa62a1283f20d2be4281b086f80dad9e955e690c75fb54a2", size = 129822 },
    { url = "https://files.pythonhosted.org/packages/df/9e/ee7d1954b1331da3eddea0c4e08d9142da5f14b1321c7301f5014f49d492/multidict-6.1.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:0e5f362e895bc5b9e67fe6e4ded2492d8124bdf817827f33c5b46c2fe3ffaca6", size = 128656 },
    { url = "https://files.pythonhosted.org/packages/77/00/8538f11e3356b5d95fa4b024aa566cde7a38aa7a5f08f4912b32a037c5dc/multidict-6.1.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:3ec660d19bbc671e3a6443325f07263be452c453ac9e512f5eb935e7d4ac28b3", size = 125360 },
    { url = "https://files.pythonhosted.org/packages/be/05/5d334c1f2462d43fec2363cd00b1c44c93a78c3925d952e9a71caf662e96/multidict-6.1.0-cp312-cp312-win32.whl", hash = "sha256:58130ecf8f7b8112cdb841486404f1282b9c86ccb30d3519faf301b2e5659133", size = 26382 },
    { url = "https://files.pythonhosted.org/packages/a3/bf/f332a13486b1ed0496d624bcc7e8357bb8053823e8cd4b9a18edc1d97e73/multidict-6.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:188215fc0aafb8e03341995e7c4797860181562380f81ed0a87ff455b70bf1f1", size = 28529 },
    { url = "https://files.pythonhosted.org/packages/22/67/1c7c0f39fe069aa4e5d794f323be24bf4d33d62d2a348acdb7991f8f30db/multidict-6.1.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:d569388c381b24671589335a3be6e1d45546c2988c2ebe30fdcada8457a31008", size = 48771 },
    { url = "https://files.pythonhosted.org/packages/3c/25/c186ee7b212bdf0df2519eacfb1981a017bda34392c67542c274651daf23/multidict-6.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:052e10d2d37810b99cc170b785945421141bf7bb7d2f8799d431e7db229c385f", size = 29533 },
    { url = "https://files.pythonhosted.org/packages/67/5e/04575fd837e0958e324ca035b339cea174554f6f641d3fb2b4f2e7ff44a2/multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f90c822a402cb865e396a504f9fc8173ef34212a342d92e362ca498cad308e28", size = 29595 },
    { url = "https://files.pythonhosted.org/packages/d3/b2/e56388f86663810c07cfe4a3c3d87227f3811eeb2d08450b9e5d19d78876/multidict-6.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b225d95519a5bf73860323e633a664b0d85ad3d5bede6d30d95b35d4dfe8805b", size = 130094 },
    { url = "https://files.pythonhosted.org/packages/6c/ee/30ae9b4186a644d284543d55d491fbd4239b015d36b23fea43b4c94f7052/multidict-6.1.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:23bfd518810af7de1116313ebd9092cb9aa629beb12f6ed631ad53356ed6b86c", size = 134876 },
    { url = "https://files.pythonhosted.org/packages/84/c7/70461c13ba8ce3c779503c70ec9d0345ae84de04521c1f45a04d5f48943d/multidict-6.1.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5c09fcfdccdd0b57867577b719c69e347a436b86cd83747f179dbf0cc0d4c1f3", size = 133500 },
    { url = "https://files.pythonhosted.org/packages/4a/9f/002af221253f10f99959561123fae676148dd730e2daa2cd053846a58507/multidict-6.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bf6bea52ec97e95560af5ae576bdac3aa3aae0b6758c6efa115236d9e07dae44", size = 131099 },
    { url = "https://files.pythonhosted.org/packages/82/42/d1c7a7301d52af79d88548a97e297f9d99c961ad76bbe6f67442bb77f097/multidict-6.1.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:57feec87371dbb3520da6192213c7d6fc892d5589a93db548331954de8248fd2", size = 120403 },
    { url = "https://files.pythonhosted.org/packages/68/f3/471985c2c7ac707547553e8f37cff5158030d36bdec4414cb825fbaa5327/multidict-6.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0c3f390dc53279cbc8ba976e5f8035eab997829066756d811616b652b00a23a3", size = 125348 },
    { url = "https://files.pythonhosted.org/packages/67/2c/e6df05c77e0e433c214ec1d21ddd203d9a4770a1f2866a8ca40a545869a0/multidict-6.1.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:59bfeae4b25ec05b34f1956eaa1cb38032282cd4dfabc5056d0a1ec4d696d3aa", size = 119673 },
    { url = "https://files.pythonhosted.org/packages/c5/cd/bc8608fff06239c9fb333f9db7743a1b2eafe98c2666c9a196e867a3a0a4/multidict-6.1.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:b2f59caeaf7632cc633b5cf6fc449372b83bbdf0da4ae04d5be36118e46cc0aa", size = 129927 },
    { url = "https://files.pythonhosted.org/packages/44/8e/281b69b7bc84fc963a44dc6e0bbcc7150e517b91df368a27834299a526ac/multidict-6.1.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:37bb93b2178e02b7b618893990941900fd25b6b9ac0fa49931a40aecdf083fe4", size = 128711 },
    { url = "https://files.pythonhosted.org/packages/12/a4/63e7cd38ed29dd9f1881d5119f272c898ca92536cdb53ffe0843197f6c85/multidict-6.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4e9f48f58c2c523d5a06faea47866cd35b32655c46b443f163d08c6d0ddb17d6", size = 125519 },
    { url = "https://files.pythonhosted.org/packages/38/e0/4f5855037a72cd8a7a2f60a3952d9aa45feedb37ae7831642102604e8a37/multidict-6.1.0-cp313-cp313-win32.whl", hash = "sha256:3a37ffb35399029b45c6cc33640a92bef403c9fd388acce75cdc88f58bd19a81", size = 26426 },
    { url = "https://files.pythonhosted.org/packages/7e/a5/17ee3a4db1e310b7405f5d25834460073a8ccd86198ce044dfaf69eac073/multidict-6.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:e9aa71e15d9d9beaad2c6b9319edcdc0a49a43ef5c0a4c8265ca9ee7d6c67774", size = 28531 },
    { url = "https://files.pythonhosted.org/packages/99/b7/b9e70fde2c0f0c9af4cc5277782a89b66d35948ea3369ec9f598358c3ac5/multidict-6.1.0-py3-none-any.whl", hash = "sha256:48e171e52d1c4d33888e529b999e5900356b9ae588c2f09a52dcefb158b27506", size = 10051 },
]

[[package]]
name = "openai"
version = "1.65.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "distro" },
    { name = "httpx" },
    { name = "jiter" },
    { name = "pydantic" },
    { name = "sniffio" },
    { name = "tqdm" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fa/8d/1f7aace801afbbe4d6b8c7fa89b76eb9a3a8eeff38b84d4005d47b226b30/openai-1.65.4.tar.gz", hash = "sha256:0b08c58625d556f5c6654701af1023689c173eb0989ce8f73c7fd0eb22203c76", size = 359365 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ba/db/7bab832be24631a793492c1c61ecbf029018b99696f435db3b63d690bf1c/openai-1.65.4-py3-none-any.whl", hash = "sha256:15566d46574b94eae3d18efc2f9a4ebd1366d1d44bfc1bdafeea7a5cf8271bcb", size = 473523 },
]

[[package]]
name = "orjson"
version = "3.10.12"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e0/04/bb9f72987e7f62fb591d6c880c0caaa16238e4e530cbc3bdc84a7372d75f/orjson-3.10.12.tar.gz", hash = "sha256:0a78bbda3aea0f9f079057ee1ee8a1ecf790d4f1af88dd67493c6b8ee52506ff", size = 5438647 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d3/48/7c3cd094488f5a3bc58488555244609a8c4d105bc02f2b77e509debf0450/orjson-3.10.12-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:a734c62efa42e7df94926d70fe7d37621c783dea9f707a98cdea796964d4cf74", size = 248687 },
    { url = "https://files.pythonhosted.org/packages/ff/90/e55f0e25c7fdd1f82551fe787f85df6f378170caca863c04c810cd8f2730/orjson-3.10.12-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:750f8b27259d3409eda8350c2919a58b0cfcd2054ddc1bd317a643afc646ef23", size = 136953 },
    { url = "https://files.pythonhosted.org/packages/2a/b3/109c020cf7fee747d400de53b43b183ca9d3ebda3906ad0b858eb5479718/orjson-3.10.12-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:bb52c22bfffe2857e7aa13b4622afd0dd9d16ea7cc65fd2bf318d3223b1b6252", size = 149090 },
    { url = "https://files.pythonhosted.org/packages/96/d4/35c0275dc1350707d182a1b5da16d1184b9439848060af541285407f18f9/orjson-3.10.12-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:440d9a337ac8c199ff8251e100c62e9488924c92852362cd27af0e67308c16ef", size = 140480 },
    { url = "https://files.pythonhosted.org/packages/3b/79/f863ff460c291ad2d882cc3b580cc444bd4ec60c9df55f6901e6c9a3f519/orjson-3.10.12-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a9e15c06491c69997dfa067369baab3bf094ecb74be9912bdc4339972323f252", size = 156564 },
    { url = "https://files.pythonhosted.org/packages/98/7e/8d5835449ddd873424ee7b1c4ba73a0369c1055750990d824081652874d6/orjson-3.10.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:362d204ad4b0b8724cf370d0cd917bb2dc913c394030da748a3bb632445ce7c4", size = 131279 },
    { url = "https://files.pythonhosted.org/packages/46/f5/d34595b6d7f4f984c6fef289269a7f98abcdc2445ebdf90e9273487dda6b/orjson-3.10.12-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2b57cbb4031153db37b41622eac67329c7810e5f480fda4cfd30542186f006ae", size = 139764 },
    { url = "https://files.pythonhosted.org/packages/b3/5b/ee6e9ddeab54a7b7806768151c2090a2d36025bc346a944f51cf172ef7f7/orjson-3.10.12-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:165c89b53ef03ce0d7c59ca5c82fa65fe13ddf52eeb22e859e58c237d4e33b9b", size = 131915 },
    { url = "https://files.pythonhosted.org/packages/c4/45/febee5951aef6db5cd8cdb260548101d7ece0ca9d4ddadadf1766306b7a4/orjson-3.10.12-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:5dee91b8dfd54557c1a1596eb90bcd47dbcd26b0baaed919e6861f076583e9da", size = 415783 },
    { url = "https://files.pythonhosted.org/packages/27/a5/5a8569e49f3a6c093bee954a3de95062a231196f59e59df13a48e2420081/orjson-3.10.12-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:77a4e1cfb72de6f905bdff061172adfb3caf7a4578ebf481d8f0530879476c07", size = 142387 },
    { url = "https://files.pythonhosted.org/packages/6e/05/02550fb38c5bf758f3994f55401233a2ef304e175f473f2ac6dbf464cc8b/orjson-3.10.12-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:038d42c7bc0606443459b8fe2d1f121db474c49067d8d14c6a075bbea8bf14dd", size = 130664 },
    { url = "https://files.pythonhosted.org/packages/8c/f4/ba31019d0646ce51f7ac75af6dabf98fd89dbf8ad87a9086da34710738e7/orjson-3.10.12-cp311-none-win32.whl", hash = "sha256:03b553c02ab39bed249bedd4abe37b2118324d1674e639b33fab3d1dafdf4d79", size = 143623 },
    { url = "https://files.pythonhosted.org/packages/83/fe/babf08842b989acf4c46103fefbd7301f026423fab47e6f3ba07b54d7837/orjson-3.10.12-cp311-none-win_amd64.whl", hash = "sha256:8b8713b9e46a45b2af6b96f559bfb13b1e02006f4242c156cbadef27800a55a8", size = 135074 },
    { url = "https://files.pythonhosted.org/packages/a1/2f/989adcafad49afb535da56b95d8f87d82e748548b2a86003ac129314079c/orjson-3.10.12-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:53206d72eb656ca5ac7d3a7141e83c5bbd3ac30d5eccfe019409177a57634b0d", size = 248678 },
    { url = "https://files.pythonhosted.org/packages/69/b9/8c075e21a50c387649db262b618ebb7e4d40f4197b949c146fc225dd23da/orjson-3.10.12-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ac8010afc2150d417ebda810e8df08dd3f544e0dd2acab5370cfa6bcc0662f8f", size = 136763 },
    { url = "https://files.pythonhosted.org/packages/87/d3/78edf10b4ab14c19f6d918cf46a145818f4aca2b5a1773c894c5490d3a4c/orjson-3.10.12-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ed459b46012ae950dd2e17150e838ab08215421487371fa79d0eced8d1461d70", size = 149137 },
    { url = "https://files.pythonhosted.org/packages/16/81/5db8852bdf990a0ddc997fa8f16b80895b8cc77c0fe3701569ed2b4b9e78/orjson-3.10.12-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8dcb9673f108a93c1b52bfc51b0af422c2d08d4fc710ce9c839faad25020bb69", size = 140567 },
    { url = "https://files.pythonhosted.org/packages/fa/a6/9ce1e3e3db918512efadad489630c25841eb148513d21dab96f6b4157fa1/orjson-3.10.12-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:22a51ae77680c5c4652ebc63a83d5255ac7d65582891d9424b566fb3b5375ee9", size = 156620 },
    { url = "https://files.pythonhosted.org/packages/47/d4/05133d6bea24e292d2f7628b1e19986554f7d97b6412b3e51d812e38db2d/orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:910fdf2ac0637b9a77d1aad65f803bac414f0b06f720073438a7bd8906298192", size = 131555 },
    { url = "https://files.pythonhosted.org/packages/b9/7a/b3fbffda8743135c7811e95dc2ab7cdbc5f04999b83c2957d046f1b3fac9/orjson-3.10.12-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:24ce85f7100160936bc2116c09d1a8492639418633119a2224114f67f63a4559", size = 139743 },
    { url = "https://files.pythonhosted.org/packages/b5/13/95bbcc9a6584aa083da5ce5004ce3d59ea362a542a0b0938d884fd8790b6/orjson-3.10.12-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:8a76ba5fc8dd9c913640292df27bff80a685bed3a3c990d59aa6ce24c352f8fc", size = 131733 },
    { url = "https://files.pythonhosted.org/packages/e8/29/dddbb2ea6e7af426fcc3da65a370618a88141de75c6603313d70768d1df1/orjson-3.10.12-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:ff70ef093895fd53f4055ca75f93f047e088d1430888ca1229393a7c0521100f", size = 415788 },
    { url = "https://files.pythonhosted.org/packages/53/df/4aea59324ac539975919b4705ee086aced38e351a6eb3eea0f5071dd5661/orjson-3.10.12-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:f4244b7018b5753ecd10a6d324ec1f347da130c953a9c88432c7fbc8875d13be", size = 142347 },
    { url = "https://files.pythonhosted.org/packages/55/55/a52d83d7c49f8ff44e0daab10554490447d6c658771569e1c662aa7057fe/orjson-3.10.12-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:16135ccca03445f37921fa4b585cff9a58aa8d81ebcb27622e69bfadd220b32c", size = 130829 },
    { url = "https://files.pythonhosted.org/packages/a1/8b/b1beb1624dd4adf7d72e2d9b73c4b529e7851c0c754f17858ea13e368b33/orjson-3.10.12-cp312-none-win32.whl", hash = "sha256:2d879c81172d583e34153d524fcba5d4adafbab8349a7b9f16ae511c2cee8708", size = 143659 },
    { url = "https://files.pythonhosted.org/packages/13/91/634c9cd0bfc6a857fc8fab9bf1a1bd9f7f3345e0d6ca5c3d4569ceb6dcfa/orjson-3.10.12-cp312-none-win_amd64.whl", hash = "sha256:fc23f691fa0f5c140576b8c365bc942d577d861a9ee1142e4db468e4e17094fb", size = 135221 },
    { url = "https://files.pythonhosted.org/packages/1b/bb/3f560735f46fa6f875a9d7c4c2171a58cfb19f56a633d5ad5037a924f35f/orjson-3.10.12-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:47962841b2a8aa9a258b377f5188db31ba49af47d4003a32f55d6f8b19006543", size = 248662 },
    { url = "https://files.pythonhosted.org/packages/a3/df/54817902350636cc9270db20486442ab0e4db33b38555300a1159b439d16/orjson-3.10.12-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6334730e2532e77b6054e87ca84f3072bee308a45a452ea0bffbbbc40a67e296", size = 126055 },
    { url = "https://files.pythonhosted.org/packages/2e/77/55835914894e00332601a74540840f7665e81f20b3e2b9a97614af8565ed/orjson-3.10.12-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:accfe93f42713c899fdac2747e8d0d5c659592df2792888c6c5f829472e4f85e", size = 131507 },
    { url = "https://files.pythonhosted.org/packages/33/9e/b91288361898e3158062a876b5013c519a5d13e692ac7686e3486c4133ab/orjson-3.10.12-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a7974c490c014c48810d1dede6c754c3cc46598da758c25ca3b4001ac45b703f", size = 131686 },
    { url = "https://files.pythonhosted.org/packages/b2/15/08ce117d60a4d2d3fd24e6b21db463139a658e9f52d22c9c30af279b4187/orjson-3.10.12-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:3f250ce7727b0b2682f834a3facff88e310f52f07a5dcfd852d99637d386e79e", size = 415710 },
    { url = "https://files.pythonhosted.org/packages/71/af/c09da5ed58f9c002cf83adff7a4cdf3e6cee742aa9723395f8dcdb397233/orjson-3.10.12-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:f31422ff9486ae484f10ffc51b5ab2a60359e92d0716fcce1b3593d7bb8a9af6", size = 142305 },
    { url = "https://files.pythonhosted.org/packages/17/d1/8612038d44f33fae231e9ba480d273bac2b0383ce9e77cb06bede1224ae3/orjson-3.10.12-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5f29c5d282bb2d577c2a6bbde88d8fdcc4919c593f806aac50133f01b733846e", size = 130815 },
    { url = "https://files.pythonhosted.org/packages/67/2c/d5f87834be3591555cfaf9aecdf28f480a6f0b4afeaac53bad534bf9518f/orjson-3.10.12-cp313-none-win32.whl", hash = "sha256:f45653775f38f63dc0e6cd4f14323984c3149c05d6007b58cb154dd080ddc0dc", size = 143664 },
    { url = "https://files.pythonhosted.org/packages/6a/05/7d768fa3ca23c9b3e1e09117abeded1501119f1d8de0ab722938c91ab25d/orjson-3.10.12-cp313-none-win_amd64.whl", hash = "sha256:229994d0c376d5bdc91d92b3c9e6be2f1fbabd4cc1b59daae1443a46ee5e9825", size = 134944 },
]

[[package]]
name = "packaging"
version = "24.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d0/63/68dbb6eb2de9cb10ee4c9c14a0148804425e13c4fb20d61cce69f53106da/packaging-24.2.tar.gz", hash = "sha256:c228a6dc5e932d346bc5739379109d49e8853dd8223571c7c5b55260edc0b97f", size = 163950 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl", hash = "sha256:09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759", size = 65451 },
]

[[package]]
name = "propcache"
version = "0.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/92/76/f941e63d55c0293ff7829dd21e7cf1147e90a526756869a9070f287a68c9/propcache-0.3.0.tar.gz", hash = "sha256:a8fd93de4e1d278046345f49e2238cdb298589325849b2645d4a94c53faeffc5", size = 42722 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/45/c9/cf09ff7e6d09f14149094f7cd50d2dec032b24e61af21fc4540da2b17bfb/propcache-0.3.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:9ddd49258610499aab83b4f5b61b32e11fce873586282a0e972e5ab3bcadee51", size = 79568 },
    { url = "https://files.pythonhosted.org/packages/c8/32/2424d89da88cd81b7d148e0d2b3131461b570a02aa9d84a2e567509adb0d/propcache-0.3.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:2578541776769b500bada3f8a4eeaf944530516b6e90c089aa368266ed70c49e", size = 45895 },
    { url = "https://files.pythonhosted.org/packages/f6/91/ee5b6aa7aa31754fefcf0c5180e09223cac380ef195c4ddc8c266eb641ea/propcache-0.3.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:d8074c5dd61c8a3e915fa8fc04754fa55cfa5978200d2daa1e2d4294c1f136aa", size = 45427 },
    { url = "https://files.pythonhosted.org/packages/bf/73/38f0128462b8b616181d8c53bd5d04eac41c50c449b07615c65d56ba0a9b/propcache-0.3.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b58229a844931bca61b3a20efd2be2a2acb4ad1622fc026504309a6883686fbf", size = 232427 },
    { url = "https://files.pythonhosted.org/packages/59/82/f3d4e84f4539dcfc9c3d338282b9e915f5b63c921986ecfdf7af2d12f87c/propcache-0.3.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e45377d5d6fefe1677da2a2c07b024a6dac782088e37c0b1efea4cfe2b1be19b", size = 239985 },
    { url = "https://files.pythonhosted.org/packages/42/e8/029f58cccbae83c9969a7ee7a06558d5b83a93dfc54e0f4f70234bbaea1b/propcache-0.3.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ec5060592d83454e8063e487696ac3783cc48c9a329498bafae0d972bc7816c9", size = 238827 },
    { url = "https://files.pythonhosted.org/packages/8b/a2/c373561777c0cb9b9e7b9b9a10b9b3a7b6bde75a2535b962231cecc8fdb8/propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:15010f29fbed80e711db272909a074dc79858c6d28e2915704cfc487a8ac89c6", size = 231348 },
    { url = "https://files.pythonhosted.org/packages/d7/d2/4673f715beedf6038b485bcd976813149231d9df5bb6196cb69a09c185c9/propcache-0.3.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a254537b9b696ede293bfdbc0a65200e8e4507bc9f37831e2a0318a9b333c85c", size = 220426 },
    { url = "https://files.pythonhosted.org/packages/e0/f6/1da65f900927bafd4675a16e890618ec7643f2f922bf0e4d84bb38645618/propcache-0.3.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:2b975528998de037dfbc10144b8aed9b8dd5a99ec547f14d1cb7c5665a43f075", size = 220294 },
    { url = "https://files.pythonhosted.org/packages/ff/86/620451bdc02e91b1712cd71890c17077ee97e2a28493836a87e47b8e70ff/propcache-0.3.0-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:19d36bb351ad5554ff20f2ae75f88ce205b0748c38b146c75628577020351e3c", size = 212492 },
    { url = "https://files.pythonhosted.org/packages/6e/1b/e8f86921ed4016da80faf3b8f515f7829decabdbff106736bfff353bceba/propcache-0.3.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:6032231d4a5abd67c7f71168fd64a47b6b451fbcb91c8397c2f7610e67683810", size = 215113 },
    { url = "https://files.pythonhosted.org/packages/1a/95/a61d86cc49aa0945f6c06f3a4614fc543e311a50558c92861f5e9691a37c/propcache-0.3.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:6985a593417cdbc94c7f9c3403747335e450c1599da1647a5af76539672464d3", size = 228330 },
    { url = "https://files.pythonhosted.org/packages/8f/7d/10dbae48ff2bb189e92c2b3487a48f3229146a25941ad0d485934d1104d4/propcache-0.3.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:6a1948df1bb1d56b5e7b0553c0fa04fd0e320997ae99689488201f19fa90d2e7", size = 231942 },
    { url = "https://files.pythonhosted.org/packages/39/ce/82d16aec96c5513ae7db13ab901a65a1e54c915292fb5b2390e33275b61d/propcache-0.3.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:8319293e85feadbbfe2150a5659dbc2ebc4afdeaf7d98936fb9a2f2ba0d4c35c", size = 223077 },
    { url = "https://files.pythonhosted.org/packages/c8/e0/cb077e8e7a583c733df7f53327fcbdb92e42be59b976ce60bf1d904a0efe/propcache-0.3.0-cp311-cp311-win32.whl", hash = "sha256:63f26258a163c34542c24808f03d734b338da66ba91f410a703e505c8485791d", size = 40455 },
    { url = "https://files.pythonhosted.org/packages/d8/35/57abeb6146fe3c19081eeaf3d9d4cfea256f87f1e5101acf80d3332c1820/propcache-0.3.0-cp311-cp311-win_amd64.whl", hash = "sha256:cacea77ef7a2195f04f9279297684955e3d1ae4241092ff0cfcef532bb7a1c32", size = 44705 },
    { url = "https://files.pythonhosted.org/packages/8d/2c/921f15dc365796ec23975b322b0078eae72995c7b4d49eba554c6a308d70/propcache-0.3.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:e53d19c2bf7d0d1e6998a7e693c7e87300dd971808e6618964621ccd0e01fe4e", size = 79867 },
    { url = "https://files.pythonhosted.org/packages/11/a5/4a6cc1a559d1f2fb57ea22edc4245158cdffae92f7f92afcee2913f84417/propcache-0.3.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:a61a68d630e812b67b5bf097ab84e2cd79b48c792857dc10ba8a223f5b06a2af", size = 46109 },
    { url = "https://files.pythonhosted.org/packages/e1/6d/28bfd3af3a567ad7d667348e7f46a520bda958229c4d545ba138a044232f/propcache-0.3.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:fb91d20fa2d3b13deea98a690534697742029f4fb83673a3501ae6e3746508b5", size = 45635 },
    { url = "https://files.pythonhosted.org/packages/73/20/d75b42eaffe5075eac2f4e168f6393d21c664c91225288811d85451b2578/propcache-0.3.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:67054e47c01b7b349b94ed0840ccae075449503cf1fdd0a1fdd98ab5ddc2667b", size = 242159 },
    { url = "https://files.pythonhosted.org/packages/a5/fb/4b537dd92f9fd4be68042ec51c9d23885ca5fafe51ec24c58d9401034e5f/propcache-0.3.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:997e7b8f173a391987df40f3b52c423e5850be6f6df0dcfb5376365440b56667", size = 248163 },
    { url = "https://files.pythonhosted.org/packages/e7/af/8a9db04ac596d531ca0ef7dde518feaadfcdabef7b17d6a5ec59ee3effc2/propcache-0.3.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:8d663fd71491dde7dfdfc899d13a067a94198e90695b4321084c6e450743b8c7", size = 248794 },
    { url = "https://files.pythonhosted.org/packages/9d/c4/ecfc988879c0fd9db03228725b662d76cf484b6b46f7e92fee94e4b52490/propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8884ba1a0fe7210b775106b25850f5e5a9dc3c840d1ae9924ee6ea2eb3acbfe7", size = 243912 },
    { url = "https://files.pythonhosted.org/packages/04/a2/298dd27184faa8b7d91cc43488b578db218b3cc85b54d912ed27b8c5597a/propcache-0.3.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:aa806bbc13eac1ab6291ed21ecd2dd426063ca5417dd507e6be58de20e58dfcf", size = 229402 },
    { url = "https://files.pythonhosted.org/packages/be/0d/efe7fec316ca92dbf4bc4a9ba49ca889c43ca6d48ab1d6fa99fc94e5bb98/propcache-0.3.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:6f4d7a7c0aff92e8354cceca6fe223973ddf08401047920df0fcb24be2bd5138", size = 226896 },
    { url = "https://files.pythonhosted.org/packages/60/63/72404380ae1d9c96d96e165aa02c66c2aae6072d067fc4713da5cde96762/propcache-0.3.0-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:9be90eebc9842a93ef8335291f57b3b7488ac24f70df96a6034a13cb58e6ff86", size = 221447 },
    { url = "https://files.pythonhosted.org/packages/9d/18/b8392cab6e0964b67a30a8f4dadeaff64dc7022b5a34bb1d004ea99646f4/propcache-0.3.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:bf15fc0b45914d9d1b706f7c9c4f66f2b7b053e9517e40123e137e8ca8958b3d", size = 222440 },
    { url = "https://files.pythonhosted.org/packages/6f/be/105d9ceda0f97eff8c06bac1673448b2db2a497444de3646464d3f5dc881/propcache-0.3.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:5a16167118677d94bb48bfcd91e420088854eb0737b76ec374b91498fb77a70e", size = 234104 },
    { url = "https://files.pythonhosted.org/packages/cb/c9/f09a4ec394cfcce4053d8b2a04d622b5f22d21ba9bb70edd0cad061fa77b/propcache-0.3.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:41de3da5458edd5678b0f6ff66691507f9885f5fe6a0fb99a5d10d10c0fd2d64", size = 239086 },
    { url = "https://files.pythonhosted.org/packages/ea/aa/96f7f9ed6def82db67c972bdb7bd9f28b95d7d98f7e2abaf144c284bf609/propcache-0.3.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:728af36011bb5d344c4fe4af79cfe186729efb649d2f8b395d1572fb088a996c", size = 230991 },
    { url = "https://files.pythonhosted.org/packages/5a/11/bee5439de1307d06fad176f7143fec906e499c33d7aff863ea8428b8e98b/propcache-0.3.0-cp312-cp312-win32.whl", hash = "sha256:6b5b7fd6ee7b54e01759f2044f936dcf7dea6e7585f35490f7ca0420fe723c0d", size = 40337 },
    { url = "https://files.pythonhosted.org/packages/e4/17/e5789a54a0455a61cb9efc4ca6071829d992220c2998a27c59aeba749f6f/propcache-0.3.0-cp312-cp312-win_amd64.whl", hash = "sha256:2d15bc27163cd4df433e75f546b9ac31c1ba7b0b128bfb1b90df19082466ff57", size = 44404 },
    { url = "https://files.pythonhosted.org/packages/3a/0f/a79dd23a0efd6ee01ab0dc9750d8479b343bfd0c73560d59d271eb6a99d4/propcache-0.3.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a2b9bf8c79b660d0ca1ad95e587818c30ccdb11f787657458d6f26a1ea18c568", size = 77287 },
    { url = "https://files.pythonhosted.org/packages/b8/51/76675703c90de38ac75adb8deceb3f3ad99b67ff02a0fa5d067757971ab8/propcache-0.3.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b0c1a133d42c6fc1f5fbcf5c91331657a1ff822e87989bf4a6e2e39b818d0ee9", size = 44923 },
    { url = "https://files.pythonhosted.org/packages/01/9b/fd5ddbee66cf7686e73c516227c2fd9bf471dbfed0f48329d095ea1228d3/propcache-0.3.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:bb2f144c6d98bb5cbc94adeb0447cfd4c0f991341baa68eee3f3b0c9c0e83767", size = 44325 },
    { url = "https://files.pythonhosted.org/packages/13/1c/6961f11eb215a683b34b903b82bde486c606516c1466bf1fa67f26906d51/propcache-0.3.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d1323cd04d6e92150bcc79d0174ce347ed4b349d748b9358fd2e497b121e03c8", size = 225116 },
    { url = "https://files.pythonhosted.org/packages/ef/ea/f8410c40abcb2e40dffe9adeed017898c930974650a63e5c79b886aa9f73/propcache-0.3.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3b812b3cb6caacd072276ac0492d249f210006c57726b6484a1e1805b3cfeea0", size = 229905 },
    { url = "https://files.pythonhosted.org/packages/ef/5a/a9bf90894001468bf8e6ea293bb00626cc9ef10f8eb7996e9ec29345c7ed/propcache-0.3.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:742840d1d0438eb7ea4280f3347598f507a199a35a08294afdcc560c3739989d", size = 233221 },
    { url = "https://files.pythonhosted.org/packages/dd/ce/fffdddd9725b690b01d345c1156b4c2cc6dca09ab5c23a6d07b8f37d6e2f/propcache-0.3.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c6e7e4f9167fddc438cd653d826f2222222564daed4116a02a184b464d3ef05", size = 227627 },
    { url = "https://files.pythonhosted.org/packages/58/ae/45c89a5994a334735a3032b48e8e4a98c05d9536ddee0719913dc27da548/propcache-0.3.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a94ffc66738da99232ddffcf7910e0f69e2bbe3a0802e54426dbf0714e1c2ffe", size = 214217 },
    { url = "https://files.pythonhosted.org/packages/01/84/bc60188c3290ff8f5f4a92b9ca2d93a62e449c8daf6fd11ad517ad136926/propcache-0.3.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:3c6ec957025bf32b15cbc6b67afe233c65b30005e4c55fe5768e4bb518d712f1", size = 212921 },
    { url = "https://files.pythonhosted.org/packages/14/b3/39d60224048feef7a96edabb8217dc3f75415457e5ebbef6814f8b2a27b5/propcache-0.3.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:549722908de62aa0b47a78b90531c022fa6e139f9166be634f667ff45632cc92", size = 208200 },
    { url = "https://files.pythonhosted.org/packages/9d/b3/0a6720b86791251273fff8a01bc8e628bc70903513bd456f86cde1e1ef84/propcache-0.3.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:5d62c4f6706bff5d8a52fd51fec6069bef69e7202ed481486c0bc3874912c787", size = 208400 },
    { url = "https://files.pythonhosted.org/packages/e9/4f/bb470f3e687790547e2e78105fb411f54e0cdde0d74106ccadd2521c6572/propcache-0.3.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:24c04f8fbf60094c531667b8207acbae54146661657a1b1be6d3ca7773b7a545", size = 218116 },
    { url = "https://files.pythonhosted.org/packages/34/71/277f7f9add469698ac9724c199bfe06f85b199542121a71f65a80423d62a/propcache-0.3.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:7c5f5290799a3f6539cc5e6f474c3e5c5fbeba74a5e1e5be75587746a940d51e", size = 222911 },
    { url = "https://files.pythonhosted.org/packages/92/e3/a7b9782aef5a2fc765b1d97da9ec7aed2f25a4e985703608e73232205e3f/propcache-0.3.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4fa0e7c9c3cf7c276d4f6ab9af8adddc127d04e0fcabede315904d2ff76db626", size = 216563 },
    { url = "https://files.pythonhosted.org/packages/ab/76/0583ca2c551aa08ffcff87b2c6849c8f01c1f6fb815a5226f0c5c202173e/propcache-0.3.0-cp313-cp313-win32.whl", hash = "sha256:ee0bd3a7b2e184e88d25c9baa6a9dc609ba25b76daae942edfb14499ac7ec374", size = 39763 },
    { url = "https://files.pythonhosted.org/packages/80/ec/c6a84f9a36f608379b95f0e786c111d5465926f8c62f12be8cdadb02b15c/propcache-0.3.0-cp313-cp313-win_amd64.whl", hash = "sha256:1c8f7d896a16da9455f882870a507567d4f58c53504dc2d4b1e1d386dfe4588a", size = 43650 },
    { url = "https://files.pythonhosted.org/packages/ee/95/7d32e3560f5bf83fc2f2a4c1b0c181d327d53d5f85ebd045ab89d4d97763/propcache-0.3.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:e560fd75aaf3e5693b91bcaddd8b314f4d57e99aef8a6c6dc692f935cc1e6bbf", size = 82140 },
    { url = "https://files.pythonhosted.org/packages/86/89/752388f12e6027a5e63f5d075f15291ded48e2d8311314fff039da5a9b11/propcache-0.3.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:65a37714b8ad9aba5780325228598a5b16c47ba0f8aeb3dc0514701e4413d7c0", size = 47296 },
    { url = "https://files.pythonhosted.org/packages/1b/4c/b55c98d586c69180d3048984a57a5ea238bdeeccf82dbfcd598e935e10bb/propcache-0.3.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:07700939b2cbd67bfb3b76a12e1412405d71019df00ca5697ce75e5ef789d829", size = 46724 },
    { url = "https://files.pythonhosted.org/packages/0f/b6/67451a437aed90c4e951e320b5b3d7eb584ade1d5592f6e5e8f678030989/propcache-0.3.0-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7c0fdbdf6983526e269e5a8d53b7ae3622dd6998468821d660d0daf72779aefa", size = 291499 },
    { url = "https://files.pythonhosted.org/packages/ee/ff/e4179facd21515b24737e1e26e02615dfb5ed29416eed4cf5bc6ac5ce5fb/propcache-0.3.0-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:794c3dd744fad478b6232289c866c25406ecdfc47e294618bdf1697e69bd64a6", size = 293911 },
    { url = "https://files.pythonhosted.org/packages/76/8d/94a8585992a064a23bd54f56c5e58c3b8bf0c0a06ae10e56f2353ae16c3d/propcache-0.3.0-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4544699674faf66fb6b4473a1518ae4999c1b614f0b8297b1cef96bac25381db", size = 293301 },
    { url = "https://files.pythonhosted.org/packages/b0/b8/2c860c92b4134f68c7716c6f30a0d723973f881c32a6d7a24c4ddca05fdf/propcache-0.3.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fddb8870bdb83456a489ab67c6b3040a8d5a55069aa6f72f9d872235fbc52f54", size = 281947 },
    { url = "https://files.pythonhosted.org/packages/cd/72/b564be7411b525d11757b713c757c21cd4dc13b6569c3b2b8f6d3c96fd5e/propcache-0.3.0-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f857034dc68d5ceb30fb60afb6ff2103087aea10a01b613985610e007053a121", size = 268072 },
    { url = "https://files.pythonhosted.org/packages/37/68/d94649e399e8d7fc051e5a4f2334efc567993525af083db145a70690a121/propcache-0.3.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:02df07041e0820cacc8f739510078f2aadcfd3fc57eaeeb16d5ded85c872c89e", size = 275190 },
    { url = "https://files.pythonhosted.org/packages/d8/3c/446e125f5bbbc1922964dd67cb541c01cdb678d811297b79a4ff6accc843/propcache-0.3.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:f47d52fd9b2ac418c4890aad2f6d21a6b96183c98021f0a48497a904199f006e", size = 254145 },
    { url = "https://files.pythonhosted.org/packages/f4/80/fd3f741483dc8e59f7ba7e05eaa0f4e11677d7db2077522b92ff80117a2a/propcache-0.3.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:9ff4e9ecb6e4b363430edf2c6e50173a63e0820e549918adef70515f87ced19a", size = 257163 },
    { url = "https://files.pythonhosted.org/packages/dc/cf/6292b5ce6ed0017e6a89024a827292122cc41b6259b30ada0c6732288513/propcache-0.3.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:ecc2920630283e0783c22e2ac94427f8cca29a04cfdf331467d4f661f4072dac", size = 280249 },
    { url = "https://files.pythonhosted.org/packages/e8/f0/fd9b8247b449fe02a4f96538b979997e229af516d7462b006392badc59a1/propcache-0.3.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:c441c841e82c5ba7a85ad25986014be8d7849c3cfbdb6004541873505929a74e", size = 288741 },
    { url = "https://files.pythonhosted.org/packages/64/71/cf831fdc2617f86cfd7f414cfc487d018e722dac8acc098366ce9bba0941/propcache-0.3.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:6c929916cbdb540d3407c66f19f73387f43e7c12fa318a66f64ac99da601bcdf", size = 277061 },
    { url = "https://files.pythonhosted.org/packages/42/78/9432542a35d944abeca9e02927a0de38cd7a298466d8ffa171536e2381c3/propcache-0.3.0-cp313-cp313t-win32.whl", hash = "sha256:0c3e893c4464ebd751b44ae76c12c5f5c1e4f6cbd6fbf67e3783cd93ad221863", size = 42252 },
    { url = "https://files.pythonhosted.org/packages/6f/45/960365f4f8978f48ebb56b1127adf33a49f2e69ecd46ac1f46d6cf78a79d/propcache-0.3.0-cp313-cp313t-win_amd64.whl", hash = "sha256:75e872573220d1ee2305b35c9813626e620768248425f58798413e9c39741f46", size = 46425 },
    { url = "https://files.pythonhosted.org/packages/b5/35/6c4c6fc8774a9e3629cd750dc24a7a4fb090a25ccd5c3246d127b70f9e22/propcache-0.3.0-py3-none-any.whl", hash = "sha256:67dda3c7325691c2081510e92c561f465ba61b975f481735aefdfc845d2cd043", size = 12101 },
]

[[package]]
name = "pycparser"
version = "2.22"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1d/b2/31537cf4b1ca988837256c910a668b553fceb8f069bedc4b1c826024b52c/pycparser-2.22.tar.gz", hash = "sha256:491c8be9c040f5390f5bf44a5b07752bd07f56edf992381b05c701439eec10f6", size = 172736 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl", hash = "sha256:c3702b6d3dd8c7abc1afa565d7e63d53a1d0bd86cdc24edd75470f4de499cfcc", size = 117552 },
]

[[package]]
name = "pydantic"
version = "2.10.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/70/7e/fb60e6fee04d0ef8f15e4e01ff187a196fa976eb0f0ab524af4599e5754c/pydantic-2.10.4.tar.gz", hash = "sha256:82f12e9723da6de4fe2ba888b5971157b3be7ad914267dea8f05f82b28254f06", size = 762094 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f3/26/3e1bbe954fde7ee22a6e7d31582c642aad9e84ffe4b5fb61e63b87cd326f/pydantic-2.10.4-py3-none-any.whl", hash = "sha256:597e135ea68be3a37552fb524bc7d0d66dcf93d395acd93a00682f1efcb8ee3d", size = 431765 },
]

[[package]]
name = "pydantic-core"
version = "2.27.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fc/01/f3e5ac5e7c25833db5eb555f7b7ab24cd6f8c322d3a3ad2d67a952dc0abc/pydantic_core-2.27.2.tar.gz", hash = "sha256:eb026e5a4c1fee05726072337ff51d1efb6f59090b7da90d30ea58625b1ffb39", size = 413443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/89/f3450af9d09d44eea1f2c369f49e8f181d742f28220f88cc4dfaae91ea6e/pydantic_core-2.27.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:8e10c99ef58cfdf2a66fc15d66b16c4a04f62bca39db589ae8cba08bc55331bc", size = 1893421 },
    { url = "https://files.pythonhosted.org/packages/9e/e3/71fe85af2021f3f386da42d291412e5baf6ce7716bd7101ea49c810eda90/pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:26f32e0adf166a84d0cb63be85c562ca8a6fa8de28e5f0d92250c6b7e9e2aff7", size = 1814998 },
    { url = "https://files.pythonhosted.org/packages/a6/3c/724039e0d848fd69dbf5806894e26479577316c6f0f112bacaf67aa889ac/pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8c19d1ea0673cd13cc2f872f6c9ab42acc4e4f492a7ca9d3795ce2b112dd7e15", size = 1826167 },
    { url = "https://files.pythonhosted.org/packages/2b/5b/1b29e8c1fb5f3199a9a57c1452004ff39f494bbe9bdbe9a81e18172e40d3/pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5e68c4446fe0810e959cdff46ab0a41ce2f2c86d227d96dc3847af0ba7def306", size = 1865071 },
    { url = "https://files.pythonhosted.org/packages/89/6c/3985203863d76bb7d7266e36970d7e3b6385148c18a68cc8915fd8c84d57/pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d9640b0059ff4f14d1f37321b94061c6db164fbe49b334b31643e0528d100d99", size = 2036244 },
    { url = "https://files.pythonhosted.org/packages/0e/41/f15316858a246b5d723f7d7f599f79e37493b2e84bfc789e58d88c209f8a/pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:40d02e7d45c9f8af700f3452f329ead92da4c5f4317ca9b896de7ce7199ea459", size = 2737470 },
    { url = "https://files.pythonhosted.org/packages/a8/7c/b860618c25678bbd6d1d99dbdfdf0510ccb50790099b963ff78a124b754f/pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1c1fd185014191700554795c99b347d64f2bb637966c4cfc16998a0ca700d048", size = 1992291 },
    { url = "https://files.pythonhosted.org/packages/bf/73/42c3742a391eccbeab39f15213ecda3104ae8682ba3c0c28069fbcb8c10d/pydantic_core-2.27.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d81d2068e1c1228a565af076598f9e7451712700b673de8f502f0334f281387d", size = 1994613 },
    { url = "https://files.pythonhosted.org/packages/94/7a/941e89096d1175d56f59340f3a8ebaf20762fef222c298ea96d36a6328c5/pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:1a4207639fb02ec2dbb76227d7c751a20b1a6b4bc52850568e52260cae64ca3b", size = 2002355 },
    { url = "https://files.pythonhosted.org/packages/6e/95/2359937a73d49e336a5a19848713555605d4d8d6940c3ec6c6c0ca4dcf25/pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:3de3ce3c9ddc8bbd88f6e0e304dea0e66d843ec9de1b0042b0911c1663ffd474", size = 2126661 },
    { url = "https://files.pythonhosted.org/packages/2b/4c/ca02b7bdb6012a1adef21a50625b14f43ed4d11f1fc237f9d7490aa5078c/pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:30c5f68ded0c36466acede341551106821043e9afaad516adfb6e8fa80a4e6a6", size = 2153261 },
    { url = "https://files.pythonhosted.org/packages/72/9d/a241db83f973049a1092a079272ffe2e3e82e98561ef6214ab53fe53b1c7/pydantic_core-2.27.2-cp311-cp311-win32.whl", hash = "sha256:c70c26d2c99f78b125a3459f8afe1aed4d9687c24fd677c6a4436bc042e50d6c", size = 1812361 },
    { url = "https://files.pythonhosted.org/packages/e8/ef/013f07248041b74abd48a385e2110aa3a9bbfef0fbd97d4e6d07d2f5b89a/pydantic_core-2.27.2-cp311-cp311-win_amd64.whl", hash = "sha256:08e125dbdc505fa69ca7d9c499639ab6407cfa909214d500897d02afb816e7cc", size = 1982484 },
    { url = "https://files.pythonhosted.org/packages/10/1c/16b3a3e3398fd29dca77cea0a1d998d6bde3902fa2706985191e2313cc76/pydantic_core-2.27.2-cp311-cp311-win_arm64.whl", hash = "sha256:26f0d68d4b235a2bae0c3fc585c585b4ecc51382db0e3ba402a22cbc440915e4", size = 1867102 },
    { url = "https://files.pythonhosted.org/packages/d6/74/51c8a5482ca447871c93e142d9d4a92ead74de6c8dc5e66733e22c9bba89/pydantic_core-2.27.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:9e0c8cfefa0ef83b4da9588448b6d8d2a2bf1a53c3f1ae5fca39eb3061e2f0b0", size = 1893127 },
    { url = "https://files.pythonhosted.org/packages/d3/f3/c97e80721735868313c58b89d2de85fa80fe8dfeeed84dc51598b92a135e/pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:83097677b8e3bd7eaa6775720ec8e0405f1575015a463285a92bfdfe254529ef", size = 1811340 },
    { url = "https://files.pythonhosted.org/packages/9e/91/840ec1375e686dbae1bd80a9e46c26a1e0083e1186abc610efa3d9a36180/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:172fce187655fece0c90d90a678424b013f8fbb0ca8b036ac266749c09438cb7", size = 1822900 },
    { url = "https://files.pythonhosted.org/packages/f6/31/4240bc96025035500c18adc149aa6ffdf1a0062a4b525c932065ceb4d868/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:519f29f5213271eeeeb3093f662ba2fd512b91c5f188f3bb7b27bc5973816934", size = 1869177 },
    { url = "https://files.pythonhosted.org/packages/fa/20/02fbaadb7808be578317015c462655c317a77a7c8f0ef274bc016a784c54/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:05e3a55d124407fffba0dd6b0c0cd056d10e983ceb4e5dbd10dda135c31071d6", size = 2038046 },
    { url = "https://files.pythonhosted.org/packages/06/86/7f306b904e6c9eccf0668248b3f272090e49c275bc488a7b88b0823444a4/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9c3ed807c7b91de05e63930188f19e921d1fe90de6b4f5cd43ee7fcc3525cb8c", size = 2685386 },
    { url = "https://files.pythonhosted.org/packages/8d/f0/49129b27c43396581a635d8710dae54a791b17dfc50c70164866bbf865e3/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6fb4aadc0b9a0c063206846d603b92030eb6f03069151a625667f982887153e2", size = 1997060 },
    { url = "https://files.pythonhosted.org/packages/0d/0f/943b4af7cd416c477fd40b187036c4f89b416a33d3cc0ab7b82708a667aa/pydantic_core-2.27.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:28ccb213807e037460326424ceb8b5245acb88f32f3d2777427476e1b32c48c4", size = 2004870 },
    { url = "https://files.pythonhosted.org/packages/35/40/aea70b5b1a63911c53a4c8117c0a828d6790483f858041f47bab0b779f44/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:de3cd1899e2c279b140adde9357c4495ed9d47131b4a4eaff9052f23398076b3", size = 1999822 },
    { url = "https://files.pythonhosted.org/packages/f2/b3/807b94fd337d58effc5498fd1a7a4d9d59af4133e83e32ae39a96fddec9d/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:220f892729375e2d736b97d0e51466252ad84c51857d4d15f5e9692f9ef12be4", size = 2130364 },
    { url = "https://files.pythonhosted.org/packages/fc/df/791c827cd4ee6efd59248dca9369fb35e80a9484462c33c6649a8d02b565/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:a0fcd29cd6b4e74fe8ddd2c90330fd8edf2e30cb52acda47f06dd615ae72da57", size = 2158303 },
    { url = "https://files.pythonhosted.org/packages/9b/67/4e197c300976af185b7cef4c02203e175fb127e414125916bf1128b639a9/pydantic_core-2.27.2-cp312-cp312-win32.whl", hash = "sha256:1e2cb691ed9834cd6a8be61228471d0a503731abfb42f82458ff27be7b2186fc", size = 1834064 },
    { url = "https://files.pythonhosted.org/packages/1f/ea/cd7209a889163b8dcca139fe32b9687dd05249161a3edda62860430457a5/pydantic_core-2.27.2-cp312-cp312-win_amd64.whl", hash = "sha256:cc3f1a99a4f4f9dd1de4fe0312c114e740b5ddead65bb4102884b384c15d8bc9", size = 1989046 },
    { url = "https://files.pythonhosted.org/packages/bc/49/c54baab2f4658c26ac633d798dab66b4c3a9bbf47cff5284e9c182f4137a/pydantic_core-2.27.2-cp312-cp312-win_arm64.whl", hash = "sha256:3911ac9284cd8a1792d3cb26a2da18f3ca26c6908cc434a18f730dc0db7bfa3b", size = 1885092 },
    { url = "https://files.pythonhosted.org/packages/41/b1/9bc383f48f8002f99104e3acff6cba1231b29ef76cfa45d1506a5cad1f84/pydantic_core-2.27.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:7d14bd329640e63852364c306f4d23eb744e0f8193148d4044dd3dacdaacbd8b", size = 1892709 },
    { url = "https://files.pythonhosted.org/packages/10/6c/e62b8657b834f3eb2961b49ec8e301eb99946245e70bf42c8817350cbefc/pydantic_core-2.27.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:82f91663004eb8ed30ff478d77c4d1179b3563df6cdb15c0817cd1cdaf34d154", size = 1811273 },
    { url = "https://files.pythonhosted.org/packages/ba/15/52cfe49c8c986e081b863b102d6b859d9defc63446b642ccbbb3742bf371/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:71b24c7d61131bb83df10cc7e687433609963a944ccf45190cfc21e0887b08c9", size = 1823027 },
    { url = "https://files.pythonhosted.org/packages/b1/1c/b6f402cfc18ec0024120602bdbcebc7bdd5b856528c013bd4d13865ca473/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:fa8e459d4954f608fa26116118bb67f56b93b209c39b008277ace29937453dc9", size = 1868888 },
    { url = "https://files.pythonhosted.org/packages/bd/7b/8cb75b66ac37bc2975a3b7de99f3c6f355fcc4d89820b61dffa8f1e81677/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ce8918cbebc8da707ba805b7fd0b382816858728ae7fe19a942080c24e5b7cd1", size = 2037738 },
    { url = "https://files.pythonhosted.org/packages/c8/f1/786d8fe78970a06f61df22cba58e365ce304bf9b9f46cc71c8c424e0c334/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:eda3f5c2a021bbc5d976107bb302e0131351c2ba54343f8a496dc8783d3d3a6a", size = 2685138 },
    { url = "https://files.pythonhosted.org/packages/a6/74/d12b2cd841d8724dc8ffb13fc5cef86566a53ed358103150209ecd5d1999/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bd8086fa684c4775c27f03f062cbb9eaa6e17f064307e86b21b9e0abc9c0f02e", size = 1997025 },
    { url = "https://files.pythonhosted.org/packages/a0/6e/940bcd631bc4d9a06c9539b51f070b66e8f370ed0933f392db6ff350d873/pydantic_core-2.27.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:8d9b3388db186ba0c099a6d20f0604a44eabdeef1777ddd94786cdae158729e4", size = 2004633 },
    { url = "https://files.pythonhosted.org/packages/50/cc/a46b34f1708d82498c227d5d80ce615b2dd502ddcfd8376fc14a36655af1/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:7a66efda2387de898c8f38c0cf7f14fca0b51a8ef0b24bfea5849f1b3c95af27", size = 1999404 },
    { url = "https://files.pythonhosted.org/packages/ca/2d/c365cfa930ed23bc58c41463bae347d1005537dc8db79e998af8ba28d35e/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:18a101c168e4e092ab40dbc2503bdc0f62010e95d292b27827871dc85450d7ee", size = 2130130 },
    { url = "https://files.pythonhosted.org/packages/f4/d7/eb64d015c350b7cdb371145b54d96c919d4db516817f31cd1c650cae3b21/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ba5dd002f88b78a4215ed2f8ddbdf85e8513382820ba15ad5ad8955ce0ca19a1", size = 2157946 },
    { url = "https://files.pythonhosted.org/packages/a4/99/bddde3ddde76c03b65dfd5a66ab436c4e58ffc42927d4ff1198ffbf96f5f/pydantic_core-2.27.2-cp313-cp313-win32.whl", hash = "sha256:1ebaf1d0481914d004a573394f4be3a7616334be70261007e47c2a6fe7e50130", size = 1834387 },
    { url = "https://files.pythonhosted.org/packages/71/47/82b5e846e01b26ac6f1893d3c5f9f3a2eb6ba79be26eef0b759b4fe72946/pydantic_core-2.27.2-cp313-cp313-win_amd64.whl", hash = "sha256:953101387ecf2f5652883208769a79e48db18c6df442568a0b5ccd8c2723abee", size = 1990453 },
    { url = "https://files.pythonhosted.org/packages/51/b2/b2b50d5ecf21acf870190ae5d093602d95f66c9c31f9d5de6062eb329ad1/pydantic_core-2.27.2-cp313-cp313-win_arm64.whl", hash = "sha256:ac4dbfd1691affb8f48c2c13241a2e3b60ff23247cbcf981759c768b6633cf8b", size = 1885186 },
]

[[package]]
name = "pyjwt"
version = "2.10.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/46/bd74733ff231675599650d3e47f361794b22ef3e3770998dda30d3b63726/pyjwt-2.10.1.tar.gz", hash = "sha256:3cc5772eb20009233caf06e9d8a0577824723b44e6648ee0a2aedb6cf9381953", size = 87785 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/61/ad/689f02752eeec26aed679477e80e632ef1b682313be70793d798c1d5fc8f/PyJWT-2.10.1-py3-none-any.whl", hash = "sha256:dcdd193e30abefd5debf142f9adfcdd2b58004e644f25406ffaebd50bd98dacb", size = 22997 },
]

[[package]]
name = "python-dotenv"
version = "1.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bc/57/e84d88dfe0aec03b7a2d4327012c1627ab5f03652216c63d49846d7a6c58/python-dotenv-1.0.1.tar.gz", hash = "sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca", size = 39115 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl", hash = "sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a", size = 19863 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f8/aa/7af4e81f7acba21a4c6be026da38fd2b872ca46226673c89a758ebdc4fd2/PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774", size = 184612 },
    { url = "https://files.pythonhosted.org/packages/8b/62/b9faa998fd185f65c1371643678e4d58254add437edb764a08c5a98fb986/PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee", size = 172040 },
    { url = "https://files.pythonhosted.org/packages/ad/0c/c804f5f922a9a6563bab712d8dcc70251e8af811fce4524d57c2c0fd49a4/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c", size = 736829 },
    { url = "https://files.pythonhosted.org/packages/51/16/6af8d6a6b210c8e54f1406a6b9481febf9c64a3109c541567e35a49aa2e7/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317", size = 764167 },
    { url = "https://files.pythonhosted.org/packages/75/e4/2c27590dfc9992f73aabbeb9241ae20220bd9452df27483b6e56d3975cc5/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85", size = 762952 },
    { url = "https://files.pythonhosted.org/packages/9b/97/ecc1abf4a823f5ac61941a9c00fe501b02ac3ab0e373c3857f7d4b83e2b6/PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4", size = 735301 },
    { url = "https://files.pythonhosted.org/packages/45/73/0f49dacd6e82c9430e46f4a027baa4ca205e8b0a9dce1397f44edc23559d/PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e", size = 756638 },
    { url = "https://files.pythonhosted.org/packages/22/5f/956f0f9fc65223a58fbc14459bf34b4cc48dec52e00535c79b8db361aabd/PyYAML-6.0.2-cp311-cp311-win32.whl", hash = "sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5", size = 143850 },
    { url = "https://files.pythonhosted.org/packages/ed/23/8da0bbe2ab9dcdd11f4f4557ccaf95c10b9811b13ecced089d43ce59c3c8/PyYAML-6.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44", size = 161980 },
    { url = "https://files.pythonhosted.org/packages/86/0c/c581167fc46d6d6d7ddcfb8c843a4de25bdd27e4466938109ca68492292c/PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab", size = 183873 },
    { url = "https://files.pythonhosted.org/packages/a8/0c/38374f5bb272c051e2a69281d71cba6fdb983413e6758b84482905e29a5d/PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725", size = 173302 },
    { url = "https://files.pythonhosted.org/packages/c3/93/9916574aa8c00aa06bbac729972eb1071d002b8e158bd0e83a3b9a20a1f7/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5", size = 739154 },
    { url = "https://files.pythonhosted.org/packages/95/0f/b8938f1cbd09739c6da569d172531567dbcc9789e0029aa070856f123984/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425", size = 766223 },
    { url = "https://files.pythonhosted.org/packages/b9/2b/614b4752f2e127db5cc206abc23a8c19678e92b23c3db30fc86ab731d3bd/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476", size = 767542 },
    { url = "https://files.pythonhosted.org/packages/d4/00/dd137d5bcc7efea1836d6264f049359861cf548469d18da90cd8216cf05f/PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48", size = 731164 },
    { url = "https://files.pythonhosted.org/packages/c9/1f/4f998c900485e5c0ef43838363ba4a9723ac0ad73a9dc42068b12aaba4e4/PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b", size = 756611 },
    { url = "https://files.pythonhosted.org/packages/df/d1/f5a275fdb252768b7a11ec63585bc38d0e87c9e05668a139fea92b80634c/PyYAML-6.0.2-cp312-cp312-win32.whl", hash = "sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4", size = 140591 },
    { url = "https://files.pythonhosted.org/packages/0c/e8/4f648c598b17c3d06e8753d7d13d57542b30d56e6c2dedf9c331ae56312e/PyYAML-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8", size = 156338 },
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/58/58/7e4d9493a66c88a7da6d205768119f51af0f684fe7be7bac8328e217a52c/regex-2024.11.6-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:5478c6962ad548b54a591778e93cd7c456a7a29f8eca9c49e4f9a806dcc5d638", size = 482669 },
    { url = "https://files.pythonhosted.org/packages/34/4c/8f8e631fcdc2ff978609eaeef1d6994bf2f028b59d9ac67640ed051f1218/regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:2c89a8cc122b25ce6945f0423dc1352cb9593c68abd19223eebbd4e56612c5b7", size = 287684 },
    { url = "https://files.pythonhosted.org/packages/c5/1b/f0e4d13e6adf866ce9b069e191f303a30ab1277e037037a365c3aad5cc9c/regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:94d87b689cdd831934fa3ce16cc15cd65748e6d689f5d2b8f4f4df2065c9fa20", size = 284589 },
    { url = "https://files.pythonhosted.org/packages/25/4d/ab21047f446693887f25510887e6820b93f791992994f6498b0318904d4a/regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1062b39a0a2b75a9c694f7a08e7183a80c63c0d62b301418ffd9c35f55aaa114", size = 792121 },
    { url = "https://files.pythonhosted.org/packages/45/ee/c867e15cd894985cb32b731d89576c41a4642a57850c162490ea34b78c3b/regex-2024.11.6-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:167ed4852351d8a750da48712c3930b031f6efdaa0f22fa1933716bfcd6bf4a3", size = 831275 },
    { url = "https://files.pythonhosted.org/packages/b3/12/b0f480726cf1c60f6536fa5e1c95275a77624f3ac8fdccf79e6727499e28/regex-2024.11.6-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d548dafee61f06ebdb584080621f3e0c23fff312f0de1afc776e2a2ba99a74f", size = 818257 },
    { url = "https://files.pythonhosted.org/packages/bf/ce/0d0e61429f603bac433910d99ef1a02ce45a8967ffbe3cbee48599e62d88/regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f2a19f302cd1ce5dd01a9099aaa19cae6173306d1302a43b627f62e21cf18ac0", size = 792727 },
    { url = "https://files.pythonhosted.org/packages/e4/c1/243c83c53d4a419c1556f43777ccb552bccdf79d08fda3980e4e77dd9137/regex-2024.11.6-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:bec9931dfb61ddd8ef2ebc05646293812cb6b16b60cf7c9511a832b6f1854b55", size = 780667 },
    { url = "https://files.pythonhosted.org/packages/c5/f4/75eb0dd4ce4b37f04928987f1d22547ddaf6c4bae697623c1b05da67a8aa/regex-2024.11.6-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:9714398225f299aa85267fd222f7142fcb5c769e73d7733344efc46f2ef5cf89", size = 776963 },
    { url = "https://files.pythonhosted.org/packages/16/5d/95c568574e630e141a69ff8a254c2f188b4398e813c40d49228c9bbd9875/regex-2024.11.6-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:202eb32e89f60fc147a41e55cb086db2a3f8cb82f9a9a88440dcfc5d37faae8d", size = 784700 },
    { url = "https://files.pythonhosted.org/packages/8e/b5/f8495c7917f15cc6fee1e7f395e324ec3e00ab3c665a7dc9d27562fd5290/regex-2024.11.6-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:4181b814e56078e9b00427ca358ec44333765f5ca1b45597ec7446d3a1ef6e34", size = 848592 },
    { url = "https://files.pythonhosted.org/packages/1c/80/6dd7118e8cb212c3c60b191b932dc57db93fb2e36fb9e0e92f72a5909af9/regex-2024.11.6-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:068376da5a7e4da51968ce4c122a7cd31afaaec4fccc7856c92f63876e57b51d", size = 852929 },
    { url = "https://files.pythonhosted.org/packages/11/9b/5a05d2040297d2d254baf95eeeb6df83554e5e1df03bc1a6687fc4ba1f66/regex-2024.11.6-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:ac10f2c4184420d881a3475fb2c6f4d95d53a8d50209a2500723d831036f7c45", size = 781213 },
    { url = "https://files.pythonhosted.org/packages/26/b7/b14e2440156ab39e0177506c08c18accaf2b8932e39fb092074de733d868/regex-2024.11.6-cp311-cp311-win32.whl", hash = "sha256:c36f9b6f5f8649bb251a5f3f66564438977b7ef8386a52460ae77e6070d309d9", size = 261734 },
    { url = "https://files.pythonhosted.org/packages/80/32/763a6cc01d21fb3819227a1cc3f60fd251c13c37c27a73b8ff4315433a8e/regex-2024.11.6-cp311-cp311-win_amd64.whl", hash = "sha256:02e28184be537f0e75c1f9b2f8847dc51e08e6e171c6bde130b2687e0c33cf60", size = 274052 },
    { url = "https://files.pythonhosted.org/packages/ba/30/9a87ce8336b172cc232a0db89a3af97929d06c11ceaa19d97d84fa90a8f8/regex-2024.11.6-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:52fb28f528778f184f870b7cf8f225f5eef0a8f6e3778529bdd40c7b3920796a", size = 483781 },
    { url = "https://files.pythonhosted.org/packages/01/e8/00008ad4ff4be8b1844786ba6636035f7ef926db5686e4c0f98093612add/regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:fdd6028445d2460f33136c55eeb1f601ab06d74cb3347132e1c24250187500d9", size = 288455 },
    { url = "https://files.pythonhosted.org/packages/60/85/cebcc0aff603ea0a201667b203f13ba75d9fc8668fab917ac5b2de3967bc/regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:805e6b60c54bf766b251e94526ebad60b7de0c70f70a4e6210ee2891acb70bf2", size = 284759 },
    { url = "https://files.pythonhosted.org/packages/94/2b/701a4b0585cb05472a4da28ee28fdfe155f3638f5e1ec92306d924e5faf0/regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b85c2530be953a890eaffde05485238f07029600e8f098cdf1848d414a8b45e4", size = 794976 },
    { url = "https://files.pythonhosted.org/packages/4b/bf/fa87e563bf5fee75db8915f7352e1887b1249126a1be4813837f5dbec965/regex-2024.11.6-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bb26437975da7dc36b7efad18aa9dd4ea569d2357ae6b783bf1118dabd9ea577", size = 833077 },
    { url = "https://files.pythonhosted.org/packages/a1/56/7295e6bad94b047f4d0834e4779491b81216583c00c288252ef625c01d23/regex-2024.11.6-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:abfa5080c374a76a251ba60683242bc17eeb2c9818d0d30117b4486be10c59d3", size = 823160 },
    { url = "https://files.pythonhosted.org/packages/fb/13/e3b075031a738c9598c51cfbc4c7879e26729c53aa9cca59211c44235314/regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b7fa6606c2881c1db9479b0eaa11ed5dfa11c8d60a474ff0e095099f39d98e", size = 796896 },
    { url = "https://files.pythonhosted.org/packages/24/56/0b3f1b66d592be6efec23a795b37732682520b47c53da5a32c33ed7d84e3/regex-2024.11.6-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0c32f75920cf99fe6b6c539c399a4a128452eaf1af27f39bce8909c9a3fd8cbe", size = 783997 },
    { url = "https://files.pythonhosted.org/packages/f9/a1/eb378dada8b91c0e4c5f08ffb56f25fcae47bf52ad18f9b2f33b83e6d498/regex-2024.11.6-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:982e6d21414e78e1f51cf595d7f321dcd14de1f2881c5dc6a6e23bbbbd68435e", size = 781725 },
    { url = "https://files.pythonhosted.org/packages/83/f2/033e7dec0cfd6dda93390089864732a3409246ffe8b042e9554afa9bff4e/regex-2024.11.6-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:a7c2155f790e2fb448faed6dd241386719802296ec588a8b9051c1f5c481bc29", size = 789481 },
    { url = "https://files.pythonhosted.org/packages/83/23/15d4552ea28990a74e7696780c438aadd73a20318c47e527b47a4a5a596d/regex-2024.11.6-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:149f5008d286636e48cd0b1dd65018548944e495b0265b45e1bffecce1ef7f39", size = 852896 },
    { url = "https://files.pythonhosted.org/packages/e3/39/ed4416bc90deedbfdada2568b2cb0bc1fdb98efe11f5378d9892b2a88f8f/regex-2024.11.6-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:e5364a4502efca094731680e80009632ad6624084aff9a23ce8c8c6820de3e51", size = 860138 },
    { url = "https://files.pythonhosted.org/packages/93/2d/dd56bb76bd8e95bbce684326302f287455b56242a4f9c61f1bc76e28360e/regex-2024.11.6-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:0a86e7eeca091c09e021db8eb72d54751e527fa47b8d5787caf96d9831bd02ad", size = 787692 },
    { url = "https://files.pythonhosted.org/packages/0b/55/31877a249ab7a5156758246b9c59539abbeba22461b7d8adc9e8475ff73e/regex-2024.11.6-cp312-cp312-win32.whl", hash = "sha256:32f9a4c643baad4efa81d549c2aadefaeba12249b2adc5af541759237eee1c54", size = 262135 },
    { url = "https://files.pythonhosted.org/packages/38/ec/ad2d7de49a600cdb8dd78434a1aeffe28b9d6fc42eb36afab4a27ad23384/regex-2024.11.6-cp312-cp312-win_amd64.whl", hash = "sha256:a93c194e2df18f7d264092dc8539b8ffb86b45b899ab976aa15d48214138e81b", size = 273567 },
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/63/70/2bf7780ad2d390a8d301ad0b550f1581eadbd9a20f896afe06353c2a2913/requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760", size = 131218 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6", size = 64928 },
]

[[package]]
name = "requests-toolbelt"
version = "1.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f3/61/d7545dafb7ac2230c70d38d31cbfe4cc64f7144dc41f6e4e4b78ecd9f5bb/requests-toolbelt-1.0.0.tar.gz", hash = "sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6", size = 206888 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl", hash = "sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06", size = 54481 },
]

[[package]]
name = "ruff"
version = "0.8.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/34/37/9c02181ef38d55b77d97c68b78e705fd14c0de0e5d085202bb2b52ce5be9/ruff-0.8.4.tar.gz", hash = "sha256:0d5f89f254836799af1615798caa5f80b7f935d7a670fad66c5007928e57ace8", size = 3402103 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/05/67/f480bf2f2723b2e49af38ed2be75ccdb2798fca7d56279b585c8f553aaab/ruff-0.8.4-py3-none-linux_armv6l.whl", hash = "sha256:58072f0c06080276804c6a4e21a9045a706584a958e644353603d36ca1eb8a60", size = 10546415 },
    { url = "https://files.pythonhosted.org/packages/eb/7a/5aba20312c73f1ce61814e520d1920edf68ca3b9c507bd84d8546a8ecaa8/ruff-0.8.4-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:ffb60904651c00a1e0b8df594591770018a0f04587f7deeb3838344fe3adabac", size = 10346113 },
    { url = "https://files.pythonhosted.org/packages/76/f4/c41de22b3728486f0aa95383a44c42657b2db4062f3234ca36fc8cf52d8b/ruff-0.8.4-py3-none-macosx_11_0_arm64.whl", hash = "sha256:6ddf5d654ac0d44389f6bf05cee4caeefc3132a64b58ea46738111d687352296", size = 9943564 },
    { url = "https://files.pythonhosted.org/packages/0e/f0/afa0d2191af495ac82d4cbbfd7a94e3df6f62a04ca412033e073b871fc6d/ruff-0.8.4-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e248b1f0fa2749edd3350a2a342b67b43a2627434c059a063418e3d375cfe643", size = 10805522 },
    { url = "https://files.pythonhosted.org/packages/12/57/5d1e9a0fd0c228e663894e8e3a8e7063e5ee90f8e8e60cf2085f362bfa1a/ruff-0.8.4-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:bf197b98ed86e417412ee3b6c893f44c8864f816451441483253d5ff22c0e81e", size = 10306763 },
    { url = "https://files.pythonhosted.org/packages/04/df/f069fdb02e408be8aac6853583572a2873f87f866fe8515de65873caf6b8/ruff-0.8.4-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c41319b85faa3aadd4d30cb1cffdd9ac6b89704ff79f7664b853785b48eccdf3", size = 11359574 },
    { url = "https://files.pythonhosted.org/packages/d3/04/37c27494cd02e4a8315680debfc6dfabcb97e597c07cce0044db1f9dfbe2/ruff-0.8.4-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:9f8402b7c4f96463f135e936d9ab77b65711fcd5d72e5d67597b543bbb43cf3f", size = 12094851 },
    { url = "https://files.pythonhosted.org/packages/81/b1/c5d7fb68506cab9832d208d03ea4668da9a9887a4a392f4f328b1bf734ad/ruff-0.8.4-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e4e56b3baa9c23d324ead112a4fdf20db9a3f8f29eeabff1355114dd96014604", size = 11655539 },
    { url = "https://files.pythonhosted.org/packages/ef/38/8f8f2c8898dc8a7a49bc340cf6f00226917f0f5cb489e37075bcb2ce3671/ruff-0.8.4-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:736272574e97157f7edbbb43b1d046125fce9e7d8d583d5d65d0c9bf2c15addf", size = 12912805 },
    { url = "https://files.pythonhosted.org/packages/06/dd/fa6660c279f4eb320788876d0cff4ea18d9af7d9ed7216d7bd66877468d0/ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e5fe710ab6061592521f902fca7ebcb9fabd27bc7c57c764298b1c1f15fff720", size = 11205976 },
    { url = "https://files.pythonhosted.org/packages/a8/d7/de94cc89833b5de455750686c17c9e10f4e1ab7ccdc5521b8fe911d1477e/ruff-0.8.4-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:13e9ec6d6b55f6da412d59953d65d66e760d583dd3c1c72bf1f26435b5bfdbae", size = 10792039 },
    { url = "https://files.pythonhosted.org/packages/6d/15/3e4906559248bdbb74854af684314608297a05b996062c9d72e0ef7c7097/ruff-0.8.4-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:97d9aefef725348ad77d6db98b726cfdb075a40b936c7984088804dfd38268a7", size = 10400088 },
    { url = "https://files.pythonhosted.org/packages/a2/21/9ed4c0e8133cb4a87a18d470f534ad1a8a66d7bec493bcb8bda2d1a5d5be/ruff-0.8.4-py3-none-musllinux_1_2_i686.whl", hash = "sha256:ab78e33325a6f5374e04c2ab924a3367d69a0da36f8c9cb6b894a62017506111", size = 10900814 },
    { url = "https://files.pythonhosted.org/packages/0d/5d/122a65a18955bd9da2616b69bc839351f8baf23b2805b543aa2f0aed72b5/ruff-0.8.4-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:8ef06f66f4a05c3ddbc9121a8b0cecccd92c5bf3dd43b5472ffe40b8ca10f0f8", size = 11268828 },
    { url = "https://files.pythonhosted.org/packages/43/a9/1676ee9106995381e3d34bccac5bb28df70194167337ed4854c20f27c7ba/ruff-0.8.4-py3-none-win32.whl", hash = "sha256:552fb6d861320958ca5e15f28b20a3d071aa83b93caee33a87b471f99a6c0835", size = 8805621 },
    { url = "https://files.pythonhosted.org/packages/10/98/ed6b56a30ee76771c193ff7ceeaf1d2acc98d33a1a27b8479cbdb5c17a23/ruff-0.8.4-py3-none-win_amd64.whl", hash = "sha256:f21a1143776f8656d7f364bd264a9d60f01b7f52243fbe90e7670c0dfe0cf65d", size = 9660086 },
    { url = "https://files.pythonhosted.org/packages/13/9f/026e18ca7d7766783d779dae5e9c656746c6ede36ef73c6d934aaf4a6dec/ruff-0.8.4-py3-none-win_arm64.whl", hash = "sha256:9183dd615d8df50defa8b1d9a074053891ba39025cf5ae88e8bcb52edcc4bf08", size = 9074500 },
]

[[package]]
name = "slack-bolt"
version = "1.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "slack-sdk" },
]
sdist = { url = "https://files.pythonhosted.org/packages/68/c4/50b9009135d3189e0120692034f1ae95a2db695253517f14a3a3f12a5a3f/slack_bolt-1.22.0.tar.gz", hash = "sha256:b9c66d088fe3ec8bdd0494278eb500fe58092c2941de86d6822d00f4b3c7c88b", size = 130600 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5d/2d/fb23c998c43ff8398d7fa1e58bb82e7e735fbdaa0bd4ddaac04b3865bd4c/slack_bolt-1.22.0-py2.py3-none-any.whl", hash = "sha256:349097136a586617e5fb71f40f58a30fa847f664c598577f67a01f99faa1a9eb", size = 229675 },
]

[[package]]
name = "slack-sdk"
version = "3.34.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6e/ff/6eb67fd5bd179fa804dbd859d88d872d3ae343955e63a319a73a132d406f/slack_sdk-3.34.0.tar.gz", hash = "sha256:ff61db7012160eed742285ea91f11c72b7a38a6500a7f6c5335662b4bc6b853d", size = 233629 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/25/2d/8724ef191cb64907de1e4e4436462955501e00f859a53d0aa794d0d060ff/slack_sdk-3.34.0-py2.py3-none-any.whl", hash = "sha256:c61f57f310d85be83466db5a98ab6ae3bb2e5587437b54fa0daa8fae6a0feffa", size = 292480 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "sqlalchemy"
version = "2.0.38"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "greenlet", marker = "(python_full_version < '3.14' and platform_machine == 'AMD64') or (python_full_version < '3.14' and platform_machine == 'WIN32') or (python_full_version < '3.14' and platform_machine == 'aarch64') or (python_full_version < '3.14' and platform_machine == 'amd64') or (python_full_version < '3.14' and platform_machine == 'ppc64le') or (python_full_version < '3.14' and platform_machine == 'win32') or (python_full_version < '3.14' and platform_machine == 'x86_64')" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e4/08/9a90962ea72acd532bda71249a626344d855c4032603924b1b547694b837/sqlalchemy-2.0.38.tar.gz", hash = "sha256:e5a4d82bdb4bf1ac1285a68eab02d253ab73355d9f0fe725a97e1e0fa689decb", size = 9634782 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/00/6c/9d3a638f297fce288ba12a4e5dbd08ef1841d119abee9300c100eba00217/SQLAlchemy-2.0.38-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:bf89e0e4a30714b357f5d46b6f20e0099d38b30d45fa68ea48589faf5f12f62d", size = 2106330 },
    { url = "https://files.pythonhosted.org/packages/0e/57/d5fdee56f418491267701965795805662b1744de40915d4764451390536d/SQLAlchemy-2.0.38-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:8455aa60da49cb112df62b4721bd8ad3654a3a02b9452c783e651637a1f21fa2", size = 2096730 },
    { url = "https://files.pythonhosted.org/packages/42/84/205f423f8b28329c47237b7e130a7f93c234a49fab20b4534bd1ff26a06a/SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f53c0d6a859b2db58332e0e6a921582a02c1677cc93d4cbb36fdf49709b327b2", size = 3215023 },
    { url = "https://files.pythonhosted.org/packages/77/41/94a558d47bffae5a361b0cfb3721324ea4154829dd5432f80bd4cfeecbc9/SQLAlchemy-2.0.38-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b3c4817dff8cef5697f5afe5fec6bc1783994d55a68391be24cb7d80d2dbc3a6", size = 3214991 },
    { url = "https://files.pythonhosted.org/packages/74/a0/cc3c030e7440bd17ce67c1875f50edb41d0ef17b9c76fbc290ef27bbe37f/SQLAlchemy-2.0.38-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:c9cea5b756173bb86e2235f2f871b406a9b9d722417ae31e5391ccaef5348f2c", size = 3151854 },
    { url = "https://files.pythonhosted.org/packages/24/ab/8ba2588c2eb1d092944551354d775ef4fc0250badede324d786a4395d10e/SQLAlchemy-2.0.38-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:40e9cdbd18c1f84631312b64993f7d755d85a3930252f6276a77432a2b25a2f3", size = 3172158 },
    { url = "https://files.pythonhosted.org/packages/e0/73/2a3d6217e8e6abb553ed410ce5adc0bdec7effd684716f0fbaee5831d677/SQLAlchemy-2.0.38-cp311-cp311-win32.whl", hash = "sha256:cb39ed598aaf102251483f3e4675c5dd6b289c8142210ef76ba24aae0a8f8aba", size = 2076965 },
    { url = "https://files.pythonhosted.org/packages/a4/17/364a99c8c5698492c7fa40fc463bf388f05b0b03b74028828b71a79dc89d/SQLAlchemy-2.0.38-cp311-cp311-win_amd64.whl", hash = "sha256:f9d57f1b3061b3e21476b0ad5f0397b112b94ace21d1f439f2db472e568178ae", size = 2102169 },
    { url = "https://files.pythonhosted.org/packages/5a/f8/6d0424af1442c989b655a7b5f608bc2ae5e4f94cdf6df9f6054f629dc587/SQLAlchemy-2.0.38-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:12d5b06a1f3aeccf295a5843c86835033797fea292c60e72b07bcb5d820e6dd3", size = 2104927 },
    { url = "https://files.pythonhosted.org/packages/25/80/fc06e65fca0a19533e2bfab633a5633ed8b6ee0b9c8d580acf84609ce4da/SQLAlchemy-2.0.38-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:e036549ad14f2b414c725349cce0772ea34a7ab008e9cd67f9084e4f371d1f32", size = 2095317 },
    { url = "https://files.pythonhosted.org/packages/98/2d/5d66605f76b8e344813237dc160a01f03b987201e974b46056a7fb94a874/SQLAlchemy-2.0.38-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ee3bee874cb1fadee2ff2b79fc9fc808aa638670f28b2145074538d4a6a5028e", size = 3244735 },
    { url = "https://files.pythonhosted.org/packages/73/8d/b0539e8dce90861efc38fea3eefb15a5d0cfeacf818614762e77a9f192f9/SQLAlchemy-2.0.38-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e185ea07a99ce8b8edfc788c586c538c4b1351007e614ceb708fd01b095ef33e", size = 3255581 },
    { url = "https://files.pythonhosted.org/packages/ac/a5/94e1e44bf5bdffd1782807fcc072542b110b950f0be53f49e68b5f5eca1b/SQLAlchemy-2.0.38-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:b79ee64d01d05a5476d5cceb3c27b5535e6bb84ee0f872ba60d9a8cd4d0e6579", size = 3190877 },
    { url = "https://files.pythonhosted.org/packages/91/13/f08b09996dce945aec029c64f61c13b4788541ac588d9288e31e0d3d8850/SQLAlchemy-2.0.38-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:afd776cf1ebfc7f9aa42a09cf19feadb40a26366802d86c1fba080d8e5e74bdd", size = 3217485 },
    { url = "https://files.pythonhosted.org/packages/13/8f/8cfe2ba5ba6d8090f4de0e658330c53be6b7bf430a8df1b141c2b180dcdf/SQLAlchemy-2.0.38-cp312-cp312-win32.whl", hash = "sha256:a5645cd45f56895cfe3ca3459aed9ff2d3f9aaa29ff7edf557fa7a23515a3725", size = 2075254 },
    { url = "https://files.pythonhosted.org/packages/c2/5c/e3c77fae41862be1da966ca98eec7fbc07cdd0b00f8b3e1ef2a13eaa6cca/SQLAlchemy-2.0.38-cp312-cp312-win_amd64.whl", hash = "sha256:1052723e6cd95312f6a6eff9a279fd41bbae67633415373fdac3c430eca3425d", size = 2100865 },
    { url = "https://files.pythonhosted.org/packages/21/77/caa875a1f5a8a8980b564cc0e6fee1bc992d62d29101252561d0a5e9719c/SQLAlchemy-2.0.38-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:ecef029b69843b82048c5b347d8e6049356aa24ed644006c9a9d7098c3bd3bfd", size = 2100201 },
    { url = "https://files.pythonhosted.org/packages/f4/ec/94bb036ec78bf9a20f8010c807105da9152dd84f72e8c51681ad2f30b3fd/SQLAlchemy-2.0.38-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:9c8bcad7fc12f0cc5896d8e10fdf703c45bd487294a986903fe032c72201596b", size = 2090678 },
    { url = "https://files.pythonhosted.org/packages/7b/61/63ff1893f146e34d3934c0860209fdd3925c25ee064330e6c2152bacc335/SQLAlchemy-2.0.38-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2a0ef3f98175d77180ffdc623d38e9f1736e8d86b6ba70bff182a7e68bed7727", size = 3177107 },
    { url = "https://files.pythonhosted.org/packages/a9/4f/b933bea41a602b5f274065cc824fae25780ed38664d735575192490a021b/SQLAlchemy-2.0.38-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8b0ac78898c50e2574e9f938d2e5caa8fe187d7a5b69b65faa1ea4648925b096", size = 3190435 },
    { url = "https://files.pythonhosted.org/packages/f5/23/9e654b4059e385988de08c5d3b38a369ea042f4c4d7c8902376fd737096a/SQLAlchemy-2.0.38-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9eb4fa13c8c7a2404b6a8e3772c17a55b1ba18bc711e25e4d6c0c9f5f541b02a", size = 3123648 },
    { url = "https://files.pythonhosted.org/packages/83/59/94c6d804e76ebc6412a08d2b086a8cb3e5a056cd61508e18ddaf3ec70100/SQLAlchemy-2.0.38-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5dba1cdb8f319084f5b00d41207b2079822aa8d6a4667c0f369fce85e34b0c86", size = 3151789 },
    { url = "https://files.pythonhosted.org/packages/b2/27/17f143013aabbe1256dce19061eafdce0b0142465ce32168cdb9a18c04b1/SQLAlchemy-2.0.38-cp313-cp313-win32.whl", hash = "sha256:eae27ad7580529a427cfdd52c87abb2dfb15ce2b7a3e0fc29fbb63e2ed6f8120", size = 2073023 },
    { url = "https://files.pythonhosted.org/packages/e2/3e/259404b03c3ed2e7eee4c179e001a07d9b61070334be91124cf4ad32eec7/SQLAlchemy-2.0.38-cp313-cp313-win_amd64.whl", hash = "sha256:b335a7c958bc945e10c522c069cd6e5804f4ff20f9a744dd38e748eb602cbbda", size = 2096908 },
    { url = "https://files.pythonhosted.org/packages/aa/e4/592120713a314621c692211eba034d09becaf6bc8848fabc1dc2a54d8c16/SQLAlchemy-2.0.38-py3-none-any.whl", hash = "sha256:63178c675d4c80def39f1febd625a6333f44c0ba269edd8a468b156394b27753", size = 1896347 },
]

[[package]]
name = "sse-starlette"
version = "2.1.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
    { name = "uvicorn" },
]
sdist = { url = "https://files.pythonhosted.org/packages/72/fc/56ab9f116b2133521f532fce8d03194cf04dcac25f583cf3d839be4c0496/sse_starlette-2.1.3.tar.gz", hash = "sha256:9cd27eb35319e1414e3d2558ee7414487f9529ce3b3cf9b21434fd110e017169", size = 19678 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/52/aa/36b271bc4fa1d2796311ee7c7283a3a1c348bad426d37293609ca4300eef/sse_starlette-2.1.3-py3-none-any.whl", hash = "sha256:8ec846438b4665b9e8c560fcdea6bc8081a3abf7942faa95e5a744999d219772", size = 9383 },
]

[[package]]
name = "starlette"
version = "0.41.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/1a/4c/9b5764bd22eec91c4039ef4c55334e9187085da2d8a2df7bd570869aae18/starlette-0.41.3.tar.gz", hash = "sha256:0e4ab3d16522a255be6b28260b938eae2482f98ce5cc934cb08dce8dc3ba5835", size = 2574159 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/96/00/2b325970b3060c7cecebab6d295afe763365822b1306a12eeab198f74323/starlette-0.41.3-py3-none-any.whl", hash = "sha256:44cedb2b7c77a9de33a8b74b2b90e9f50d11fcf25d8270ea525ad71a25374ff7", size = 73225 },
]

[[package]]
name = "structlog"
version = "25.1.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/25/fe/578db23e17392a8693b45a7b7dc6985370f51dd937157def8ecc7b20930d/structlog-25.1.0.tar.gz", hash = "sha256:2ef2a572e0e27f09664965d31a576afe64e46ac6084ef5cec3c2b8cd6e4e3ad3", size = 1364973 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c1/14/e9aed6c820fa166e8a19a19e663e98bd5538004d68a70c5752458ca3656e/structlog-25.1.0-py3-none-any.whl", hash = "sha256:843fe4f254540329f380812cbe612e1af5ec5b8172205ae634679cd35a6d6321", size = 68921 },
]

[[package]]
name = "tenacity"
version = "9.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cd/94/91fccdb4b8110642462e653d5dcb27e7b674742ad68efd146367da7bdb10/tenacity-9.0.0.tar.gz", hash = "sha256:807f37ca97d62aa361264d497b0e31e92b8027044942bfa756160d908320d73b", size = 47421 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b6/cb/b86984bed139586d01532a587464b5805f12e397594f19f931c4c2fbfa61/tenacity-9.0.0-py3-none-any.whl", hash = "sha256:93de0c98785b27fcf659856aa9f54bfbd399e29969b0621bc7f762bd441b4539", size = 28169 },
]

[[package]]
name = "tiktoken"
version = "0.9.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "regex" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ea/cf/756fedf6981e82897f2d570dd25fa597eb3f4459068ae0572d7e888cfd6f/tiktoken-0.9.0.tar.gz", hash = "sha256:d02a5ca6a938e0490e1ff957bc48c8b078c88cb83977be1625b1fd8aac792c5d", size = 35991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4d/ae/4613a59a2a48e761c5161237fc850eb470b4bb93696db89da51b79a871f1/tiktoken-0.9.0-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:f32cc56168eac4851109e9b5d327637f15fd662aa30dd79f964b7c39fbadd26e", size = 1065987 },
    { url = "https://files.pythonhosted.org/packages/3f/86/55d9d1f5b5a7e1164d0f1538a85529b5fcba2b105f92db3622e5d7de6522/tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:45556bc41241e5294063508caf901bf92ba52d8ef9222023f83d2483a3055348", size = 1009155 },
    { url = "https://files.pythonhosted.org/packages/03/58/01fb6240df083b7c1916d1dcb024e2b761213c95d576e9f780dfb5625a76/tiktoken-0.9.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:03935988a91d6d3216e2ec7c645afbb3d870b37bcb67ada1943ec48678e7ee33", size = 1142898 },
    { url = "https://files.pythonhosted.org/packages/b1/73/41591c525680cd460a6becf56c9b17468d3711b1df242c53d2c7b2183d16/tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8b3d80aad8d2c6b9238fc1a5524542087c52b860b10cbf952429ffb714bc1136", size = 1197535 },
    { url = "https://files.pythonhosted.org/packages/7d/7c/1069f25521c8f01a1a182f362e5c8e0337907fae91b368b7da9c3e39b810/tiktoken-0.9.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:b2a21133be05dc116b1d0372af051cd2c6aa1d2188250c9b553f9fa49301b336", size = 1259548 },
    { url = "https://files.pythonhosted.org/packages/6f/07/c67ad1724b8e14e2b4c8cca04b15da158733ac60136879131db05dda7c30/tiktoken-0.9.0-cp311-cp311-win_amd64.whl", hash = "sha256:11a20e67fdf58b0e2dea7b8654a288e481bb4fc0289d3ad21291f8d0849915fb", size = 893895 },
    { url = "https://files.pythonhosted.org/packages/cf/e5/21ff33ecfa2101c1bb0f9b6df750553bd873b7fb532ce2cb276ff40b197f/tiktoken-0.9.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:e88f121c1c22b726649ce67c089b90ddda8b9662545a8aeb03cfef15967ddd03", size = 1065073 },
    { url = "https://files.pythonhosted.org/packages/8e/03/a95e7b4863ee9ceec1c55983e4cc9558bcfd8f4f80e19c4f8a99642f697d/tiktoken-0.9.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:a6600660f2f72369acb13a57fb3e212434ed38b045fd8cc6cdd74947b4b5d210", size = 1008075 },
    { url = "https://files.pythonhosted.org/packages/40/10/1305bb02a561595088235a513ec73e50b32e74364fef4de519da69bc8010/tiktoken-0.9.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:95e811743b5dfa74f4b227927ed86cbc57cad4df859cb3b643be797914e41794", size = 1140754 },
    { url = "https://files.pythonhosted.org/packages/1b/40/da42522018ca496432ffd02793c3a72a739ac04c3794a4914570c9bb2925/tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:99376e1370d59bcf6935c933cb9ba64adc29033b7e73f5f7569f3aad86552b22", size = 1196678 },
    { url = "https://files.pythonhosted.org/packages/5c/41/1e59dddaae270ba20187ceb8aa52c75b24ffc09f547233991d5fd822838b/tiktoken-0.9.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:badb947c32739fb6ddde173e14885fb3de4d32ab9d8c591cbd013c22b4c31dd2", size = 1259283 },
    { url = "https://files.pythonhosted.org/packages/5b/64/b16003419a1d7728d0d8c0d56a4c24325e7b10a21a9dd1fc0f7115c02f0a/tiktoken-0.9.0-cp312-cp312-win_amd64.whl", hash = "sha256:5a62d7a25225bafed786a524c1b9f0910a1128f4232615bf3f8257a73aaa3b16", size = 894897 },
    { url = "https://files.pythonhosted.org/packages/7a/11/09d936d37f49f4f494ffe660af44acd2d99eb2429d60a57c71318af214e0/tiktoken-0.9.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:2b0e8e05a26eda1249e824156d537015480af7ae222ccb798e5234ae0285dbdb", size = 1064919 },
    { url = "https://files.pythonhosted.org/packages/80/0e/f38ba35713edb8d4197ae602e80837d574244ced7fb1b6070b31c29816e0/tiktoken-0.9.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:27d457f096f87685195eea0165a1807fae87b97b2161fe8c9b1df5bd74ca6f63", size = 1007877 },
    { url = "https://files.pythonhosted.org/packages/fe/82/9197f77421e2a01373e27a79dd36efdd99e6b4115746ecc553318ecafbf0/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2cf8ded49cddf825390e36dd1ad35cd49589e8161fdcb52aa25f0583e90a3e01", size = 1140095 },
    { url = "https://files.pythonhosted.org/packages/f2/bb/4513da71cac187383541facd0291c4572b03ec23c561de5811781bbd988f/tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cc156cb314119a8bb9748257a2eaebd5cc0753b6cb491d26694ed42fc7cb3139", size = 1195649 },
    { url = "https://files.pythonhosted.org/packages/fa/5c/74e4c137530dd8504e97e3a41729b1103a4ac29036cbfd3250b11fd29451/tiktoken-0.9.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:cd69372e8c9dd761f0ab873112aba55a0e3e506332dd9f7522ca466e817b1b7a", size = 1258465 },
    { url = "https://files.pythonhosted.org/packages/de/a8/8f499c179ec900783ffe133e9aab10044481679bb9aad78436d239eee716/tiktoken-0.9.0-cp313-cp313-win_amd64.whl", hash = "sha256:5ea0edb6f83dc56d794723286215918c1cde03712cbbafa0348b33448faf5b95", size = 894669 },
]

[[package]]
name = "tqdm"
version = "4.67.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "platform_system == 'Windows'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a8/4b/29b4ef32e036bb34e4ab51796dd745cdba7ed47ad142a9f4a1eb8e0c744d/tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2", size = 169737 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2", size = 78540 },
]

[[package]]
name = "trustcall"
version = "0.0.38"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "dydantic" },
    { name = "jsonpatch" },
    { name = "langgraph" },
    { name = "langgraph-prebuilt" },
]
sdist = { url = "https://files.pythonhosted.org/packages/aa/b9/6e3ecf9617ab778e2ca3fb1ed1dfb6a739d8ca6d8d9ce09c34a080bd6ab5/trustcall-0.0.38.tar.gz", hash = "sha256:318d451737d88188254c468ae813d16b7c7b1d19da17d402a52629a0198f4646", size = 35973 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/50/1e/36dafe25a5edbfab01f92561e99da14c42d4575d5a628369137098af149f/trustcall-0.0.38-py3-none-any.whl", hash = "sha256:90d5441f792059a6d5a08f90d306818363be7aa0f096002e30cfbcceee706351", size = 26554 },
]

[[package]]
name = "typing-extensions"
version = "4.12.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/df/db/f35a00659bc03fec321ba8bce9420de607a1d37f8342eee1863174c69557/typing_extensions-4.12.2.tar.gz", hash = "sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8", size = 85321 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl", hash = "sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d", size = 37438 },
]

[[package]]
name = "urllib3"
version = "2.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/aa/63/e53da845320b757bf29ef6a9062f5c669fe997973f966045cb019c3f4b66/urllib3-2.3.0.tar.gz", hash = "sha256:f8c5449b3cf0861679ce7e0503c7b44b5ec981bec0d1d3795a07f1ba96f0204d", size = 307268 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl", hash = "sha256:1cee9ad369867bfdbbb48b7dd50374c0967a0bb7710050facf0dd6911440e3df", size = 128369 },
]

[[package]]
name = "uvicorn"
version = "0.34.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/4b/4d/938bd85e5bf2edeec766267a5015ad969730bb91e31b44021dfe8b22df6c/uvicorn-0.34.0.tar.gz", hash = "sha256:404051050cd7e905de2c9a7e61790943440b3416f49cb409f965d9dcd0fa73e9", size = 76568 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/61/14/33a3a1352cfa71812a3a21e8c9bfb83f60b0011f5e36f2b1399d51928209/uvicorn-0.34.0-py3-none-any.whl", hash = "sha256:023dc038422502fa28a09c7a30bf2b6991512da7dcdb8fd35fe57cfc154126f4", size = 62315 },
]

[[package]]
name = "watchfiles"
version = "1.0.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3c/7e/4569184ea04b501840771b8fcecee19b2233a8b72c196061263c0ef23c0b/watchfiles-1.0.3.tar.gz", hash = "sha256:f3ff7da165c99a5412fe5dd2304dd2dbaaaa5da718aad942dcb3a178eaa70c56", size = 38185 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/24/a8/06e2d5f840b285718a09be7c71ea19b7177b005cec87b8923dd7e8541b20/watchfiles-1.0.3-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:ffe709b1d0bc2e9921257569675674cafb3a5f8af689ab9f3f2b3f88775b960f", size = 394821 },
    { url = "https://files.pythonhosted.org/packages/57/9f/f98a57ada3d4b1fcd0e325aa6c307e2248ecb048f71c96fba34a602f02e7/watchfiles-1.0.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:418c5ce332f74939ff60691e5293e27c206c8164ce2b8ce0d9abf013003fb7fe", size = 384898 },
    { url = "https://files.pythonhosted.org/packages/a3/31/33ba914010cbfd01033ca3727aff6585b6b2ea2b051b6fbaecdf4e2160b9/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2f492d2907263d6d0d52f897a68647195bc093dafed14508a8d6817973586b6b", size = 441710 },
    { url = "https://files.pythonhosted.org/packages/d9/dd/e56b2ef07c2c34e4152950f0ce98a1081215ef027cf39e5dab61a0f8bd95/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:48c9f3bc90c556a854f4cab6a79c16974099ccfa3e3e150673d82d47a4bc92c9", size = 447681 },
    { url = "https://files.pythonhosted.org/packages/60/8f/3837df33f3d0cbef8ae59559891d688490bf2960373ea077ff11cbf79115/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:75d3bcfa90454dba8df12adc86b13b6d85fda97d90e708efc036c2760cc6ba44", size = 472312 },
    { url = "https://files.pythonhosted.org/packages/5a/b3/95d103e5bb609b20f175e8acdf8b32c4b091f96f781c066fd3bff2b17778/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5691340f259b8f76b45fb31b98e594d46c36d1dc8285efa7975f7f50230c9093", size = 494779 },
    { url = "https://files.pythonhosted.org/packages/4f/f0/9fdc60daf5abf7b0deb225c9b0a37fd72dc407fa33f071ae2f70e84e268c/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1e263cc718545b7f897baeac1f00299ab6fabe3e18caaacacb0edf6d5f35513c", size = 492090 },
    { url = "https://files.pythonhosted.org/packages/96/e5/a9967e77f173280ab1abbfd7ead90f2b94060574968baf5e6d7cbe9dd490/watchfiles-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1c6cf7709ed3e55704cc06f6e835bf43c03bc8e3cb8ff946bf69a2e0a78d9d77", size = 443713 },
    { url = "https://files.pythonhosted.org/packages/60/38/e5390d4633a558878113e45d32e39d30cf58eb94e0359f41737be209321b/watchfiles-1.0.3-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:703aa5e50e465be901e0e0f9d5739add15e696d8c26c53bc6fc00eb65d7b9469", size = 615306 },
    { url = "https://files.pythonhosted.org/packages/5c/27/8a1ee74544c93e3242ca073087b45c64367aeb6897b622e43c8172c2b421/watchfiles-1.0.3-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:bfcae6aecd9e0cb425f5145afee871465b98b75862e038d42fe91fd753ddd780", size = 614333 },
    { url = "https://files.pythonhosted.org/packages/fc/f8/25698f5b734907662b50acf3e81996053abdfe26fcf38804d028412876a8/watchfiles-1.0.3-cp311-cp311-win32.whl", hash = "sha256:6a76494d2c5311584f22416c5a87c1e2cb954ff9b5f0988027bc4ef2a8a67181", size = 270987 },
    { url = "https://files.pythonhosted.org/packages/39/78/f600dee7b387e6088c8d1f4c898a4340d07aecfe6406bd90ec4c1925ef08/watchfiles-1.0.3-cp311-cp311-win_amd64.whl", hash = "sha256:cf745cbfad6389c0e331786e5fe9ae3f06e9d9c2ce2432378e1267954793975c", size = 284098 },
    { url = "https://files.pythonhosted.org/packages/ca/6f/27ba8aec0a4b45a6063454465eefb42777158081d9df18eab5f1d6a3bd8a/watchfiles-1.0.3-cp311-cp311-win_arm64.whl", hash = "sha256:2dcc3f60c445f8ce14156854a072ceb36b83807ed803d37fdea2a50e898635d6", size = 276804 },
    { url = "https://files.pythonhosted.org/packages/bf/a9/c8b5ab33444306e1a324cb2b51644f8458dd459e30c3841f925012893e6a/watchfiles-1.0.3-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:93436ed550e429da007fbafb723e0769f25bae178fbb287a94cb4ccdf42d3af3", size = 391395 },
    { url = "https://files.pythonhosted.org/packages/ad/d3/403af5f07359863c03951796ddab265ee8cce1a6147510203d0bf43950e7/watchfiles-1.0.3-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:c18f3502ad0737813c7dad70e3e1cc966cc147fbaeef47a09463bbffe70b0a00", size = 381432 },
    { url = "https://files.pythonhosted.org/packages/f6/5f/921f2f2beabaf24b1ad81ac22bb69df8dd5771fdb68d6f34a5912a420941/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6a5bc3ca468bb58a2ef50441f953e1f77b9a61bd1b8c347c8223403dc9b4ac9a", size = 441448 },
    { url = "https://files.pythonhosted.org/packages/63/d7/67d0d750b246f248ccdb400a85a253e93e419ea5b6cbe968fa48b97a5f30/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:0d1ec043f02ca04bf21b1b32cab155ce90c651aaf5540db8eb8ad7f7e645cba8", size = 446852 },
    { url = "https://files.pythonhosted.org/packages/53/7c/d7cd94c7d0905f1e2f1c2232ea9bc39b1a48affd007e09c547ead96edb8f/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f58d3bfafecf3d81c15d99fc0ecf4319e80ac712c77cf0ce2661c8cf8bf84066", size = 471662 },
    { url = "https://files.pythonhosted.org/packages/26/81/738f8e66f7525753996b8aa292f78dcec1ef77887d62e6cdfb04cc2f352f/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1df924ba82ae9e77340101c28d56cbaff2c991bd6fe8444a545d24075abb0a87", size = 493765 },
    { url = "https://files.pythonhosted.org/packages/d2/50/78e21f5da24ab39114e9b24f7b0945ea1c6fc7bc9ae86cd87f8eaeb47325/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:632a52dcaee44792d0965c17bdfe5dc0edad5b86d6a29e53d6ad4bf92dc0ff49", size = 490558 },
    { url = "https://files.pythonhosted.org/packages/a8/93/1873fea6354b2858eae8970991d64e9a449d87726d596490d46bf00af8ed/watchfiles-1.0.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bf4b459d94a0387617a1b499f314aa04d8a64b7a0747d15d425b8c8b151da0", size = 442808 },
    { url = "https://files.pythonhosted.org/packages/4f/b4/2fc4c92fb28b029f66d04a4d430fe929284e9ff717b04bb7a3bb8a7a5605/watchfiles-1.0.3-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:ca94c85911601b097d53caeeec30201736ad69a93f30d15672b967558df02885", size = 615287 },
    { url = "https://files.pythonhosted.org/packages/1e/d4/93da24db39257e440240d338b617c5153ad11d361c34108f5c0e1e0743eb/watchfiles-1.0.3-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:65ab1fb635476f6170b07e8e21db0424de94877e4b76b7feabfe11f9a5fc12b5", size = 612812 },
    { url = "https://files.pythonhosted.org/packages/c6/67/9fd3661c2dc0309abd6021876653d91e8b64fb279529e2cadaa3520ef3e3/watchfiles-1.0.3-cp312-cp312-win32.whl", hash = "sha256:49bc1bc26abf4f32e132652f4b3bfeec77d8f8f62f57652703ef127e85a3e38d", size = 271642 },
    { url = "https://files.pythonhosted.org/packages/ae/aa/8c887edb78cd67f5d4d6a35c3aeb46d748643ebf962163130fb1871e2ee0/watchfiles-1.0.3-cp312-cp312-win_amd64.whl", hash = "sha256:48681c86f2cb08348631fed788a116c89c787fdf1e6381c5febafd782f6c3b44", size = 285505 },
    { url = "https://files.pythonhosted.org/packages/7b/31/d212fa6390f0e73a91913ada0b925b294a78d67794795371208baf73f0b5/watchfiles-1.0.3-cp312-cp312-win_arm64.whl", hash = "sha256:9e080cf917b35b20c889225a13f290f2716748362f6071b859b60b8847a6aa43", size = 277263 },
    { url = "https://files.pythonhosted.org/packages/36/77/0ceb864c854c59bc5326484f88a900c70b4a05e3792e0ce340689988dd5e/watchfiles-1.0.3-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:e153a690b7255c5ced17895394b4f109d5dcc2a4f35cb809374da50f0e5c456a", size = 391061 },
    { url = "https://files.pythonhosted.org/packages/00/66/327046cfe276a6e4af1a9a58fc99321e25783e501dc68c4c82de2d1bd3a7/watchfiles-1.0.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:ac1be85fe43b4bf9a251978ce5c3bb30e1ada9784290441f5423a28633a958a7", size = 381177 },
    { url = "https://files.pythonhosted.org/packages/66/8a/420e2833deaa88e8ca7d94a497ec60fde610c66206a1776f049dc5ad3a4e/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a2ec98e31e1844eac860e70d9247db9d75440fc8f5f679c37d01914568d18721", size = 441293 },
    { url = "https://files.pythonhosted.org/packages/58/56/2627795ecdf3f0f361458cfa74c583d5041615b9ad81bc25f8c66a6c44a2/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:0179252846be03fa97d4d5f8233d1c620ef004855f0717712ae1c558f1974a16", size = 446209 },
    { url = "https://files.pythonhosted.org/packages/8f/d0/11c8dcd8a9995f0c075d76f1d06068bbb7a17583a19c5be75361497a4074/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:995c374e86fa82126c03c5b4630c4e312327ecfe27761accb25b5e1d7ab50ec8", size = 471227 },
    { url = "https://files.pythonhosted.org/packages/cb/8f/baa06574eaf48173882c4cdc3636993d0854661be7d88193e015ef996c73/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:29b9cb35b7f290db1c31fb2fdf8fc6d3730cfa4bca4b49761083307f441cac5a", size = 493205 },
    { url = "https://files.pythonhosted.org/packages/ee/e8/9af886b4d3daa281047b542ffd2eb8f76dae9dd6ca0e21c5df4593b98574/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:6f8dc09ae69af50bead60783180f656ad96bd33ffbf6e7a6fce900f6d53b08f1", size = 489090 },
    { url = "https://files.pythonhosted.org/packages/81/02/62085db54b151fc02e22d47b288d19e99031dc9af73151289a7ab6621f9a/watchfiles-1.0.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:489b80812f52a8d8c7b0d10f0d956db0efed25df2821c7a934f6143f76938bd6", size = 442610 },
    { url = "https://files.pythonhosted.org/packages/61/81/980439c5d3fd3c69ba7124a56e1016d0b824ced2192ffbfe7062d53f524b/watchfiles-1.0.3-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:228e2247de583475d4cebf6b9af5dc9918abb99d1ef5ee737155bb39fb33f3c0", size = 614781 },
    { url = "https://files.pythonhosted.org/packages/55/98/e11401d8e9cd5d2bd0e95e9bf750f397489681965ee0c72fb84732257912/watchfiles-1.0.3-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:1550be1a5cb3be08a3fb84636eaafa9b7119b70c71b0bed48726fd1d5aa9b868", size = 612637 },
    { url = "https://files.pythonhosted.org/packages/50/be/8393b68f2add0f839be6863f151bd6a7b242efc6eb2ce0c9f7d135d529cc/watchfiles-1.0.3-cp313-cp313-win32.whl", hash = "sha256:16db2d7e12f94818cbf16d4c8938e4d8aaecee23826344addfaaa671a1527b07", size = 271170 },
    { url = "https://files.pythonhosted.org/packages/f0/da/725f97a8b1b4e7b3e4331cce3ef921b12568af3af403b9f0f61ede036898/watchfiles-1.0.3-cp313-cp313-win_amd64.whl", hash = "sha256:160eff7d1267d7b025e983ca8460e8cc67b328284967cbe29c05f3c3163711a3", size = 285246 },
]

[[package]]
name = "yarl"
version = "1.18.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b7/9d/4b94a8e6d2b51b599516a5cb88e5bc99b4d8d4583e468057eaa29d5f0918/yarl-1.18.3.tar.gz", hash = "sha256:ac1801c45cbf77b6c99242eeff4fffb5e4e73a800b5c4ad4fc0be5def634d2e1", size = 181062 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/40/93/282b5f4898d8e8efaf0790ba6d10e2245d2c9f30e199d1a85cae9356098c/yarl-1.18.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:8503ad47387b8ebd39cbbbdf0bf113e17330ffd339ba1144074da24c545f0069", size = 141555 },
    { url = "https://files.pythonhosted.org/packages/6d/9c/0a49af78df099c283ca3444560f10718fadb8a18dc8b3edf8c7bd9fd7d89/yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193", size = 94351 },
    { url = "https://files.pythonhosted.org/packages/5a/a1/205ab51e148fdcedad189ca8dd587794c6f119882437d04c33c01a75dece/yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:67a283dd2882ac98cc6318384f565bffc751ab564605959df4752d42483ad889", size = 92286 },
    { url = "https://files.pythonhosted.org/packages/ed/fe/88b690b30f3f59275fb674f5f93ddd4a3ae796c2b62e5bb9ece8a4914b83/yarl-1.18.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d980e0325b6eddc81331d3f4551e2a333999fb176fd153e075c6d1c2530aa8a8", size = 340649 },
    { url = "https://files.pythonhosted.org/packages/07/eb/3b65499b568e01f36e847cebdc8d7ccb51fff716dbda1ae83c3cbb8ca1c9/yarl-1.18.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b643562c12680b01e17239be267bc306bbc6aac1f34f6444d1bded0c5ce438ca", size = 356623 },
    { url = "https://files.pythonhosted.org/packages/33/46/f559dc184280b745fc76ec6b1954de2c55595f0ec0a7614238b9ebf69618/yarl-1.18.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c017a3b6df3a1bd45b9fa49a0f54005e53fbcad16633870104b66fa1a30a29d8", size = 354007 },
    { url = "https://files.pythonhosted.org/packages/af/ba/1865d85212351ad160f19fb99808acf23aab9a0f8ff31c8c9f1b4d671fc9/yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:75674776d96d7b851b6498f17824ba17849d790a44d282929c42dbb77d4f17ae", size = 344145 },
    { url = "https://files.pythonhosted.org/packages/94/cb/5c3e975d77755d7b3d5193e92056b19d83752ea2da7ab394e22260a7b824/yarl-1.18.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ccaa3a4b521b780a7e771cc336a2dba389a0861592bbce09a476190bb0c8b4b3", size = 336133 },
    { url = "https://files.pythonhosted.org/packages/19/89/b77d3fd249ab52a5c40859815765d35c91425b6bb82e7427ab2f78f5ff55/yarl-1.18.3-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:2d06d3005e668744e11ed80812e61efd77d70bb7f03e33c1598c301eea20efbb", size = 347967 },
    { url = "https://files.pythonhosted.org/packages/35/bd/f6b7630ba2cc06c319c3235634c582a6ab014d52311e7d7c22f9518189b5/yarl-1.18.3-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:9d41beda9dc97ca9ab0b9888cb71f7539124bc05df02c0cff6e5acc5a19dcc6e", size = 346397 },
    { url = "https://files.pythonhosted.org/packages/18/1a/0b4e367d5a72d1f095318344848e93ea70da728118221f84f1bf6c1e39e7/yarl-1.18.3-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:ba23302c0c61a9999784e73809427c9dbedd79f66a13d84ad1b1943802eaaf59", size = 350206 },
    { url = "https://files.pythonhosted.org/packages/b5/cf/320fff4367341fb77809a2d8d7fe75b5d323a8e1b35710aafe41fdbf327b/yarl-1.18.3-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:6748dbf9bfa5ba1afcc7556b71cda0d7ce5f24768043a02a58846e4a443d808d", size = 362089 },
    { url = "https://files.pythonhosted.org/packages/57/cf/aadba261d8b920253204085268bad5e8cdd86b50162fcb1b10c10834885a/yarl-1.18.3-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:0b0cad37311123211dc91eadcb322ef4d4a66008d3e1bdc404808992260e1a0e", size = 366267 },
    { url = "https://files.pythonhosted.org/packages/54/58/fb4cadd81acdee6dafe14abeb258f876e4dd410518099ae9a35c88d8097c/yarl-1.18.3-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:0fb2171a4486bb075316ee754c6d8382ea6eb8b399d4ec62fde2b591f879778a", size = 359141 },
    { url = "https://files.pythonhosted.org/packages/9a/7a/4c571597589da4cd5c14ed2a0b17ac56ec9ee7ee615013f74653169e702d/yarl-1.18.3-cp311-cp311-win32.whl", hash = "sha256:61b1a825a13bef4a5f10b1885245377d3cd0bf87cba068e1d9a88c2ae36880e1", size = 84402 },
    { url = "https://files.pythonhosted.org/packages/ae/7b/8600250b3d89b625f1121d897062f629883c2f45339623b69b1747ec65fa/yarl-1.18.3-cp311-cp311-win_amd64.whl", hash = "sha256:b9d60031cf568c627d028239693fd718025719c02c9f55df0a53e587aab951b5", size = 91030 },
    { url = "https://files.pythonhosted.org/packages/33/85/bd2e2729752ff4c77338e0102914897512e92496375e079ce0150a6dc306/yarl-1.18.3-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:1dd4bdd05407ced96fed3d7f25dbbf88d2ffb045a0db60dbc247f5b3c5c25d50", size = 142644 },
    { url = "https://files.pythonhosted.org/packages/ff/74/1178322cc0f10288d7eefa6e4a85d8d2e28187ccab13d5b844e8b5d7c88d/yarl-1.18.3-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7c33dd1931a95e5d9a772d0ac5e44cac8957eaf58e3c8da8c1414de7dd27c576", size = 94962 },
    { url = "https://files.pythonhosted.org/packages/be/75/79c6acc0261e2c2ae8a1c41cf12265e91628c8c58ae91f5ff59e29c0787f/yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:25b411eddcfd56a2f0cd6a384e9f4f7aa3efee14b188de13048c25b5e91f1640", size = 92795 },
    { url = "https://files.pythonhosted.org/packages/6b/32/927b2d67a412c31199e83fefdce6e645247b4fb164aa1ecb35a0f9eb2058/yarl-1.18.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:436c4fc0a4d66b2badc6c5fc5ef4e47bb10e4fd9bf0c79524ac719a01f3607c2", size = 332368 },
    { url = "https://files.pythonhosted.org/packages/19/e5/859fca07169d6eceeaa4fde1997c91d8abde4e9a7c018e371640c2da2b71/yarl-1.18.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e35ef8683211db69ffe129a25d5634319a677570ab6b2eba4afa860f54eeaf75", size = 342314 },
    { url = "https://files.pythonhosted.org/packages/08/75/76b63ccd91c9e03ab213ef27ae6add2e3400e77e5cdddf8ed2dbc36e3f21/yarl-1.18.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:84b2deecba4a3f1a398df819151eb72d29bfeb3b69abb145a00ddc8d30094512", size = 341987 },
    { url = "https://files.pythonhosted.org/packages/1a/e1/a097d5755d3ea8479a42856f51d97eeff7a3a7160593332d98f2709b3580/yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba", size = 336914 },
    { url = "https://files.pythonhosted.org/packages/0b/42/e1b4d0e396b7987feceebe565286c27bc085bf07d61a59508cdaf2d45e63/yarl-1.18.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d0e883008013c0e4aef84dcfe2a0b172c4d23c2669412cf5b3371003941f72bb", size = 325765 },
    { url = "https://files.pythonhosted.org/packages/7e/18/03a5834ccc9177f97ca1bbb245b93c13e58e8225276f01eedc4cc98ab820/yarl-1.18.3-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:5a3f356548e34a70b0172d8890006c37be92995f62d95a07b4a42e90fba54272", size = 344444 },
    { url = "https://files.pythonhosted.org/packages/c8/03/a713633bdde0640b0472aa197b5b86e90fbc4c5bc05b727b714cd8a40e6d/yarl-1.18.3-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:ccd17349166b1bee6e529b4add61727d3f55edb7babbe4069b5764c9587a8cc6", size = 340760 },
    { url = "https://files.pythonhosted.org/packages/eb/99/f6567e3f3bbad8fd101886ea0276c68ecb86a2b58be0f64077396cd4b95e/yarl-1.18.3-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:b958ddd075ddba5b09bb0be8a6d9906d2ce933aee81100db289badbeb966f54e", size = 346484 },
    { url = "https://files.pythonhosted.org/packages/8e/a9/84717c896b2fc6cb15bd4eecd64e34a2f0a9fd6669e69170c73a8b46795a/yarl-1.18.3-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:c7d79f7d9aabd6011004e33b22bc13056a3e3fb54794d138af57f5ee9d9032cb", size = 359864 },
    { url = "https://files.pythonhosted.org/packages/1e/2e/d0f5f1bef7ee93ed17e739ec8dbcb47794af891f7d165fa6014517b48169/yarl-1.18.3-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:4891ed92157e5430874dad17b15eb1fda57627710756c27422200c52d8a4e393", size = 364537 },
    { url = "https://files.pythonhosted.org/packages/97/8a/568d07c5d4964da5b02621a517532adb8ec5ba181ad1687191fffeda0ab6/yarl-1.18.3-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:ce1af883b94304f493698b00d0f006d56aea98aeb49d75ec7d98cd4a777e9285", size = 357861 },
    { url = "https://files.pythonhosted.org/packages/7d/e3/924c3f64b6b3077889df9a1ece1ed8947e7b61b0a933f2ec93041990a677/yarl-1.18.3-cp312-cp312-win32.whl", hash = "sha256:f91c4803173928a25e1a55b943c81f55b8872f0018be83e3ad4938adffb77dd2", size = 84097 },
    { url = "https://files.pythonhosted.org/packages/34/45/0e055320daaabfc169b21ff6174567b2c910c45617b0d79c68d7ab349b02/yarl-1.18.3-cp312-cp312-win_amd64.whl", hash = "sha256:7e2ee16578af3b52ac2f334c3b1f92262f47e02cc6193c598502bd46f5cd1477", size = 90399 },
    { url = "https://files.pythonhosted.org/packages/30/c7/c790513d5328a8390be8f47be5d52e141f78b66c6c48f48d241ca6bd5265/yarl-1.18.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:90adb47ad432332d4f0bc28f83a5963f426ce9a1a8809f5e584e704b82685dcb", size = 140789 },
    { url = "https://files.pythonhosted.org/packages/30/aa/a2f84e93554a578463e2edaaf2300faa61c8701f0898725842c704ba5444/yarl-1.18.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:913829534200eb0f789d45349e55203a091f45c37a2674678744ae52fae23efa", size = 94144 },
    { url = "https://files.pythonhosted.org/packages/c6/fc/d68d8f83714b221a85ce7866832cba36d7c04a68fa6a960b908c2c84f325/yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:ef9f7768395923c3039055c14334ba4d926f3baf7b776c923c93d80195624782", size = 91974 },
    { url = "https://files.pythonhosted.org/packages/56/4e/d2563d8323a7e9a414b5b25341b3942af5902a2263d36d20fb17c40411e2/yarl-1.18.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:88a19f62ff30117e706ebc9090b8ecc79aeb77d0b1f5ec10d2d27a12bc9f66d0", size = 333587 },
    { url = "https://files.pythonhosted.org/packages/25/c9/cfec0bc0cac8d054be223e9f2c7909d3e8442a856af9dbce7e3442a8ec8d/yarl-1.18.3-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e17c9361d46a4d5addf777c6dd5eab0715a7684c2f11b88c67ac37edfba6c482", size = 344386 },
    { url = "https://files.pythonhosted.org/packages/ab/5d/4c532190113b25f1364d25f4c319322e86232d69175b91f27e3ebc2caf9a/yarl-1.18.3-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186", size = 345421 },
    { url = "https://files.pythonhosted.org/packages/23/d1/6cdd1632da013aa6ba18cee4d750d953104a5e7aac44e249d9410a972bf5/yarl-1.18.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:41f7ce59d6ee7741af71d82020346af364949314ed3d87553763a2df1829cc58", size = 339384 },
    { url = "https://files.pythonhosted.org/packages/9a/c4/6b3c39bec352e441bd30f432cda6ba51681ab19bb8abe023f0d19777aad1/yarl-1.18.3-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f52a265001d830bc425f82ca9eabda94a64a4d753b07d623a9f2863fde532b53", size = 326689 },
    { url = "https://files.pythonhosted.org/packages/23/30/07fb088f2eefdc0aa4fc1af4e3ca4eb1a3aadd1ce7d866d74c0f124e6a85/yarl-1.18.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:82123d0c954dc58db301f5021a01854a85bf1f3bb7d12ae0c01afc414a882ca2", size = 345453 },
    { url = "https://files.pythonhosted.org/packages/63/09/d54befb48f9cd8eec43797f624ec37783a0266855f4930a91e3d5c7717f8/yarl-1.18.3-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:2ec9bbba33b2d00999af4631a3397d1fd78290c48e2a3e52d8dd72db3a067ac8", size = 341872 },
    { url = "https://files.pythonhosted.org/packages/91/26/fd0ef9bf29dd906a84b59f0cd1281e65b0c3e08c6aa94b57f7d11f593518/yarl-1.18.3-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:fbd6748e8ab9b41171bb95c6142faf068f5ef1511935a0aa07025438dd9a9bc1", size = 347497 },
    { url = "https://files.pythonhosted.org/packages/d9/b5/14ac7a256d0511b2ac168d50d4b7d744aea1c1aa20c79f620d1059aab8b2/yarl-1.18.3-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:877d209b6aebeb5b16c42cbb377f5f94d9e556626b1bfff66d7b0d115be88d0a", size = 359981 },
    { url = "https://files.pythonhosted.org/packages/ca/b3/d493221ad5cbd18bc07e642894030437e405e1413c4236dd5db6e46bcec9/yarl-1.18.3-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:b464c4ab4bfcb41e3bfd3f1c26600d038376c2de3297760dfe064d2cb7ea8e10", size = 366229 },
    { url = "https://files.pythonhosted.org/packages/04/56/6a3e2a5d9152c56c346df9b8fb8edd2c8888b1e03f96324d457e5cf06d34/yarl-1.18.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8d39d351e7faf01483cc7ff7c0213c412e38e5a340238826be7e0e4da450fdc8", size = 360383 },
    { url = "https://files.pythonhosted.org/packages/fd/b7/4b3c7c7913a278d445cc6284e59b2e62fa25e72758f888b7a7a39eb8423f/yarl-1.18.3-cp313-cp313-win32.whl", hash = "sha256:61ee62ead9b68b9123ec24bc866cbef297dd266175d53296e2db5e7f797f902d", size = 310152 },
    { url = "https://files.pythonhosted.org/packages/f5/d5/688db678e987c3e0fb17867970700b92603cadf36c56e5fb08f23e822a0c/yarl-1.18.3-cp313-cp313-win_amd64.whl", hash = "sha256:578e281c393af575879990861823ef19d66e2b1d0098414855dd367e234f5b3c", size = 315723 },
    { url = "https://files.pythonhosted.org/packages/f5/4b/a06e0ec3d155924f77835ed2d167ebd3b211a7b0853da1cf8d8414d784ef/yarl-1.18.3-py3-none-any.whl", hash = "sha256:b57f4f58099328dfb26c6a771d09fb20dbbae81d20cfb66141251ea063bd101b", size = 45109 },
]

[[package]]
name = "zstandard"
version = "0.23.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "cffi", marker = "platform_python_implementation == 'PyPy'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ed/f6/2ac0287b442160a89d726b17a9184a4c615bb5237db763791a7fd16d9df1/zstandard-0.23.0.tar.gz", hash = "sha256:b2d8c62d08e7255f68f7a740bae85b3c9b8e5466baa9cbf7f57f1cde0ac6bc09", size = 681701 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9e/40/f67e7d2c25a0e2dc1744dd781110b0b60306657f8696cafb7ad7579469bd/zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:34895a41273ad33347b2fc70e1bff4240556de3c46c6ea430a7ed91f9042aa4e", size = 788699 },
    { url = "https://files.pythonhosted.org/packages/e8/46/66d5b55f4d737dd6ab75851b224abf0afe5774976fe511a54d2eb9063a41/zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:77ea385f7dd5b5676d7fd943292ffa18fbf5c72ba98f7d09fc1fb9e819b34c23", size = 633681 },
    { url = "https://files.pythonhosted.org/packages/63/b6/677e65c095d8e12b66b8f862b069bcf1f1d781b9c9c6f12eb55000d57583/zstandard-0.23.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:983b6efd649723474f29ed42e1467f90a35a74793437d0bc64a5bf482bedfa0a", size = 4944328 },
    { url = "https://files.pythonhosted.org/packages/59/cc/e76acb4c42afa05a9d20827116d1f9287e9c32b7ad58cc3af0721ce2b481/zstandard-0.23.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:80a539906390591dd39ebb8d773771dc4db82ace6372c4d41e2d293f8e32b8db", size = 5311955 },
    { url = "https://files.pythonhosted.org/packages/78/e4/644b8075f18fc7f632130c32e8f36f6dc1b93065bf2dd87f03223b187f26/zstandard-0.23.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:445e4cb5048b04e90ce96a79b4b63140e3f4ab5f662321975679b5f6360b90e2", size = 5344944 },
    { url = "https://files.pythonhosted.org/packages/76/3f/dbafccf19cfeca25bbabf6f2dd81796b7218f768ec400f043edc767015a6/zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd30d9c67d13d891f2360b2a120186729c111238ac63b43dbd37a5a40670b8ca", size = 5442927 },
    { url = "https://files.pythonhosted.org/packages/0c/c3/d24a01a19b6733b9f218e94d1a87c477d523237e07f94899e1c10f6fd06c/zstandard-0.23.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d20fd853fbb5807c8e84c136c278827b6167ded66c72ec6f9a14b863d809211c", size = 4864910 },
    { url = "https://files.pythonhosted.org/packages/1c/a9/cf8f78ead4597264f7618d0875be01f9bc23c9d1d11afb6d225b867cb423/zstandard-0.23.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ed1708dbf4d2e3a1c5c69110ba2b4eb6678262028afd6c6fbcc5a8dac9cda68e", size = 4935544 },
    { url = "https://files.pythonhosted.org/packages/2c/96/8af1e3731b67965fb995a940c04a2c20997a7b3b14826b9d1301cf160879/zstandard-0.23.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:be9b5b8659dff1f913039c2feee1aca499cfbc19e98fa12bc85e037c17ec6ca5", size = 5467094 },
    { url = "https://files.pythonhosted.org/packages/ff/57/43ea9df642c636cb79f88a13ab07d92d88d3bfe3e550b55a25a07a26d878/zstandard-0.23.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:65308f4b4890aa12d9b6ad9f2844b7ee42c7f7a4fd3390425b242ffc57498f48", size = 4860440 },
    { url = "https://files.pythonhosted.org/packages/46/37/edb78f33c7f44f806525f27baa300341918fd4c4af9472fbc2c3094be2e8/zstandard-0.23.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:98da17ce9cbf3bfe4617e836d561e433f871129e3a7ac16d6ef4c680f13a839c", size = 4700091 },
    { url = "https://files.pythonhosted.org/packages/c1/f1/454ac3962671a754f3cb49242472df5c2cced4eb959ae203a377b45b1a3c/zstandard-0.23.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:8ed7d27cb56b3e058d3cf684d7200703bcae623e1dcc06ed1e18ecda39fee003", size = 5208682 },
    { url = "https://files.pythonhosted.org/packages/85/b2/1734b0fff1634390b1b887202d557d2dd542de84a4c155c258cf75da4773/zstandard-0.23.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:b69bb4f51daf461b15e7b3db033160937d3ff88303a7bc808c67bbc1eaf98c78", size = 5669707 },
    { url = "https://files.pythonhosted.org/packages/52/5a/87d6971f0997c4b9b09c495bf92189fb63de86a83cadc4977dc19735f652/zstandard-0.23.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:034b88913ecc1b097f528e42b539453fa82c3557e414b3de9d5632c80439a473", size = 5201792 },
    { url = "https://files.pythonhosted.org/packages/79/02/6f6a42cc84459d399bd1a4e1adfc78d4dfe45e56d05b072008d10040e13b/zstandard-0.23.0-cp311-cp311-win32.whl", hash = "sha256:f2d4380bf5f62daabd7b751ea2339c1a21d1c9463f1feb7fc2bdcea2c29c3160", size = 430586 },
    { url = "https://files.pythonhosted.org/packages/be/a2/4272175d47c623ff78196f3c10e9dc7045c1b9caf3735bf041e65271eca4/zstandard-0.23.0-cp311-cp311-win_amd64.whl", hash = "sha256:62136da96a973bd2557f06ddd4e8e807f9e13cbb0bfb9cc06cfe6d98ea90dfe0", size = 495420 },
    { url = "https://files.pythonhosted.org/packages/7b/83/f23338c963bd9de687d47bf32efe9fd30164e722ba27fb59df33e6b1719b/zstandard-0.23.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:b4567955a6bc1b20e9c31612e615af6b53733491aeaa19a6b3b37f3b65477094", size = 788713 },
    { url = "https://files.pythonhosted.org/packages/5b/b3/1a028f6750fd9227ee0b937a278a434ab7f7fdc3066c3173f64366fe2466/zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:1e172f57cd78c20f13a3415cc8dfe24bf388614324d25539146594c16d78fcc8", size = 633459 },
    { url = "https://files.pythonhosted.org/packages/26/af/36d89aae0c1f95a0a98e50711bc5d92c144939efc1f81a2fcd3e78d7f4c1/zstandard-0.23.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b0e166f698c5a3e914947388c162be2583e0c638a4703fc6a543e23a88dea3c1", size = 4945707 },
    { url = "https://files.pythonhosted.org/packages/cd/2e/2051f5c772f4dfc0aae3741d5fc72c3dcfe3aaeb461cc231668a4db1ce14/zstandard-0.23.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:12a289832e520c6bd4dcaad68e944b86da3bad0d339ef7989fb7e88f92e96072", size = 5306545 },
    { url = "https://files.pythonhosted.org/packages/0a/9e/a11c97b087f89cab030fa71206963090d2fecd8eb83e67bb8f3ffb84c024/zstandard-0.23.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d50d31bfedd53a928fed6707b15a8dbeef011bb6366297cc435accc888b27c20", size = 5337533 },
    { url = "https://files.pythonhosted.org/packages/fc/79/edeb217c57fe1bf16d890aa91a1c2c96b28c07b46afed54a5dcf310c3f6f/zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:72c68dda124a1a138340fb62fa21b9bf4848437d9ca60bd35db36f2d3345f373", size = 5436510 },
    { url = "https://files.pythonhosted.org/packages/81/4f/c21383d97cb7a422ddf1ae824b53ce4b51063d0eeb2afa757eb40804a8ef/zstandard-0.23.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:53dd9d5e3d29f95acd5de6802e909ada8d8d8cfa37a3ac64836f3bc4bc5512db", size = 4859973 },
    { url = "https://files.pythonhosted.org/packages/ab/15/08d22e87753304405ccac8be2493a495f529edd81d39a0870621462276ef/zstandard-0.23.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:6a41c120c3dbc0d81a8e8adc73312d668cd34acd7725f036992b1b72d22c1772", size = 4936968 },
    { url = "https://files.pythonhosted.org/packages/eb/fa/f3670a597949fe7dcf38119a39f7da49a8a84a6f0b1a2e46b2f71a0ab83f/zstandard-0.23.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:40b33d93c6eddf02d2c19f5773196068d875c41ca25730e8288e9b672897c105", size = 5467179 },
    { url = "https://files.pythonhosted.org/packages/4e/a9/dad2ab22020211e380adc477a1dbf9f109b1f8d94c614944843e20dc2a99/zstandard-0.23.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:9206649ec587e6b02bd124fb7799b86cddec350f6f6c14bc82a2b70183e708ba", size = 4848577 },
    { url = "https://files.pythonhosted.org/packages/08/03/dd28b4484b0770f1e23478413e01bee476ae8227bbc81561f9c329e12564/zstandard-0.23.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:76e79bc28a65f467e0409098fa2c4376931fd3207fbeb6b956c7c476d53746dd", size = 4693899 },
    { url = "https://files.pythonhosted.org/packages/2b/64/3da7497eb635d025841e958bcd66a86117ae320c3b14b0ae86e9e8627518/zstandard-0.23.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:66b689c107857eceabf2cf3d3fc699c3c0fe8ccd18df2219d978c0283e4c508a", size = 5199964 },
    { url = "https://files.pythonhosted.org/packages/43/a4/d82decbab158a0e8a6ebb7fc98bc4d903266bce85b6e9aaedea1d288338c/zstandard-0.23.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9c236e635582742fee16603042553d276cca506e824fa2e6489db04039521e90", size = 5655398 },
    { url = "https://files.pythonhosted.org/packages/f2/61/ac78a1263bc83a5cf29e7458b77a568eda5a8f81980691bbc6eb6a0d45cc/zstandard-0.23.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:a8fffdbd9d1408006baaf02f1068d7dd1f016c6bcb7538682622c556e7b68e35", size = 5191313 },
    { url = "https://files.pythonhosted.org/packages/e7/54/967c478314e16af5baf849b6ee9d6ea724ae5b100eb506011f045d3d4e16/zstandard-0.23.0-cp312-cp312-win32.whl", hash = "sha256:dc1d33abb8a0d754ea4763bad944fd965d3d95b5baef6b121c0c9013eaf1907d", size = 430877 },
    { url = "https://files.pythonhosted.org/packages/75/37/872d74bd7739639c4553bf94c84af7d54d8211b626b352bc57f0fd8d1e3f/zstandard-0.23.0-cp312-cp312-win_amd64.whl", hash = "sha256:64585e1dba664dc67c7cdabd56c1e5685233fbb1fc1966cfba2a340ec0dfff7b", size = 495595 },
    { url = "https://files.pythonhosted.org/packages/80/f1/8386f3f7c10261fe85fbc2c012fdb3d4db793b921c9abcc995d8da1b7a80/zstandard-0.23.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:576856e8594e6649aee06ddbfc738fec6a834f7c85bf7cadd1c53d4a58186ef9", size = 788975 },
    { url = "https://files.pythonhosted.org/packages/16/e8/cbf01077550b3e5dc86089035ff8f6fbbb312bc0983757c2d1117ebba242/zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:38302b78a850ff82656beaddeb0bb989a0322a8bbb1bf1ab10c17506681d772a", size = 633448 },
    { url = "https://files.pythonhosted.org/packages/06/27/4a1b4c267c29a464a161aeb2589aff212b4db653a1d96bffe3598f3f0d22/zstandard-0.23.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d2240ddc86b74966c34554c49d00eaafa8200a18d3a5b6ffbf7da63b11d74ee2", size = 4945269 },
    { url = "https://files.pythonhosted.org/packages/7c/64/d99261cc57afd9ae65b707e38045ed8269fbdae73544fd2e4a4d50d0ed83/zstandard-0.23.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2ef230a8fd217a2015bc91b74f6b3b7d6522ba48be29ad4ea0ca3a3775bf7dd5", size = 5306228 },
    { url = "https://files.pythonhosted.org/packages/7a/cf/27b74c6f22541f0263016a0fd6369b1b7818941de639215c84e4e94b2a1c/zstandard-0.23.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:774d45b1fac1461f48698a9d4b5fa19a69d47ece02fa469825b442263f04021f", size = 5336891 },
    { url = "https://files.pythonhosted.org/packages/fa/18/89ac62eac46b69948bf35fcd90d37103f38722968e2981f752d69081ec4d/zstandard-0.23.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6f77fa49079891a4aab203d0b1744acc85577ed16d767b52fc089d83faf8d8ed", size = 5436310 },
    { url = "https://files.pythonhosted.org/packages/a8/a8/5ca5328ee568a873f5118d5b5f70d1f36c6387716efe2e369010289a5738/zstandard-0.23.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ac184f87ff521f4840e6ea0b10c0ec90c6b1dcd0bad2f1e4a9a1b4fa177982ea", size = 4859912 },
    { url = "https://files.pythonhosted.org/packages/ea/ca/3781059c95fd0868658b1cf0440edd832b942f84ae60685d0cfdb808bca1/zstandard-0.23.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c363b53e257246a954ebc7c488304b5592b9c53fbe74d03bc1c64dda153fb847", size = 4936946 },
    { url = "https://files.pythonhosted.org/packages/ce/11/41a58986f809532742c2b832c53b74ba0e0a5dae7e8ab4642bf5876f35de/zstandard-0.23.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e7792606d606c8df5277c32ccb58f29b9b8603bf83b48639b7aedf6df4fe8171", size = 5466994 },
    { url = "https://files.pythonhosted.org/packages/83/e3/97d84fe95edd38d7053af05159465d298c8b20cebe9ccb3d26783faa9094/zstandard-0.23.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a0817825b900fcd43ac5d05b8b3079937073d2b1ff9cf89427590718b70dd840", size = 4848681 },
    { url = "https://files.pythonhosted.org/packages/6e/99/cb1e63e931de15c88af26085e3f2d9af9ce53ccafac73b6e48418fd5a6e6/zstandard-0.23.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:9da6bc32faac9a293ddfdcb9108d4b20416219461e4ec64dfea8383cac186690", size = 4694239 },
    { url = "https://files.pythonhosted.org/packages/ab/50/b1e703016eebbc6501fc92f34db7b1c68e54e567ef39e6e59cf5fb6f2ec0/zstandard-0.23.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:fd7699e8fd9969f455ef2926221e0233f81a2542921471382e77a9e2f2b57f4b", size = 5200149 },
    { url = "https://files.pythonhosted.org/packages/aa/e0/932388630aaba70197c78bdb10cce2c91fae01a7e553b76ce85471aec690/zstandard-0.23.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:d477ed829077cd945b01fc3115edd132c47e6540ddcd96ca169facff28173057", size = 5655392 },
    { url = "https://files.pythonhosted.org/packages/02/90/2633473864f67a15526324b007a9f96c96f56d5f32ef2a56cc12f9548723/zstandard-0.23.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa6ce8b52c5987b3e34d5674b0ab529a4602b632ebab0a93b07bfb4dfc8f8a33", size = 5191299 },
    { url = "https://files.pythonhosted.org/packages/b0/4c/315ca5c32da7e2dc3455f3b2caee5c8c2246074a61aac6ec3378a97b7136/zstandard-0.23.0-cp313-cp313-win32.whl", hash = "sha256:a9b07268d0c3ca5c170a385a0ab9fb7fdd9f5fd866be004c4ea39e44edce47dd", size = 430862 },
    { url = "https://files.pythonhosted.org/packages/a2/bf/c6aaba098e2d04781e8f4f7c0ba3c7aa73d00e4c436bcc0cf059a66691d1/zstandard-0.23.0-cp313-cp313-win_amd64.whl", hash = "sha256:f3513916e8c645d0610815c257cbfd3242adfd5c4cfa78be514e5a3ebb42a41b", size = 495578 },
]



================================================
FILE: .codespellignore
================================================
IST
afterAll


================================================
FILE: .dockerignore
================================================
node_modules
dist


================================================
FILE: .editorconfig
================================================
root = true

[*]
end_of_line = lf
insert_final_newline = true

[*.{js,json,yml}]
charset = utf-8
indent_style = space
indent_size = 2



================================================
FILE: .env.full.example
================================================
# ------------------LangSmith tracing------------------
LANGCHAIN_API_KEY=
LANGCHAIN_TRACING_V2=true
# -----------------------------------------------------

# LangGraph Server URL
LANGGRAPH_API_URL="http://localhost:54367" # Change the port if needed


#  ------------------Social Media Authentication------------------

# Arcade API key - used for fetching Tweets, and scheduling LinkedIn/Twitter posts
ARCADE_API_KEY=
# Setting this to false will not use Arcade for reading, or posting Tweets
USE_ARCADE_AUTH="true"

# Optional - set the Twitter and/or LinkedIn user ID(s) once instead
# of passing via configurable fields every time the graph is invoked.
TWITTER_USER_ID=
LINKEDIN_USER_ID=

# LinkedIn secrets
LINKEDIN_USER_ID=
# Generated after creating a LinkedIn developer account & app.
LINKEDIN_CLIENT_ID=
LINKEDIN_CLIENT_SECRET=
# Generated after running the auth server and logging in
LINKEDIN_ACCESS_TOKEN=
LINKEDIN_PERSON_URN=
# Found in the URL of the company page
LINKEDIN_ORGANIZATION_ID=
# Set this to true if you want to post to a company account
POST_TO_LINKEDIN_ORGANIZATION=false

# Twitter secrets (optional if running in basic setup mode with Arcade)
TWITTER_USER_ID=
# Generated after creating a Twitter developer account & app.
TWITTER_API_KEY=
TWITTER_API_KEY_SECRET=
TWITTER_BEARER_TOKEN=
# Generated after setting up `User Authentication settings`:
TWITTER_CLIENT_ID=
TWITTER_CLIENT_SECRET=
# Generated after running the auth server and logging in
TWITTER_USER_TOKEN=
TWITTER_USER_TOKEN_SECRET=
# If set to true, this will use your Twitter developer account for reading tweets, uploading media, and posting tweets.
# If set to false (default), your Twitter developer account will still be used for uploading media, however reading
# tweets will be done via Arcade. This will override `USE_ARCADE_AUTH` if set.
USE_TWITTER_API_ONLY=false
#  ---------------------------------------------------------------


# LLM API Keys
ANTHROPIC_API_KEY=
# Optional, only set if not running in basic setup mode
GOOGLE_VERTEX_AI_WEB_CREDENTIALS=

# Slack API key (only required if running `ingest-data` graph)
SLACK_BOT_OAUTH_TOKEN=

# FireCrawl API key - used for scraping web pages
FIRECRAWL_API_KEY=

# Supabase - used for storing images (optional if running in basic setup mode)
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY= 

# Used for fetching GitHub repo contents (optional)
GITHUB_TOKEN=

# A randomly generated string that is used in the Twitter OAuth flow
# Can be generated by running the following command in a terminal:
# openssl rand -hex 64
SESSION_SECRET=

# Slack channel ID to ingest messages from
SLACK_CHANNEL_ID=

# ENV VARS FOR SLACK MESSAGING

# These are available in the oauth section of your Slack App
SLACK_SIGNING_SECRET=
SLACK_BOT_TOKEN=

# This is either a specific ID of an assistant or the name of a graph
# (the key of a graph in the key-value pairs in the langgraph.json file)
LANGGRAPH_ASSISTANT_ID="ingest_repurposed_data"

# Whether or not to skip the verification step. If "true", all links will be assumed to be valid.
SKIP_CONTENT_RELEVANCY_CHECK="false"

# Whether or not to skip the used URLs check. If "true", the graph will not prevent duplicate links from being used, or save those links for future checks.
SKIP_USED_URLS_CHECK="false"



================================================
FILE: .env.quickstart.example
================================================
# For LangSmith tracing
LANGCHAIN_API_KEY=
LANGCHAIN_TRACING_V2=true

# For LLM generations
ANTHROPIC_API_KEY=

# For web scraping
FIRECRAWL_API_KEY=

# Arcade API key - used for fetching Tweets, and scheduling LinkedIn/Twitter posts
ARCADE_API_KEY=
# Setting this to false will not use Arcade for reading, or posting Tweets
USE_ARCADE_AUTH="true"
# Set this to true if your LinkedIn account is tied to a company page, and you want
# to post from the company page
POST_TO_LINKEDIN_ORGANIZATION="false"

# The Twitter/LinkedIn user ID/email/username of the account you want to use to post from
# This field can be passed via configurable fields (`twitterUserId`, `linkedinUserId`),
# _or_ set as an environment variable here.
TWITTER_USER_ID=
LINKEDIN_USER_ID=

# Whether or not to skip the verification step. If "true", all links will be assumed to be valid.
SKIP_CONTENT_RELEVANCY_CHECK="false"

# Whether or not to skip the used URLs check. If "true", the graph will not prevent duplicate links from being used, or save those links for future checks.
SKIP_USED_URLS_CHECK="false"



================================================
FILE: .eslintrc.cjs
================================================
module.exports = {
  extends: [
    "eslint:recommended",
    "prettier",
    "plugin:@typescript-eslint/recommended",
  ],
  parserOptions: {
    ecmaVersion: 12,
    parser: "@typescript-eslint/parser",
    project: "./tsconfig.json",
    sourceType: "module",
  },
  plugins: ["import", "@typescript-eslint", "no-instanceof"],
  ignorePatterns: [
    ".eslintrc.cjs",
    "scripts",
    "src/utils/lodash/*",
    "node_modules",
    "dist",
    "dist-cjs",
    "*.js",
    "*.cjs",
    "*.d.ts",
  ],
  rules: {
    "@typescript-eslint/explicit-module-boundary-types": 0,
    "@typescript-eslint/no-empty-function": 0,
    "@typescript-eslint/no-shadow": 0,
    "@typescript-eslint/no-empty-interface": 0,
    "@typescript-eslint/no-use-before-define": ["error", "nofunc"],
    "@typescript-eslint/no-unused-vars": [
      "error",
      {
        argsIgnorePattern: "^_",
        varsIgnorePattern: "^_|^UNUSED_",
        caughtErrorsIgnorePattern: "^_",
        destructuredArrayIgnorePattern: "^_",
      },
    ],
    "@typescript-eslint/no-floating-promises": "error",
    "@typescript-eslint/no-misused-promises": "error",
    "@typescript-eslint/await-thenable": "error",
    "@typescript-eslint/no-explicit-any": 0,
    camelcase: 0,
    "class-methods-use-this": 0,
    "import/extensions": [2, "ignorePackages"],
    "import/no-extraneous-dependencies": [
      "error",
      { devDependencies: ["**/*.test.ts"] },
    ],
    "import/no-unresolved": 0,
    "import/prefer-default-export": 0,
    "keyword-spacing": "error",
    "max-classes-per-file": 0,
    "max-len": 0,
    "no-await-in-loop": 0,
    "no-bitwise": 0,
    "no-console": 0,
    "no-restricted-syntax": 0,
    "no-shadow": 0,
    "no-continue": 0,
    "no-underscore-dangle": 0,
    "no-use-before-define": 0,
    "no-useless-constructor": 0,
    "no-return-await": 0,
    "consistent-return": 0,
    "no-else-return": 0,
    "new-cap": ["error", { properties: false, capIsNew: false }],
  },
};



================================================
FILE: memory-v2/README.md
================================================



================================================
FILE: memory-v2/langgraph.json
================================================
{
  "graphs": {
    "reflection_v2": "./memory_v2/graph.py:graph"
  },
  "env": "../.env",
  "dependencies": ["."]
}



================================================
FILE: memory-v2/Makefile
================================================
.PHONY: all format lint test tests test_watch integration_tests docker_tests help extended_tests

# Default target executed when no arguments are given to make.
all: help

# Define a variable for the test file path.
TEST_FILE ?= tests/unit_tests/

test:
	python -m pytest $(TEST_FILE)

integration_tests:
	python -m pytest tests/integration_tests 

test_watch:
	python -m ptw --snapshot-update --now . -- -vv tests/unit_tests

test_profile:
	python -m pytest -vv tests/unit_tests/ --profile-svg

extended_tests:
	python -m pytest --only-extended $(TEST_FILE)


######################
# LINTING AND FORMATTING
######################

# Define a variable for Python and notebook files.
PYTHON_FILES=src/
MYPY_CACHE=.mypy_cache
lint format: PYTHON_FILES=.
lint_diff format_diff: PYTHON_FILES=$(shell git diff --name-only --diff-filter=d main | grep -E '\.py$$|\.ipynb$$')
lint_package: PYTHON_FILES=src
lint_tests: PYTHON_FILES=tests
lint_tests: MYPY_CACHE=.mypy_cache_test

lint lint_diff lint_package lint_tests:
	python -m ruff check .
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff format $(PYTHON_FILES) --diff
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff check --select I $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || python -m mypy --strict $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || mkdir -p $(MYPY_CACHE) && python -m mypy --strict $(PYTHON_FILES) --cache-dir $(MYPY_CACHE)

format format_diff:
	ruff format $(PYTHON_FILES)
	ruff check --select I --fix $(PYTHON_FILES)

spell_check:
	codespell --toml pyproject.toml

spell_fix:
	codespell --toml pyproject.toml -w

######################
# HELP
######################

help:
	@echo '----'
	@echo 'format                       - run code formatters'
	@echo 'lint                         - run linters'
	@echo 'test                         - run unit tests'
	@echo 'tests                        - run unit tests'
	@echo 'test TEST_FILE=<test_file>   - run all tests in file'
	@echo 'test_watch                   - run unit tests in watch mode'



================================================
FILE: memory-v2/poetry.lock
================================================
# This file is automatically @generated by Poetry 1.8.2 and should not be changed by hand.

[[package]]
name = "aiohappyeyeballs"
version = "2.4.4"
description = "Happy Eyeballs for asyncio"
optional = false
python-versions = ">=3.8"
files = [
    {file = "aiohappyeyeballs-2.4.4-py3-none-any.whl", hash = "sha256:a980909d50efcd44795c4afeca523296716d50cd756ddca6af8c65b996e27de8"},
    {file = "aiohappyeyeballs-2.4.4.tar.gz", hash = "sha256:5fdd7d87889c63183afc18ce9271f9b0a7d32c2303e394468dd45d514a757745"},
]

[[package]]
name = "aiohttp"
version = "3.11.11"
description = "Async http client/server framework (asyncio)"
optional = false
python-versions = ">=3.9"
files = [
    {file = "aiohttp-3.11.11-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:a60804bff28662cbcf340a4d61598891f12eea3a66af48ecfdc975ceec21e3c8"},
    {file = "aiohttp-3.11.11-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:4b4fa1cb5f270fb3eab079536b764ad740bb749ce69a94d4ec30ceee1b5940d5"},
    {file = "aiohttp-3.11.11-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:731468f555656767cda219ab42e033355fe48c85fbe3ba83a349631541715ba2"},
    {file = "aiohttp-3.11.11-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:cb23d8bb86282b342481cad4370ea0853a39e4a32a0042bb52ca6bdde132df43"},
    {file = "aiohttp-3.11.11-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f047569d655f81cb70ea5be942ee5d4421b6219c3f05d131f64088c73bb0917f"},
    {file = "aiohttp-3.11.11-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:dd7659baae9ccf94ae5fe8bfaa2c7bc2e94d24611528395ce88d009107e00c6d"},
    {file = "aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:af01e42ad87ae24932138f154105e88da13ce7d202a6de93fafdafb2883a00ef"},
    {file = "aiohttp-3.11.11-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5854be2f3e5a729800bac57a8d76af464e160f19676ab6aea74bde18ad19d438"},
    {file = "aiohttp-3.11.11-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:6526e5fb4e14f4bbf30411216780c9967c20c5a55f2f51d3abd6de68320cc2f3"},
    {file = "aiohttp-3.11.11-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:85992ee30a31835fc482468637b3e5bd085fa8fe9392ba0bdcbdc1ef5e9e3c55"},
    {file = "aiohttp-3.11.11-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:88a12ad8ccf325a8a5ed80e6d7c3bdc247d66175afedbe104ee2aaca72960d8e"},
    {file = "aiohttp-3.11.11-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:0a6d3fbf2232e3a08c41eca81ae4f1dff3d8f1a30bae415ebe0af2d2458b8a33"},
    {file = "aiohttp-3.11.11-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:84a585799c58b795573c7fa9b84c455adf3e1d72f19a2bf498b54a95ae0d194c"},
    {file = "aiohttp-3.11.11-cp310-cp310-win32.whl", hash = "sha256:bfde76a8f430cf5c5584553adf9926534352251d379dcb266ad2b93c54a29745"},
    {file = "aiohttp-3.11.11-cp310-cp310-win_amd64.whl", hash = "sha256:0fd82b8e9c383af11d2b26f27a478640b6b83d669440c0a71481f7c865a51da9"},
    {file = "aiohttp-3.11.11-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:ba74ec819177af1ef7f59063c6d35a214a8fde6f987f7661f4f0eecc468a8f76"},
    {file = "aiohttp-3.11.11-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:4af57160800b7a815f3fe0eba9b46bf28aafc195555f1824555fa2cfab6c1538"},
    {file = "aiohttp-3.11.11-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:ffa336210cf9cd8ed117011085817d00abe4c08f99968deef0013ea283547204"},
    {file = "aiohttp-3.11.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:81b8fe282183e4a3c7a1b72f5ade1094ed1c6345a8f153506d114af5bf8accd9"},
    {file = "aiohttp-3.11.11-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3af41686ccec6a0f2bdc66686dc0f403c41ac2089f80e2214a0f82d001052c03"},
    {file = "aiohttp-3.11.11-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:70d1f9dde0e5dd9e292a6d4d00058737052b01f3532f69c0c65818dac26dc287"},
    {file = "aiohttp-3.11.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:249cc6912405917344192b9f9ea5cd5b139d49e0d2f5c7f70bdfaf6b4dbf3a2e"},
    {file = "aiohttp-3.11.11-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0eb98d90b6690827dcc84c246811feeb4e1eea683c0eac6caed7549be9c84665"},
    {file = "aiohttp-3.11.11-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:ec82bf1fda6cecce7f7b915f9196601a1bd1a3079796b76d16ae4cce6d0ef89b"},
    {file = "aiohttp-3.11.11-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:9fd46ce0845cfe28f108888b3ab17abff84ff695e01e73657eec3f96d72eef34"},
    {file = "aiohttp-3.11.11-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:bd176afcf8f5d2aed50c3647d4925d0db0579d96f75a31e77cbaf67d8a87742d"},
    {file = "aiohttp-3.11.11-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:ec2aa89305006fba9ffb98970db6c8221541be7bee4c1d027421d6f6df7d1ce2"},
    {file = "aiohttp-3.11.11-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:92cde43018a2e17d48bb09c79e4d4cb0e236de5063ce897a5e40ac7cb4878773"},
    {file = "aiohttp-3.11.11-cp311-cp311-win32.whl", hash = "sha256:aba807f9569455cba566882c8938f1a549f205ee43c27b126e5450dc9f83cc62"},
    {file = "aiohttp-3.11.11-cp311-cp311-win_amd64.whl", hash = "sha256:ae545f31489548c87b0cced5755cfe5a5308d00407000e72c4fa30b19c3220ac"},
    {file = "aiohttp-3.11.11-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:e595c591a48bbc295ebf47cb91aebf9bd32f3ff76749ecf282ea7f9f6bb73886"},
    {file = "aiohttp-3.11.11-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:3ea1b59dc06396b0b424740a10a0a63974c725b1c64736ff788a3689d36c02d2"},
    {file = "aiohttp-3.11.11-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:8811f3f098a78ffa16e0ea36dffd577eb031aea797cbdba81be039a4169e242c"},
    {file = "aiohttp-3.11.11-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bd7227b87a355ce1f4bf83bfae4399b1f5bb42e0259cb9405824bd03d2f4336a"},
    {file = "aiohttp-3.11.11-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d40f9da8cabbf295d3a9dae1295c69975b86d941bc20f0a087f0477fa0a66231"},
    {file = "aiohttp-3.11.11-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ffb3dc385f6bb1568aa974fe65da84723210e5d9707e360e9ecb51f59406cd2e"},
    {file = "aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a8f5f7515f3552d899c61202d99dcb17d6e3b0de777900405611cd747cecd1b8"},
    {file = "aiohttp-3.11.11-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3499c7ffbfd9c6a3d8d6a2b01c26639da7e43d47c7b4f788016226b1e711caa8"},
    {file = "aiohttp-3.11.11-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:8e2bf8029dbf0810c7bfbc3e594b51c4cc9101fbffb583a3923aea184724203c"},
    {file = "aiohttp-3.11.11-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:b6212a60e5c482ef90f2d788835387070a88d52cf6241d3916733c9176d39eab"},
    {file = "aiohttp-3.11.11-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:d119fafe7b634dbfa25a8c597718e69a930e4847f0b88e172744be24515140da"},
    {file = "aiohttp-3.11.11-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:6fba278063559acc730abf49845d0e9a9e1ba74f85f0ee6efd5803f08b285853"},
    {file = "aiohttp-3.11.11-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:92fc484e34b733704ad77210c7957679c5c3877bd1e6b6d74b185e9320cc716e"},
    {file = "aiohttp-3.11.11-cp312-cp312-win32.whl", hash = "sha256:9f5b3c1ed63c8fa937a920b6c1bec78b74ee09593b3f5b979ab2ae5ef60d7600"},
    {file = "aiohttp-3.11.11-cp312-cp312-win_amd64.whl", hash = "sha256:1e69966ea6ef0c14ee53ef7a3d68b564cc408121ea56c0caa2dc918c1b2f553d"},
    {file = "aiohttp-3.11.11-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:541d823548ab69d13d23730a06f97460f4238ad2e5ed966aaf850d7c369782d9"},
    {file = "aiohttp-3.11.11-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:929f3ed33743a49ab127c58c3e0a827de0664bfcda566108989a14068f820194"},
    {file = "aiohttp-3.11.11-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:0882c2820fd0132240edbb4a51eb8ceb6eef8181db9ad5291ab3332e0d71df5f"},
    {file = "aiohttp-3.11.11-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b63de12e44935d5aca7ed7ed98a255a11e5cb47f83a9fded7a5e41c40277d104"},
    {file = "aiohttp-3.11.11-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:aa54f8ef31d23c506910c21163f22b124facb573bff73930735cf9fe38bf7dff"},
    {file = "aiohttp-3.11.11-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a344d5dc18074e3872777b62f5f7d584ae4344cd6006c17ba12103759d407af3"},
    {file = "aiohttp-3.11.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0b7fb429ab1aafa1f48578eb315ca45bd46e9c37de11fe45c7f5f4138091e2f1"},
    {file = "aiohttp-3.11.11-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c341c7d868750e31961d6d8e60ff040fb9d3d3a46d77fd85e1ab8e76c3e9a5c4"},
    {file = "aiohttp-3.11.11-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:ed9ee95614a71e87f1a70bc81603f6c6760128b140bc4030abe6abaa988f1c3d"},
    {file = "aiohttp-3.11.11-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:de8d38f1c2810fa2a4f1d995a2e9c70bb8737b18da04ac2afbf3971f65781d87"},
    {file = "aiohttp-3.11.11-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:a9b7371665d4f00deb8f32208c7c5e652059b0fda41cf6dbcac6114a041f1cc2"},
    {file = "aiohttp-3.11.11-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:620598717fce1b3bd14dd09947ea53e1ad510317c85dda2c9c65b622edc96b12"},
    {file = "aiohttp-3.11.11-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:bf8d9bfee991d8acc72d060d53860f356e07a50f0e0d09a8dfedea1c554dd0d5"},
    {file = "aiohttp-3.11.11-cp313-cp313-win32.whl", hash = "sha256:9d73ee3725b7a737ad86c2eac5c57a4a97793d9f442599bea5ec67ac9f4bdc3d"},
    {file = "aiohttp-3.11.11-cp313-cp313-win_amd64.whl", hash = "sha256:c7a06301c2fb096bdb0bd25fe2011531c1453b9f2c163c8031600ec73af1cc99"},
    {file = "aiohttp-3.11.11-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:3e23419d832d969f659c208557de4a123e30a10d26e1e14b73431d3c13444c2e"},
    {file = "aiohttp-3.11.11-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:21fef42317cf02e05d3b09c028712e1d73a9606f02467fd803f7c1f39cc59add"},
    {file = "aiohttp-3.11.11-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:1f21bb8d0235fc10c09ce1d11ffbd40fc50d3f08a89e4cf3a0c503dc2562247a"},
    {file = "aiohttp-3.11.11-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1642eceeaa5ab6c9b6dfeaaa626ae314d808188ab23ae196a34c9d97efb68350"},
    {file = "aiohttp-3.11.11-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2170816e34e10f2fd120f603e951630f8a112e1be3b60963a1f159f5699059a6"},
    {file = "aiohttp-3.11.11-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:8be8508d110d93061197fd2d6a74f7401f73b6d12f8822bbcd6d74f2b55d71b1"},
    {file = "aiohttp-3.11.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4eed954b161e6b9b65f6be446ed448ed3921763cc432053ceb606f89d793927e"},
    {file = "aiohttp-3.11.11-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d6c9af134da4bc9b3bd3e6a70072509f295d10ee60c697826225b60b9959acdd"},
    {file = "aiohttp-3.11.11-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:44167fc6a763d534a6908bdb2592269b4bf30a03239bcb1654781adf5e49caf1"},
    {file = "aiohttp-3.11.11-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:479b8c6ebd12aedfe64563b85920525d05d394b85f166b7873c8bde6da612f9c"},
    {file = "aiohttp-3.11.11-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:10b4ff0ad793d98605958089fabfa350e8e62bd5d40aa65cdc69d6785859f94e"},
    {file = "aiohttp-3.11.11-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:b540bd67cfb54e6f0865ceccd9979687210d7ed1a1cc8c01f8e67e2f1e883d28"},
    {file = "aiohttp-3.11.11-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:1dac54e8ce2ed83b1f6b1a54005c87dfed139cf3f777fdc8afc76e7841101226"},
    {file = "aiohttp-3.11.11-cp39-cp39-win32.whl", hash = "sha256:568c1236b2fde93b7720f95a890741854c1200fba4a3471ff48b2934d2d93fd3"},
    {file = "aiohttp-3.11.11-cp39-cp39-win_amd64.whl", hash = "sha256:943a8b052e54dfd6439fd7989f67fc6a7f2138d0a2cf0a7de5f18aa4fe7eb3b1"},
    {file = "aiohttp-3.11.11.tar.gz", hash = "sha256:bb49c7f1e6ebf3821a42d81d494f538107610c3a705987f53068546b0e90303e"},
]

[package.dependencies]
aiohappyeyeballs = ">=2.3.0"
aiosignal = ">=1.1.2"
attrs = ">=17.3.0"
frozenlist = ">=1.1.1"
multidict = ">=4.5,<7.0"
propcache = ">=0.2.0"
yarl = ">=1.17.0,<2.0"

[package.extras]
speedups = ["Brotli", "aiodns (>=3.2.0)", "brotlicffi"]

[[package]]
name = "aiosignal"
version = "1.3.2"
description = "aiosignal: a list of registered asynchronous callbacks"
optional = false
python-versions = ">=3.9"
files = [
    {file = "aiosignal-1.3.2-py2.py3-none-any.whl", hash = "sha256:45cde58e409a301715980c2b01d0c28bdde3770d8290b5eb2173759d9acb31a5"},
    {file = "aiosignal-1.3.2.tar.gz", hash = "sha256:a8c255c66fafb1e499c9351d0bf32ff2d8a0321595ebac3b93713656d2436f54"},
]

[package.dependencies]
frozenlist = ">=1.1.0"

[[package]]
name = "annotated-types"
version = "0.7.0"
description = "Reusable constraint types to use with typing.Annotated"
optional = false
python-versions = ">=3.8"
files = [
    {file = "annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53"},
    {file = "annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89"},
]

[[package]]
name = "anthropic"
version = "0.45.0"
description = "The official Python library for the anthropic API"
optional = false
python-versions = ">=3.8"
files = [
    {file = "anthropic-0.45.0-py3-none-any.whl", hash = "sha256:f36aff71d2c232945e64d1970be68a91b05a2ef5e3afa6c1ff195c3303a95ad3"},
    {file = "anthropic-0.45.0.tar.gz", hash = "sha256:4e8541dc355332090bfc51b84549c19b649a13a23dbd6bd68e1d012e08551025"},
]

[package.dependencies]
anyio = ">=3.5.0,<5"
distro = ">=1.7.0,<2"
httpx = ">=0.23.0,<1"
jiter = ">=0.4.0,<1"
pydantic = ">=1.9.0,<3"
sniffio = "*"
typing-extensions = ">=4.10,<5"

[package.extras]
bedrock = ["boto3 (>=1.28.57)", "botocore (>=1.31.57)"]
vertex = ["google-auth (>=2,<3)"]

[[package]]
name = "anyio"
version = "4.8.0"
description = "High level compatibility layer for multiple asynchronous event loop implementations"
optional = false
python-versions = ">=3.9"
files = [
    {file = "anyio-4.8.0-py3-none-any.whl", hash = "sha256:b5011f270ab5eb0abf13385f851315585cc37ef330dd88e27ec3d34d651fd47a"},
    {file = "anyio-4.8.0.tar.gz", hash = "sha256:1d9fe889df5212298c0c0723fa20479d1b94883a2df44bd3897aa91083316f7a"},
]

[package.dependencies]
idna = ">=2.8"
sniffio = ">=1.1"
typing_extensions = {version = ">=4.5", markers = "python_version < \"3.13\""}

[package.extras]
doc = ["Sphinx (>=7.4,<8.0)", "packaging", "sphinx-autodoc-typehints (>=1.2.0)", "sphinx_rtd_theme"]
test = ["anyio[trio]", "coverage[toml] (>=7)", "exceptiongroup (>=1.2.0)", "hypothesis (>=4.0)", "psutil (>=5.9)", "pytest (>=7.0)", "trustme", "truststore (>=0.9.1)", "uvloop (>=0.21)"]
trio = ["trio (>=0.26.1)"]

[[package]]
name = "attrs"
version = "24.3.0"
description = "Classes Without Boilerplate"
optional = false
python-versions = ">=3.8"
files = [
    {file = "attrs-24.3.0-py3-none-any.whl", hash = "sha256:ac96cd038792094f438ad1f6ff80837353805ac950cd2aa0e0625ef19850c308"},
    {file = "attrs-24.3.0.tar.gz", hash = "sha256:8f5c07333d543103541ba7be0e2ce16eeee8130cb0b3f9238ab904ce1e85baff"},
]

[package.extras]
benchmark = ["cloudpickle", "hypothesis", "mypy (>=1.11.1)", "pympler", "pytest (>=4.3.0)", "pytest-codspeed", "pytest-mypy-plugins", "pytest-xdist[psutil]"]
cov = ["cloudpickle", "coverage[toml] (>=5.3)", "hypothesis", "mypy (>=1.11.1)", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins", "pytest-xdist[psutil]"]
dev = ["cloudpickle", "hypothesis", "mypy (>=1.11.1)", "pre-commit-uv", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins", "pytest-xdist[psutil]"]
docs = ["cogapp", "furo", "myst-parser", "sphinx", "sphinx-notfound-page", "sphinxcontrib-towncrier", "towncrier (<24.7)"]
tests = ["cloudpickle", "hypothesis", "mypy (>=1.11.1)", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins", "pytest-xdist[psutil]"]
tests-mypy = ["mypy (>=1.11.1)", "pytest-mypy-plugins"]

[[package]]
name = "certifi"
version = "2024.12.14"
description = "Python package for providing Mozilla's CA Bundle."
optional = false
python-versions = ">=3.6"
files = [
    {file = "certifi-2024.12.14-py3-none-any.whl", hash = "sha256:1275f7a45be9464efc1173084eaa30f866fe2e47d389406136d332ed4967ec56"},
    {file = "certifi-2024.12.14.tar.gz", hash = "sha256:b650d30f370c2b724812bee08008be0c4163b163ddaec3f2546c1caf65f191db"},
]

[[package]]
name = "cffi"
version = "1.17.1"
description = "Foreign Function Interface for Python calling C code."
optional = false
python-versions = ">=3.8"
files = [
    {file = "cffi-1.17.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:df8b1c11f177bc2313ec4b2d46baec87a5f3e71fc8b45dab2ee7cae86d9aba14"},
    {file = "cffi-1.17.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:8f2cdc858323644ab277e9bb925ad72ae0e67f69e804f4898c070998d50b1a67"},
    {file = "cffi-1.17.1-cp310-cp310-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:edae79245293e15384b51f88b00613ba9f7198016a5948b5dddf4917d4d26382"},
    {file = "cffi-1.17.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:45398b671ac6d70e67da8e4224a065cec6a93541bb7aebe1b198a61b58c7b702"},
    {file = "cffi-1.17.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ad9413ccdeda48c5afdae7e4fa2192157e991ff761e7ab8fdd8926f40b160cc3"},
    {file = "cffi-1.17.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5da5719280082ac6bd9aa7becb3938dc9f9cbd57fac7d2871717b1feb0902ab6"},
    {file = "cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2bb1a08b8008b281856e5971307cc386a8e9c5b625ac297e853d36da6efe9c17"},
    {file = "cffi-1.17.1-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:045d61c734659cc045141be4bae381a41d89b741f795af1dd018bfb532fd0df8"},
    {file = "cffi-1.17.1-cp310-cp310-musllinux_1_1_i686.whl", hash = "sha256:6883e737d7d9e4899a8a695e00ec36bd4e5e4f18fabe0aca0efe0a4b44cdb13e"},
    {file = "cffi-1.17.1-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:6b8b4a92e1c65048ff98cfe1f735ef8f1ceb72e3d5f0c25fdb12087a23da22be"},
    {file = "cffi-1.17.1-cp310-cp310-win32.whl", hash = "sha256:c9c3d058ebabb74db66e431095118094d06abf53284d9c81f27300d0e0d8bc7c"},
    {file = "cffi-1.17.1-cp310-cp310-win_amd64.whl", hash = "sha256:0f048dcf80db46f0098ccac01132761580d28e28bc0f78ae0d58048063317e15"},
    {file = "cffi-1.17.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:a45e3c6913c5b87b3ff120dcdc03f6131fa0065027d0ed7ee6190736a74cd401"},
    {file = "cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:30c5e0cb5ae493c04c8b42916e52ca38079f1b235c2f8ae5f4527b963c401caf"},
    {file = "cffi-1.17.1-cp311-cp311-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f75c7ab1f9e4aca5414ed4d8e5c0e303a34f4421f8a0d47a4d019ceff0ab6af4"},
    {file = "cffi-1.17.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a1ed2dd2972641495a3ec98445e09766f077aee98a1c896dcb4ad0d303628e41"},
    {file = "cffi-1.17.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:46bf43160c1a35f7ec506d254e5c890f3c03648a4dbac12d624e4490a7046cd1"},
    {file = "cffi-1.17.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a24ed04c8ffd54b0729c07cee15a81d964e6fee0e3d4d342a27b020d22959dc6"},
    {file = "cffi-1.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:610faea79c43e44c71e1ec53a554553fa22321b65fae24889706c0a84d4ad86d"},
    {file = "cffi-1.17.1-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:a9b15d491f3ad5d692e11f6b71f7857e7835eb677955c00cc0aefcd0669adaf6"},
    {file = "cffi-1.17.1-cp311-cp311-musllinux_1_1_i686.whl", hash = "sha256:de2ea4b5833625383e464549fec1bc395c1bdeeb5f25c4a3a82b5a8c756ec22f"},
    {file = "cffi-1.17.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:fc48c783f9c87e60831201f2cce7f3b2e4846bf4d8728eabe54d60700b318a0b"},
    {file = "cffi-1.17.1-cp311-cp311-win32.whl", hash = "sha256:85a950a4ac9c359340d5963966e3e0a94a676bd6245a4b55bc43949eee26a655"},
    {file = "cffi-1.17.1-cp311-cp311-win_amd64.whl", hash = "sha256:caaf0640ef5f5517f49bc275eca1406b0ffa6aa184892812030f04c2abf589a0"},
    {file = "cffi-1.17.1-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:805b4371bf7197c329fcb3ead37e710d1bca9da5d583f5073b799d5c5bd1eee4"},
    {file = "cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:733e99bc2df47476e3848417c5a4540522f234dfd4ef3ab7fafdf555b082ec0c"},
    {file = "cffi-1.17.1-cp312-cp312-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1257bdabf294dceb59f5e70c64a3e2f462c30c7ad68092d01bbbfb1c16b1ba36"},
    {file = "cffi-1.17.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:da95af8214998d77a98cc14e3a3bd00aa191526343078b530ceb0bd710fb48a5"},
    {file = "cffi-1.17.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d63afe322132c194cf832bfec0dc69a99fb9bb6bbd550f161a49e9e855cc78ff"},
    {file = "cffi-1.17.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f79fc4fc25f1c8698ff97788206bb3c2598949bfe0fef03d299eb1b5356ada99"},
    {file = "cffi-1.17.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b62ce867176a75d03a665bad002af8e6d54644fad99a3c70905c543130e39d93"},
    {file = "cffi-1.17.1-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:386c8bf53c502fff58903061338ce4f4950cbdcb23e2902d86c0f722b786bbe3"},
    {file = "cffi-1.17.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:4ceb10419a9adf4460ea14cfd6bc43d08701f0835e979bf821052f1805850fe8"},
    {file = "cffi-1.17.1-cp312-cp312-win32.whl", hash = "sha256:a08d7e755f8ed21095a310a693525137cfe756ce62d066e53f502a83dc550f65"},
    {file = "cffi-1.17.1-cp312-cp312-win_amd64.whl", hash = "sha256:51392eae71afec0d0c8fb1a53b204dbb3bcabcb3c9b807eedf3e1e6ccf2de903"},
    {file = "cffi-1.17.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:f3a2b4222ce6b60e2e8b337bb9596923045681d71e5a082783484d845390938e"},
    {file = "cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:0984a4925a435b1da406122d4d7968dd861c1385afe3b45ba82b750f229811e2"},
    {file = "cffi-1.17.1-cp313-cp313-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d01b12eeeb4427d3110de311e1774046ad344f5b1a7403101878976ecd7a10f3"},
    {file = "cffi-1.17.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:706510fe141c86a69c8ddc029c7910003a17353970cff3b904ff0686a5927683"},
    {file = "cffi-1.17.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:de55b766c7aa2e2a3092c51e0483d700341182f08e67c63630d5b6f200bb28e5"},
    {file = "cffi-1.17.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c59d6e989d07460165cc5ad3c61f9fd8f1b4796eacbd81cee78957842b834af4"},
    {file = "cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dd398dbc6773384a17fe0d3e7eeb8d1a21c2200473ee6806bb5e6a8e62bb73dd"},
    {file = "cffi-1.17.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:3edc8d958eb099c634dace3c7e16560ae474aa3803a5df240542b305d14e14ed"},
    {file = "cffi-1.17.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:72e72408cad3d5419375fc87d289076ee319835bdfa2caad331e377589aebba9"},
    {file = "cffi-1.17.1-cp313-cp313-win32.whl", hash = "sha256:e03eab0a8677fa80d646b5ddece1cbeaf556c313dcfac435ba11f107ba117b5d"},
    {file = "cffi-1.17.1-cp313-cp313-win_amd64.whl", hash = "sha256:f6a16c31041f09ead72d69f583767292f750d24913dadacf5756b966aacb3f1a"},
    {file = "cffi-1.17.1-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:636062ea65bd0195bc012fea9321aca499c0504409f413dc88af450b57ffd03b"},
    {file = "cffi-1.17.1-cp38-cp38-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c7eac2ef9b63c79431bc4b25f1cd649d7f061a28808cbc6c47b534bd789ef964"},
    {file = "cffi-1.17.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e221cf152cff04059d011ee126477f0d9588303eb57e88923578ace7baad17f9"},
    {file = "cffi-1.17.1-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:31000ec67d4221a71bd3f67df918b1f88f676f1c3b535a7eb473255fdc0b83fc"},
    {file = "cffi-1.17.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:6f17be4345073b0a7b8ea599688f692ac3ef23ce28e5df79c04de519dbc4912c"},
    {file = "cffi-1.17.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0e2b1fac190ae3ebfe37b979cc1ce69c81f4e4fe5746bb401dca63a9062cdaf1"},
    {file = "cffi-1.17.1-cp38-cp38-win32.whl", hash = "sha256:7596d6620d3fa590f677e9ee430df2958d2d6d6de2feeae5b20e82c00b76fbf8"},
    {file = "cffi-1.17.1-cp38-cp38-win_amd64.whl", hash = "sha256:78122be759c3f8a014ce010908ae03364d00a1f81ab5c7f4a7a5120607ea56e1"},
    {file = "cffi-1.17.1-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:b2ab587605f4ba0bf81dc0cb08a41bd1c0a5906bd59243d56bad7668a6fc6c16"},
    {file = "cffi-1.17.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:28b16024becceed8c6dfbc75629e27788d8a3f9030691a1dbf9821a128b22c36"},
    {file = "cffi-1.17.1-cp39-cp39-manylinux_2_12_i686.manylinux2010_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1d599671f396c4723d016dbddb72fe8e0397082b0a77a4fab8028923bec050e8"},
    {file = "cffi-1.17.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ca74b8dbe6e8e8263c0ffd60277de77dcee6c837a3d0881d8c1ead7268c9e576"},
    {file = "cffi-1.17.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f7f5baafcc48261359e14bcd6d9bff6d4b28d9103847c9e136694cb0501aef87"},
    {file = "cffi-1.17.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98e3969bcff97cae1b2def8ba499ea3d6f31ddfdb7635374834cf89a1a08ecf0"},
    {file = "cffi-1.17.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cdf5ce3acdfd1661132f2a9c19cac174758dc2352bfe37d98aa7512c6b7178b3"},
    {file = "cffi-1.17.1-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:9755e4345d1ec879e3849e62222a18c7174d65a6a92d5b346b1863912168b595"},
    {file = "cffi-1.17.1-cp39-cp39-musllinux_1_1_i686.whl", hash = "sha256:f1e22e8c4419538cb197e4dd60acc919d7696e5ef98ee4da4e01d3f8cfa4cc5a"},
    {file = "cffi-1.17.1-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:c03e868a0b3bc35839ba98e74211ed2b05d2119be4e8a0f224fba9384f1fe02e"},
    {file = "cffi-1.17.1-cp39-cp39-win32.whl", hash = "sha256:e31ae45bc2e29f6b2abd0de1cc3b9d5205aa847cafaecb8af1476a609a2f6eb7"},
    {file = "cffi-1.17.1-cp39-cp39-win_amd64.whl", hash = "sha256:d016c76bdd850f3c626af19b0542c9677ba156e4ee4fccfdd7848803533ef662"},
    {file = "cffi-1.17.1.tar.gz", hash = "sha256:1c39c6016c32bc48dd54561950ebd6836e1670f2ae46128f67cf49e789c52824"},
]

[package.dependencies]
pycparser = "*"

[[package]]
name = "charset-normalizer"
version = "3.4.1"
description = "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet."
optional = false
python-versions = ">=3.7"
files = [
    {file = "charset_normalizer-3.4.1-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:91b36a978b5ae0ee86c394f5a54d6ef44db1de0815eb43de826d41d21e4af3de"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7461baadb4dc00fd9e0acbe254e3d7d2112e7f92ced2adc96e54ef6501c5f176"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e218488cd232553829be0664c2292d3af2eeeb94b32bea483cf79ac6a694e037"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:80ed5e856eb7f30115aaf94e4a08114ccc8813e6ed1b5efa74f9f82e8509858f"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b010a7a4fd316c3c484d482922d13044979e78d1861f0e0650423144c616a46a"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4532bff1b8421fd0a320463030c7520f56a79c9024a4e88f01c537316019005a"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:d973f03c0cb71c5ed99037b870f2be986c3c05e63622c017ea9816881d2dd247"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:3a3bd0dcd373514dcec91c411ddb9632c0d7d92aed7093b8c3bbb6d69ca74408"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:d9c3cdf5390dcd29aa8056d13e8e99526cda0305acc038b96b30352aff5ff2bb"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:2bdfe3ac2e1bbe5b59a1a63721eb3b95fc9b6817ae4a46debbb4e11f6232428d"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:eab677309cdb30d047996b36d34caeda1dc91149e4fdca0b1a039b3f79d9a807"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-win32.whl", hash = "sha256:c0429126cf75e16c4f0ad00ee0eae4242dc652290f940152ca8c75c3a4b6ee8f"},
    {file = "charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl", hash = "sha256:9f0b8b1c6d84c8034a44893aba5e767bf9c7a211e313a9605d9c617d7083829f"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:8bfa33f4f2672964266e940dd22a195989ba31669bd84629f05fab3ef4e2d125"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:28bf57629c75e810b6ae989f03c0828d64d6b26a5e205535585f96093e405ed1"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f08ff5e948271dc7e18a35641d2f11a4cd8dfd5634f55228b691e62b37125eb3"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:234ac59ea147c59ee4da87a0c0f098e9c8d169f4dc2a159ef720f1a61bbe27cd"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd4ec41f914fa74ad1b8304bbc634b3de73d2a0889bd32076342a573e0779e00"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:eea6ee1db730b3483adf394ea72f808b6e18cf3cb6454b4d86e04fa8c4327a12"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:c96836c97b1238e9c9e3fe90844c947d5afbf4f4c92762679acfe19927d81d77"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:4d86f7aff21ee58f26dcf5ae81a9addbd914115cdebcbb2217e4f0ed8982e146"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:09b5e6733cbd160dcc09589227187e242a30a49ca5cefa5a7edd3f9d19ed53fd"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:5777ee0881f9499ed0f71cc82cf873d9a0ca8af166dfa0af8ec4e675b7df48e6"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:237bdbe6159cff53b4f24f397d43c6336c6b0b42affbe857970cefbb620911c8"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-win32.whl", hash = "sha256:8417cb1f36cc0bc7eaba8ccb0e04d55f0ee52df06df3ad55259b9a323555fc8b"},
    {file = "charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl", hash = "sha256:d7f50a1f8c450f3925cb367d011448c39239bb3eb4117c36a6d354794de4ce76"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:73d94b58ec7fecbc7366247d3b0b10a21681004153238750bb67bd9012414545"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dad3e487649f498dd991eeb901125411559b22e8d7ab25d3aeb1af367df5efd7"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c30197aa96e8eed02200a83fba2657b4c3acd0f0aa4bdc9f6c1af8e8962e0757"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2369eea1ee4a7610a860d88f268eb39b95cb588acd7235e02fd5a5601773d4fa"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bc2722592d8998c870fa4e290c2eec2c1569b87fe58618e67d38b4665dfa680d"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ffc9202a29ab3920fa812879e95a9e78b2465fd10be7fcbd042899695d75e616"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:804a4d582ba6e5b747c625bf1255e6b1507465494a40a2130978bda7b932c90b"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:0f55e69f030f7163dffe9fd0752b32f070566451afe180f99dbeeb81f511ad8d"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:c4c3e6da02df6fa1410a7680bd3f63d4f710232d3139089536310d027950696a"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:5df196eb874dae23dcfb968c83d4f8fdccb333330fe1fc278ac5ceeb101003a9"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:e358e64305fe12299a08e08978f51fc21fac060dcfcddd95453eabe5b93ed0e1"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-win32.whl", hash = "sha256:9b23ca7ef998bc739bf6ffc077c2116917eabcc901f88da1b9856b210ef63f35"},
    {file = "charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl", hash = "sha256:6ff8a4a60c227ad87030d76e99cd1698345d4491638dfa6673027c48b3cd395f"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:aabfa34badd18f1da5ec1bc2715cadc8dca465868a4e73a0173466b688f29dda"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:22e14b5d70560b8dd51ec22863f370d1e595ac3d024cb8ad7d308b4cd95f8313"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8436c508b408b82d87dc5f62496973a1805cd46727c34440b0d29d8a2f50a6c9"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d074908e1aecee37a7635990b2c6d504cd4766c7bc9fc86d63f9c09af3fa11b"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:955f8851919303c92343d2f66165294848d57e9bba6cf6e3625485a70a038d11"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:44ecbf16649486d4aebafeaa7ec4c9fed8b88101f4dd612dcaf65d5e815f837f"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0924e81d3d5e70f8126529951dac65c1010cdf117bb75eb02dd12339b57749dd"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:2967f74ad52c3b98de4c3b32e1a44e32975e008a9cd2a8cc8966d6a5218c5cb2"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c75cb2a3e389853835e84a2d8fb2b81a10645b503eca9bcb98df6b5a43eb8886"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:09b26ae6b1abf0d27570633b2b078a2a20419c99d66fb2823173d73f188ce601"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa88b843d6e211393a37219e6a1c1df99d35e8fd90446f1118f4216e307e48cd"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-win32.whl", hash = "sha256:eb8178fe3dba6450a3e024e95ac49ed3400e506fd4e9e5c32d30adda88cbd407"},
    {file = "charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:b1ac5992a838106edb89654e0aebfc24f5848ae2547d22c2c3f66454daa11971"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f30bf9fd9be89ecb2360c7d94a711f00c09b976258846efe40db3d05828e8089"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:97f68b8d6831127e4787ad15e6757232e14e12060bec17091b85eb1486b91d8d"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7974a0b5ecd505609e3b19742b60cee7aa2aa2fb3151bc917e6e2646d7667dcf"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fc54db6c8593ef7d4b2a331b58653356cf04f67c960f584edb7c3d8c97e8f39e"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:311f30128d7d333eebd7896965bfcfbd0065f1716ec92bd5638d7748eb6f936a"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-musllinux_1_2_aarch64.whl", hash = "sha256:7d053096f67cd1241601111b698f5cad775f97ab25d81567d3f59219b5f1adbd"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-musllinux_1_2_i686.whl", hash = "sha256:807f52c1f798eef6cf26beb819eeb8819b1622ddfeef9d0977a8502d4db6d534"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-musllinux_1_2_ppc64le.whl", hash = "sha256:dccbe65bd2f7f7ec22c4ff99ed56faa1e9f785482b9bbd7c717e26fd723a1d1e"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-musllinux_1_2_s390x.whl", hash = "sha256:2fb9bd477fdea8684f78791a6de97a953c51831ee2981f8e4f583ff3b9d9687e"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-musllinux_1_2_x86_64.whl", hash = "sha256:01732659ba9b5b873fc117534143e4feefecf3b2078b0a6a2e925271bb6f4cfa"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-win32.whl", hash = "sha256:7a4f97a081603d2050bfaffdefa5b02a9ec823f8348a572e39032caa8404a487"},
    {file = "charset_normalizer-3.4.1-cp37-cp37m-win_amd64.whl", hash = "sha256:7b1bef6280950ee6c177b326508f86cad7ad4dff12454483b51d8b7d673a2c5d"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:ecddf25bee22fe4fe3737a399d0d177d72bc22be6913acfab364b40bce1ba83c"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8c60ca7339acd497a55b0ea5d506b2a2612afb2826560416f6894e8b5770d4a9"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b7b2d86dd06bfc2ade3312a83a5c364c7ec2e3498f8734282c6c3d4b07b346b8"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:dd78cfcda14a1ef52584dbb008f7ac81c1328c0f58184bf9a84c49c605002da6"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6e27f48bcd0957c6d4cb9d6fa6b61d192d0b13d5ef563e5f2ae35feafc0d179c"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:01ad647cdd609225c5350561d084b42ddf732f4eeefe6e678765636791e78b9a"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:619a609aa74ae43d90ed2e89bdd784765de0a25ca761b93e196d938b8fd1dbbd"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:89149166622f4db9b4b6a449256291dc87a99ee53151c74cbd82a53c8c2f6ccd"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:7709f51f5f7c853f0fb938bcd3bc59cdfdc5203635ffd18bf354f6967ea0f824"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:345b0426edd4e18138d6528aed636de7a9ed169b4aaf9d61a8c19e39d26838ca"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:0907f11d019260cdc3f94fbdb23ff9125f6b5d1039b76003b5b0ac9d6a6c9d5b"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-win32.whl", hash = "sha256:ea0d8d539afa5eb2728aa1932a988a9a7af94f18582ffae4bc10b3fbdad0626e"},
    {file = "charset_normalizer-3.4.1-cp38-cp38-win_amd64.whl", hash = "sha256:329ce159e82018d646c7ac45b01a430369d526569ec08516081727a20e9e4af4"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:b97e690a2118911e39b4042088092771b4ae3fc3aa86518f84b8cf6888dbdb41"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:78baa6d91634dfb69ec52a463534bc0df05dbd546209b79a3880a34487f4b84f"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1a2bc9f351a75ef49d664206d51f8e5ede9da246602dc2d2726837620ea034b2"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:75832c08354f595c760a804588b9357d34ec00ba1c940c15e31e96d902093770"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0af291f4fe114be0280cdd29d533696a77b5b49cfde5467176ecab32353395c4"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0167ddc8ab6508fe81860a57dd472b2ef4060e8d378f0cc555707126830f2537"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:2a75d49014d118e4198bcee5ee0a6f25856b29b12dbf7cd012791f8a6cc5c496"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:363e2f92b0f0174b2f8238240a1a30142e3db7b957a5dd5689b0e75fb717cc78"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:ab36c8eb7e454e34e60eb55ca5d241a5d18b2c6244f6827a30e451c42410b5f7"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:4c0907b1928a36d5a998d72d64d8eaa7244989f7aaaf947500d3a800c83a3fd6"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:04432ad9479fa40ec0f387795ddad4437a2b50417c69fa275e212933519ff294"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-win32.whl", hash = "sha256:3bed14e9c89dcb10e8f3a29f9ccac4955aebe93c71ae803af79265c9ca5644c5"},
    {file = "charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl", hash = "sha256:49402233c892a461407c512a19435d1ce275543138294f7ef013f0b63d5d3765"},
    {file = "charset_normalizer-3.4.1-py3-none-any.whl", hash = "sha256:d98b1668f06378c6dbefec3b92299716b931cd4e6061f3c875a71ced1780ab85"},
    {file = "charset_normalizer-3.4.1.tar.gz", hash = "sha256:44251f18cd68a75b56585dd00dae26183e102cd5e0f9f1466e6df5da2ed64ea3"},
]

[[package]]
name = "colorama"
version = "0.4.6"
description = "Cross-platform colored terminal text."
optional = false
python-versions = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
files = [
    {file = "colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6"},
    {file = "colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44"},
]

[[package]]
name = "defusedxml"
version = "0.7.1"
description = "XML bomb protection for Python stdlib modules"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*"
files = [
    {file = "defusedxml-0.7.1-py2.py3-none-any.whl", hash = "sha256:a352e7e428770286cc899e2542b6cdaedb2b4953ff269a210103ec58f6198a61"},
    {file = "defusedxml-0.7.1.tar.gz", hash = "sha256:1bb3032db185915b62d7c6209c5a8792be6a32ab2fedacc84e01b52c51aa3e69"},
]

[[package]]
name = "distro"
version = "1.9.0"
description = "Distro - an OS platform information API"
optional = false
python-versions = ">=3.6"
files = [
    {file = "distro-1.9.0-py3-none-any.whl", hash = "sha256:7bffd925d65168f85027d8da9af6bddab658135b840670a223589bc0c8ef02b2"},
    {file = "distro-1.9.0.tar.gz", hash = "sha256:2fa77c6fd8940f116ee1d6b94a2f90b13b5ea8d019b98bc8bafdcabcdd9bdbed"},
]

[[package]]
name = "dydantic"
version = "0.0.7"
description = "Dynamically generate pydantic models from JSON schema."
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "dydantic-0.0.7-py3-none-any.whl", hash = "sha256:5531220d876b77451bb045155f83534c69d13bf76d4d628dcaa920d933a7ae89"},
    {file = "dydantic-0.0.7.tar.gz", hash = "sha256:6430f038fd32b72782722ef1ddf39837ba251fbf7a183ced3e1597fac4e4862e"},
]

[package.dependencies]
pydantic = ">=2,<3"

[package.extras]
email = ["email-validator (>=2.1,<3.0)"]

[[package]]
name = "frozenlist"
version = "1.5.0"
description = "A list-like structure which implements collections.abc.MutableSequence"
optional = false
python-versions = ">=3.8"
files = [
    {file = "frozenlist-1.5.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:5b6a66c18b5b9dd261ca98dffcb826a525334b2f29e7caa54e182255c5f6a65a"},
    {file = "frozenlist-1.5.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:d1b3eb7b05ea246510b43a7e53ed1653e55c2121019a97e60cad7efb881a97bb"},
    {file = "frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:15538c0cbf0e4fa11d1e3a71f823524b0c46299aed6e10ebb4c2089abd8c3bec"},
    {file = "frozenlist-1.5.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e79225373c317ff1e35f210dd5f1344ff31066ba8067c307ab60254cd3a78ad5"},
    {file = "frozenlist-1.5.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:9272fa73ca71266702c4c3e2d4a28553ea03418e591e377a03b8e3659d94fa76"},
    {file = "frozenlist-1.5.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:498524025a5b8ba81695761d78c8dd7382ac0b052f34e66939c42df860b8ff17"},
    {file = "frozenlist-1.5.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:92b5278ed9d50fe610185ecd23c55d8b307d75ca18e94c0e7de328089ac5dcba"},
    {file = "frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7f3c8c1dacd037df16e85227bac13cca58c30da836c6f936ba1df0c05d046d8d"},
    {file = "frozenlist-1.5.0-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:f2ac49a9bedb996086057b75bf93538240538c6d9b38e57c82d51f75a73409d2"},
    {file = "frozenlist-1.5.0-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:e66cc454f97053b79c2ab09c17fbe3c825ea6b4de20baf1be28919460dd7877f"},
    {file = "frozenlist-1.5.0-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:5a3ba5f9a0dfed20337d3e966dc359784c9f96503674c2faf015f7fe8e96798c"},
    {file = "frozenlist-1.5.0-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:6321899477db90bdeb9299ac3627a6a53c7399c8cd58d25da094007402b039ab"},
    {file = "frozenlist-1.5.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:76e4753701248476e6286f2ef492af900ea67d9706a0155335a40ea21bf3b2f5"},
    {file = "frozenlist-1.5.0-cp310-cp310-win32.whl", hash = "sha256:977701c081c0241d0955c9586ffdd9ce44f7a7795df39b9151cd9a6fd0ce4cfb"},
    {file = "frozenlist-1.5.0-cp310-cp310-win_amd64.whl", hash = "sha256:189f03b53e64144f90990d29a27ec4f7997d91ed3d01b51fa39d2dbe77540fd4"},
    {file = "frozenlist-1.5.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:fd74520371c3c4175142d02a976aee0b4cb4a7cc912a60586ffd8d5929979b30"},
    {file = "frozenlist-1.5.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:2f3f7a0fbc219fb4455264cae4d9f01ad41ae6ee8524500f381de64ffaa077d5"},
    {file = "frozenlist-1.5.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:f47c9c9028f55a04ac254346e92977bf0f166c483c74b4232bee19a6697e4778"},
    {file = "frozenlist-1.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0996c66760924da6e88922756d99b47512a71cfd45215f3570bf1e0b694c206a"},
    {file = "frozenlist-1.5.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a2fe128eb4edeabe11896cb6af88fca5346059f6c8d807e3b910069f39157869"},
    {file = "frozenlist-1.5.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1a8ea951bbb6cacd492e3948b8da8c502a3f814f5d20935aae74b5df2b19cf3d"},
    {file = "frozenlist-1.5.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:de537c11e4aa01d37db0d403b57bd6f0546e71a82347a97c6a9f0dcc532b3a45"},
    {file = "frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9c2623347b933fcb9095841f1cc5d4ff0b278addd743e0e966cb3d460278840d"},
    {file = "frozenlist-1.5.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:cee6798eaf8b1416ef6909b06f7dc04b60755206bddc599f52232606e18179d3"},
    {file = "frozenlist-1.5.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:f5f9da7f5dbc00a604fe74aa02ae7c98bcede8a3b8b9666f9f86fc13993bc71a"},
    {file = "frozenlist-1.5.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:90646abbc7a5d5c7c19461d2e3eeb76eb0b204919e6ece342feb6032c9325ae9"},
    {file = "frozenlist-1.5.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:bdac3c7d9b705d253b2ce370fde941836a5f8b3c5c2b8fd70940a3ea3af7f4f2"},
    {file = "frozenlist-1.5.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:03d33c2ddbc1816237a67f66336616416e2bbb6beb306e5f890f2eb22b959cdf"},
    {file = "frozenlist-1.5.0-cp311-cp311-win32.whl", hash = "sha256:237f6b23ee0f44066219dae14c70ae38a63f0440ce6750f868ee08775073f942"},
    {file = "frozenlist-1.5.0-cp311-cp311-win_amd64.whl", hash = "sha256:0cc974cc93d32c42e7b0f6cf242a6bd941c57c61b618e78b6c0a96cb72788c1d"},
    {file = "frozenlist-1.5.0-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:31115ba75889723431aa9a4e77d5f398f5cf976eea3bdf61749731f62d4a4a21"},
    {file = "frozenlist-1.5.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7437601c4d89d070eac8323f121fcf25f88674627505334654fd027b091db09d"},
    {file = "frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:7948140d9f8ece1745be806f2bfdf390127cf1a763b925c4a805c603df5e697e"},
    {file = "frozenlist-1.5.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:feeb64bc9bcc6b45c6311c9e9b99406660a9c05ca8a5b30d14a78555088b0b3a"},
    {file = "frozenlist-1.5.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:683173d371daad49cffb8309779e886e59c2f369430ad28fe715f66d08d4ab1a"},
    {file = "frozenlist-1.5.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7d57d8f702221405a9d9b40f9da8ac2e4a1a8b5285aac6100f3393675f0a85ee"},
    {file = "frozenlist-1.5.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:30c72000fbcc35b129cb09956836c7d7abf78ab5416595e4857d1cae8d6251a6"},
    {file = "frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:000a77d6034fbad9b6bb880f7ec073027908f1b40254b5d6f26210d2dab1240e"},
    {file = "frozenlist-1.5.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:5d7f5a50342475962eb18b740f3beecc685a15b52c91f7d975257e13e029eca9"},
    {file = "frozenlist-1.5.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:87f724d055eb4785d9be84e9ebf0f24e392ddfad00b3fe036e43f489fafc9039"},
    {file = "frozenlist-1.5.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:6e9080bb2fb195a046e5177f10d9d82b8a204c0736a97a153c2466127de87784"},
    {file = "frozenlist-1.5.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9b93d7aaa36c966fa42efcaf716e6b3900438632a626fb09c049f6a2f09fc631"},
    {file = "frozenlist-1.5.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:52ef692a4bc60a6dd57f507429636c2af8b6046db8b31b18dac02cbc8f507f7f"},
    {file = "frozenlist-1.5.0-cp312-cp312-win32.whl", hash = "sha256:29d94c256679247b33a3dc96cce0f93cbc69c23bf75ff715919332fdbb6a32b8"},
    {file = "frozenlist-1.5.0-cp312-cp312-win_amd64.whl", hash = "sha256:8969190d709e7c48ea386db202d708eb94bdb29207a1f269bab1196ce0dcca1f"},
    {file = "frozenlist-1.5.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:7a1a048f9215c90973402e26c01d1cff8a209e1f1b53f72b95c13db61b00f953"},
    {file = "frozenlist-1.5.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:dd47a5181ce5fcb463b5d9e17ecfdb02b678cca31280639255ce9d0e5aa67af0"},
    {file = "frozenlist-1.5.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1431d60b36d15cda188ea222033eec8e0eab488f39a272461f2e6d9e1a8e63c2"},
    {file = "frozenlist-1.5.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6482a5851f5d72767fbd0e507e80737f9c8646ae7fd303def99bfe813f76cf7f"},
    {file = "frozenlist-1.5.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:44c49271a937625619e862baacbd037a7ef86dd1ee215afc298a417ff3270608"},
    {file = "frozenlist-1.5.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:12f78f98c2f1c2429d42e6a485f433722b0061d5c0b0139efa64f396efb5886b"},
    {file = "frozenlist-1.5.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ce3aa154c452d2467487765e3adc730a8c153af77ad84096bc19ce19a2400840"},
    {file = "frozenlist-1.5.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9b7dc0c4338e6b8b091e8faf0db3168a37101943e687f373dce00959583f7439"},
    {file = "frozenlist-1.5.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:45e0896250900b5aa25180f9aec243e84e92ac84bd4a74d9ad4138ef3f5c97de"},
    {file = "frozenlist-1.5.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:561eb1c9579d495fddb6da8959fd2a1fca2c6d060d4113f5844b433fc02f2641"},
    {file = "frozenlist-1.5.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:df6e2f325bfee1f49f81aaac97d2aa757c7646534a06f8f577ce184afe2f0a9e"},
    {file = "frozenlist-1.5.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:140228863501b44b809fb39ec56b5d4071f4d0aa6d216c19cbb08b8c5a7eadb9"},
    {file = "frozenlist-1.5.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:7707a25d6a77f5d27ea7dc7d1fc608aa0a478193823f88511ef5e6b8a48f9d03"},
    {file = "frozenlist-1.5.0-cp313-cp313-win32.whl", hash = "sha256:31a9ac2b38ab9b5a8933b693db4939764ad3f299fcaa931a3e605bc3460e693c"},
    {file = "frozenlist-1.5.0-cp313-cp313-win_amd64.whl", hash = "sha256:11aabdd62b8b9c4b84081a3c246506d1cddd2dd93ff0ad53ede5defec7886b28"},
    {file = "frozenlist-1.5.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:dd94994fc91a6177bfaafd7d9fd951bc8689b0a98168aa26b5f543868548d3ca"},
    {file = "frozenlist-1.5.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:2d0da8bbec082bf6bf18345b180958775363588678f64998c2b7609e34719b10"},
    {file = "frozenlist-1.5.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:73f2e31ea8dd7df61a359b731716018c2be196e5bb3b74ddba107f694fbd7604"},
    {file = "frozenlist-1.5.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:828afae9f17e6de596825cf4228ff28fbdf6065974e5ac1410cecc22f699d2b3"},
    {file = "frozenlist-1.5.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f1577515d35ed5649d52ab4319db757bb881ce3b2b796d7283e6634d99ace307"},
    {file = "frozenlist-1.5.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2150cc6305a2c2ab33299453e2968611dacb970d2283a14955923062c8d00b10"},
    {file = "frozenlist-1.5.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a72b7a6e3cd2725eff67cd64c8f13335ee18fc3c7befc05aed043d24c7b9ccb9"},
    {file = "frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:c16d2fa63e0800723139137d667e1056bee1a1cf7965153d2d104b62855e9b99"},
    {file = "frozenlist-1.5.0-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:17dcc32fc7bda7ce5875435003220a457bcfa34ab7924a49a1c19f55b6ee185c"},
    {file = "frozenlist-1.5.0-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:97160e245ea33d8609cd2b8fd997c850b56db147a304a262abc2b3be021a9171"},
    {file = "frozenlist-1.5.0-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:f1e6540b7fa044eee0bb5111ada694cf3dc15f2b0347ca125ee9ca984d5e9e6e"},
    {file = "frozenlist-1.5.0-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:91d6c171862df0a6c61479d9724f22efb6109111017c87567cfeb7b5d1449fdf"},
    {file = "frozenlist-1.5.0-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:c1fac3e2ace2eb1052e9f7c7db480818371134410e1f5c55d65e8f3ac6d1407e"},
    {file = "frozenlist-1.5.0-cp38-cp38-win32.whl", hash = "sha256:b97f7b575ab4a8af9b7bc1d2ef7f29d3afee2226bd03ca3875c16451ad5a7723"},
    {file = "frozenlist-1.5.0-cp38-cp38-win_amd64.whl", hash = "sha256:374ca2dabdccad8e2a76d40b1d037f5bd16824933bf7bcea3e59c891fd4a0923"},
    {file = "frozenlist-1.5.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:9bbcdfaf4af7ce002694a4e10a0159d5a8d20056a12b05b45cea944a4953f972"},
    {file = "frozenlist-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:1893f948bf6681733aaccf36c5232c231e3b5166d607c5fa77773611df6dc336"},
    {file = "frozenlist-1.5.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:2b5e23253bb709ef57a8e95e6ae48daa9ac5f265637529e4ce6b003a37b2621f"},
    {file = "frozenlist-1.5.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0f253985bb515ecd89629db13cb58d702035ecd8cfbca7d7a7e29a0e6d39af5f"},
    {file = "frozenlist-1.5.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:04a5c6babd5e8fb7d3c871dc8b321166b80e41b637c31a995ed844a6139942b6"},
    {file = "frozenlist-1.5.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a9fe0f1c29ba24ba6ff6abf688cb0b7cf1efab6b6aa6adc55441773c252f7411"},
    {file = "frozenlist-1.5.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:226d72559fa19babe2ccd920273e767c96a49b9d3d38badd7c91a0fdeda8ea08"},
    {file = "frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:15b731db116ab3aedec558573c1a5eec78822b32292fe4f2f0345b7f697745c2"},
    {file = "frozenlist-1.5.0-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:366d8f93e3edfe5a918c874702f78faac300209a4d5bf38352b2c1bdc07a766d"},
    {file = "frozenlist-1.5.0-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:1b96af8c582b94d381a1c1f51ffaedeb77c821c690ea5f01da3d70a487dd0a9b"},
    {file = "frozenlist-1.5.0-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:c03eff4a41bd4e38415cbed054bbaff4a075b093e2394b6915dca34a40d1e38b"},
    {file = "frozenlist-1.5.0-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:50cf5e7ee9b98f22bdecbabf3800ae78ddcc26e4a435515fc72d97903e8488e0"},
    {file = "frozenlist-1.5.0-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:1e76bfbc72353269c44e0bc2cfe171900fbf7f722ad74c9a7b638052afe6a00c"},
    {file = "frozenlist-1.5.0-cp39-cp39-win32.whl", hash = "sha256:666534d15ba8f0fda3f53969117383d5dc021266b3c1a42c9ec4855e4b58b9d3"},
    {file = "frozenlist-1.5.0-cp39-cp39-win_amd64.whl", hash = "sha256:5c28f4b5dbef8a0d8aad0d4de24d1e9e981728628afaf4ea0792f5d0939372f0"},
    {file = "frozenlist-1.5.0-py3-none-any.whl", hash = "sha256:d994863bba198a4a518b467bb971c56e1db3f180a25c6cf7bb1949c267f748c3"},
    {file = "frozenlist-1.5.0.tar.gz", hash = "sha256:81d5af29e61b9c8348e876d442253723928dce6433e0e76cd925cd83f1b4b817"},
]

[[package]]
name = "greenlet"
version = "3.1.1"
description = "Lightweight in-process concurrent programming"
optional = false
python-versions = ">=3.7"
files = [
    {file = "greenlet-3.1.1-cp310-cp310-macosx_11_0_universal2.whl", hash = "sha256:0bbae94a29c9e5c7e4a2b7f0aae5c17e8e90acbfd3bf6270eeba60c39fce3563"},
    {file = "greenlet-3.1.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0fde093fb93f35ca72a556cf72c92ea3ebfda3d79fc35bb19fbe685853869a83"},
    {file = "greenlet-3.1.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:36b89d13c49216cadb828db8dfa6ce86bbbc476a82d3a6c397f0efae0525bdd0"},
    {file = "greenlet-3.1.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:94b6150a85e1b33b40b1464a3f9988dcc5251d6ed06842abff82e42632fac120"},
    {file = "greenlet-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:93147c513fac16385d1036b7e5b102c7fbbdb163d556b791f0f11eada7ba65dc"},
    {file = "greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:da7a9bff22ce038e19bf62c4dd1ec8391062878710ded0a845bcf47cc0200617"},
    {file = "greenlet-3.1.1-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:b2795058c23988728eec1f36a4e5e4ebad22f8320c85f3587b539b9ac84128d7"},
    {file = "greenlet-3.1.1-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:ed10eac5830befbdd0c32f83e8aa6288361597550ba669b04c48f0f9a2c843c6"},
    {file = "greenlet-3.1.1-cp310-cp310-win_amd64.whl", hash = "sha256:77c386de38a60d1dfb8e55b8c1101d68c79dfdd25c7095d51fec2dd800892b80"},
    {file = "greenlet-3.1.1-cp311-cp311-macosx_11_0_universal2.whl", hash = "sha256:e4d333e558953648ca09d64f13e6d8f0523fa705f51cae3f03b5983489958c70"},
    {file = "greenlet-3.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:09fc016b73c94e98e29af67ab7b9a879c307c6731a2c9da0db5a7d9b7edd1159"},
    {file = "greenlet-3.1.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d5e975ca70269d66d17dd995dafc06f1b06e8cb1ec1e9ed54c1d1e4a7c4cf26e"},
    {file = "greenlet-3.1.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3b2813dc3de8c1ee3f924e4d4227999285fd335d1bcc0d2be6dc3f1f6a318ec1"},
    {file = "greenlet-3.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e347b3bfcf985a05e8c0b7d462ba6f15b1ee1c909e2dcad795e49e91b152c383"},
    {file = "greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9e8f8c9cb53cdac7ba9793c276acd90168f416b9ce36799b9b885790f8ad6c0a"},
    {file = "greenlet-3.1.1-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:62ee94988d6b4722ce0028644418d93a52429e977d742ca2ccbe1c4f4a792511"},
    {file = "greenlet-3.1.1-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:1776fd7f989fc6b8d8c8cb8da1f6b82c5814957264d1f6cf818d475ec2bf6395"},
    {file = "greenlet-3.1.1-cp311-cp311-win_amd64.whl", hash = "sha256:48ca08c771c268a768087b408658e216133aecd835c0ded47ce955381105ba39"},
    {file = "greenlet-3.1.1-cp312-cp312-macosx_11_0_universal2.whl", hash = "sha256:4afe7ea89de619adc868e087b4d2359282058479d7cfb94970adf4b55284574d"},
    {file = "greenlet-3.1.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f406b22b7c9a9b4f8aa9d2ab13d6ae0ac3e85c9a809bd590ad53fed2bf70dc79"},
    {file = "greenlet-3.1.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c3a701fe5a9695b238503ce5bbe8218e03c3bcccf7e204e455e7462d770268aa"},
    {file = "greenlet-3.1.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2846930c65b47d70b9d178e89c7e1a69c95c1f68ea5aa0a58646b7a96df12441"},
    {file = "greenlet-3.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:99cfaa2110534e2cf3ba31a7abcac9d328d1d9f1b95beede58294a60348fba36"},
    {file = "greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:1443279c19fca463fc33e65ef2a935a5b09bb90f978beab37729e1c3c6c25fe9"},
    {file = "greenlet-3.1.1-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:b7cede291382a78f7bb5f04a529cb18e068dd29e0fb27376074b6d0317bf4dd0"},
    {file = "greenlet-3.1.1-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:23f20bb60ae298d7d8656c6ec6db134bca379ecefadb0b19ce6f19d1f232a942"},
    {file = "greenlet-3.1.1-cp312-cp312-win_amd64.whl", hash = "sha256:7124e16b4c55d417577c2077be379514321916d5790fa287c9ed6f23bd2ffd01"},
    {file = "greenlet-3.1.1-cp313-cp313-macosx_11_0_universal2.whl", hash = "sha256:05175c27cb459dcfc05d026c4232f9de8913ed006d42713cb8a5137bd49375f1"},
    {file = "greenlet-3.1.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:935e943ec47c4afab8965954bf49bfa639c05d4ccf9ef6e924188f762145c0ff"},
    {file = "greenlet-3.1.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:667a9706c970cb552ede35aee17339a18e8f2a87a51fba2ed39ceeeb1004798a"},
    {file = "greenlet-3.1.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:b8a678974d1f3aa55f6cc34dc480169d58f2e6d8958895d68845fa4ab566509e"},
    {file = "greenlet-3.1.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:efc0f674aa41b92da8c49e0346318c6075d734994c3c4e4430b1c3f853e498e4"},
    {file = "greenlet-3.1.1-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0153404a4bb921f0ff1abeb5ce8a5131da56b953eda6e14b88dc6bbc04d2049e"},
    {file = "greenlet-3.1.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:275f72decf9932639c1c6dd1013a1bc266438eb32710016a1c742df5da6e60a1"},
    {file = "greenlet-3.1.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:c4aab7f6381f38a4b42f269057aee279ab0fc7bf2e929e3d4abfae97b682a12c"},
    {file = "greenlet-3.1.1-cp313-cp313-win_amd64.whl", hash = "sha256:b42703b1cf69f2aa1df7d1030b9d77d3e584a70755674d60e710f0af570f3761"},
    {file = "greenlet-3.1.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f1695e76146579f8c06c1509c7ce4dfe0706f49c6831a817ac04eebb2fd02011"},
    {file = "greenlet-3.1.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:7876452af029456b3f3549b696bb36a06db7c90747740c5302f74a9e9fa14b13"},
    {file = "greenlet-3.1.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4ead44c85f8ab905852d3de8d86f6f8baf77109f9da589cb4fa142bd3b57b475"},
    {file = "greenlet-3.1.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8320f64b777d00dd7ccdade271eaf0cad6636343293a25074cc5566160e4de7b"},
    {file = "greenlet-3.1.1-cp313-cp313t-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:6510bf84a6b643dabba74d3049ead221257603a253d0a9873f55f6a59a65f822"},
    {file = "greenlet-3.1.1-cp313-cp313t-musllinux_1_1_aarch64.whl", hash = "sha256:04b013dc07c96f83134b1e99888e7a79979f1a247e2a9f59697fa14b5862ed01"},
    {file = "greenlet-3.1.1-cp313-cp313t-musllinux_1_1_x86_64.whl", hash = "sha256:411f015496fec93c1c8cd4e5238da364e1da7a124bcb293f085bf2860c32c6f6"},
    {file = "greenlet-3.1.1-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:47da355d8687fd65240c364c90a31569a133b7b60de111c255ef5b606f2ae291"},
    {file = "greenlet-3.1.1-cp37-cp37m-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:98884ecf2ffb7d7fe6bd517e8eb99d31ff7855a840fa6d0d63cd07c037f6a981"},
    {file = "greenlet-3.1.1-cp37-cp37m-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f1d4aeb8891338e60d1ab6127af1fe45def5259def8094b9c7e34690c8858803"},
    {file = "greenlet-3.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:db32b5348615a04b82240cc67983cb315309e88d444a288934ee6ceaebcad6cc"},
    {file = "greenlet-3.1.1-cp37-cp37m-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:dcc62f31eae24de7f8dce72134c8651c58000d3b1868e01392baea7c32c247de"},
    {file = "greenlet-3.1.1-cp37-cp37m-musllinux_1_1_aarch64.whl", hash = "sha256:1d3755bcb2e02de341c55b4fca7a745a24a9e7212ac953f6b3a48d117d7257aa"},
    {file = "greenlet-3.1.1-cp37-cp37m-musllinux_1_1_x86_64.whl", hash = "sha256:b8da394b34370874b4572676f36acabac172602abf054cbc4ac910219f3340af"},
    {file = "greenlet-3.1.1-cp37-cp37m-win32.whl", hash = "sha256:a0dfc6c143b519113354e780a50381508139b07d2177cb6ad6a08278ec655798"},
    {file = "greenlet-3.1.1-cp37-cp37m-win_amd64.whl", hash = "sha256:54558ea205654b50c438029505def3834e80f0869a70fb15b871c29b4575ddef"},
    {file = "greenlet-3.1.1-cp38-cp38-macosx_11_0_universal2.whl", hash = "sha256:346bed03fe47414091be4ad44786d1bd8bef0c3fcad6ed3dee074a032ab408a9"},
    {file = "greenlet-3.1.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dfc59d69fc48664bc693842bd57acfdd490acafda1ab52c7836e3fc75c90a111"},
    {file = "greenlet-3.1.1-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d21e10da6ec19b457b82636209cbe2331ff4306b54d06fa04b7c138ba18c8a81"},
    {file = "greenlet-3.1.1-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:37b9de5a96111fc15418819ab4c4432e4f3c2ede61e660b1e33971eba26ef9ba"},
    {file = "greenlet-3.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6ef9ea3f137e5711f0dbe5f9263e8c009b7069d8a1acea822bd5e9dae0ae49c8"},
    {file = "greenlet-3.1.1-cp38-cp38-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:85f3ff71e2e60bd4b4932a043fbbe0f499e263c628390b285cb599154a3b03b1"},
    {file = "greenlet-3.1.1-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:95ffcf719966dd7c453f908e208e14cde192e09fde6c7186c8f1896ef778d8cd"},
    {file = "greenlet-3.1.1-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:03a088b9de532cbfe2ba2034b2b85e82df37874681e8c470d6fb2f8c04d7e4b7"},
    {file = "greenlet-3.1.1-cp38-cp38-win32.whl", hash = "sha256:8b8b36671f10ba80e159378df9c4f15c14098c4fd73a36b9ad715f057272fbef"},
    {file = "greenlet-3.1.1-cp38-cp38-win_amd64.whl", hash = "sha256:7017b2be767b9d43cc31416aba48aab0d2309ee31b4dbf10a1d38fb7972bdf9d"},
    {file = "greenlet-3.1.1-cp39-cp39-macosx_11_0_universal2.whl", hash = "sha256:396979749bd95f018296af156201d6211240e7a23090f50a8d5d18c370084dc3"},
    {file = "greenlet-3.1.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ca9d0ff5ad43e785350894d97e13633a66e2b50000e8a183a50a88d834752d42"},
    {file = "greenlet-3.1.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f6ff3b14f2df4c41660a7dec01045a045653998784bf8cfcb5a525bdffffbc8f"},
    {file = "greenlet-3.1.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:94ebba31df2aa506d7b14866fed00ac141a867e63143fe5bca82a8e503b36437"},
    {file = "greenlet-3.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:73aaad12ac0ff500f62cebed98d8789198ea0e6f233421059fa68a5aa7220145"},
    {file = "greenlet-3.1.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:63e4844797b975b9af3a3fb8f7866ff08775f5426925e1e0bbcfe7932059a12c"},
    {file = "greenlet-3.1.1-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:7939aa3ca7d2a1593596e7ac6d59391ff30281ef280d8632fa03d81f7c5f955e"},
    {file = "greenlet-3.1.1-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:d0028e725ee18175c6e422797c407874da24381ce0690d6b9396c204c7f7276e"},
    {file = "greenlet-3.1.1-cp39-cp39-win32.whl", hash = "sha256:5e06afd14cbaf9e00899fae69b24a32f2196c19de08fcb9f4779dd4f004e5e7c"},
    {file = "greenlet-3.1.1-cp39-cp39-win_amd64.whl", hash = "sha256:3319aa75e0e0639bc15ff54ca327e8dc7a6fe404003496e3c6925cd3142e0e22"},
    {file = "greenlet-3.1.1.tar.gz", hash = "sha256:4ce3ac6cdb6adf7946475d7ef31777c26d94bccc377e070a7986bd2d5c515467"},
]

[package.extras]
docs = ["Sphinx", "furo"]
test = ["objgraph", "psutil"]

[[package]]
name = "h11"
version = "0.14.0"
description = "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1"
optional = false
python-versions = ">=3.7"
files = [
    {file = "h11-0.14.0-py3-none-any.whl", hash = "sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761"},
    {file = "h11-0.14.0.tar.gz", hash = "sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d"},
]

[[package]]
name = "httpcore"
version = "1.0.7"
description = "A minimal low-level HTTP client."
optional = false
python-versions = ">=3.8"
files = [
    {file = "httpcore-1.0.7-py3-none-any.whl", hash = "sha256:a3fff8f43dc260d5bd363d9f9cf1830fa3a458b332856f34282de498ed420edd"},
    {file = "httpcore-1.0.7.tar.gz", hash = "sha256:8551cb62a169ec7162ac7be8d4817d561f60e08eaa485234898414bb5a8a0b4c"},
]

[package.dependencies]
certifi = "*"
h11 = ">=0.13,<0.15"

[package.extras]
asyncio = ["anyio (>=4.0,<5.0)"]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (==1.*)"]
trio = ["trio (>=0.22.0,<1.0)"]

[[package]]
name = "httpx"
version = "0.28.1"
description = "The next generation HTTP client."
optional = false
python-versions = ">=3.8"
files = [
    {file = "httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad"},
    {file = "httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc"},
]

[package.dependencies]
anyio = "*"
certifi = "*"
httpcore = "==1.*"
idna = "*"

[package.extras]
brotli = ["brotli", "brotlicffi"]
cli = ["click (==8.*)", "pygments (==2.*)", "rich (>=10,<14)"]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (==1.*)"]
zstd = ["zstandard (>=0.18.0)"]

[[package]]
name = "idna"
version = "3.10"
description = "Internationalized Domain Names in Applications (IDNA)"
optional = false
python-versions = ">=3.6"
files = [
    {file = "idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3"},
    {file = "idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9"},
]

[package.extras]
all = ["flake8 (>=7.1.1)", "mypy (>=1.11.2)", "pytest (>=8.3.2)", "ruff (>=0.6.2)"]

[[package]]
name = "jiter"
version = "0.8.2"
description = "Fast iterable JSON parser."
optional = false
python-versions = ">=3.8"
files = [
    {file = "jiter-0.8.2-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:ca8577f6a413abe29b079bc30f907894d7eb07a865c4df69475e868d73e71c7b"},
    {file = "jiter-0.8.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:b25bd626bde7fb51534190c7e3cb97cee89ee76b76d7585580e22f34f5e3f393"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d5c826a221851a8dc028eb6d7d6429ba03184fa3c7e83ae01cd6d3bd1d4bd17d"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d35c864c2dff13dfd79fb070fc4fc6235d7b9b359efe340e1261deb21b9fcb66"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f557c55bc2b7676e74d39d19bcb8775ca295c7a028246175d6a8b431e70835e5"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:580ccf358539153db147e40751a0b41688a5ceb275e6f3e93d91c9467f42b2e3"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:af102d3372e917cffce49b521e4c32c497515119dc7bd8a75665e90a718bbf08"},
    {file = "jiter-0.8.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:cadcc978f82397d515bb2683fc0d50103acff2a180552654bb92d6045dec2c49"},
    {file = "jiter-0.8.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:ba5bdf56969cad2019d4e8ffd3f879b5fdc792624129741d3d83fc832fef8c7d"},
    {file = "jiter-0.8.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:3b94a33a241bee9e34b8481cdcaa3d5c2116f575e0226e421bed3f7a6ea71cff"},
    {file = "jiter-0.8.2-cp310-cp310-win32.whl", hash = "sha256:6e5337bf454abddd91bd048ce0dca5134056fc99ca0205258766db35d0a2ea43"},
    {file = "jiter-0.8.2-cp310-cp310-win_amd64.whl", hash = "sha256:4a9220497ca0cb1fe94e3f334f65b9b5102a0b8147646118f020d8ce1de70105"},
    {file = "jiter-0.8.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:2dd61c5afc88a4fda7d8b2cf03ae5947c6ac7516d32b7a15bf4b49569a5c076b"},
    {file = "jiter-0.8.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:a6c710d657c8d1d2adbbb5c0b0c6bfcec28fd35bd6b5f016395f9ac43e878a15"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a9584de0cd306072635fe4b89742bf26feae858a0683b399ad0c2509011b9dc0"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5a90a923338531b7970abb063cfc087eebae6ef8ec8139762007188f6bc69a9f"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d21974d246ed0181558087cd9f76e84e8321091ebfb3a93d4c341479a736f099"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:32475a42b2ea7b344069dc1e81445cfc00b9d0e3ca837f0523072432332e9f74"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8b9931fd36ee513c26b5bf08c940b0ac875de175341cbdd4fa3be109f0492586"},
    {file = "jiter-0.8.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:ce0820f4a3a59ddced7fce696d86a096d5cc48d32a4183483a17671a61edfddc"},
    {file = "jiter-0.8.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:8ffc86ae5e3e6a93765d49d1ab47b6075a9c978a2b3b80f0f32628f39caa0c88"},
    {file = "jiter-0.8.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:5127dc1abd809431172bc3fbe8168d6b90556a30bb10acd5ded41c3cfd6f43b6"},
    {file = "jiter-0.8.2-cp311-cp311-win32.whl", hash = "sha256:66227a2c7b575720c1871c8800d3a0122bb8ee94edb43a5685aa9aceb2782d44"},
    {file = "jiter-0.8.2-cp311-cp311-win_amd64.whl", hash = "sha256:cde031d8413842a1e7501e9129b8e676e62a657f8ec8166e18a70d94d4682855"},
    {file = "jiter-0.8.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:e6ec2be506e7d6f9527dae9ff4b7f54e68ea44a0ef6b098256ddf895218a2f8f"},
    {file = "jiter-0.8.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:76e324da7b5da060287c54f2fabd3db5f76468006c811831f051942bf68c9d44"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:180a8aea058f7535d1c84183c0362c710f4750bef66630c05f40c93c2b152a0f"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:025337859077b41548bdcbabe38698bcd93cfe10b06ff66617a48ff92c9aec60"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ecff0dc14f409599bbcafa7e470c00b80f17abc14d1405d38ab02e4b42e55b57"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ffd9fee7d0775ebaba131f7ca2e2d83839a62ad65e8e02fe2bd8fc975cedeb9e"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:14601dcac4889e0a1c75ccf6a0e4baf70dbc75041e51bcf8d0e9274519df6887"},
    {file = "jiter-0.8.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:92249669925bc1c54fcd2ec73f70f2c1d6a817928480ee1c65af5f6b81cdf12d"},
    {file = "jiter-0.8.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:e725edd0929fa79f8349ab4ec7f81c714df51dc4e991539a578e5018fa4a7152"},
    {file = "jiter-0.8.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:bf55846c7b7a680eebaf9c3c48d630e1bf51bdf76c68a5f654b8524335b0ad29"},
    {file = "jiter-0.8.2-cp312-cp312-win32.whl", hash = "sha256:7efe4853ecd3d6110301665a5178b9856be7e2a9485f49d91aa4d737ad2ae49e"},
    {file = "jiter-0.8.2-cp312-cp312-win_amd64.whl", hash = "sha256:83c0efd80b29695058d0fd2fa8a556490dbce9804eac3e281f373bbc99045f6c"},
    {file = "jiter-0.8.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:ca1f08b8e43dc3bd0594c992fb1fd2f7ce87f7bf0d44358198d6da8034afdf84"},
    {file = "jiter-0.8.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:5672a86d55416ccd214c778efccf3266b84f87b89063b582167d803246354be4"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:58dc9bc9767a1101f4e5e22db1b652161a225874d66f0e5cb8e2c7d1c438b587"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:37b2998606d6dadbb5ccda959a33d6a5e853252d921fec1792fc902351bb4e2c"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4ab9a87f3784eb0e098f84a32670cfe4a79cb6512fd8f42ae3d0709f06405d18"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:79aec8172b9e3c6d05fd4b219d5de1ac616bd8da934107325a6c0d0e866a21b6"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:711e408732d4e9a0208008e5892c2966b485c783cd2d9a681f3eb147cf36c7ef"},
    {file = "jiter-0.8.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:653cf462db4e8c41995e33d865965e79641ef45369d8a11f54cd30888b7e6ff1"},
    {file = "jiter-0.8.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:9c63eaef32b7bebac8ebebf4dabebdbc6769a09c127294db6babee38e9f405b9"},
    {file = "jiter-0.8.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:eb21aaa9a200d0a80dacc7a81038d2e476ffe473ffdd9c91eb745d623561de05"},
    {file = "jiter-0.8.2-cp313-cp313-win32.whl", hash = "sha256:789361ed945d8d42850f919342a8665d2dc79e7e44ca1c97cc786966a21f627a"},
    {file = "jiter-0.8.2-cp313-cp313-win_amd64.whl", hash = "sha256:ab7f43235d71e03b941c1630f4b6e3055d46b6cb8728a17663eaac9d8e83a865"},
    {file = "jiter-0.8.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:b426f72cd77da3fec300ed3bc990895e2dd6b49e3bfe6c438592a3ba660e41ca"},
    {file = "jiter-0.8.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b2dd880785088ff2ad21ffee205e58a8c1ddabc63612444ae41e5e4b321b39c0"},
    {file = "jiter-0.8.2-cp313-cp313t-win_amd64.whl", hash = "sha256:3ac9f578c46f22405ff7f8b1f5848fb753cc4b8377fbec8470a7dc3997ca7566"},
    {file = "jiter-0.8.2-cp38-cp38-macosx_10_12_x86_64.whl", hash = "sha256:9e1fa156ee9454642adb7e7234a383884452532bc9d53d5af2d18d98ada1d79c"},
    {file = "jiter-0.8.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:0cf5dfa9956d96ff2efb0f8e9c7d055904012c952539a774305aaaf3abdf3d6c"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e52bf98c7e727dd44f7c4acb980cb988448faeafed8433c867888268899b298b"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a2ecaa3c23e7a7cf86d00eda3390c232f4d533cd9ddea4b04f5d0644faf642c5"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:08d4c92bf480e19fc3f2717c9ce2aa31dceaa9163839a311424b6862252c943e"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:99d9a1eded738299ba8e106c6779ce5c3893cffa0e32e4485d680588adae6db8"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d20be8b7f606df096e08b0b1b4a3c6f0515e8dac296881fe7461dfa0fb5ec817"},
    {file = "jiter-0.8.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d33f94615fcaf872f7fd8cd98ac3b429e435c77619777e8a449d9d27e01134d1"},
    {file = "jiter-0.8.2-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:317b25e98a35ffec5c67efe56a4e9970852632c810d35b34ecdd70cc0e47b3b6"},
    {file = "jiter-0.8.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:fc9043259ee430ecd71d178fccabd8c332a3bf1e81e50cae43cc2b28d19e4cb7"},
    {file = "jiter-0.8.2-cp38-cp38-win32.whl", hash = "sha256:fc5adda618205bd4678b146612ce44c3cbfdee9697951f2c0ffdef1f26d72b63"},
    {file = "jiter-0.8.2-cp38-cp38-win_amd64.whl", hash = "sha256:cd646c827b4f85ef4a78e4e58f4f5854fae0caf3db91b59f0d73731448a970c6"},
    {file = "jiter-0.8.2-cp39-cp39-macosx_10_12_x86_64.whl", hash = "sha256:e41e75344acef3fc59ba4765df29f107f309ca9e8eace5baacabd9217e52a5ee"},
    {file = "jiter-0.8.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:7f22b16b35d5c1df9dfd58843ab2cd25e6bf15191f5a236bed177afade507bfc"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f7200b8f7619d36aa51c803fd52020a2dfbea36ffec1b5e22cab11fd34d95a6d"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:70bf4c43652cc294040dbb62256c83c8718370c8b93dd93d934b9a7bf6c4f53c"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f9d471356dc16f84ed48768b8ee79f29514295c7295cb41e1133ec0b2b8d637d"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:859e8eb3507894093d01929e12e267f83b1d5f6221099d3ec976f0c995cb6bd9"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:eaa58399c01db555346647a907b4ef6d4f584b123943be6ed5588c3f2359c9f4"},
    {file = "jiter-0.8.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:8f2d5ed877f089862f4c7aacf3a542627c1496f972a34d0474ce85ee7d939c27"},
    {file = "jiter-0.8.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:03c9df035d4f8d647f8c210ddc2ae0728387275340668fb30d2421e17d9a0841"},
    {file = "jiter-0.8.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8bd2a824d08d8977bb2794ea2682f898ad3d8837932e3a74937e93d62ecbb637"},
    {file = "jiter-0.8.2-cp39-cp39-win32.whl", hash = "sha256:ca29b6371ebc40e496995c94b988a101b9fbbed48a51190a4461fcb0a68b4a36"},
    {file = "jiter-0.8.2-cp39-cp39-win_amd64.whl", hash = "sha256:1c0dfbd1be3cbefc7510102370d86e35d1d53e5a93d48519688b1bf0f761160a"},
    {file = "jiter-0.8.2.tar.gz", hash = "sha256:cd73d3e740666d0e639f678adb176fad25c1bcbdae88d8d7b857e1783bb4212d"},
]

[[package]]
name = "jsonpatch"
version = "1.33"
description = "Apply JSON-Patches (RFC 6902)"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*, !=3.6.*"
files = [
    {file = "jsonpatch-1.33-py2.py3-none-any.whl", hash = "sha256:0ae28c0cd062bbd8b8ecc26d7d164fbbea9652a1a3693f3b956c1eae5145dade"},
    {file = "jsonpatch-1.33.tar.gz", hash = "sha256:9fcd4009c41e6d12348b4a0ff2563ba56a2923a7dfee731d004e212e1ee5030c"},
]

[package.dependencies]
jsonpointer = ">=1.9"

[[package]]
name = "jsonpointer"
version = "3.0.0"
description = "Identify specific nodes in a JSON document (RFC 6901)"
optional = false
python-versions = ">=3.7"
files = [
    {file = "jsonpointer-3.0.0-py2.py3-none-any.whl", hash = "sha256:13e088adc14fca8b6aa8177c044e12701e6ad4b28ff10e65f2267a90109c9942"},
    {file = "jsonpointer-3.0.0.tar.gz", hash = "sha256:2b2d729f2091522d61c3b31f82e11870f60b68f43fbc705cb76bf4b832af59ef"},
]

[[package]]
name = "langchain"
version = "0.3.15"
description = "Building applications with LLMs through composability"
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langchain-0.3.15-py3-none-any.whl", hash = "sha256:2657735184054cae8181ac43fce6cbc9ee64ca81a2ad2aed3ccd6e5d6fe1f19f"},
    {file = "langchain-0.3.15.tar.gz", hash = "sha256:1204d67f8469cd8da5621d2b39501650a824d4c0d5a74264dfe3df9a7528897e"},
]

[package.dependencies]
aiohttp = ">=3.8.3,<4.0.0"
langchain-core = ">=0.3.31,<0.4.0"
langchain-text-splitters = ">=0.3.3,<0.4.0"
langsmith = ">=0.1.17,<0.4"
numpy = [
    {version = ">=1.22.4,<2", markers = "python_version < \"3.12\""},
    {version = ">=1.26.2,<3", markers = "python_version >= \"3.12\""},
]
pydantic = ">=2.7.4,<3.0.0"
PyYAML = ">=5.3"
requests = ">=2,<3"
SQLAlchemy = ">=1.4,<3"
tenacity = ">=8.1.0,<8.4.0 || >8.4.0,<10"

[[package]]
name = "langchain-anthropic"
version = "0.3.4"
description = "An integration package connecting AnthropicMessages and LangChain"
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langchain_anthropic-0.3.4-py3-none-any.whl", hash = "sha256:1d244b8d33aae43096fda64b9153195dcac8ff211dc2cdf1930393dbaf48476d"},
    {file = "langchain_anthropic-0.3.4.tar.gz", hash = "sha256:c80c4972f577ff5c0f92006273d706be23728a32c5eb884a0528515a9a969697"},
]

[package.dependencies]
anthropic = ">=0.41.0,<1"
defusedxml = ">=0.7.1,<0.8.0"
langchain-core = ">=0.3.31,<0.4.0"
pydantic = ">=2.7.4,<3.0.0"

[[package]]
name = "langchain-core"
version = "0.3.31"
description = "Building applications with LLMs through composability"
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langchain_core-0.3.31-py3-none-any.whl", hash = "sha256:882e64ad95887c951dce8e835889e43263b11848c394af3b73e06912624bd743"},
    {file = "langchain_core-0.3.31.tar.gz", hash = "sha256:5ffa56354c07de9efaa4139609659c63e7d9b29da2c825f6bab9392ec98300df"},
]

[package.dependencies]
jsonpatch = ">=1.33,<2.0"
langsmith = ">=0.1.125,<0.4"
packaging = ">=23.2,<25"
pydantic = [
    {version = ">=2.5.2,<3.0.0", markers = "python_full_version < \"3.12.4\""},
    {version = ">=2.7.4,<3.0.0", markers = "python_full_version >= \"3.12.4\""},
]
PyYAML = ">=5.3"
tenacity = ">=8.1.0,<8.4.0 || >8.4.0,<10.0.0"
typing-extensions = ">=4.7"

[[package]]
name = "langchain-openai"
version = "0.3.2"
description = "An integration package connecting OpenAI and LangChain"
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langchain_openai-0.3.2-py3-none-any.whl", hash = "sha256:8674183805e26d3ae3f78cc44f79fe0b2066f61e2de0e7e18be3b86f0d3b2759"},
    {file = "langchain_openai-0.3.2.tar.gz", hash = "sha256:c2c80ac0208eb7cefdef96f6353b00fa217979ffe83f0a21cc8666001df828c1"},
]

[package.dependencies]
langchain-core = ">=0.3.31,<0.4.0"
openai = ">=1.58.1,<2.0.0"
tiktoken = ">=0.7,<1"

[[package]]
name = "langchain-text-splitters"
version = "0.3.5"
description = "LangChain text splitting utilities"
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langchain_text_splitters-0.3.5-py3-none-any.whl", hash = "sha256:8c9b059827438c5fa8f327b4df857e307828a5ec815163c9b5c9569a3e82c8ee"},
    {file = "langchain_text_splitters-0.3.5.tar.gz", hash = "sha256:11cb7ca3694e5bdd342bc16d3875b7f7381651d4a53cbb91d34f22412ae16443"},
]

[package.dependencies]
langchain-core = ">=0.3.29,<0.4.0"

[[package]]
name = "langgraph"
version = "0.2.67"
description = "Building stateful, multi-actor applications with LLMs"
optional = false
python-versions = "<4.0,>=3.9.0"
files = [
    {file = "langgraph-0.2.67-py3-none-any.whl", hash = "sha256:79a76d8fd7581bf117aaa90453d780df2c96a49ed6f5e25086d9adadef8e945f"},
    {file = "langgraph-0.2.67.tar.gz", hash = "sha256:80d034b612d998ec5f6a77362ba8ebe697b5e7b25f8ebbc4333b82bbd9bca8f6"},
]

[package.dependencies]
langchain-core = ">=0.2.43,<0.3.0 || >0.3.0,<0.3.1 || >0.3.1,<0.3.2 || >0.3.2,<0.3.3 || >0.3.3,<0.3.4 || >0.3.4,<0.3.5 || >0.3.5,<0.3.6 || >0.3.6,<0.3.7 || >0.3.7,<0.3.8 || >0.3.8,<0.3.9 || >0.3.9,<0.3.10 || >0.3.10,<0.3.11 || >0.3.11,<0.3.12 || >0.3.12,<0.3.13 || >0.3.13,<0.3.14 || >0.3.14,<0.3.15 || >0.3.15,<0.3.16 || >0.3.16,<0.3.17 || >0.3.17,<0.3.18 || >0.3.18,<0.3.19 || >0.3.19,<0.3.20 || >0.3.20,<0.3.21 || >0.3.21,<0.3.22 || >0.3.22,<0.4.0"
langgraph-checkpoint = ">=2.0.10,<3.0.0"
langgraph-sdk = ">=0.1.42,<0.2.0"

[[package]]
name = "langgraph-checkpoint"
version = "2.0.10"
description = "Library with base interfaces for LangGraph checkpoint savers."
optional = false
python-versions = "<4.0.0,>=3.9.0"
files = [
    {file = "langgraph_checkpoint-2.0.10-py3-none-any.whl", hash = "sha256:0d592cfda2df93844c6ea44d142170a8f7e5ba5320274e0e5e60e27f2749392c"},
    {file = "langgraph_checkpoint-2.0.10.tar.gz", hash = "sha256:2dcc04e09091d588bb6209e49d83ff5406d7231c2590d6ff18fb29ab8b140129"},
]

[package.dependencies]
langchain-core = ">=0.2.38,<0.4"
msgpack = ">=1.1.0,<2.0.0"

[[package]]
name = "langgraph-sdk"
version = "0.1.51"
description = "SDK for interacting with LangGraph API"
optional = false
python-versions = "<4.0.0,>=3.9.0"
files = [
    {file = "langgraph_sdk-0.1.51-py3-none-any.whl", hash = "sha256:ce2b58466d1700d06149782ed113157a8694a6d7932c801f316cd13fab315fe4"},
    {file = "langgraph_sdk-0.1.51.tar.gz", hash = "sha256:dea1363e72562cb1e82a2d156be8d5b1a69ff3fe8815eee0e1e7a2f423242ec1"},
]

[package.dependencies]
httpx = ">=0.25.2"
orjson = ">=3.10.1"

[[package]]
name = "langmem"
version = "0.0.5rc5"
description = "Prebuilt utilities for memory management and retrieval."
optional = false
python-versions = ">=3.11"
files = [
    {file = "langmem-0.0.5rc5-py3-none-any.whl", hash = "sha256:2f39a1fd6719cc45e894e8320d0e88fef9b035f2f607c19330a3882c07dce341"},
    {file = "langmem-0.0.5rc5.tar.gz", hash = "sha256:bca5774265c56c2a6742aaa9bfe0654d2cf62aba3b7e1fd7a7bf397356933230"},
]

[package.dependencies]
langchain = ">=0.3.15"
langchain-openai = ">=0.3.1"
langgraph = ">=0.2.66"
trustcall = ">=0.0.28"

[[package]]
name = "langsmith"
version = "0.3.1"
description = "Client library to connect to the LangSmith LLM Tracing and Evaluation Platform."
optional = false
python-versions = "<4.0,>=3.9"
files = [
    {file = "langsmith-0.3.1-py3-none-any.whl", hash = "sha256:b6afbb214ae82b6d96b8134718db3a7d2598b2a7eb4ab1212bcd6d96e04eda10"},
    {file = "langsmith-0.3.1.tar.gz", hash = "sha256:9242a49d37e2176a344ddec97bf57b958dc0e1f0437e150cefd0fb70195f0e26"},
]

[package.dependencies]
httpx = ">=0.23.0,<1"
orjson = {version = ">=3.9.14,<4.0.0", markers = "platform_python_implementation != \"PyPy\""}
pydantic = [
    {version = ">=1,<3", markers = "python_full_version < \"3.12.4\""},
    {version = ">=2.7.4,<3.0.0", markers = "python_full_version >= \"3.12.4\""},
]
requests = ">=2,<3"
requests-toolbelt = ">=1.0.0,<2.0.0"
zstandard = ">=0.23.0,<0.24.0"

[package.extras]
langsmith-pyo3 = ["langsmith-pyo3 (>=0.1.0rc2,<0.2.0)"]
pytest = ["pytest (>=7.0.0)", "rich (>=13.9.4,<14.0.0)"]

[[package]]
name = "msgpack"
version = "1.1.0"
description = "MessagePack serializer"
optional = false
python-versions = ">=3.8"
files = [
    {file = "msgpack-1.1.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:7ad442d527a7e358a469faf43fda45aaf4ac3249c8310a82f0ccff9164e5dccd"},
    {file = "msgpack-1.1.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:74bed8f63f8f14d75eec75cf3d04ad581da6b914001b474a5d3cd3372c8cc27d"},
    {file = "msgpack-1.1.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:914571a2a5b4e7606997e169f64ce53a8b1e06f2cf2c3a7273aa106236d43dd5"},
    {file = "msgpack-1.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c921af52214dcbb75e6bdf6a661b23c3e6417f00c603dd2070bccb5c3ef499f5"},
    {file = "msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d8ce0b22b890be5d252de90d0e0d119f363012027cf256185fc3d474c44b1b9e"},
    {file = "msgpack-1.1.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:73322a6cc57fcee3c0c57c4463d828e9428275fb85a27aa2aa1a92fdc42afd7b"},
    {file = "msgpack-1.1.0-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:e1f3c3d21f7cf67bcf2da8e494d30a75e4cf60041d98b3f79875afb5b96f3a3f"},
    {file = "msgpack-1.1.0-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:64fc9068d701233effd61b19efb1485587560b66fe57b3e50d29c5d78e7fef68"},
    {file = "msgpack-1.1.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:42f754515e0f683f9c79210a5d1cad631ec3d06cea5172214d2176a42e67e19b"},
    {file = "msgpack-1.1.0-cp310-cp310-win32.whl", hash = "sha256:3df7e6b05571b3814361e8464f9304c42d2196808e0119f55d0d3e62cd5ea044"},
    {file = "msgpack-1.1.0-cp310-cp310-win_amd64.whl", hash = "sha256:685ec345eefc757a7c8af44a3032734a739f8c45d1b0ac45efc5d8977aa4720f"},
    {file = "msgpack-1.1.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3d364a55082fb2a7416f6c63ae383fbd903adb5a6cf78c5b96cc6316dc1cedc7"},
    {file = "msgpack-1.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:79ec007767b9b56860e0372085f8504db5d06bd6a327a335449508bbee9648fa"},
    {file = "msgpack-1.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6ad622bf7756d5a497d5b6836e7fc3752e2dd6f4c648e24b1803f6048596f701"},
    {file = "msgpack-1.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8e59bca908d9ca0de3dc8684f21ebf9a690fe47b6be93236eb40b99af28b6ea6"},
    {file = "msgpack-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:5e1da8f11a3dd397f0a32c76165cf0c4eb95b31013a94f6ecc0b280c05c91b59"},
    {file = "msgpack-1.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:452aff037287acb1d70a804ffd022b21fa2bb7c46bee884dbc864cc9024128a0"},
    {file = "msgpack-1.1.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:8da4bf6d54ceed70e8861f833f83ce0814a2b72102e890cbdfe4b34764cdd66e"},
    {file = "msgpack-1.1.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:41c991beebf175faf352fb940bf2af9ad1fb77fd25f38d9142053914947cdbf6"},
    {file = "msgpack-1.1.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:a52a1f3a5af7ba1c9ace055b659189f6c669cf3657095b50f9602af3a3ba0fe5"},
    {file = "msgpack-1.1.0-cp311-cp311-win32.whl", hash = "sha256:58638690ebd0a06427c5fe1a227bb6b8b9fdc2bd07701bec13c2335c82131a88"},
    {file = "msgpack-1.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:fd2906780f25c8ed5d7b323379f6138524ba793428db5d0e9d226d3fa6aa1788"},
    {file = "msgpack-1.1.0-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:d46cf9e3705ea9485687aa4001a76e44748b609d260af21c4ceea7f2212a501d"},
    {file = "msgpack-1.1.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:5dbad74103df937e1325cc4bfeaf57713be0b4f15e1c2da43ccdd836393e2ea2"},
    {file = "msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:58dfc47f8b102da61e8949708b3eafc3504509a5728f8b4ddef84bd9e16ad420"},
    {file = "msgpack-1.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4676e5be1b472909b2ee6356ff425ebedf5142427842aa06b4dfd5117d1ca8a2"},
    {file = "msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:17fb65dd0bec285907f68b15734a993ad3fc94332b5bb21b0435846228de1f39"},
    {file = "msgpack-1.1.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a51abd48c6d8ac89e0cfd4fe177c61481aca2d5e7ba42044fd218cfd8ea9899f"},
    {file = "msgpack-1.1.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:2137773500afa5494a61b1208619e3871f75f27b03bcfca7b3a7023284140247"},
    {file = "msgpack-1.1.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:398b713459fea610861c8a7b62a6fec1882759f308ae0795b5413ff6a160cf3c"},
    {file = "msgpack-1.1.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:06f5fd2f6bb2a7914922d935d3b8bb4a7fff3a9a91cfce6d06c13bc42bec975b"},
    {file = "msgpack-1.1.0-cp312-cp312-win32.whl", hash = "sha256:ad33e8400e4ec17ba782f7b9cf868977d867ed784a1f5f2ab46e7ba53b6e1e1b"},
    {file = "msgpack-1.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:115a7af8ee9e8cddc10f87636767857e7e3717b7a2e97379dc2054712693e90f"},
    {file = "msgpack-1.1.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:071603e2f0771c45ad9bc65719291c568d4edf120b44eb36324dcb02a13bfddf"},
    {file = "msgpack-1.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:0f92a83b84e7c0749e3f12821949d79485971f087604178026085f60ce109330"},
    {file = "msgpack-1.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:4a1964df7b81285d00a84da4e70cb1383f2e665e0f1f2a7027e683956d04b734"},
    {file = "msgpack-1.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:59caf6a4ed0d164055ccff8fe31eddc0ebc07cf7326a2aaa0dbf7a4001cd823e"},
    {file = "msgpack-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0907e1a7119b337971a689153665764adc34e89175f9a34793307d9def08e6ca"},
    {file = "msgpack-1.1.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:65553c9b6da8166e819a6aa90ad15288599b340f91d18f60b2061f402b9a4915"},
    {file = "msgpack-1.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:7a946a8992941fea80ed4beae6bff74ffd7ee129a90b4dd5cf9c476a30e9708d"},
    {file = "msgpack-1.1.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:4b51405e36e075193bc051315dbf29168d6141ae2500ba8cd80a522964e31434"},
    {file = "msgpack-1.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:b4c01941fd2ff87c2a934ee6055bda4ed353a7846b8d4f341c428109e9fcde8c"},
    {file = "msgpack-1.1.0-cp313-cp313-win32.whl", hash = "sha256:7c9a35ce2c2573bada929e0b7b3576de647b0defbd25f5139dcdaba0ae35a4cc"},
    {file = "msgpack-1.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:bce7d9e614a04d0883af0b3d4d501171fbfca038f12c77fa838d9f198147a23f"},
    {file = "msgpack-1.1.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c40ffa9a15d74e05ba1fe2681ea33b9caffd886675412612d93ab17b58ea2fec"},
    {file = "msgpack-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f1ba6136e650898082d9d5a5217d5906d1e138024f836ff48691784bbe1adf96"},
    {file = "msgpack-1.1.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e0856a2b7e8dcb874be44fea031d22e5b3a19121be92a1e098f46068a11b0870"},
    {file = "msgpack-1.1.0-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:471e27a5787a2e3f974ba023f9e265a8c7cfd373632247deb225617e3100a3c7"},
    {file = "msgpack-1.1.0-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:646afc8102935a388ffc3914b336d22d1c2d6209c773f3eb5dd4d6d3b6f8c1cb"},
    {file = "msgpack-1.1.0-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:13599f8829cfbe0158f6456374e9eea9f44eee08076291771d8ae93eda56607f"},
    {file = "msgpack-1.1.0-cp38-cp38-win32.whl", hash = "sha256:8a84efb768fb968381e525eeeb3d92857e4985aacc39f3c47ffd00eb4509315b"},
    {file = "msgpack-1.1.0-cp38-cp38-win_amd64.whl", hash = "sha256:879a7b7b0ad82481c52d3c7eb99bf6f0645dbdec5134a4bddbd16f3506947feb"},
    {file = "msgpack-1.1.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:53258eeb7a80fc46f62fd59c876957a2d0e15e6449a9e71842b6d24419d88ca1"},
    {file = "msgpack-1.1.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:7e7b853bbc44fb03fbdba34feb4bd414322180135e2cb5164f20ce1c9795ee48"},
    {file = "msgpack-1.1.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:f3e9b4936df53b970513eac1758f3882c88658a220b58dcc1e39606dccaaf01c"},
    {file = "msgpack-1.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:46c34e99110762a76e3911fc923222472c9d681f1094096ac4102c18319e6468"},
    {file = "msgpack-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8a706d1e74dd3dea05cb54580d9bd8b2880e9264856ce5068027eed09680aa74"},
    {file = "msgpack-1.1.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:534480ee5690ab3cbed89d4c8971a5c631b69a8c0883ecfea96c19118510c846"},
    {file = "msgpack-1.1.0-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:8cf9e8c3a2153934a23ac160cc4cba0ec035f6867c8013cc6077a79823370346"},
    {file = "msgpack-1.1.0-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:3180065ec2abbe13a4ad37688b61b99d7f9e012a535b930e0e683ad6bc30155b"},
    {file = "msgpack-1.1.0-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:c5a91481a3cc573ac8c0d9aace09345d989dc4a0202b7fcb312c88c26d4e71a8"},
    {file = "msgpack-1.1.0-cp39-cp39-win32.whl", hash = "sha256:f80bc7d47f76089633763f952e67f8214cb7b3ee6bfa489b3cb6a84cfac114cd"},
    {file = "msgpack-1.1.0-cp39-cp39-win_amd64.whl", hash = "sha256:4d1b7ff2d6146e16e8bd665ac726a89c74163ef8cd39fa8c1087d4e52d3a2325"},
    {file = "msgpack-1.1.0.tar.gz", hash = "sha256:dd432ccc2c72b914e4cb77afce64aab761c1137cc698be3984eee260bcb2896e"},
]

[[package]]
name = "multidict"
version = "6.1.0"
description = "multidict implementation"
optional = false
python-versions = ">=3.8"
files = [
    {file = "multidict-6.1.0-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:3380252550e372e8511d49481bd836264c009adb826b23fefcc5dd3c69692f60"},
    {file = "multidict-6.1.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:99f826cbf970077383d7de805c0681799491cb939c25450b9b5b3ced03ca99f1"},
    {file = "multidict-6.1.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:a114d03b938376557927ab23f1e950827c3b893ccb94b62fd95d430fd0e5cf53"},
    {file = "multidict-6.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b1c416351ee6271b2f49b56ad7f308072f6f44b37118d69c2cad94f3fa8a40d5"},
    {file = "multidict-6.1.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6b5d83030255983181005e6cfbac1617ce9746b219bc2aad52201ad121226581"},
    {file = "multidict-6.1.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3e97b5e938051226dc025ec80980c285b053ffb1e25a3db2a3aa3bc046bf7f56"},
    {file = "multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d618649d4e70ac6efcbba75be98b26ef5078faad23592f9b51ca492953012429"},
    {file = "multidict-6.1.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:10524ebd769727ac77ef2278390fb0068d83f3acb7773792a5080f2b0abf7748"},
    {file = "multidict-6.1.0-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:ff3827aef427c89a25cc96ded1759271a93603aba9fb977a6d264648ebf989db"},
    {file = "multidict-6.1.0-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:06809f4f0f7ab7ea2cabf9caca7d79c22c0758b58a71f9d32943ae13c7ace056"},
    {file = "multidict-6.1.0-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:f179dee3b863ab1c59580ff60f9d99f632f34ccb38bf67a33ec6b3ecadd0fd76"},
    {file = "multidict-6.1.0-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:aaed8b0562be4a0876ee3b6946f6869b7bcdb571a5d1496683505944e268b160"},
    {file = "multidict-6.1.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:3c8b88a2ccf5493b6c8da9076fb151ba106960a2df90c2633f342f120751a9e7"},
    {file = "multidict-6.1.0-cp310-cp310-win32.whl", hash = "sha256:4a9cb68166a34117d6646c0023c7b759bf197bee5ad4272f420a0141d7eb03a0"},
    {file = "multidict-6.1.0-cp310-cp310-win_amd64.whl", hash = "sha256:20b9b5fbe0b88d0bdef2012ef7dee867f874b72528cf1d08f1d59b0e3850129d"},
    {file = "multidict-6.1.0-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:3efe2c2cb5763f2f1b275ad2bf7a287d3f7ebbef35648a9726e3b69284a4f3d6"},
    {file = "multidict-6.1.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:c7053d3b0353a8b9de430a4f4b4268ac9a4fb3481af37dfe49825bf45ca24156"},
    {file = "multidict-6.1.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:27e5fc84ccef8dfaabb09d82b7d179c7cf1a3fbc8a966f8274fcb4ab2eb4cadb"},
    {file = "multidict-6.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0e2b90b43e696f25c62656389d32236e049568b39320e2735d51f08fd362761b"},
    {file = "multidict-6.1.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d83a047959d38a7ff552ff94be767b7fd79b831ad1cd9920662db05fec24fe72"},
    {file = "multidict-6.1.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d1a9dd711d0877a1ece3d2e4fea11a8e75741ca21954c919406b44e7cf971304"},
    {file = "multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ec2abea24d98246b94913b76a125e855eb5c434f7c46546046372fe60f666351"},
    {file = "multidict-6.1.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:4867cafcbc6585e4b678876c489b9273b13e9fff9f6d6d66add5e15d11d926cb"},
    {file = "multidict-6.1.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:5b48204e8d955c47c55b72779802b219a39acc3ee3d0116d5080c388970b76e3"},
    {file = "multidict-6.1.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:d8fff389528cad1618fb4b26b95550327495462cd745d879a8c7c2115248e399"},
    {file = "multidict-6.1.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:a7a9541cd308eed5e30318430a9c74d2132e9a8cb46b901326272d780bf2d423"},
    {file = "multidict-6.1.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:da1758c76f50c39a2efd5e9859ce7d776317eb1dd34317c8152ac9251fc574a3"},
    {file = "multidict-6.1.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:c943a53e9186688b45b323602298ab727d8865d8c9ee0b17f8d62d14b56f0753"},
    {file = "multidict-6.1.0-cp311-cp311-win32.whl", hash = "sha256:90f8717cb649eea3504091e640a1b8568faad18bd4b9fcd692853a04475a4b80"},
    {file = "multidict-6.1.0-cp311-cp311-win_amd64.whl", hash = "sha256:82176036e65644a6cc5bd619f65f6f19781e8ec2e5330f51aa9ada7504cc1926"},
    {file = "multidict-6.1.0-cp312-cp312-macosx_10_9_universal2.whl", hash = "sha256:b04772ed465fa3cc947db808fa306d79b43e896beb677a56fb2347ca1a49c1fa"},
    {file = "multidict-6.1.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:6180c0ae073bddeb5a97a38c03f30c233e0a4d39cd86166251617d1bbd0af436"},
    {file = "multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:071120490b47aa997cca00666923a83f02c7fbb44f71cf7f136df753f7fa8761"},
    {file = "multidict-6.1.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:50b3a2710631848991d0bf7de077502e8994c804bb805aeb2925a981de58ec2e"},
    {file = "multidict-6.1.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b58c621844d55e71c1b7f7c498ce5aa6985d743a1a59034c57a905b3f153c1ef"},
    {file = "multidict-6.1.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:55b6d90641869892caa9ca42ff913f7ff1c5ece06474fbd32fb2cf6834726c95"},
    {file = "multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4b820514bfc0b98a30e3d85462084779900347e4d49267f747ff54060cc33925"},
    {file = "multidict-6.1.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:10a9b09aba0c5b48c53761b7c720aaaf7cf236d5fe394cd399c7ba662d5f9966"},
    {file = "multidict-6.1.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:1e16bf3e5fc9f44632affb159d30a437bfe286ce9e02754759be5536b169b305"},
    {file = "multidict-6.1.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:76f364861c3bfc98cbbcbd402d83454ed9e01a5224bb3a28bf70002a230f73e2"},
    {file = "multidict-6.1.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:820c661588bd01a0aa62a1283f20d2be4281b086f80dad9e955e690c75fb54a2"},
    {file = "multidict-6.1.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:0e5f362e895bc5b9e67fe6e4ded2492d8124bdf817827f33c5b46c2fe3ffaca6"},
    {file = "multidict-6.1.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:3ec660d19bbc671e3a6443325f07263be452c453ac9e512f5eb935e7d4ac28b3"},
    {file = "multidict-6.1.0-cp312-cp312-win32.whl", hash = "sha256:58130ecf8f7b8112cdb841486404f1282b9c86ccb30d3519faf301b2e5659133"},
    {file = "multidict-6.1.0-cp312-cp312-win_amd64.whl", hash = "sha256:188215fc0aafb8e03341995e7c4797860181562380f81ed0a87ff455b70bf1f1"},
    {file = "multidict-6.1.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:d569388c381b24671589335a3be6e1d45546c2988c2ebe30fdcada8457a31008"},
    {file = "multidict-6.1.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:052e10d2d37810b99cc170b785945421141bf7bb7d2f8799d431e7db229c385f"},
    {file = "multidict-6.1.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f90c822a402cb865e396a504f9fc8173ef34212a342d92e362ca498cad308e28"},
    {file = "multidict-6.1.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b225d95519a5bf73860323e633a664b0d85ad3d5bede6d30d95b35d4dfe8805b"},
    {file = "multidict-6.1.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:23bfd518810af7de1116313ebd9092cb9aa629beb12f6ed631ad53356ed6b86c"},
    {file = "multidict-6.1.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5c09fcfdccdd0b57867577b719c69e347a436b86cd83747f179dbf0cc0d4c1f3"},
    {file = "multidict-6.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bf6bea52ec97e95560af5ae576bdac3aa3aae0b6758c6efa115236d9e07dae44"},
    {file = "multidict-6.1.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:57feec87371dbb3520da6192213c7d6fc892d5589a93db548331954de8248fd2"},
    {file = "multidict-6.1.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0c3f390dc53279cbc8ba976e5f8035eab997829066756d811616b652b00a23a3"},
    {file = "multidict-6.1.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:59bfeae4b25ec05b34f1956eaa1cb38032282cd4dfabc5056d0a1ec4d696d3aa"},
    {file = "multidict-6.1.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:b2f59caeaf7632cc633b5cf6fc449372b83bbdf0da4ae04d5be36118e46cc0aa"},
    {file = "multidict-6.1.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:37bb93b2178e02b7b618893990941900fd25b6b9ac0fa49931a40aecdf083fe4"},
    {file = "multidict-6.1.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4e9f48f58c2c523d5a06faea47866cd35b32655c46b443f163d08c6d0ddb17d6"},
    {file = "multidict-6.1.0-cp313-cp313-win32.whl", hash = "sha256:3a37ffb35399029b45c6cc33640a92bef403c9fd388acce75cdc88f58bd19a81"},
    {file = "multidict-6.1.0-cp313-cp313-win_amd64.whl", hash = "sha256:e9aa71e15d9d9beaad2c6b9319edcdc0a49a43ef5c0a4c8265ca9ee7d6c67774"},
    {file = "multidict-6.1.0-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:db7457bac39421addd0c8449933ac32d8042aae84a14911a757ae6ca3eef1392"},
    {file = "multidict-6.1.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:d094ddec350a2fb899fec68d8353c78233debde9b7d8b4beeafa70825f1c281a"},
    {file = "multidict-6.1.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:5845c1fd4866bb5dd3125d89b90e57ed3138241540897de748cdf19de8a2fca2"},
    {file = "multidict-6.1.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9079dfc6a70abe341f521f78405b8949f96db48da98aeb43f9907f342f627cdc"},
    {file = "multidict-6.1.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3914f5aaa0f36d5d60e8ece6a308ee1c9784cd75ec8151062614657a114c4478"},
    {file = "multidict-6.1.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c08be4f460903e5a9d0f76818db3250f12e9c344e79314d1d570fc69d7f4eae4"},
    {file = "multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d093be959277cb7dee84b801eb1af388b6ad3ca6a6b6bf1ed7585895789d027d"},
    {file = "multidict-6.1.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:3702ea6872c5a2a4eeefa6ffd36b042e9773f05b1f37ae3ef7264b1163c2dcf6"},
    {file = "multidict-6.1.0-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:2090f6a85cafc5b2db085124d752757c9d251548cedabe9bd31afe6363e0aff2"},
    {file = "multidict-6.1.0-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:f67f217af4b1ff66c68a87318012de788dd95fcfeb24cc889011f4e1c7454dfd"},
    {file = "multidict-6.1.0-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:189f652a87e876098bbc67b4da1049afb5f5dfbaa310dd67c594b01c10388db6"},
    {file = "multidict-6.1.0-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:6bb5992037f7a9eff7991ebe4273ea7f51f1c1c511e6a2ce511d0e7bdb754492"},
    {file = "multidict-6.1.0-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:ac10f4c2b9e770c4e393876e35a7046879d195cd123b4f116d299d442b335bcd"},
    {file = "multidict-6.1.0-cp38-cp38-win32.whl", hash = "sha256:e27bbb6d14416713a8bd7aaa1313c0fc8d44ee48d74497a0ff4c3a1b6ccb5167"},
    {file = "multidict-6.1.0-cp38-cp38-win_amd64.whl", hash = "sha256:22f3105d4fb15c8f57ff3959a58fcab6ce36814486500cd7485651230ad4d4ef"},
    {file = "multidict-6.1.0-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:4e18b656c5e844539d506a0a06432274d7bd52a7487e6828c63a63d69185626c"},
    {file = "multidict-6.1.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:a185f876e69897a6f3325c3f19f26a297fa058c5e456bfcff8015e9a27e83ae1"},
    {file = "multidict-6.1.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:ab7c4ceb38d91570a650dba194e1ca87c2b543488fe9309b4212694174fd539c"},
    {file = "multidict-6.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e617fb6b0b6953fffd762669610c1c4ffd05632c138d61ac7e14ad187870669c"},
    {file = "multidict-6.1.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:16e5f4bf4e603eb1fdd5d8180f1a25f30056f22e55ce51fb3d6ad4ab29f7d96f"},
    {file = "multidict-6.1.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f4c035da3f544b1882bac24115f3e2e8760f10a0107614fc9839fd232200b875"},
    {file = "multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:957cf8e4b6e123a9eea554fa7ebc85674674b713551de587eb318a2df3e00255"},
    {file = "multidict-6.1.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:483a6aea59cb89904e1ceabd2b47368b5600fb7de78a6e4a2c2987b2d256cf30"},
    {file = "multidict-6.1.0-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:87701f25a2352e5bf7454caa64757642734da9f6b11384c1f9d1a8e699758057"},
    {file = "multidict-6.1.0-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:682b987361e5fd7a139ed565e30d81fd81e9629acc7d925a205366877d8c8657"},
    {file = "multidict-6.1.0-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:ce2186a7df133a9c895dea3331ddc5ddad42cdd0d1ea2f0a51e5d161e4762f28"},
    {file = "multidict-6.1.0-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:9f636b730f7e8cb19feb87094949ba54ee5357440b9658b2a32a5ce4bce53972"},
    {file = "multidict-6.1.0-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:73eae06aa53af2ea5270cc066dcaf02cc60d2994bbb2c4ef5764949257d10f43"},
    {file = "multidict-6.1.0-cp39-cp39-win32.whl", hash = "sha256:1ca0083e80e791cffc6efce7660ad24af66c8d4079d2a750b29001b53ff59ada"},
    {file = "multidict-6.1.0-cp39-cp39-win_amd64.whl", hash = "sha256:aa466da5b15ccea564bdab9c89175c762bc12825f4659c11227f515cee76fa4a"},
    {file = "multidict-6.1.0-py3-none-any.whl", hash = "sha256:48e171e52d1c4d33888e529b999e5900356b9ae588c2f09a52dcefb158b27506"},
    {file = "multidict-6.1.0.tar.gz", hash = "sha256:22ae2ebf9b0c69d206c003e2f6a914ea33f0a932d4aa16f236afc049d9958f4a"},
]

[[package]]
name = "mypy"
version = "1.14.1"
description = "Optional static typing for Python"
optional = false
python-versions = ">=3.8"
files = [
    {file = "mypy-1.14.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:52686e37cf13d559f668aa398dd7ddf1f92c5d613e4f8cb262be2fb4fedb0fcb"},
    {file = "mypy-1.14.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:1fb545ca340537d4b45d3eecdb3def05e913299ca72c290326be19b3804b39c0"},
    {file = "mypy-1.14.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:90716d8b2d1f4cd503309788e51366f07c56635a3309b0f6a32547eaaa36a64d"},
    {file = "mypy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:2ae753f5c9fef278bcf12e1a564351764f2a6da579d4a81347e1d5a15819997b"},
    {file = "mypy-1.14.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:e0fe0f5feaafcb04505bcf439e991c6d8f1bf8b15f12b05feeed96e9e7bf1427"},
    {file = "mypy-1.14.1-cp310-cp310-win_amd64.whl", hash = "sha256:7d54bd85b925e501c555a3227f3ec0cfc54ee8b6930bd6141ec872d1c572f81f"},
    {file = "mypy-1.14.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:f995e511de847791c3b11ed90084a7a0aafdc074ab88c5a9711622fe4751138c"},
    {file = "mypy-1.14.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:d64169ec3b8461311f8ce2fd2eb5d33e2d0f2c7b49116259c51d0d96edee48d1"},
    {file = "mypy-1.14.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:ba24549de7b89b6381b91fbc068d798192b1b5201987070319889e93038967a8"},
    {file = "mypy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:183cf0a45457d28ff9d758730cd0210419ac27d4d3f285beda038c9083363b1f"},
    {file = "mypy-1.14.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:f2a0ecc86378f45347f586e4163d1769dd81c5a223d577fe351f26b179e148b1"},
    {file = "mypy-1.14.1-cp311-cp311-win_amd64.whl", hash = "sha256:ad3301ebebec9e8ee7135d8e3109ca76c23752bac1e717bc84cd3836b4bf3eae"},
    {file = "mypy-1.14.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:30ff5ef8519bbc2e18b3b54521ec319513a26f1bba19a7582e7b1f58a6e69f14"},
    {file = "mypy-1.14.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:cb9f255c18052343c70234907e2e532bc7e55a62565d64536dbc7706a20b78b9"},
    {file = "mypy-1.14.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:8b4e3413e0bddea671012b063e27591b953d653209e7a4fa5e48759cda77ca11"},
    {file = "mypy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:553c293b1fbdebb6c3c4030589dab9fafb6dfa768995a453d8a5d3b23784af2e"},
    {file = "mypy-1.14.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:fad79bfe3b65fe6a1efaed97b445c3d37f7be9fdc348bdb2d7cac75579607c89"},
    {file = "mypy-1.14.1-cp312-cp312-win_amd64.whl", hash = "sha256:8fa2220e54d2946e94ab6dbb3ba0a992795bd68b16dc852db33028df2b00191b"},
    {file = "mypy-1.14.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:92c3ed5afb06c3a8e188cb5da4984cab9ec9a77ba956ee419c68a388b4595255"},
    {file = "mypy-1.14.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:dbec574648b3e25f43d23577309b16534431db4ddc09fda50841f1e34e64ed34"},
    {file = "mypy-1.14.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:8c6d94b16d62eb3e947281aa7347d78236688e21081f11de976376cf010eb31a"},
    {file = "mypy-1.14.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d4b19b03fdf54f3c5b2fa474c56b4c13c9dbfb9a2db4370ede7ec11a2c5927d9"},
    {file = "mypy-1.14.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:0c911fde686394753fff899c409fd4e16e9b294c24bfd5e1ea4675deae1ac6fd"},
    {file = "mypy-1.14.1-cp313-cp313-win_amd64.whl", hash = "sha256:8b21525cb51671219f5307be85f7e646a153e5acc656e5cebf64bfa076c50107"},
    {file = "mypy-1.14.1-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:7084fb8f1128c76cd9cf68fe5971b37072598e7c31b2f9f95586b65c741a9d31"},
    {file = "mypy-1.14.1-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:8f845a00b4f420f693f870eaee5f3e2692fa84cc8514496114649cfa8fd5e2c6"},
    {file = "mypy-1.14.1-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:44bf464499f0e3a2d14d58b54674dee25c031703b2ffc35064bd0df2e0fac319"},
    {file = "mypy-1.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:c99f27732c0b7dc847adb21c9d47ce57eb48fa33a17bc6d7d5c5e9f9e7ae5bac"},
    {file = "mypy-1.14.1-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:bce23c7377b43602baa0bd22ea3265c49b9ff0b76eb315d6c34721af4cdf1d9b"},
    {file = "mypy-1.14.1-cp38-cp38-win_amd64.whl", hash = "sha256:8edc07eeade7ebc771ff9cf6b211b9a7d93687ff892150cb5692e4f4272b0837"},
    {file = "mypy-1.14.1-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:3888a1816d69f7ab92092f785a462944b3ca16d7c470d564165fe703b0970c35"},
    {file = "mypy-1.14.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:46c756a444117c43ee984bd055db99e498bc613a70bbbc120272bd13ca579fbc"},
    {file = "mypy-1.14.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:27fc248022907e72abfd8e22ab1f10e903915ff69961174784a3900a8cba9ad9"},
    {file = "mypy-1.14.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:499d6a72fb7e5de92218db961f1a66d5f11783f9ae549d214617edab5d4dbdbb"},
    {file = "mypy-1.14.1-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:57961db9795eb566dc1d1b4e9139ebc4c6b0cb6e7254ecde69d1552bf7613f60"},
    {file = "mypy-1.14.1-cp39-cp39-win_amd64.whl", hash = "sha256:07ba89fdcc9451f2ebb02853deb6aaaa3d2239a236669a63ab3801bbf923ef5c"},
    {file = "mypy-1.14.1-py3-none-any.whl", hash = "sha256:b66a60cc4073aeb8ae00057f9c1f64d49e90f918fbcef9a977eb121da8b8f1d1"},
    {file = "mypy-1.14.1.tar.gz", hash = "sha256:7ec88144fe9b510e8475ec2f5f251992690fcf89ccb4500b214b4226abcd32d6"},
]

[package.dependencies]
mypy_extensions = ">=1.0.0"
typing_extensions = ">=4.6.0"

[package.extras]
dmypy = ["psutil (>=4.0)"]
faster-cache = ["orjson"]
install-types = ["pip"]
mypyc = ["setuptools (>=50)"]
reports = ["lxml"]

[[package]]
name = "mypy-extensions"
version = "1.0.0"
description = "Type system extensions for programs checked with the mypy type checker."
optional = false
python-versions = ">=3.5"
files = [
    {file = "mypy_extensions-1.0.0-py3-none-any.whl", hash = "sha256:4392f6c0eb8a5668a69e23d168ffa70f0be9ccfd32b5cc2d26a34ae5b844552d"},
    {file = "mypy_extensions-1.0.0.tar.gz", hash = "sha256:75dbf8955dc00442a438fc4d0666508a9a97b6bd41aa2f0ffe9d2f2725af0782"},
]

[[package]]
name = "numpy"
version = "1.26.4"
description = "Fundamental package for array computing in Python"
optional = false
python-versions = ">=3.9"
files = [
    {file = "numpy-1.26.4-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0"},
    {file = "numpy-1.26.4-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:2e4ee3380d6de9c9ec04745830fd9e2eccb3e6cf790d39d7b98ffd19b0dd754a"},
    {file = "numpy-1.26.4-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4"},
    {file = "numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f"},
    {file = "numpy-1.26.4-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:62b8e4b1e28009ef2846b4c7852046736bab361f7aeadeb6a5b89ebec3c7055a"},
    {file = "numpy-1.26.4-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2"},
    {file = "numpy-1.26.4-cp310-cp310-win32.whl", hash = "sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07"},
    {file = "numpy-1.26.4-cp310-cp310-win_amd64.whl", hash = "sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5"},
    {file = "numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:4c66707fabe114439db9068ee468c26bbdf909cac0fb58686a42a24de1760c71"},
    {file = "numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef"},
    {file = "numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e"},
    {file = "numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:666dbfb6ec68962c033a450943ded891bed2d54e6755e35e5835d63f4f6931d5"},
    {file = "numpy-1.26.4-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a"},
    {file = "numpy-1.26.4-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:60dedbb91afcbfdc9bc0b1f3f402804070deed7392c23eb7a7f07fa857868e8a"},
    {file = "numpy-1.26.4-cp311-cp311-win32.whl", hash = "sha256:1af303d6b2210eb850fcf03064d364652b7120803a0b872f5211f5234b399f20"},
    {file = "numpy-1.26.4-cp311-cp311-win_amd64.whl", hash = "sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2"},
    {file = "numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218"},
    {file = "numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:03a8c78d01d9781b28a6989f6fa1bb2c4f2d51201cf99d3dd875df6fbd96b23b"},
    {file = "numpy-1.26.4-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b"},
    {file = "numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:675d61ffbfa78604709862923189bad94014bef562cc35cf61d3a07bba02a7ed"},
    {file = "numpy-1.26.4-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a"},
    {file = "numpy-1.26.4-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:1dda2e7b4ec9dd512f84935c5f126c8bd8b9f2fc001e9f54af255e8c5f16b0e0"},
    {file = "numpy-1.26.4-cp312-cp312-win32.whl", hash = "sha256:50193e430acfc1346175fcbdaa28ffec49947a06918b7b92130744e81e640110"},
    {file = "numpy-1.26.4-cp312-cp312-win_amd64.whl", hash = "sha256:08beddf13648eb95f8d867350f6a018a4be2e5ad54c8d8caed89ebca558b2818"},
    {file = "numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:7349ab0fa0c429c82442a27a9673fc802ffdb7c7775fad780226cb234965e53c"},
    {file = "numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:52b8b60467cd7dd1e9ed082188b4e6bb35aa5cdd01777621a1658910745b90be"},
    {file = "numpy-1.26.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764"},
    {file = "numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3"},
    {file = "numpy-1.26.4-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:679b0076f67ecc0138fd2ede3a8fd196dddc2ad3254069bcb9faf9a79b1cebcd"},
    {file = "numpy-1.26.4-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:47711010ad8555514b434df65f7d7b076bb8261df1ca9bb78f53d3b2db02e95c"},
    {file = "numpy-1.26.4-cp39-cp39-win32.whl", hash = "sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6"},
    {file = "numpy-1.26.4-cp39-cp39-win_amd64.whl", hash = "sha256:3373d5d70a5fe74a2c1bb6d2cfd9609ecf686d47a2d7b1d37a8f3b6bf6003aea"},
    {file = "numpy-1.26.4-pp39-pypy39_pp73-macosx_10_9_x86_64.whl", hash = "sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30"},
    {file = "numpy-1.26.4-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c"},
    {file = "numpy-1.26.4-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0"},
    {file = "numpy-1.26.4.tar.gz", hash = "sha256:2a02aba9ed12e4ac4eb3ea9421c420301a0c6460d9830d74a9df87efa4912010"},
]

[[package]]
name = "numpy"
version = "2.2.2"
description = "Fundamental package for array computing in Python"
optional = false
python-versions = ">=3.10"
files = [
    {file = "numpy-2.2.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:7079129b64cb78bdc8d611d1fd7e8002c0a2565da6a47c4df8062349fee90e3e"},
    {file = "numpy-2.2.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:2ec6c689c61df613b783aeb21f945c4cbe6c51c28cb70aae8430577ab39f163e"},
    {file = "numpy-2.2.2-cp310-cp310-macosx_14_0_arm64.whl", hash = "sha256:40c7ff5da22cd391944a28c6a9c638a5eef77fcf71d6e3a79e1d9d9e82752715"},
    {file = "numpy-2.2.2-cp310-cp310-macosx_14_0_x86_64.whl", hash = "sha256:995f9e8181723852ca458e22de5d9b7d3ba4da3f11cc1cb113f093b271d7965a"},
    {file = "numpy-2.2.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b78ea78450fd96a498f50ee096f69c75379af5138f7881a51355ab0e11286c97"},
    {file = "numpy-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3fbe72d347fbc59f94124125e73fc4976a06927ebc503ec5afbfb35f193cd957"},
    {file = "numpy-2.2.2-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:8e6da5cffbbe571f93588f562ed130ea63ee206d12851b60819512dd3e1ba50d"},
    {file = "numpy-2.2.2-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:09d6a2032faf25e8d0cadde7fd6145118ac55d2740132c1d845f98721b5ebcfd"},
    {file = "numpy-2.2.2-cp310-cp310-win32.whl", hash = "sha256:159ff6ee4c4a36a23fe01b7c3d07bd8c14cc433d9720f977fcd52c13c0098160"},
    {file = "numpy-2.2.2-cp310-cp310-win_amd64.whl", hash = "sha256:64bd6e1762cd7f0986a740fee4dff927b9ec2c5e4d9a28d056eb17d332158014"},
    {file = "numpy-2.2.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:642199e98af1bd2b6aeb8ecf726972d238c9877b0f6e8221ee5ab945ec8a2189"},
    {file = "numpy-2.2.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:6d9fc9d812c81e6168b6d405bf00b8d6739a7f72ef22a9214c4241e0dc70b323"},
    {file = "numpy-2.2.2-cp311-cp311-macosx_14_0_arm64.whl", hash = "sha256:c7d1fd447e33ee20c1f33f2c8e6634211124a9aabde3c617687d8b739aa69eac"},
    {file = "numpy-2.2.2-cp311-cp311-macosx_14_0_x86_64.whl", hash = "sha256:451e854cfae0febe723077bd0cf0a4302a5d84ff25f0bfece8f29206c7bed02e"},
    {file = "numpy-2.2.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bd249bc894af67cbd8bad2c22e7cbcd46cf87ddfca1f1289d1e7e54868cc785c"},
    {file = "numpy-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:02935e2c3c0c6cbe9c7955a8efa8908dd4221d7755644c59d1bba28b94fd334f"},
    {file = "numpy-2.2.2-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:a972cec723e0563aa0823ee2ab1df0cb196ed0778f173b381c871a03719d4826"},
    {file = "numpy-2.2.2-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:d6d6a0910c3b4368d89dde073e630882cdb266755565155bc33520283b2d9df8"},
    {file = "numpy-2.2.2-cp311-cp311-win32.whl", hash = "sha256:860fd59990c37c3ef913c3ae390b3929d005243acca1a86facb0773e2d8d9e50"},
    {file = "numpy-2.2.2-cp311-cp311-win_amd64.whl", hash = "sha256:da1eeb460ecce8d5b8608826595c777728cdf28ce7b5a5a8c8ac8d949beadcf2"},
    {file = "numpy-2.2.2-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:ac9bea18d6d58a995fac1b2cb4488e17eceeac413af014b1dd26170b766d8467"},
    {file = "numpy-2.2.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:23ae9f0c2d889b7b2d88a3791f6c09e2ef827c2446f1c4a3e3e76328ee4afd9a"},
    {file = "numpy-2.2.2-cp312-cp312-macosx_14_0_arm64.whl", hash = "sha256:3074634ea4d6df66be04f6728ee1d173cfded75d002c75fac79503a880bf3825"},
    {file = "numpy-2.2.2-cp312-cp312-macosx_14_0_x86_64.whl", hash = "sha256:8ec0636d3f7d68520afc6ac2dc4b8341ddb725039de042faf0e311599f54eb37"},
    {file = "numpy-2.2.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2ffbb1acd69fdf8e89dd60ef6182ca90a743620957afb7066385a7bbe88dc748"},
    {file = "numpy-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0349b025e15ea9d05c3d63f9657707a4e1d471128a3b1d876c095f328f8ff7f0"},
    {file = "numpy-2.2.2-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:463247edcee4a5537841d5350bc87fe8e92d7dd0e8c71c995d2c6eecb8208278"},
    {file = "numpy-2.2.2-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:9dd47ff0cb2a656ad69c38da850df3454da88ee9a6fde0ba79acceee0e79daba"},
    {file = "numpy-2.2.2-cp312-cp312-win32.whl", hash = "sha256:4525b88c11906d5ab1b0ec1f290996c0020dd318af8b49acaa46f198b1ffc283"},
    {file = "numpy-2.2.2-cp312-cp312-win_amd64.whl", hash = "sha256:5acea83b801e98541619af398cc0109ff48016955cc0818f478ee9ef1c5c3dcb"},
    {file = "numpy-2.2.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:b208cfd4f5fe34e1535c08983a1a6803fdbc7a1e86cf13dd0c61de0b51a0aadc"},
    {file = "numpy-2.2.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:d0bbe7dd86dca64854f4b6ce2ea5c60b51e36dfd597300057cf473d3615f2369"},
    {file = "numpy-2.2.2-cp313-cp313-macosx_14_0_arm64.whl", hash = "sha256:22ea3bb552ade325530e72a0c557cdf2dea8914d3a5e1fecf58fa5dbcc6f43cd"},
    {file = "numpy-2.2.2-cp313-cp313-macosx_14_0_x86_64.whl", hash = "sha256:128c41c085cab8a85dc29e66ed88c05613dccf6bc28b3866cd16050a2f5448be"},
    {file = "numpy-2.2.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:250c16b277e3b809ac20d1f590716597481061b514223c7badb7a0f9993c7f84"},
    {file = "numpy-2.2.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e0c8854b09bc4de7b041148d8550d3bd712b5c21ff6a8ed308085f190235d7ff"},
    {file = "numpy-2.2.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b6fb9c32a91ec32a689ec6410def76443e3c750e7cfc3fb2206b985ffb2b85f0"},
    {file = "numpy-2.2.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:57b4012e04cc12b78590a334907e01b3a85efb2107df2b8733ff1ed05fce71de"},
    {file = "numpy-2.2.2-cp313-cp313-win32.whl", hash = "sha256:4dbd80e453bd34bd003b16bd802fac70ad76bd463f81f0c518d1245b1c55e3d9"},
    {file = "numpy-2.2.2-cp313-cp313-win_amd64.whl", hash = "sha256:5a8c863ceacae696aff37d1fd636121f1a512117652e5dfb86031c8d84836369"},
    {file = "numpy-2.2.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:b3482cb7b3325faa5f6bc179649406058253d91ceda359c104dac0ad320e1391"},
    {file = "numpy-2.2.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:9491100aba630910489c1d0158034e1c9a6546f0b1340f716d522dc103788e39"},
    {file = "numpy-2.2.2-cp313-cp313t-macosx_14_0_arm64.whl", hash = "sha256:41184c416143defa34cc8eb9d070b0a5ba4f13a0fa96a709e20584638254b317"},
    {file = "numpy-2.2.2-cp313-cp313t-macosx_14_0_x86_64.whl", hash = "sha256:7dca87ca328f5ea7dafc907c5ec100d187911f94825f8700caac0b3f4c384b49"},
    {file = "numpy-2.2.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0bc61b307655d1a7f9f4b043628b9f2b721e80839914ede634e3d485913e1fb2"},
    {file = "numpy-2.2.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fad446ad0bc886855ddf5909cbf8cb5d0faa637aaa6277fb4b19ade134ab3c7"},
    {file = "numpy-2.2.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:149d1113ac15005652e8d0d3f6fd599360e1a708a4f98e43c9c77834a28238cb"},
    {file = "numpy-2.2.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:106397dbbb1896f99e044efc90360d098b3335060375c26aa89c0d8a97c5f648"},
    {file = "numpy-2.2.2-cp313-cp313t-win32.whl", hash = "sha256:0eec19f8af947a61e968d5429f0bd92fec46d92b0008d0a6685b40d6adf8a4f4"},
    {file = "numpy-2.2.2-cp313-cp313t-win_amd64.whl", hash = "sha256:97b974d3ba0fb4612b77ed35d7627490e8e3dff56ab41454d9e8b23448940576"},
    {file = "numpy-2.2.2-pp310-pypy310_pp73-macosx_10_15_x86_64.whl", hash = "sha256:b0531f0b0e07643eb089df4c509d30d72c9ef40defa53e41363eca8a8cc61495"},
    {file = "numpy-2.2.2-pp310-pypy310_pp73-macosx_14_0_x86_64.whl", hash = "sha256:e9e82dcb3f2ebbc8cb5ce1102d5f1c5ed236bf8a11730fb45ba82e2841ec21df"},
    {file = "numpy-2.2.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e0d4142eb40ca6f94539e4db929410f2a46052a0fe7a2c1c59f6179c39938d2a"},
    {file = "numpy-2.2.2-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:356ca982c188acbfa6af0d694284d8cf20e95b1c3d0aefa8929376fea9146f60"},
    {file = "numpy-2.2.2.tar.gz", hash = "sha256:ed6906f61834d687738d25988ae117683705636936cc605be0bb208b23df4d8f"},
]

[[package]]
name = "openai"
version = "1.60.1"
description = "The official Python library for the openai API"
optional = false
python-versions = ">=3.8"
files = [
    {file = "openai-1.60.1-py3-none-any.whl", hash = "sha256:714181ec1c452353d456f143c22db892de7b373e3165063d02a2b798ed575ba1"},
    {file = "openai-1.60.1.tar.gz", hash = "sha256:beb1541dfc38b002bd629ab68b0d6fe35b870c5f4311d9bc4404d85af3214d5e"},
]

[package.dependencies]
anyio = ">=3.5.0,<5"
distro = ">=1.7.0,<2"
httpx = ">=0.23.0,<1"
jiter = ">=0.4.0,<1"
pydantic = ">=1.9.0,<3"
sniffio = "*"
tqdm = ">4"
typing-extensions = ">=4.11,<5"

[package.extras]
datalib = ["numpy (>=1)", "pandas (>=1.2.3)", "pandas-stubs (>=1.1.0.11)"]
realtime = ["websockets (>=13,<15)"]

[[package]]
name = "orjson"
version = "3.10.15"
description = "Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy"
optional = false
python-versions = ">=3.8"
files = [
    {file = "orjson-3.10.15-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:552c883d03ad185f720d0c09583ebde257e41b9521b74ff40e08b7dec4559c04"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:616e3e8d438d02e4854f70bfdc03a6bcdb697358dbaa6bcd19cbe24d24ece1f8"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:7c2c79fa308e6edb0ffab0a31fd75a7841bf2a79a20ef08a3c6e3b26814c8ca8"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:73cb85490aa6bf98abd20607ab5c8324c0acb48d6da7863a51be48505646c814"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:763dadac05e4e9d2bc14938a45a2d0560549561287d41c465d3c58aec818b164"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a330b9b4734f09a623f74a7490db713695e13b67c959713b78369f26b3dee6bf"},
    {file = "orjson-3.10.15-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:a61a4622b7ff861f019974f73d8165be1bd9a0855e1cad18ee167acacabeb061"},
    {file = "orjson-3.10.15-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:acd271247691574416b3228db667b84775c497b245fa275c6ab90dc1ffbbd2b3"},
    {file = "orjson-3.10.15-cp310-cp310-musllinux_1_2_armv7l.whl", hash = "sha256:e4759b109c37f635aa5c5cc93a1b26927bfde24b254bcc0e1149a9fada253d2d"},
    {file = "orjson-3.10.15-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:9e992fd5cfb8b9f00bfad2fd7a05a4299db2bbe92e6440d9dd2fab27655b3182"},
    {file = "orjson-3.10.15-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:f95fb363d79366af56c3f26b71df40b9a583b07bbaaf5b317407c4d58497852e"},
    {file = "orjson-3.10.15-cp310-cp310-win32.whl", hash = "sha256:f9875f5fea7492da8ec2444839dcc439b0ef298978f311103d0b7dfd775898ab"},
    {file = "orjson-3.10.15-cp310-cp310-win_amd64.whl", hash = "sha256:17085a6aa91e1cd70ca8533989a18b5433e15d29c574582f76f821737c8d5806"},
    {file = "orjson-3.10.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:c4cc83960ab79a4031f3119cc4b1a1c627a3dc09df125b27c4201dff2af7eaa6"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ddbeef2481d895ab8be5185f2432c334d6dec1f5d1933a9c83014d188e102cef"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:9e590a0477b23ecd5b0ac865b1b907b01b3c5535f5e8a8f6ab0e503efb896334"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:a6be38bd103d2fd9bdfa31c2720b23b5d47c6796bcb1d1b598e3924441b4298d"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:ff4f6edb1578960ed628a3b998fa54d78d9bb3e2eb2cfc5c2a09732431c678d0"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b0482b21d0462eddd67e7fce10b89e0b6ac56570424662b685a0d6fccf581e13"},
    {file = "orjson-3.10.15-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:bb5cc3527036ae3d98b65e37b7986a918955f85332c1ee07f9d3f82f3a6899b5"},
    {file = "orjson-3.10.15-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:d569c1c462912acdd119ccbf719cf7102ea2c67dd03b99edcb1a3048651ac96b"},
    {file = "orjson-3.10.15-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:1e6d33efab6b71d67f22bf2962895d3dc6f82a6273a965fab762e64fa90dc399"},
    {file = "orjson-3.10.15-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:c33be3795e299f565681d69852ac8c1bc5c84863c0b0030b2b3468843be90388"},
    {file = "orjson-3.10.15-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:eea80037b9fae5339b214f59308ef0589fc06dc870578b7cce6d71eb2096764c"},
    {file = "orjson-3.10.15-cp311-cp311-win32.whl", hash = "sha256:d5ac11b659fd798228a7adba3e37c010e0152b78b1982897020a8e019a94882e"},
    {file = "orjson-3.10.15-cp311-cp311-win_amd64.whl", hash = "sha256:cf45e0214c593660339ef63e875f32ddd5aa3b4adc15e662cdb80dc49e194f8e"},
    {file = "orjson-3.10.15-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:9d11c0714fc85bfcf36ada1179400862da3288fc785c30e8297844c867d7505a"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dba5a1e85d554e3897fa9fe6fbcff2ed32d55008973ec9a2b992bd9a65d2352d"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:7723ad949a0ea502df656948ddd8b392780a5beaa4c3b5f97e525191b102fff0"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6fd9bc64421e9fe9bd88039e7ce8e58d4fead67ca88e3a4014b143cec7684fd4"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:dadba0e7b6594216c214ef7894c4bd5f08d7c0135f4dd0145600be4fbcc16767"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b48f59114fe318f33bbaee8ebeda696d8ccc94c9e90bc27dbe72153094e26f41"},
    {file = "orjson-3.10.15-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:035fb83585e0f15e076759b6fedaf0abb460d1765b6a36f48018a52858443514"},
    {file = "orjson-3.10.15-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:d13b7fe322d75bf84464b075eafd8e7dd9eae05649aa2a5354cfa32f43c59f17"},
    {file = "orjson-3.10.15-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:7066b74f9f259849629e0d04db6609db4cf5b973248f455ba5d3bd58a4daaa5b"},
    {file = "orjson-3.10.15-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:88dc3f65a026bd3175eb157fea994fca6ac7c4c8579fc5a86fc2114ad05705b7"},
    {file = "orjson-3.10.15-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:b342567e5465bd99faa559507fe45e33fc76b9fb868a63f1642c6bc0735ad02a"},
    {file = "orjson-3.10.15-cp312-cp312-win32.whl", hash = "sha256:0a4f27ea5617828e6b58922fdbec67b0aa4bb844e2d363b9244c47fa2180e665"},
    {file = "orjson-3.10.15-cp312-cp312-win_amd64.whl", hash = "sha256:ef5b87e7aa9545ddadd2309efe6824bd3dd64ac101c15dae0f2f597911d46eaa"},
    {file = "orjson-3.10.15-cp313-cp313-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:bae0e6ec2b7ba6895198cd981b7cca95d1487d0147c8ed751e5632ad16f031a6"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f93ce145b2db1252dd86af37d4165b6faa83072b46e3995ecc95d4b2301b725a"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:7c203f6f969210128af3acae0ef9ea6aab9782939f45f6fe02d05958fe761ef9"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8918719572d662e18b8af66aef699d8c21072e54b6c82a3f8f6404c1f5ccd5e0"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f71eae9651465dff70aa80db92586ad5b92df46a9373ee55252109bb6b703307"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:e117eb299a35f2634e25ed120c37c641398826c2f5a3d3cc39f5993b96171b9e"},
    {file = "orjson-3.10.15-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:13242f12d295e83c2955756a574ddd6741c81e5b99f2bef8ed8d53e47a01e4b7"},
    {file = "orjson-3.10.15-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:7946922ada8f3e0b7b958cc3eb22cfcf6c0df83d1fe5521b4a100103e3fa84c8"},
    {file = "orjson-3.10.15-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:b7155eb1623347f0f22c38c9abdd738b287e39b9982e1da227503387b81b34ca"},
    {file = "orjson-3.10.15-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:208beedfa807c922da4e81061dafa9c8489c6328934ca2a562efa707e049e561"},
    {file = "orjson-3.10.15-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:eca81f83b1b8c07449e1d6ff7074e82e3fd6777e588f1a6632127f286a968825"},
    {file = "orjson-3.10.15-cp313-cp313-win32.whl", hash = "sha256:c03cd6eea1bd3b949d0d007c8d57049aa2b39bd49f58b4b2af571a5d3833d890"},
    {file = "orjson-3.10.15-cp313-cp313-win_amd64.whl", hash = "sha256:fd56a26a04f6ba5fb2045b0acc487a63162a958ed837648c5781e1fe3316cfbf"},
    {file = "orjson-3.10.15-cp38-cp38-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:5e8afd6200e12771467a1a44e5ad780614b86abb4b11862ec54861a82d677746"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:da9a18c500f19273e9e104cca8c1f0b40a6470bcccfc33afcc088045d0bf5ea6"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:bb00b7bfbdf5d34a13180e4805d76b4567025da19a197645ca746fc2fb536586"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:33aedc3d903378e257047fee506f11e0833146ca3e57a1a1fb0ddb789876c1e1"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:dd0099ae6aed5eb1fc84c9eb72b95505a3df4267e6962eb93cdd5af03be71c98"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7c864a80a2d467d7786274fce0e4f93ef2a7ca4ff31f7fc5634225aaa4e9e98c"},
    {file = "orjson-3.10.15-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:c25774c9e88a3e0013d7d1a6c8056926b607a61edd423b50eb5c88fd7f2823ae"},
    {file = "orjson-3.10.15-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:e78c211d0074e783d824ce7bb85bf459f93a233eb67a5b5003498232ddfb0e8a"},
    {file = "orjson-3.10.15-cp38-cp38-musllinux_1_2_armv7l.whl", hash = "sha256:43e17289ffdbbac8f39243916c893d2ae41a2ea1a9cbb060a56a4d75286351ae"},
    {file = "orjson-3.10.15-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:781d54657063f361e89714293c095f506c533582ee40a426cb6489c48a637b81"},
    {file = "orjson-3.10.15-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:6875210307d36c94873f553786a808af2788e362bd0cf4c8e66d976791e7b528"},
    {file = "orjson-3.10.15-cp38-cp38-win32.whl", hash = "sha256:305b38b2b8f8083cc3d618927d7f424349afce5975b316d33075ef0f73576b60"},
    {file = "orjson-3.10.15-cp38-cp38-win_amd64.whl", hash = "sha256:5dd9ef1639878cc3efffed349543cbf9372bdbd79f478615a1c633fe4e4180d1"},
    {file = "orjson-3.10.15-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl", hash = "sha256:ffe19f3e8d68111e8644d4f4e267a069ca427926855582ff01fc012496d19969"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d433bf32a363823863a96561a555227c18a522a8217a6f9400f00ddc70139ae2"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:da03392674f59a95d03fa5fb9fe3a160b0511ad84b7a3914699ea5a1b3a38da2"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:3a63bb41559b05360ded9132032239e47983a39b151af1201f07ec9370715c82"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3766ac4702f8f795ff3fa067968e806b4344af257011858cc3d6d8721588b53f"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7a1c73dcc8fadbd7c55802d9aa093b36878d34a3b3222c41052ce6b0fc65f8e8"},
    {file = "orjson-3.10.15-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:b299383825eafe642cbab34be762ccff9fd3408d72726a6b2a4506d410a71ab3"},
    {file = "orjson-3.10.15-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:abc7abecdbf67a173ef1316036ebbf54ce400ef2300b4e26a7b843bd446c2480"},
    {file = "orjson-3.10.15-cp39-cp39-musllinux_1_2_armv7l.whl", hash = "sha256:3614ea508d522a621384c1d6639016a5a2e4f027f3e4a1c93a51867615d28829"},
    {file = "orjson-3.10.15-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:295c70f9dc154307777ba30fe29ff15c1bcc9dfc5c48632f37d20a607e9ba85a"},
    {file = "orjson-3.10.15-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:63309e3ff924c62404923c80b9e2048c1f74ba4b615e7584584389ada50ed428"},
    {file = "orjson-3.10.15-cp39-cp39-win32.whl", hash = "sha256:a2f708c62d026fb5340788ba94a55c23df4e1869fec74be455e0b2f5363b8507"},
    {file = "orjson-3.10.15-cp39-cp39-win_amd64.whl", hash = "sha256:efcf6c735c3d22ef60c4aa27a5238f1a477df85e9b15f2142f9d669beb2d13fd"},
    {file = "orjson-3.10.15.tar.gz", hash = "sha256:05ca7fe452a2e9d8d9d706a2984c95b9c2ebc5db417ce0b7a49b91d50642a23e"},
]

[[package]]
name = "packaging"
version = "24.2"
description = "Core utilities for Python packages"
optional = false
python-versions = ">=3.8"
files = [
    {file = "packaging-24.2-py3-none-any.whl", hash = "sha256:09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759"},
    {file = "packaging-24.2.tar.gz", hash = "sha256:c228a6dc5e932d346bc5739379109d49e8853dd8223571c7c5b55260edc0b97f"},
]

[[package]]
name = "propcache"
version = "0.2.1"
description = "Accelerated property cache"
optional = false
python-versions = ">=3.9"
files = [
    {file = "propcache-0.2.1-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:6b3f39a85d671436ee3d12c017f8fdea38509e4f25b28eb25877293c98c243f6"},
    {file = "propcache-0.2.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:39d51fbe4285d5db5d92a929e3e21536ea3dd43732c5b177c7ef03f918dff9f2"},
    {file = "propcache-0.2.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:6445804cf4ec763dc70de65a3b0d9954e868609e83850a47ca4f0cb64bd79fea"},
    {file = "propcache-0.2.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:f9479aa06a793c5aeba49ce5c5692ffb51fcd9a7016e017d555d5e2b0045d212"},
    {file = "propcache-0.2.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d9631c5e8b5b3a0fda99cb0d29c18133bca1e18aea9effe55adb3da1adef80d3"},
    {file = "propcache-0.2.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3156628250f46a0895f1f36e1d4fbe062a1af8718ec3ebeb746f1d23f0c5dc4d"},
    {file = "propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6b6fb63ae352e13748289f04f37868099e69dba4c2b3e271c46061e82c745634"},
    {file = "propcache-0.2.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:887d9b0a65404929641a9fabb6452b07fe4572b269d901d622d8a34a4e9043b2"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:a96dc1fa45bd8c407a0af03b2d5218392729e1822b0c32e62c5bf7eeb5fb3958"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_armv7l.whl", hash = "sha256:a7e65eb5c003a303b94aa2c3852ef130230ec79e349632d030e9571b87c4698c"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:999779addc413181912e984b942fbcc951be1f5b3663cd80b2687758f434c583"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:19a0f89a7bb9d8048d9c4370c9c543c396e894c76be5525f5e1ad287f1750ddf"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:1ac2f5fe02fa75f56e1ad473f1175e11f475606ec9bd0be2e78e4734ad575034"},
    {file = "propcache-0.2.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:574faa3b79e8ebac7cb1d7930f51184ba1ccf69adfdec53a12f319a06030a68b"},
    {file = "propcache-0.2.1-cp310-cp310-win32.whl", hash = "sha256:03ff9d3f665769b2a85e6157ac8b439644f2d7fd17615a82fa55739bc97863f4"},
    {file = "propcache-0.2.1-cp310-cp310-win_amd64.whl", hash = "sha256:2d3af2e79991102678f53e0dbf4c35de99b6b8b58f29a27ca0325816364caaba"},
    {file = "propcache-0.2.1-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:1ffc3cca89bb438fb9c95c13fc874012f7b9466b89328c3c8b1aa93cdcfadd16"},
    {file = "propcache-0.2.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:f174bbd484294ed9fdf09437f889f95807e5f229d5d93588d34e92106fbf6717"},
    {file = "propcache-0.2.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:70693319e0b8fd35dd863e3e29513875eb15c51945bf32519ef52927ca883bc3"},
    {file = "propcache-0.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b480c6a4e1138e1aa137c0079b9b6305ec6dcc1098a8ca5196283e8a49df95a9"},
    {file = "propcache-0.2.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d27b84d5880f6d8aa9ae3edb253c59d9f6642ffbb2c889b78b60361eed449787"},
    {file = "propcache-0.2.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:857112b22acd417c40fa4595db2fe28ab900c8c5fe4670c7989b1c0230955465"},
    {file = "propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cf6c4150f8c0e32d241436526f3c3f9cbd34429492abddbada2ffcff506c51af"},
    {file = "propcache-0.2.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:66d4cfda1d8ed687daa4bc0274fcfd5267873db9a5bc0418c2da19273040eeb7"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:c2f992c07c0fca81655066705beae35fc95a2fa7366467366db627d9f2ee097f"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:4a571d97dbe66ef38e472703067021b1467025ec85707d57e78711c085984e54"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:bb6178c241278d5fe853b3de743087be7f5f4c6f7d6d22a3b524d323eecec505"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:ad1af54a62ffe39cf34db1aa6ed1a1873bd548f6401db39d8e7cd060b9211f82"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:e7048abd75fe40712005bcfc06bb44b9dfcd8e101dda2ecf2f5aa46115ad07ca"},
    {file = "propcache-0.2.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:160291c60081f23ee43d44b08a7e5fb76681221a8e10b3139618c5a9a291b84e"},
    {file = "propcache-0.2.1-cp311-cp311-win32.whl", hash = "sha256:819ce3b883b7576ca28da3861c7e1a88afd08cc8c96908e08a3f4dd64a228034"},
    {file = "propcache-0.2.1-cp311-cp311-win_amd64.whl", hash = "sha256:edc9fc7051e3350643ad929df55c451899bb9ae6d24998a949d2e4c87fb596d3"},
    {file = "propcache-0.2.1-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:081a430aa8d5e8876c6909b67bd2d937bfd531b0382d3fdedb82612c618bc41a"},
    {file = "propcache-0.2.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:d2ccec9ac47cf4e04897619c0e0c1a48c54a71bdf045117d3a26f80d38ab1fb0"},
    {file = "propcache-0.2.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:14d86fe14b7e04fa306e0c43cdbeebe6b2c2156a0c9ce56b815faacc193e320d"},
    {file = "propcache-0.2.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:049324ee97bb67285b49632132db351b41e77833678432be52bdd0289c0e05e4"},
    {file = "propcache-0.2.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1cd9a1d071158de1cc1c71a26014dcdfa7dd3d5f4f88c298c7f90ad6f27bb46d"},
    {file = "propcache-0.2.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:98110aa363f1bb4c073e8dcfaefd3a5cea0f0834c2aab23dda657e4dab2f53b5"},
    {file = "propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:647894f5ae99c4cf6bb82a1bb3a796f6e06af3caa3d32e26d2350d0e3e3faf24"},
    {file = "propcache-0.2.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:bfd3223c15bebe26518d58ccf9a39b93948d3dcb3e57a20480dfdd315356baff"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:d71264a80f3fcf512eb4f18f59423fe82d6e346ee97b90625f283df56aee103f"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:e73091191e4280403bde6c9a52a6999d69cdfde498f1fdf629105247599b57ec"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:3935bfa5fede35fb202c4b569bb9c042f337ca4ff7bd540a0aa5e37131659348"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:f508b0491767bb1f2b87fdfacaba5f7eddc2f867740ec69ece6d1946d29029a6"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:1672137af7c46662a1c2be1e8dc78cb6d224319aaa40271c9257d886be4363a6"},
    {file = "propcache-0.2.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:b74c261802d3d2b85c9df2dfb2fa81b6f90deeef63c2db9f0e029a3cac50b518"},
    {file = "propcache-0.2.1-cp312-cp312-win32.whl", hash = "sha256:d09c333d36c1409d56a9d29b3a1b800a42c76a57a5a8907eacdbce3f18768246"},
    {file = "propcache-0.2.1-cp312-cp312-win_amd64.whl", hash = "sha256:c214999039d4f2a5b2073ac506bba279945233da8c786e490d411dfc30f855c1"},
    {file = "propcache-0.2.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:aca405706e0b0a44cc6bfd41fbe89919a6a56999157f6de7e182a990c36e37bc"},
    {file = "propcache-0.2.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:12d1083f001ace206fe34b6bdc2cb94be66d57a850866f0b908972f90996b3e9"},
    {file = "propcache-0.2.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:d93f3307ad32a27bda2e88ec81134b823c240aa3abb55821a8da553eed8d9439"},
    {file = "propcache-0.2.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:ba278acf14471d36316159c94a802933d10b6a1e117b8554fe0d0d9b75c9d536"},
    {file = "propcache-0.2.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4e6281aedfca15301c41f74d7005e6e3f4ca143584ba696ac69df4f02f40d629"},
    {file = "propcache-0.2.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5b750a8e5a1262434fb1517ddf64b5de58327f1adc3524a5e44c2ca43305eb0b"},
    {file = "propcache-0.2.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bf72af5e0fb40e9babf594308911436c8efde3cb5e75b6f206c34ad18be5c052"},
    {file = "propcache-0.2.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:b2d0a12018b04f4cb820781ec0dffb5f7c7c1d2a5cd22bff7fb055a2cb19ebce"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:e800776a79a5aabdb17dcc2346a7d66d0777e942e4cd251defeb084762ecd17d"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:4160d9283bd382fa6c0c2b5e017acc95bc183570cd70968b9202ad6d8fc48dce"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:30b43e74f1359353341a7adb783c8f1b1c676367b011709f466f42fda2045e95"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:58791550b27d5488b1bb52bc96328456095d96206a250d28d874fafe11b3dfaf"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:0f022d381747f0dfe27e99d928e31bc51a18b65bb9e481ae0af1380a6725dd1f"},
    {file = "propcache-0.2.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:297878dc9d0a334358f9b608b56d02e72899f3b8499fc6044133f0d319e2ec30"},
    {file = "propcache-0.2.1-cp313-cp313-win32.whl", hash = "sha256:ddfab44e4489bd79bda09d84c430677fc7f0a4939a73d2bba3073036f487a0a6"},
    {file = "propcache-0.2.1-cp313-cp313-win_amd64.whl", hash = "sha256:556fc6c10989f19a179e4321e5d678db8eb2924131e64652a51fe83e4c3db0e1"},
    {file = "propcache-0.2.1-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:6a9a8c34fb7bb609419a211e59da8887eeca40d300b5ea8e56af98f6fbbb1541"},
    {file = "propcache-0.2.1-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:ae1aa1cd222c6d205853b3013c69cd04515f9d6ab6de4b0603e2e1c33221303e"},
    {file = "propcache-0.2.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:accb6150ce61c9c4b7738d45550806aa2b71c7668c6942f17b0ac182b6142fd4"},
    {file = "propcache-0.2.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5eee736daafa7af6d0a2dc15cc75e05c64f37fc37bafef2e00d77c14171c2097"},
    {file = "propcache-0.2.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f7a31fc1e1bd362874863fdeed71aed92d348f5336fd84f2197ba40c59f061bd"},
    {file = "propcache-0.2.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:cba4cfa1052819d16699e1d55d18c92b6e094d4517c41dd231a8b9f87b6fa681"},
    {file = "propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f089118d584e859c62b3da0892b88a83d611c2033ac410e929cb6754eec0ed16"},
    {file = "propcache-0.2.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:781e65134efaf88feb447e8c97a51772aa75e48b794352f94cb7ea717dedda0d"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:31f5af773530fd3c658b32b6bdc2d0838543de70eb9a2156c03e410f7b0d3aae"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_armv7l.whl", hash = "sha256:a7a078f5d37bee6690959c813977da5291b24286e7b962e62a94cec31aa5188b"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:cea7daf9fc7ae6687cf1e2c049752f19f146fdc37c2cc376e7d0032cf4f25347"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:8b3489ff1ed1e8315674d0775dc7d2195fb13ca17b3808721b54dbe9fd020faf"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:9403db39be1393618dd80c746cb22ccda168efce239c73af13c3763ef56ffc04"},
    {file = "propcache-0.2.1-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:5d97151bc92d2b2578ff7ce779cdb9174337390a535953cbb9452fb65164c587"},
    {file = "propcache-0.2.1-cp39-cp39-win32.whl", hash = "sha256:9caac6b54914bdf41bcc91e7eb9147d331d29235a7c967c150ef5df6464fd1bb"},
    {file = "propcache-0.2.1-cp39-cp39-win_amd64.whl", hash = "sha256:92fc4500fcb33899b05ba73276dfb684a20d31caa567b7cb5252d48f896a91b1"},
    {file = "propcache-0.2.1-py3-none-any.whl", hash = "sha256:52277518d6aae65536e9cea52d4e7fd2f7a66f4aa2d30ed3f2fcea620ace3c54"},
    {file = "propcache-0.2.1.tar.gz", hash = "sha256:3f77ce728b19cb537714499928fe800c3dda29e8d9428778fc7c186da4c09a64"},
]

[[package]]
name = "pycparser"
version = "2.22"
description = "C parser in Python"
optional = false
python-versions = ">=3.8"
files = [
    {file = "pycparser-2.22-py3-none-any.whl", hash = "sha256:c3702b6d3dd8c7abc1afa565d7e63d53a1d0bd86cdc24edd75470f4de499cfcc"},
    {file = "pycparser-2.22.tar.gz", hash = "sha256:491c8be9c040f5390f5bf44a5b07752bd07f56edf992381b05c701439eec10f6"},
]

[[package]]
name = "pydantic"
version = "2.10.6"
description = "Data validation using Python type hints"
optional = false
python-versions = ">=3.8"
files = [
    {file = "pydantic-2.10.6-py3-none-any.whl", hash = "sha256:427d664bf0b8a2b34ff5dd0f5a18df00591adcee7198fbd71981054cef37b584"},
    {file = "pydantic-2.10.6.tar.gz", hash = "sha256:ca5daa827cce33de7a42be142548b0096bf05a7e7b365aebfa5f8eeec7128236"},
]

[package.dependencies]
annotated-types = ">=0.6.0"
pydantic-core = "2.27.2"
typing-extensions = ">=4.12.2"

[package.extras]
email = ["email-validator (>=2.0.0)"]
timezone = ["tzdata"]

[[package]]
name = "pydantic-core"
version = "2.27.2"
description = "Core functionality for Pydantic validation and serialization"
optional = false
python-versions = ">=3.8"
files = [
    {file = "pydantic_core-2.27.2-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:2d367ca20b2f14095a8f4fa1210f5a7b78b8a20009ecced6b12818f455b1e9fa"},
    {file = "pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:491a2b73db93fab69731eaee494f320faa4e093dbed776be1a829c2eb222c34c"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:7969e133a6f183be60e9f6f56bfae753585680f3b7307a8e555a948d443cc05a"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:3de9961f2a346257caf0aa508a4da705467f53778e9ef6fe744c038119737ef5"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e2bb4d3e5873c37bb3dd58714d4cd0b0e6238cebc4177ac8fe878f8b3aa8e74c"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:280d219beebb0752699480fe8f1dc61ab6615c2046d76b7ab7ee38858de0a4e7"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:47956ae78b6422cbd46f772f1746799cbb862de838fd8d1fbd34a82e05b0983a"},
    {file = "pydantic_core-2.27.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:14d4a5c49d2f009d62a2a7140d3064f686d17a5d1a268bc641954ba181880236"},
    {file = "pydantic_core-2.27.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:337b443af21d488716f8d0b6164de833e788aa6bd7e3a39c005febc1284f4962"},
    {file = "pydantic_core-2.27.2-cp310-cp310-musllinux_1_1_armv7l.whl", hash = "sha256:03d0f86ea3184a12f41a2d23f7ccb79cdb5a18e06993f8a45baa8dfec746f0e9"},
    {file = "pydantic_core-2.27.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:7041c36f5680c6e0f08d922aed302e98b3745d97fe1589db0a3eebf6624523af"},
    {file = "pydantic_core-2.27.2-cp310-cp310-win32.whl", hash = "sha256:50a68f3e3819077be2c98110c1f9dcb3817e93f267ba80a2c05bb4f8799e2ff4"},
    {file = "pydantic_core-2.27.2-cp310-cp310-win_amd64.whl", hash = "sha256:e0fd26b16394ead34a424eecf8a31a1f5137094cabe84a1bcb10fa6ba39d3d31"},
    {file = "pydantic_core-2.27.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:8e10c99ef58cfdf2a66fc15d66b16c4a04f62bca39db589ae8cba08bc55331bc"},
    {file = "pydantic_core-2.27.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:26f32e0adf166a84d0cb63be85c562ca8a6fa8de28e5f0d92250c6b7e9e2aff7"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8c19d1ea0673cd13cc2f872f6c9ab42acc4e4f492a7ca9d3795ce2b112dd7e15"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:5e68c4446fe0810e959cdff46ab0a41ce2f2c86d227d96dc3847af0ba7def306"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d9640b0059ff4f14d1f37321b94061c6db164fbe49b334b31643e0528d100d99"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:40d02e7d45c9f8af700f3452f329ead92da4c5f4317ca9b896de7ce7199ea459"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1c1fd185014191700554795c99b347d64f2bb637966c4cfc16998a0ca700d048"},
    {file = "pydantic_core-2.27.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d81d2068e1c1228a565af076598f9e7451712700b673de8f502f0334f281387d"},
    {file = "pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:1a4207639fb02ec2dbb76227d7c751a20b1a6b4bc52850568e52260cae64ca3b"},
    {file = "pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:3de3ce3c9ddc8bbd88f6e0e304dea0e66d843ec9de1b0042b0911c1663ffd474"},
    {file = "pydantic_core-2.27.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:30c5f68ded0c36466acede341551106821043e9afaad516adfb6e8fa80a4e6a6"},
    {file = "pydantic_core-2.27.2-cp311-cp311-win32.whl", hash = "sha256:c70c26d2c99f78b125a3459f8afe1aed4d9687c24fd677c6a4436bc042e50d6c"},
    {file = "pydantic_core-2.27.2-cp311-cp311-win_amd64.whl", hash = "sha256:08e125dbdc505fa69ca7d9c499639ab6407cfa909214d500897d02afb816e7cc"},
    {file = "pydantic_core-2.27.2-cp311-cp311-win_arm64.whl", hash = "sha256:26f0d68d4b235a2bae0c3fc585c585b4ecc51382db0e3ba402a22cbc440915e4"},
    {file = "pydantic_core-2.27.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:9e0c8cfefa0ef83b4da9588448b6d8d2a2bf1a53c3f1ae5fca39eb3061e2f0b0"},
    {file = "pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:83097677b8e3bd7eaa6775720ec8e0405f1575015a463285a92bfdfe254529ef"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:172fce187655fece0c90d90a678424b013f8fbb0ca8b036ac266749c09438cb7"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:519f29f5213271eeeeb3093f662ba2fd512b91c5f188f3bb7b27bc5973816934"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:05e3a55d124407fffba0dd6b0c0cd056d10e983ceb4e5dbd10dda135c31071d6"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9c3ed807c7b91de05e63930188f19e921d1fe90de6b4f5cd43ee7fcc3525cb8c"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6fb4aadc0b9a0c063206846d603b92030eb6f03069151a625667f982887153e2"},
    {file = "pydantic_core-2.27.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:28ccb213807e037460326424ceb8b5245acb88f32f3d2777427476e1b32c48c4"},
    {file = "pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:de3cd1899e2c279b140adde9357c4495ed9d47131b4a4eaff9052f23398076b3"},
    {file = "pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:220f892729375e2d736b97d0e51466252ad84c51857d4d15f5e9692f9ef12be4"},
    {file = "pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:a0fcd29cd6b4e74fe8ddd2c90330fd8edf2e30cb52acda47f06dd615ae72da57"},
    {file = "pydantic_core-2.27.2-cp312-cp312-win32.whl", hash = "sha256:1e2cb691ed9834cd6a8be61228471d0a503731abfb42f82458ff27be7b2186fc"},
    {file = "pydantic_core-2.27.2-cp312-cp312-win_amd64.whl", hash = "sha256:cc3f1a99a4f4f9dd1de4fe0312c114e740b5ddead65bb4102884b384c15d8bc9"},
    {file = "pydantic_core-2.27.2-cp312-cp312-win_arm64.whl", hash = "sha256:3911ac9284cd8a1792d3cb26a2da18f3ca26c6908cc434a18f730dc0db7bfa3b"},
    {file = "pydantic_core-2.27.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:7d14bd329640e63852364c306f4d23eb744e0f8193148d4044dd3dacdaacbd8b"},
    {file = "pydantic_core-2.27.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:82f91663004eb8ed30ff478d77c4d1179b3563df6cdb15c0817cd1cdaf34d154"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:71b24c7d61131bb83df10cc7e687433609963a944ccf45190cfc21e0887b08c9"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:fa8e459d4954f608fa26116118bb67f56b93b209c39b008277ace29937453dc9"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ce8918cbebc8da707ba805b7fd0b382816858728ae7fe19a942080c24e5b7cd1"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:eda3f5c2a021bbc5d976107bb302e0131351c2ba54343f8a496dc8783d3d3a6a"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bd8086fa684c4775c27f03f062cbb9eaa6e17f064307e86b21b9e0abc9c0f02e"},
    {file = "pydantic_core-2.27.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:8d9b3388db186ba0c099a6d20f0604a44eabdeef1777ddd94786cdae158729e4"},
    {file = "pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:7a66efda2387de898c8f38c0cf7f14fca0b51a8ef0b24bfea5849f1b3c95af27"},
    {file = "pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:18a101c168e4e092ab40dbc2503bdc0f62010e95d292b27827871dc85450d7ee"},
    {file = "pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ba5dd002f88b78a4215ed2f8ddbdf85e8513382820ba15ad5ad8955ce0ca19a1"},
    {file = "pydantic_core-2.27.2-cp313-cp313-win32.whl", hash = "sha256:1ebaf1d0481914d004a573394f4be3a7616334be70261007e47c2a6fe7e50130"},
    {file = "pydantic_core-2.27.2-cp313-cp313-win_amd64.whl", hash = "sha256:953101387ecf2f5652883208769a79e48db18c6df442568a0b5ccd8c2723abee"},
    {file = "pydantic_core-2.27.2-cp313-cp313-win_arm64.whl", hash = "sha256:ac4dbfd1691affb8f48c2c13241a2e3b60ff23247cbcf981759c768b6633cf8b"},
    {file = "pydantic_core-2.27.2-cp38-cp38-macosx_10_12_x86_64.whl", hash = "sha256:d3e8d504bdd3f10835468f29008d72fc8359d95c9c415ce6e767203db6127506"},
    {file = "pydantic_core-2.27.2-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:521eb9b7f036c9b6187f0b47318ab0d7ca14bd87f776240b90b21c1f4f149320"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:85210c4d99a0114f5a9481b44560d7d1e35e32cc5634c656bc48e590b669b145"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d716e2e30c6f140d7560ef1538953a5cd1a87264c737643d481f2779fc247fe1"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f66d89ba397d92f840f8654756196d93804278457b5fbede59598a1f9f90b228"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:669e193c1c576a58f132e3158f9dfa9662969edb1a250c54d8fa52590045f046"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdbe7629b996647b99c01b37f11170a57ae675375b14b8c13b8518b8320ced5"},
    {file = "pydantic_core-2.27.2-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d262606bf386a5ba0b0af3b97f37c83d7011439e3dc1a9298f21efb292e42f1a"},
    {file = "pydantic_core-2.27.2-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:cabb9bcb7e0d97f74df8646f34fc76fbf793b7f6dc2438517d7a9e50eee4f14d"},
    {file = "pydantic_core-2.27.2-cp38-cp38-musllinux_1_1_armv7l.whl", hash = "sha256:d2d63f1215638d28221f664596b1ccb3944f6e25dd18cd3b86b0a4c408d5ebb9"},
    {file = "pydantic_core-2.27.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:bca101c00bff0adb45a833f8451b9105d9df18accb8743b08107d7ada14bd7da"},
    {file = "pydantic_core-2.27.2-cp38-cp38-win32.whl", hash = "sha256:f6f8e111843bbb0dee4cb6594cdc73e79b3329b526037ec242a3e49012495b3b"},
    {file = "pydantic_core-2.27.2-cp38-cp38-win_amd64.whl", hash = "sha256:fd1aea04935a508f62e0d0ef1f5ae968774a32afc306fb8545e06f5ff5cdf3ad"},
    {file = "pydantic_core-2.27.2-cp39-cp39-macosx_10_12_x86_64.whl", hash = "sha256:c10eb4f1659290b523af58fa7cffb452a61ad6ae5613404519aee4bfbf1df993"},
    {file = "pydantic_core-2.27.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:ef592d4bad47296fb11f96cd7dc898b92e795032b4894dfb4076cfccd43a9308"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c61709a844acc6bf0b7dce7daae75195a10aac96a596ea1b776996414791ede4"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:42c5f762659e47fdb7b16956c71598292f60a03aa92f8b6351504359dbdba6cf"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4c9775e339e42e79ec99c441d9730fccf07414af63eac2f0e48e08fd38a64d76"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:57762139821c31847cfb2df63c12f725788bd9f04bc2fb392790959b8f70f118"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0d1e85068e818c73e048fe28cfc769040bb1f475524f4745a5dc621f75ac7630"},
    {file = "pydantic_core-2.27.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:097830ed52fd9e427942ff3b9bc17fab52913b2f50f2880dc4a5611446606a54"},
    {file = "pydantic_core-2.27.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:044a50963a614ecfae59bb1eaf7ea7efc4bc62f49ed594e18fa1e5d953c40e9f"},
    {file = "pydantic_core-2.27.2-cp39-cp39-musllinux_1_1_armv7l.whl", hash = "sha256:4e0b4220ba5b40d727c7f879eac379b822eee5d8fff418e9d3381ee45b3b0362"},
    {file = "pydantic_core-2.27.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:5e4f4bb20d75e9325cc9696c6802657b58bc1dbbe3022f32cc2b2b632c3fbb96"},
    {file = "pydantic_core-2.27.2-cp39-cp39-win32.whl", hash = "sha256:cca63613e90d001b9f2f9a9ceb276c308bfa2a43fafb75c8031c4f66039e8c6e"},
    {file = "pydantic_core-2.27.2-cp39-cp39-win_amd64.whl", hash = "sha256:77d1bca19b0f7021b3a982e6f903dcd5b2b06076def36a652e3907f596e29f67"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:2bf14caea37e91198329b828eae1618c068dfb8ef17bb33287a7ad4b61ac314e"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:b0cb791f5b45307caae8810c2023a184c74605ec3bcbb67d13846c28ff731ff8"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:688d3fd9fcb71f41c4c015c023d12a79d1c4c0732ec9eb35d96e3388a120dcf3"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3d591580c34f4d731592f0e9fe40f9cc1b430d297eecc70b962e93c5c668f15f"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:82f986faf4e644ffc189a7f1aafc86e46ef70372bb153e7001e8afccc6e54133"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:bec317a27290e2537f922639cafd54990551725fc844249e64c523301d0822fc"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:0296abcb83a797db256b773f45773da397da75a08f5fcaef41f2044adec05f50"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:0d75070718e369e452075a6017fbf187f788e17ed67a3abd47fa934d001863d9"},
    {file = "pydantic_core-2.27.2-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:7e17b560be3c98a8e3aa66ce828bdebb9e9ac6ad5466fba92eb74c4c95cb1151"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:c33939a82924da9ed65dab5a65d427205a73181d8098e79b6b426bdf8ad4e656"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-macosx_11_0_arm64.whl", hash = "sha256:00bad2484fa6bda1e216e7345a798bd37c68fb2d97558edd584942aa41b7d278"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c817e2b40aba42bac6f457498dacabc568c3b7a986fc9ba7c8d9d260b71485fb"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:251136cdad0cb722e93732cb45ca5299fb56e1344a833640bf93b2803f8d1bfd"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:d2088237af596f0a524d3afc39ab3b036e8adb054ee57cbb1dcf8e09da5b29cc"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:d4041c0b966a84b4ae7a09832eb691a35aec90910cd2dbe7a208de59be77965b"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:8083d4e875ebe0b864ffef72a4304827015cff328a1be6e22cc850753bfb122b"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:f141ee28a0ad2123b6611b6ceff018039df17f32ada8b534e6aa039545a3efb2"},
    {file = "pydantic_core-2.27.2-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:7d0c8399fcc1848491f00e0314bd59fb34a9c008761bcb422a057670c3f65e35"},
    {file = "pydantic_core-2.27.2.tar.gz", hash = "sha256:eb026e5a4c1fee05726072337ff51d1efb6f59090b7da90d30ea58625b1ffb39"},
]

[package.dependencies]
typing-extensions = ">=4.6.0,<4.7.0 || >4.7.0"

[[package]]
name = "python-dotenv"
version = "1.0.1"
description = "Read key-value pairs from a .env file and set them as environment variables"
optional = false
python-versions = ">=3.8"
files = [
    {file = "python-dotenv-1.0.1.tar.gz", hash = "sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca"},
    {file = "python_dotenv-1.0.1-py3-none-any.whl", hash = "sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a"},
]

[package.extras]
cli = ["click (>=5.0)"]

[[package]]
name = "pyyaml"
version = "6.0.2"
description = "YAML parser and emitter for Python"
optional = false
python-versions = ">=3.8"
files = [
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:0a9a2848a5b7feac301353437eb7d5957887edbf81d56e903999a75a3d743086"},
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:29717114e51c84ddfba879543fb232a6ed60086602313ca38cce623c1d62cfbf"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8824b5a04a04a047e72eea5cec3bc266db09e35de6bdfe34c9436ac5ee27d237"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7c36280e6fb8385e520936c3cb3b8042851904eba0e58d277dca80a5cfed590b"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ec031d5d2feb36d1d1a24380e4db6d43695f3748343d99434e6f5f9156aaa2ed"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:936d68689298c36b53b29f23c6dbb74de12b4ac12ca6cfe0e047bedceea56180"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:23502f431948090f597378482b4812b0caae32c22213aecf3b55325e049a6c68"},
    {file = "PyYAML-6.0.2-cp310-cp310-win32.whl", hash = "sha256:2e99c6826ffa974fe6e27cdb5ed0021786b03fc98e5ee3c5bfe1fd5015f42b99"},
    {file = "PyYAML-6.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:a4d3091415f010369ae4ed1fc6b79def9416358877534caf6a0fdd2146c87a3e"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e"},
    {file = "PyYAML-6.0.2-cp311-cp311-win32.whl", hash = "sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5"},
    {file = "PyYAML-6.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b"},
    {file = "PyYAML-6.0.2-cp312-cp312-win32.whl", hash = "sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4"},
    {file = "PyYAML-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652"},
    {file = "PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183"},
    {file = "PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563"},
    {file = "PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:24471b829b3bf607e04e88d79542a9d48bb037c2267d7927a874e6c205ca7e9a"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d7fded462629cfa4b685c5416b949ebad6cec74af5e2d42905d41e257e0869f5"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d84a1718ee396f54f3a086ea0a66d8e552b2ab2017ef8b420e92edbc841c352d"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9056c1ecd25795207ad294bcf39f2db3d845767be0ea6e6a34d856f006006083"},
    {file = "PyYAML-6.0.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:82d09873e40955485746739bcb8b4586983670466c23382c19cffecbf1fd8706"},
    {file = "PyYAML-6.0.2-cp38-cp38-win32.whl", hash = "sha256:43fa96a3ca0d6b1812e01ced1044a003533c47f6ee8aca31724f78e93ccc089a"},
    {file = "PyYAML-6.0.2-cp38-cp38-win_amd64.whl", hash = "sha256:01179a4a8559ab5de078078f37e5c1a30d76bb88519906844fd7bdea1b7729ff"},
    {file = "PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:688ba32a1cffef67fd2e9398a2efebaea461578b0923624778664cc1c914db5d"},
    {file = "PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:a8786accb172bd8afb8be14490a16625cbc387036876ab6ba70912730faf8e1f"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d8e03406cac8513435335dbab54c0d385e4a49e4945d2909a581c83647ca0290"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f753120cb8181e736c57ef7636e83f31b9c0d1722c516f7e86cf15b7aa57ff12"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3b1fdb9dc17f5a7677423d508ab4f243a726dea51fa5e70992e59a7411c89d19"},
    {file = "PyYAML-6.0.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:0b69e4ce7a131fe56b7e4d770c67429700908fc0752af059838b1cfb41960e4e"},
    {file = "PyYAML-6.0.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:a9f8c2e67970f13b16084e04f134610fd1d374bf477b17ec1599185cf611d725"},
    {file = "PyYAML-6.0.2-cp39-cp39-win32.whl", hash = "sha256:6395c297d42274772abc367baaa79683958044e5d3835486c16da75d2a694631"},
    {file = "PyYAML-6.0.2-cp39-cp39-win_amd64.whl", hash = "sha256:39693e1f8320ae4f43943590b49779ffb98acb81f788220ea932a6b6c51004d8"},
    {file = "pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e"},
]

[[package]]
name = "regex"
version = "2024.11.6"
description = "Alternative regular expression module, to replace re."
optional = false
python-versions = ">=3.8"
files = [
    {file = "regex-2024.11.6-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:ff590880083d60acc0433f9c3f713c51f7ac6ebb9adf889c79a261ecf541aa91"},
    {file = "regex-2024.11.6-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:658f90550f38270639e83ce492f27d2c8d2cd63805c65a13a14d36ca126753f0"},
    {file = "regex-2024.11.6-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:164d8b7b3b4bcb2068b97428060b2a53be050085ef94eca7f240e7947f1b080e"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3660c82f209655a06b587d55e723f0b813d3a7db2e32e5e7dc64ac2a9e86fde"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:d22326fcdef5e08c154280b71163ced384b428343ae16a5ab2b3354aed12436e"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f1ac758ef6aebfc8943560194e9fd0fa18bcb34d89fd8bd2af18183afd8da3a2"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:997d6a487ff00807ba810e0f8332c18b4eb8d29463cfb7c820dc4b6e7562d0cf"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:02a02d2bb04fec86ad61f3ea7f49c015a0681bf76abb9857f945d26159d2968c"},
    {file = "regex-2024.11.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:f02f93b92358ee3f78660e43b4b0091229260c5d5c408d17d60bf26b6c900e86"},
    {file = "regex-2024.11.6-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:06eb1be98df10e81ebaded73fcd51989dcf534e3c753466e4b60c4697a003b67"},
    {file = "regex-2024.11.6-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:040df6fe1a5504eb0f04f048e6d09cd7c7110fef851d7c567a6b6e09942feb7d"},
    {file = "regex-2024.11.6-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:fdabbfc59f2c6edba2a6622c647b716e34e8e3867e0ab975412c5c2f79b82da2"},
    {file = "regex-2024.11.6-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:8447d2d39b5abe381419319f942de20b7ecd60ce86f16a23b0698f22e1b70008"},
    {file = "regex-2024.11.6-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:da8f5fc57d1933de22a9e23eec290a0d8a5927a5370d24bda9a6abe50683fe62"},
    {file = "regex-2024.11.6-cp310-cp310-win32.whl", hash = "sha256:b489578720afb782f6ccf2840920f3a32e31ba28a4b162e13900c3e6bd3f930e"},
    {file = "regex-2024.11.6-cp310-cp310-win_amd64.whl", hash = "sha256:5071b2093e793357c9d8b2929dfc13ac5f0a6c650559503bb81189d0a3814519"},
    {file = "regex-2024.11.6-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:5478c6962ad548b54a591778e93cd7c456a7a29f8eca9c49e4f9a806dcc5d638"},
    {file = "regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:2c89a8cc122b25ce6945f0423dc1352cb9593c68abd19223eebbd4e56612c5b7"},
    {file = "regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:94d87b689cdd831934fa3ce16cc15cd65748e6d689f5d2b8f4f4df2065c9fa20"},
    {file = "regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1062b39a0a2b75a9c694f7a08e7183a80c63c0d62b301418ffd9c35f55aaa114"},
    {file = "regex-2024.11.6-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:167ed4852351d8a750da48712c3930b031f6efdaa0f22fa1933716bfcd6bf4a3"},
    {file = "regex-2024.11.6-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d548dafee61f06ebdb584080621f3e0c23fff312f0de1afc776e2a2ba99a74f"},
    {file = "regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f2a19f302cd1ce5dd01a9099aaa19cae6173306d1302a43b627f62e21cf18ac0"},
    {file = "regex-2024.11.6-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:bec9931dfb61ddd8ef2ebc05646293812cb6b16b60cf7c9511a832b6f1854b55"},
    {file = "regex-2024.11.6-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:9714398225f299aa85267fd222f7142fcb5c769e73d7733344efc46f2ef5cf89"},
    {file = "regex-2024.11.6-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:202eb32e89f60fc147a41e55cb086db2a3f8cb82f9a9a88440dcfc5d37faae8d"},
    {file = "regex-2024.11.6-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:4181b814e56078e9b00427ca358ec44333765f5ca1b45597ec7446d3a1ef6e34"},
    {file = "regex-2024.11.6-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:068376da5a7e4da51968ce4c122a7cd31afaaec4fccc7856c92f63876e57b51d"},
    {file = "regex-2024.11.6-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:ac10f2c4184420d881a3475fb2c6f4d95d53a8d50209a2500723d831036f7c45"},
    {file = "regex-2024.11.6-cp311-cp311-win32.whl", hash = "sha256:c36f9b6f5f8649bb251a5f3f66564438977b7ef8386a52460ae77e6070d309d9"},
    {file = "regex-2024.11.6-cp311-cp311-win_amd64.whl", hash = "sha256:02e28184be537f0e75c1f9b2f8847dc51e08e6e171c6bde130b2687e0c33cf60"},
    {file = "regex-2024.11.6-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:52fb28f528778f184f870b7cf8f225f5eef0a8f6e3778529bdd40c7b3920796a"},
    {file = "regex-2024.11.6-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:fdd6028445d2460f33136c55eeb1f601ab06d74cb3347132e1c24250187500d9"},
    {file = "regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:805e6b60c54bf766b251e94526ebad60b7de0c70f70a4e6210ee2891acb70bf2"},
    {file = "regex-2024.11.6-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b85c2530be953a890eaffde05485238f07029600e8f098cdf1848d414a8b45e4"},
    {file = "regex-2024.11.6-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:bb26437975da7dc36b7efad18aa9dd4ea569d2357ae6b783bf1118dabd9ea577"},
    {file = "regex-2024.11.6-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:abfa5080c374a76a251ba60683242bc17eeb2c9818d0d30117b4486be10c59d3"},
    {file = "regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b7fa6606c2881c1db9479b0eaa11ed5dfa11c8d60a474ff0e095099f39d98e"},
    {file = "regex-2024.11.6-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0c32f75920cf99fe6b6c539c399a4a128452eaf1af27f39bce8909c9a3fd8cbe"},
    {file = "regex-2024.11.6-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:982e6d21414e78e1f51cf595d7f321dcd14de1f2881c5dc6a6e23bbbbd68435e"},
    {file = "regex-2024.11.6-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:a7c2155f790e2fb448faed6dd241386719802296ec588a8b9051c1f5c481bc29"},
    {file = "regex-2024.11.6-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:149f5008d286636e48cd0b1dd65018548944e495b0265b45e1bffecce1ef7f39"},
    {file = "regex-2024.11.6-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:e5364a4502efca094731680e80009632ad6624084aff9a23ce8c8c6820de3e51"},
    {file = "regex-2024.11.6-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:0a86e7eeca091c09e021db8eb72d54751e527fa47b8d5787caf96d9831bd02ad"},
    {file = "regex-2024.11.6-cp312-cp312-win32.whl", hash = "sha256:32f9a4c643baad4efa81d549c2aadefaeba12249b2adc5af541759237eee1c54"},
    {file = "regex-2024.11.6-cp312-cp312-win_amd64.whl", hash = "sha256:a93c194e2df18f7d264092dc8539b8ffb86b45b899ab976aa15d48214138e81b"},
    {file = "regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84"},
    {file = "regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4"},
    {file = "regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0"},
    {file = "regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0"},
    {file = "regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7"},
    {file = "regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7"},
    {file = "regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c"},
    {file = "regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3"},
    {file = "regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07"},
    {file = "regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e"},
    {file = "regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6"},
    {file = "regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4"},
    {file = "regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d"},
    {file = "regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff"},
    {file = "regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a"},
    {file = "regex-2024.11.6-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:3a51ccc315653ba012774efca4f23d1d2a8a8f278a6072e29c7147eee7da446b"},
    {file = "regex-2024.11.6-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:ad182d02e40de7459b73155deb8996bbd8e96852267879396fb274e8700190e3"},
    {file = "regex-2024.11.6-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:ba9b72e5643641b7d41fa1f6d5abda2c9a263ae835b917348fc3c928182ad467"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40291b1b89ca6ad8d3f2b82782cc33807f1406cf68c8d440861da6304d8ffbbd"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:cdf58d0e516ee426a48f7b2c03a332a4114420716d55769ff7108c37a09951bf"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a36fdf2af13c2b14738f6e973aba563623cb77d753bbbd8d414d18bfaa3105dd"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d1cee317bfc014c2419a76bcc87f071405e3966da434e03e13beb45f8aced1a6"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:50153825ee016b91549962f970d6a4442fa106832e14c918acd1c8e479916c4f"},
    {file = "regex-2024.11.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:ea1bfda2f7162605f6e8178223576856b3d791109f15ea99a9f95c16a7636fb5"},
    {file = "regex-2024.11.6-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:df951c5f4a1b1910f1a99ff42c473ff60f8225baa1cdd3539fe2819d9543e9df"},
    {file = "regex-2024.11.6-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:072623554418a9911446278f16ecb398fb3b540147a7828c06e2011fa531e773"},
    {file = "regex-2024.11.6-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:f654882311409afb1d780b940234208a252322c24a93b442ca714d119e68086c"},
    {file = "regex-2024.11.6-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:89d75e7293d2b3e674db7d4d9b1bee7f8f3d1609428e293771d1a962617150cc"},
    {file = "regex-2024.11.6-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:f65557897fc977a44ab205ea871b690adaef6b9da6afda4790a2484b04293a5f"},
    {file = "regex-2024.11.6-cp38-cp38-win32.whl", hash = "sha256:6f44ec28b1f858c98d3036ad5d7d0bfc568bdd7a74f9c24e25f41ef1ebfd81a4"},
    {file = "regex-2024.11.6-cp38-cp38-win_amd64.whl", hash = "sha256:bb8f74f2f10dbf13a0be8de623ba4f9491faf58c24064f32b65679b021ed0001"},
    {file = "regex-2024.11.6-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:5704e174f8ccab2026bd2f1ab6c510345ae8eac818b613d7d73e785f1310f839"},
    {file = "regex-2024.11.6-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:220902c3c5cc6af55d4fe19ead504de80eb91f786dc102fbd74894b1551f095e"},
    {file = "regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:5e7e351589da0850c125f1600a4c4ba3c722efefe16b297de54300f08d734fbf"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5056b185ca113c88e18223183aa1a50e66507769c9640a6ff75859619d73957b"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2e34b51b650b23ed3354b5a07aab37034d9f923db2a40519139af34f485f77d0"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5670bce7b200273eee1840ef307bfa07cda90b38ae56e9a6ebcc9f50da9c469b"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:08986dce1339bc932923e7d1232ce9881499a0e02925f7402fb7c982515419ef"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:93c0b12d3d3bc25af4ebbf38f9ee780a487e8bf6954c115b9f015822d3bb8e48"},
    {file = "regex-2024.11.6-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl", hash = "sha256:764e71f22ab3b305e7f4c21f1a97e1526a25ebdd22513e251cf376760213da13"},
    {file = "regex-2024.11.6-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:f056bf21105c2515c32372bbc057f43eb02aae2fda61052e2f7622c801f0b4e2"},
    {file = "regex-2024.11.6-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:69ab78f848845569401469da20df3e081e6b5a11cb086de3eed1d48f5ed57c95"},
    {file = "regex-2024.11.6-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:86fddba590aad9208e2fa8b43b4c098bb0ec74f15718bb6a704e3c63e2cef3e9"},
    {file = "regex-2024.11.6-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:684d7a212682996d21ca12ef3c17353c021fe9de6049e19ac8481ec35574a70f"},
    {file = "regex-2024.11.6-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:a03e02f48cd1abbd9f3b7e3586d97c8f7a9721c436f51a5245b3b9483044480b"},
    {file = "regex-2024.11.6-cp39-cp39-win32.whl", hash = "sha256:41758407fc32d5c3c5de163888068cfee69cb4c2be844e7ac517a52770f9af57"},
    {file = "regex-2024.11.6-cp39-cp39-win_amd64.whl", hash = "sha256:b2837718570f95dd41675328e111345f9b7095d821bac435aac173ac80b19983"},
    {file = "regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519"},
]

[[package]]
name = "requests"
version = "2.32.3"
description = "Python HTTP for Humans."
optional = false
python-versions = ">=3.8"
files = [
    {file = "requests-2.32.3-py3-none-any.whl", hash = "sha256:70761cfe03c773ceb22aa2f671b4757976145175cdfca038c02654d061d6dcc6"},
    {file = "requests-2.32.3.tar.gz", hash = "sha256:55365417734eb18255590a9ff9eb97e9e1da868d4ccd6402399eaf68af20a760"},
]

[package.dependencies]
certifi = ">=2017.4.17"
charset-normalizer = ">=2,<4"
idna = ">=2.5,<4"
urllib3 = ">=1.21.1,<3"

[package.extras]
socks = ["PySocks (>=1.5.6,!=1.5.7)"]
use-chardet-on-py3 = ["chardet (>=3.0.2,<6)"]

[[package]]
name = "requests-toolbelt"
version = "1.0.0"
description = "A utility belt for advanced users of python-requests"
optional = false
python-versions = ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*"
files = [
    {file = "requests-toolbelt-1.0.0.tar.gz", hash = "sha256:7681a0a3d047012b5bdc0ee37d7f8f07ebe76ab08caeccfc3921ce23c88d5bc6"},
    {file = "requests_toolbelt-1.0.0-py2.py3-none-any.whl", hash = "sha256:cccfdd665f0a24fcf4726e690f65639d272bb0637b9b92dfd91a5568ccf6bd06"},
]

[package.dependencies]
requests = ">=2.0.1,<3.0.0"

[[package]]
name = "ruff"
version = "0.9.3"
description = "An extremely fast Python linter and code formatter, written in Rust."
optional = false
python-versions = ">=3.7"
files = [
    {file = "ruff-0.9.3-py3-none-linux_armv6l.whl", hash = "sha256:7f39b879064c7d9670197d91124a75d118d00b0990586549949aae80cdc16624"},
    {file = "ruff-0.9.3-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:a187171e7c09efa4b4cc30ee5d0d55a8d6c5311b3e1b74ac5cb96cc89bafc43c"},
    {file = "ruff-0.9.3-py3-none-macosx_11_0_arm64.whl", hash = "sha256:c59ab92f8e92d6725b7ded9d4a31be3ef42688a115c6d3da9457a5bda140e2b4"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2dc153c25e715be41bb228bc651c1e9b1a88d5c6e5ed0194fa0dfea02b026439"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:646909a1e25e0dc28fbc529eab8eb7bb583079628e8cbe738192853dbbe43af5"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:5a5a46e09355695fbdbb30ed9889d6cf1c61b77b700a9fafc21b41f097bfbba4"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:c4bb09d2bbb394e3730d0918c00276e79b2de70ec2a5231cd4ebb51a57df9ba1"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:96a87ec31dc1044d8c2da2ebbed1c456d9b561e7d087734336518181b26b3aa5"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9bb7554aca6f842645022fe2d301c264e6925baa708b392867b7a62645304df4"},
    {file = "ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cabc332b7075a914ecea912cd1f3d4370489c8018f2c945a30bcc934e3bc06a6"},
    {file = "ruff-0.9.3-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:33866c3cc2a575cbd546f2cd02bdd466fed65118e4365ee538a3deffd6fcb730"},
    {file = "ruff-0.9.3-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:006e5de2621304c8810bcd2ee101587712fa93b4f955ed0985907a36c427e0c2"},
    {file = "ruff-0.9.3-py3-none-musllinux_1_2_i686.whl", hash = "sha256:ba6eea4459dbd6b1be4e6bfc766079fb9b8dd2e5a35aff6baee4d9b1514ea519"},
    {file = "ruff-0.9.3-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:90230a6b8055ad47d3325e9ee8f8a9ae7e273078a66401ac66df68943ced029b"},
    {file = "ruff-0.9.3-py3-none-win32.whl", hash = "sha256:eabe5eb2c19a42f4808c03b82bd313fc84d4e395133fb3fc1b1516170a31213c"},
    {file = "ruff-0.9.3-py3-none-win_amd64.whl", hash = "sha256:040ceb7f20791dfa0e78b4230ee9dce23da3b64dd5848e40e3bf3ab76468dcf4"},
    {file = "ruff-0.9.3-py3-none-win_arm64.whl", hash = "sha256:800d773f6d4d33b0a3c60e2c6ae8f4c202ea2de056365acfa519aa48acf28e0b"},
    {file = "ruff-0.9.3.tar.gz", hash = "sha256:8293f89985a090ebc3ed1064df31f3b4b56320cdfcec8b60d3295bddb955c22a"},
]

[[package]]
name = "sniffio"
version = "1.3.1"
description = "Sniff out which async library your code is running under"
optional = false
python-versions = ">=3.7"
files = [
    {file = "sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2"},
    {file = "sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc"},
]

[[package]]
name = "sqlalchemy"
version = "2.0.37"
description = "Database Abstraction Library"
optional = false
python-versions = ">=3.7"
files = [
    {file = "SQLAlchemy-2.0.37-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:da36c3b0e891808a7542c5c89f224520b9a16c7f5e4d6a1156955605e54aef0e"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:e7402ff96e2b073a98ef6d6142796426d705addd27b9d26c3b32dbaa06d7d069"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e6f5d254a22394847245f411a2956976401e84da4288aa70cbcd5190744062c1"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:41296bbcaa55ef5fdd32389a35c710133b097f7b2609d8218c0eabded43a1d84"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:bedee60385c1c0411378cbd4dc486362f5ee88deceea50002772912d798bb00f"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:6c67415258f9f3c69867ec02fea1bf6508153709ecbd731a982442a590f2b7e4"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-win32.whl", hash = "sha256:650dcb70739957a492ad8acff65d099a9586b9b8920e3507ca61ec3ce650bb72"},
    {file = "SQLAlchemy-2.0.37-cp310-cp310-win_amd64.whl", hash = "sha256:93d1543cd8359040c02b6614421c8e10cd7a788c40047dbc507ed46c29ae5636"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:78361be6dc9073ed17ab380985d1e45e48a642313ab68ab6afa2457354ff692c"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:b661b49d0cb0ab311a189b31e25576b7ac3e20783beb1e1817d72d9d02508bf5"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d57bafbab289e147d064ffbd5cca2d7b1394b63417c0636cea1f2e93d16eb9e8"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2fa2c0913f02341d25fb858e4fb2031e6b0813494cca1ba07d417674128ce11b"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:9df21b8d9e5c136ea6cde1c50d2b1c29a2b5ff2b1d610165c23ff250e0704087"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:db18ff6b8c0f1917f8b20f8eca35c28bbccb9f83afa94743e03d40203ed83de9"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-win32.whl", hash = "sha256:46954173612617a99a64aee103bcd3f078901b9a8dcfc6ae80cbf34ba23df989"},
    {file = "SQLAlchemy-2.0.37-cp311-cp311-win_amd64.whl", hash = "sha256:7b7e772dc4bc507fdec4ee20182f15bd60d2a84f1e087a8accf5b5b7a0dcf2ba"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:2952748ecd67ed3b56773c185e85fc084f6bdcdec10e5032a7c25a6bc7d682ef"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:3151822aa1db0eb5afd65ccfafebe0ef5cda3a7701a279c8d0bf17781a793bb4"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eaa8039b6d20137a4e02603aba37d12cd2dde7887500b8855356682fc33933f4"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1cdba1f73b64530c47b27118b7053b8447e6d6f3c8104e3ac59f3d40c33aa9fd"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:1b2690456528a87234a75d1a1644cdb330a6926f455403c8e4f6cad6921f9098"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:cf5ae8a9dcf657fd72144a7fd01f243236ea39e7344e579a121c4205aedf07bb"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-win32.whl", hash = "sha256:ea308cec940905ba008291d93619d92edaf83232ec85fbd514dcb329f3192761"},
    {file = "SQLAlchemy-2.0.37-cp312-cp312-win_amd64.whl", hash = "sha256:635d8a21577341dfe4f7fa59ec394b346da12420b86624a69e466d446de16aff"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8c4096727193762e72ce9437e2a86a110cf081241919ce3fab8e89c02f6b6658"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:e4fb5ac86d8fe8151966814f6720996430462e633d225497566b3996966b9bdb"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e56a139bfe136a22c438478a86f8204c1eb5eed36f4e15c4224e4b9db01cb3e4"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2f95fc8e3f34b5f6b3effb49d10ac97c569ec8e32f985612d9b25dd12d0d2e94"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:c505edd429abdfe3643fa3b2e83efb3445a34a9dc49d5f692dd087be966020e0"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:12b0f1ec623cccf058cf21cb544f0e74656618165b083d78145cafde156ea7b6"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-win32.whl", hash = "sha256:293f9ade06b2e68dd03cfb14d49202fac47b7bb94bffcff174568c951fbc7af2"},
    {file = "SQLAlchemy-2.0.37-cp313-cp313-win_amd64.whl", hash = "sha256:d70f53a0646cc418ca4853da57cf3ddddbccb8c98406791f24426f2dd77fd0e2"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-macosx_10_9_x86_64.whl", hash = "sha256:44f569d0b1eb82301b92b72085583277316e7367e038d97c3a1a899d9a05e342"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b2eae3423e538c10d93ae3e87788c6a84658c3ed6db62e6a61bb9495b0ad16bb"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dfff7be361048244c3aa0f60b5e63221c5e0f0e509f4e47b8910e22b57d10ae7"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-musllinux_1_2_aarch64.whl", hash = "sha256:5bc3339db84c5fb9130ac0e2f20347ee77b5dd2596ba327ce0d399752f4fce39"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-musllinux_1_2_x86_64.whl", hash = "sha256:84b9f23b0fa98a6a4b99d73989350a94e4a4ec476b9a7dfe9b79ba5939f5e80b"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-win32.whl", hash = "sha256:51bc9cfef83e0ac84f86bf2b10eaccb27c5a3e66a1212bef676f5bee6ef33ebb"},
    {file = "SQLAlchemy-2.0.37-cp37-cp37m-win_amd64.whl", hash = "sha256:8e47f1af09444f87c67b4f1bb6231e12ba6d4d9f03050d7fc88df6d075231a49"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:6b788f14c5bb91db7f468dcf76f8b64423660a05e57fe277d3f4fad7b9dcb7ce"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:521ef85c04c33009166777c77e76c8a676e2d8528dc83a57836b63ca9c69dcd1"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:75311559f5c9881a9808eadbeb20ed8d8ba3f7225bef3afed2000c2a9f4d49b9"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cce918ada64c956b62ca2c2af59b125767097ec1dca89650a6221e887521bfd7"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:9d087663b7e1feabea8c578d6887d59bb00388158e8bff3a76be11aa3f748ca2"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:cf95a60b36997dad99692314c4713f141b61c5b0b4cc5c3426faad570b31ca01"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-win32.whl", hash = "sha256:d75ead7dd4d255068ea0f21492ee67937bd7c90964c8f3c2bea83c7b7f81b95f"},
    {file = "SQLAlchemy-2.0.37-cp38-cp38-win_amd64.whl", hash = "sha256:74bbd1d0a9bacf34266a7907d43260c8d65d31d691bb2356f41b17c2dca5b1d0"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:648ec5acf95ad59255452ef759054f2176849662af4521db6cb245263ae4aa33"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:35bd2df269de082065d4b23ae08502a47255832cc3f17619a5cea92ce478b02b"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4f581d365af9373a738c49e0c51e8b18e08d8a6b1b15cc556773bcd8a192fa8b"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:82df02816c14f8dc9f4d74aea4cb84a92f4b0620235daa76dde002409a3fbb5a"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:94b564e38b344d3e67d2e224f0aec6ba09a77e4582ced41e7bfd0f757d926ec9"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:955a2a765aa1bd81aafa69ffda179d4fe3e2a3ad462a736ae5b6f387f78bfeb8"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-win32.whl", hash = "sha256:03f0528c53ca0b67094c4764523c1451ea15959bbf0a8a8a3096900014db0278"},
    {file = "SQLAlchemy-2.0.37-cp39-cp39-win_amd64.whl", hash = "sha256:4b12885dc85a2ab2b7d00995bac6d967bffa8594123b02ed21e8eb2205a7584b"},
    {file = "SQLAlchemy-2.0.37-py3-none-any.whl", hash = "sha256:a8998bf9f8658bd3839cbc44ddbe982955641863da0c1efe5b00c1ab4f5c16b1"},
    {file = "sqlalchemy-2.0.37.tar.gz", hash = "sha256:12b28d99a9c14eaf4055810df1001557176716de0167b91026e648e65229bffb"},
]

[package.dependencies]
greenlet = {version = "!=0.4.17", markers = "python_version < \"3.14\" and (platform_machine == \"aarch64\" or platform_machine == \"ppc64le\" or platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"AMD64\" or platform_machine == \"win32\" or platform_machine == \"WIN32\")"}
typing-extensions = ">=4.6.0"

[package.extras]
aiomysql = ["aiomysql (>=0.2.0)", "greenlet (!=0.4.17)"]
aioodbc = ["aioodbc", "greenlet (!=0.4.17)"]
aiosqlite = ["aiosqlite", "greenlet (!=0.4.17)", "typing_extensions (!=3.10.0.1)"]
asyncio = ["greenlet (!=0.4.17)"]
asyncmy = ["asyncmy (>=0.2.3,!=0.2.4,!=0.2.6)", "greenlet (!=0.4.17)"]
mariadb-connector = ["mariadb (>=1.0.1,!=1.1.2,!=1.1.5,!=1.1.10)"]
mssql = ["pyodbc"]
mssql-pymssql = ["pymssql"]
mssql-pyodbc = ["pyodbc"]
mypy = ["mypy (>=0.910)"]
mysql = ["mysqlclient (>=1.4.0)"]
mysql-connector = ["mysql-connector-python"]
oracle = ["cx_oracle (>=8)"]
oracle-oracledb = ["oracledb (>=1.0.1)"]
postgresql = ["psycopg2 (>=2.7)"]
postgresql-asyncpg = ["asyncpg", "greenlet (!=0.4.17)"]
postgresql-pg8000 = ["pg8000 (>=1.29.1)"]
postgresql-psycopg = ["psycopg (>=3.0.7)"]
postgresql-psycopg2binary = ["psycopg2-binary"]
postgresql-psycopg2cffi = ["psycopg2cffi"]
postgresql-psycopgbinary = ["psycopg[binary] (>=3.0.7)"]
pymysql = ["pymysql"]
sqlcipher = ["sqlcipher3_binary"]

[[package]]
name = "tenacity"
version = "9.0.0"
description = "Retry code until it succeeds"
optional = false
python-versions = ">=3.8"
files = [
    {file = "tenacity-9.0.0-py3-none-any.whl", hash = "sha256:93de0c98785b27fcf659856aa9f54bfbd399e29969b0621bc7f762bd441b4539"},
    {file = "tenacity-9.0.0.tar.gz", hash = "sha256:807f37ca97d62aa361264d497b0e31e92b8027044942bfa756160d908320d73b"},
]

[package.extras]
doc = ["reno", "sphinx"]
test = ["pytest", "tornado (>=4.5)", "typeguard"]

[[package]]
name = "tiktoken"
version = "0.8.0"
description = "tiktoken is a fast BPE tokeniser for use with OpenAI's models"
optional = false
python-versions = ">=3.9"
files = [
    {file = "tiktoken-0.8.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:b07e33283463089c81ef1467180e3e00ab00d46c2c4bbcef0acab5f771d6695e"},
    {file = "tiktoken-0.8.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:9269348cb650726f44dd3bbb3f9110ac19a8dcc8f54949ad3ef652ca22a38e21"},
    {file = "tiktoken-0.8.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:25e13f37bc4ef2d012731e93e0fef21dc3b7aea5bb9009618de9a4026844e560"},
    {file = "tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:f13d13c981511331eac0d01a59b5df7c0d4060a8be1e378672822213da51e0a2"},
    {file = "tiktoken-0.8.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:6b2ddbc79a22621ce8b1166afa9f9a888a664a579350dc7c09346a3b5de837d9"},
    {file = "tiktoken-0.8.0-cp310-cp310-win_amd64.whl", hash = "sha256:d8c2d0e5ba6453a290b86cd65fc51fedf247e1ba170191715b049dac1f628005"},
    {file = "tiktoken-0.8.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:d622d8011e6d6f239297efa42a2657043aaed06c4f68833550cac9e9bc723ef1"},
    {file = "tiktoken-0.8.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:2efaf6199717b4485031b4d6edb94075e4d79177a172f38dd934d911b588d54a"},
    {file = "tiktoken-0.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5637e425ce1fc49cf716d88df3092048359a4b3bbb7da762840426e937ada06d"},
    {file = "tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fb0e352d1dbe15aba082883058b3cce9e48d33101bdaac1eccf66424feb5b47"},
    {file = "tiktoken-0.8.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:56edfefe896c8f10aba372ab5706b9e3558e78db39dd497c940b47bf228bc419"},
    {file = "tiktoken-0.8.0-cp311-cp311-win_amd64.whl", hash = "sha256:326624128590def898775b722ccc327e90b073714227175ea8febbc920ac0a99"},
    {file = "tiktoken-0.8.0-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:881839cfeae051b3628d9823b2e56b5cc93a9e2efb435f4cf15f17dc45f21586"},
    {file = "tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:fe9399bdc3f29d428f16a2f86c3c8ec20be3eac5f53693ce4980371c3245729b"},
    {file = "tiktoken-0.8.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9a58deb7075d5b69237a3ff4bb51a726670419db6ea62bdcd8bd80c78497d7ab"},
    {file = "tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d2908c0d043a7d03ebd80347266b0e58440bdef5564f84f4d29fb235b5df3b04"},
    {file = "tiktoken-0.8.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:294440d21a2a51e12d4238e68a5972095534fe9878be57d905c476017bff99fc"},
    {file = "tiktoken-0.8.0-cp312-cp312-win_amd64.whl", hash = "sha256:d8f3192733ac4d77977432947d563d7e1b310b96497acd3c196c9bddb36ed9db"},
    {file = "tiktoken-0.8.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:02be1666096aff7da6cbd7cdaa8e7917bfed3467cd64b38b1f112e96d3b06a24"},
    {file = "tiktoken-0.8.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:c94ff53c5c74b535b2cbf431d907fc13c678bbd009ee633a2aca269a04389f9a"},
    {file = "tiktoken-0.8.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6b231f5e8982c245ee3065cd84a4712d64692348bc609d84467c57b4b72dcbc5"},
    {file = "tiktoken-0.8.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:4177faa809bd55f699e88c96d9bb4635d22e3f59d635ba6fd9ffedf7150b9953"},
    {file = "tiktoken-0.8.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:5376b6f8dc4753cd81ead935c5f518fa0fbe7e133d9e25f648d8c4dabdd4bad7"},
    {file = "tiktoken-0.8.0-cp313-cp313-win_amd64.whl", hash = "sha256:18228d624807d66c87acd8f25fc135665617cab220671eb65b50f5d70fa51f69"},
    {file = "tiktoken-0.8.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:7e17807445f0cf1f25771c9d86496bd8b5c376f7419912519699f3cc4dc5c12e"},
    {file = "tiktoken-0.8.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:886f80bd339578bbdba6ed6d0567a0d5c6cfe198d9e587ba6c447654c65b8edc"},
    {file = "tiktoken-0.8.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:6adc8323016d7758d6de7313527f755b0fc6c72985b7d9291be5d96d73ecd1e1"},
    {file = "tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b591fb2b30d6a72121a80be24ec7a0e9eb51c5500ddc7e4c2496516dd5e3816b"},
    {file = "tiktoken-0.8.0-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:845287b9798e476b4d762c3ebda5102be87ca26e5d2c9854002825d60cdb815d"},
    {file = "tiktoken-0.8.0-cp39-cp39-win_amd64.whl", hash = "sha256:1473cfe584252dc3fa62adceb5b1c763c1874e04511b197da4e6de51d6ce5a02"},
    {file = "tiktoken-0.8.0.tar.gz", hash = "sha256:9ccbb2740f24542534369c5635cfd9b2b3c2490754a78ac8831d99f89f94eeb2"},
]

[package.dependencies]
regex = ">=2022.1.18"
requests = ">=2.26.0"

[package.extras]
blobfile = ["blobfile (>=2)"]

[[package]]
name = "tqdm"
version = "4.67.1"
description = "Fast, Extensible Progress Meter"
optional = false
python-versions = ">=3.7"
files = [
    {file = "tqdm-4.67.1-py3-none-any.whl", hash = "sha256:26445eca388f82e72884e0d580d5464cd801a3ea01e63e5601bdff9ba6a48de2"},
    {file = "tqdm-4.67.1.tar.gz", hash = "sha256:f8aef9c52c08c13a65f30ea34f4e5aac3fd1a34959879d7e59e63027286627f2"},
]

[package.dependencies]
colorama = {version = "*", markers = "platform_system == \"Windows\""}

[package.extras]
dev = ["nbval", "pytest (>=6)", "pytest-asyncio (>=0.24)", "pytest-cov", "pytest-timeout"]
discord = ["requests"]
notebook = ["ipywidgets (>=6)"]
slack = ["slack-sdk"]
telegram = ["requests"]

[[package]]
name = "trustcall"
version = "0.0.28"
description = "Tenacious & trustworthy tool calling built on LangGraph."
optional = false
python-versions = "<4.0,>=3.10"
files = [
    {file = "trustcall-0.0.28-py3-none-any.whl", hash = "sha256:9f5aa3665397933b70c314be9541dc76a0ce3c0cf3a284e8618c7796c9cd1f65"},
    {file = "trustcall-0.0.28.tar.gz", hash = "sha256:34ce95bae988a27849658e00e5070623f5f8127065a64cb29ee46be090696372"},
]

[package.dependencies]
dydantic = ">=0.0.7,<1.0.0"
jsonpatch = ">=1.33,<2.0"
langgraph = ">=0.2.25"

[[package]]
name = "typing-extensions"
version = "4.12.2"
description = "Backported and Experimental Type Hints for Python 3.8+"
optional = false
python-versions = ">=3.8"
files = [
    {file = "typing_extensions-4.12.2-py3-none-any.whl", hash = "sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d"},
    {file = "typing_extensions-4.12.2.tar.gz", hash = "sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8"},
]

[[package]]
name = "urllib3"
version = "2.3.0"
description = "HTTP library with thread-safe connection pooling, file post, and more."
optional = false
python-versions = ">=3.9"
files = [
    {file = "urllib3-2.3.0-py3-none-any.whl", hash = "sha256:1cee9ad369867bfdbbb48b7dd50374c0967a0bb7710050facf0dd6911440e3df"},
    {file = "urllib3-2.3.0.tar.gz", hash = "sha256:f8c5449b3cf0861679ce7e0503c7b44b5ec981bec0d1d3795a07f1ba96f0204d"},
]

[package.extras]
brotli = ["brotli (>=1.0.9)", "brotlicffi (>=0.8.0)"]
h2 = ["h2 (>=4,<5)"]
socks = ["pysocks (>=1.5.6,!=1.5.7,<2.0)"]
zstd = ["zstandard (>=0.18.0)"]

[[package]]
name = "yarl"
version = "1.18.3"
description = "Yet another URL library"
optional = false
python-versions = ">=3.9"
files = [
    {file = "yarl-1.18.3-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:7df647e8edd71f000a5208fe6ff8c382a1de8edfbccdbbfe649d263de07d8c34"},
    {file = "yarl-1.18.3-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:c69697d3adff5aa4f874b19c0e4ed65180ceed6318ec856ebc423aa5850d84f7"},
    {file = "yarl-1.18.3-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:602d98f2c2d929f8e697ed274fbadc09902c4025c5a9963bf4e9edfc3ab6f7ed"},
    {file = "yarl-1.18.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c654d5207c78e0bd6d749f6dae1dcbbfde3403ad3a4b11f3c5544d9906969dde"},
    {file = "yarl-1.18.3-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:5094d9206c64181d0f6e76ebd8fb2f8fe274950a63890ee9e0ebfd58bf9d787b"},
    {file = "yarl-1.18.3-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:35098b24e0327fc4ebdc8ffe336cee0a87a700c24ffed13161af80124b7dc8e5"},
    {file = "yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3236da9272872443f81fedc389bace88408f64f89f75d1bdb2256069a8730ccc"},
    {file = "yarl-1.18.3-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e2c08cc9b16f4f4bc522771d96734c7901e7ebef70c6c5c35dd0f10845270bcd"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:80316a8bd5109320d38eef8833ccf5f89608c9107d02d2a7f985f98ed6876990"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_armv7l.whl", hash = "sha256:c1e1cc06da1491e6734f0ea1e6294ce00792193c463350626571c287c9a704db"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:fea09ca13323376a2fdfb353a5fa2e59f90cd18d7ca4eaa1fd31f0a8b4f91e62"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:e3b9fd71836999aad54084906f8663dffcd2a7fb5cdafd6c37713b2e72be1760"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:757e81cae69244257d125ff31663249b3013b5dc0a8520d73694aed497fb195b"},
    {file = "yarl-1.18.3-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:b1771de9944d875f1b98a745bc547e684b863abf8f8287da8466cf470ef52690"},
    {file = "yarl-1.18.3-cp310-cp310-win32.whl", hash = "sha256:8874027a53e3aea659a6d62751800cf6e63314c160fd607489ba5c2edd753cf6"},
    {file = "yarl-1.18.3-cp310-cp310-win_amd64.whl", hash = "sha256:93b2e109287f93db79210f86deb6b9bbb81ac32fc97236b16f7433db7fc437d8"},
    {file = "yarl-1.18.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:8503ad47387b8ebd39cbbbdf0bf113e17330ffd339ba1144074da24c545f0069"},
    {file = "yarl-1.18.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:02ddb6756f8f4517a2d5e99d8b2f272488e18dd0bfbc802f31c16c6c20f22193"},
    {file = "yarl-1.18.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:67a283dd2882ac98cc6318384f565bffc751ab564605959df4752d42483ad889"},
    {file = "yarl-1.18.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d980e0325b6eddc81331d3f4551e2a333999fb176fd153e075c6d1c2530aa8a8"},
    {file = "yarl-1.18.3-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b643562c12680b01e17239be267bc306bbc6aac1f34f6444d1bded0c5ce438ca"},
    {file = "yarl-1.18.3-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c017a3b6df3a1bd45b9fa49a0f54005e53fbcad16633870104b66fa1a30a29d8"},
    {file = "yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:75674776d96d7b851b6498f17824ba17849d790a44d282929c42dbb77d4f17ae"},
    {file = "yarl-1.18.3-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ccaa3a4b521b780a7e771cc336a2dba389a0861592bbce09a476190bb0c8b4b3"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:2d06d3005e668744e11ed80812e61efd77d70bb7f03e33c1598c301eea20efbb"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_armv7l.whl", hash = "sha256:9d41beda9dc97ca9ab0b9888cb71f7539124bc05df02c0cff6e5acc5a19dcc6e"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:ba23302c0c61a9999784e73809427c9dbedd79f66a13d84ad1b1943802eaaf59"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:6748dbf9bfa5ba1afcc7556b71cda0d7ce5f24768043a02a58846e4a443d808d"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:0b0cad37311123211dc91eadcb322ef4d4a66008d3e1bdc404808992260e1a0e"},
    {file = "yarl-1.18.3-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:0fb2171a4486bb075316ee754c6d8382ea6eb8b399d4ec62fde2b591f879778a"},
    {file = "yarl-1.18.3-cp311-cp311-win32.whl", hash = "sha256:61b1a825a13bef4a5f10b1885245377d3cd0bf87cba068e1d9a88c2ae36880e1"},
    {file = "yarl-1.18.3-cp311-cp311-win_amd64.whl", hash = "sha256:b9d60031cf568c627d028239693fd718025719c02c9f55df0a53e587aab951b5"},
    {file = "yarl-1.18.3-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:1dd4bdd05407ced96fed3d7f25dbbf88d2ffb045a0db60dbc247f5b3c5c25d50"},
    {file = "yarl-1.18.3-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:7c33dd1931a95e5d9a772d0ac5e44cac8957eaf58e3c8da8c1414de7dd27c576"},
    {file = "yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:25b411eddcfd56a2f0cd6a384e9f4f7aa3efee14b188de13048c25b5e91f1640"},
    {file = "yarl-1.18.3-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:436c4fc0a4d66b2badc6c5fc5ef4e47bb10e4fd9bf0c79524ac719a01f3607c2"},
    {file = "yarl-1.18.3-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e35ef8683211db69ffe129a25d5634319a677570ab6b2eba4afa860f54eeaf75"},
    {file = "yarl-1.18.3-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:84b2deecba4a3f1a398df819151eb72d29bfeb3b69abb145a00ddc8d30094512"},
    {file = "yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:00e5a1fea0fd4f5bfa7440a47eff01d9822a65b4488f7cff83155a0f31a2ecba"},
    {file = "yarl-1.18.3-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d0e883008013c0e4aef84dcfe2a0b172c4d23c2669412cf5b3371003941f72bb"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:5a3f356548e34a70b0172d8890006c37be92995f62d95a07b4a42e90fba54272"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_armv7l.whl", hash = "sha256:ccd17349166b1bee6e529b4add61727d3f55edb7babbe4069b5764c9587a8cc6"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:b958ddd075ddba5b09bb0be8a6d9906d2ce933aee81100db289badbeb966f54e"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:c7d79f7d9aabd6011004e33b22bc13056a3e3fb54794d138af57f5ee9d9032cb"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:4891ed92157e5430874dad17b15eb1fda57627710756c27422200c52d8a4e393"},
    {file = "yarl-1.18.3-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:ce1af883b94304f493698b00d0f006d56aea98aeb49d75ec7d98cd4a777e9285"},
    {file = "yarl-1.18.3-cp312-cp312-win32.whl", hash = "sha256:f91c4803173928a25e1a55b943c81f55b8872f0018be83e3ad4938adffb77dd2"},
    {file = "yarl-1.18.3-cp312-cp312-win_amd64.whl", hash = "sha256:7e2ee16578af3b52ac2f334c3b1f92262f47e02cc6193c598502bd46f5cd1477"},
    {file = "yarl-1.18.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:90adb47ad432332d4f0bc28f83a5963f426ce9a1a8809f5e584e704b82685dcb"},
    {file = "yarl-1.18.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:913829534200eb0f789d45349e55203a091f45c37a2674678744ae52fae23efa"},
    {file = "yarl-1.18.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:ef9f7768395923c3039055c14334ba4d926f3baf7b776c923c93d80195624782"},
    {file = "yarl-1.18.3-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:88a19f62ff30117e706ebc9090b8ecc79aeb77d0b1f5ec10d2d27a12bc9f66d0"},
    {file = "yarl-1.18.3-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e17c9361d46a4d5addf777c6dd5eab0715a7684c2f11b88c67ac37edfba6c482"},
    {file = "yarl-1.18.3-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1a74a13a4c857a84a845505fd2d68e54826a2cd01935a96efb1e9d86c728e186"},
    {file = "yarl-1.18.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:41f7ce59d6ee7741af71d82020346af364949314ed3d87553763a2df1829cc58"},
    {file = "yarl-1.18.3-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:f52a265001d830bc425f82ca9eabda94a64a4d753b07d623a9f2863fde532b53"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:82123d0c954dc58db301f5021a01854a85bf1f3bb7d12ae0c01afc414a882ca2"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:2ec9bbba33b2d00999af4631a3397d1fd78290c48e2a3e52d8dd72db3a067ac8"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:fbd6748e8ab9b41171bb95c6142faf068f5ef1511935a0aa07025438dd9a9bc1"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:877d209b6aebeb5b16c42cbb377f5f94d9e556626b1bfff66d7b0d115be88d0a"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:b464c4ab4bfcb41e3bfd3f1c26600d038376c2de3297760dfe064d2cb7ea8e10"},
    {file = "yarl-1.18.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8d39d351e7faf01483cc7ff7c0213c412e38e5a340238826be7e0e4da450fdc8"},
    {file = "yarl-1.18.3-cp313-cp313-win32.whl", hash = "sha256:61ee62ead9b68b9123ec24bc866cbef297dd266175d53296e2db5e7f797f902d"},
    {file = "yarl-1.18.3-cp313-cp313-win_amd64.whl", hash = "sha256:578e281c393af575879990861823ef19d66e2b1d0098414855dd367e234f5b3c"},
    {file = "yarl-1.18.3-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:61e5e68cb65ac8f547f6b5ef933f510134a6bf31bb178be428994b0cb46c2a04"},
    {file = "yarl-1.18.3-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:fe57328fbc1bfd0bd0514470ac692630f3901c0ee39052ae47acd1d90a436719"},
    {file = "yarl-1.18.3-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:a440a2a624683108a1b454705ecd7afc1c3438a08e890a1513d468671d90a04e"},
    {file = "yarl-1.18.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:09c7907c8548bcd6ab860e5f513e727c53b4a714f459b084f6580b49fa1b9cee"},
    {file = "yarl-1.18.3-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b4f6450109834af88cb4cc5ecddfc5380ebb9c228695afc11915a0bf82116789"},
    {file = "yarl-1.18.3-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:a9ca04806f3be0ac6d558fffc2fdf8fcef767e0489d2684a21912cc4ed0cd1b8"},
    {file = "yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:77a6e85b90a7641d2e07184df5557132a337f136250caafc9ccaa4a2a998ca2c"},
    {file = "yarl-1.18.3-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:6333c5a377c8e2f5fae35e7b8f145c617b02c939d04110c76f29ee3676b5f9a5"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:0b3c92fa08759dbf12b3a59579a4096ba9af8dd344d9a813fc7f5070d86bbab1"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_armv7l.whl", hash = "sha256:4ac515b860c36becb81bb84b667466885096b5fc85596948548b667da3bf9f24"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:045b8482ce9483ada4f3f23b3774f4e1bf4f23a2d5c912ed5170f68efb053318"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:a4bb030cf46a434ec0225bddbebd4b89e6471814ca851abb8696170adb163985"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:54d6921f07555713b9300bee9c50fb46e57e2e639027089b1d795ecd9f7fa910"},
    {file = "yarl-1.18.3-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:1d407181cfa6e70077df3377938c08012d18893f9f20e92f7d2f314a437c30b1"},
    {file = "yarl-1.18.3-cp39-cp39-win32.whl", hash = "sha256:ac36703a585e0929b032fbaab0707b75dc12703766d0b53486eabd5139ebadd5"},
    {file = "yarl-1.18.3-cp39-cp39-win_amd64.whl", hash = "sha256:ba87babd629f8af77f557b61e49e7c7cac36f22f871156b91e10a6e9d4f829e9"},
    {file = "yarl-1.18.3-py3-none-any.whl", hash = "sha256:b57f4f58099328dfb26c6a771d09fb20dbbae81d20cfb66141251ea063bd101b"},
    {file = "yarl-1.18.3.tar.gz", hash = "sha256:ac1801c45cbf77b6c99242eeff4fffb5e4e73a800b5c4ad4fc0be5def634d2e1"},
]

[package.dependencies]
idna = ">=2.0"
multidict = ">=4.0"
propcache = ">=0.2.0"

[[package]]
name = "zstandard"
version = "0.23.0"
description = "Zstandard bindings for Python"
optional = false
python-versions = ">=3.8"
files = [
    {file = "zstandard-0.23.0-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:bf0a05b6059c0528477fba9054d09179beb63744355cab9f38059548fedd46a9"},
    {file = "zstandard-0.23.0-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:fc9ca1c9718cb3b06634c7c8dec57d24e9438b2aa9a0f02b8bb36bf478538880"},
    {file = "zstandard-0.23.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:77da4c6bfa20dd5ea25cbf12c76f181a8e8cd7ea231c673828d0386b1740b8dc"},
    {file = "zstandard-0.23.0-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b2170c7e0367dde86a2647ed5b6f57394ea7f53545746104c6b09fc1f4223573"},
    {file = "zstandard-0.23.0-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c16842b846a8d2a145223f520b7e18b57c8f476924bda92aeee3a88d11cfc391"},
    {file = "zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:157e89ceb4054029a289fb504c98c6a9fe8010f1680de0201b3eb5dc20aa6d9e"},
    {file = "zstandard-0.23.0-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:203d236f4c94cd8379d1ea61db2fce20730b4c38d7f1c34506a31b34edc87bdd"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:dc5d1a49d3f8262be192589a4b72f0d03b72dcf46c51ad5852a4fdc67be7b9e4"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:752bf8a74412b9892f4e5b58f2f890a039f57037f52c89a740757ebd807f33ea"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:80080816b4f52a9d886e67f1f96912891074903238fe54f2de8b786f86baded2"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:84433dddea68571a6d6bd4fbf8ff398236031149116a7fff6f777ff95cad3df9"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:ab19a2d91963ed9e42b4e8d77cd847ae8381576585bad79dbd0a8837a9f6620a"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:59556bf80a7094d0cfb9f5e50bb2db27fefb75d5138bb16fb052b61b0e0eeeb0"},
    {file = "zstandard-0.23.0-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:27d3ef2252d2e62476389ca8f9b0cf2bbafb082a3b6bfe9d90cbcbb5529ecf7c"},
    {file = "zstandard-0.23.0-cp310-cp310-win32.whl", hash = "sha256:5d41d5e025f1e0bccae4928981e71b2334c60f580bdc8345f824e7c0a4c2a813"},
    {file = "zstandard-0.23.0-cp310-cp310-win_amd64.whl", hash = "sha256:519fbf169dfac1222a76ba8861ef4ac7f0530c35dd79ba5727014613f91613d4"},
    {file = "zstandard-0.23.0-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:34895a41273ad33347b2fc70e1bff4240556de3c46c6ea430a7ed91f9042aa4e"},
    {file = "zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:77ea385f7dd5b5676d7fd943292ffa18fbf5c72ba98f7d09fc1fb9e819b34c23"},
    {file = "zstandard-0.23.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:983b6efd649723474f29ed42e1467f90a35a74793437d0bc64a5bf482bedfa0a"},
    {file = "zstandard-0.23.0-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:80a539906390591dd39ebb8d773771dc4db82ace6372c4d41e2d293f8e32b8db"},
    {file = "zstandard-0.23.0-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:445e4cb5048b04e90ce96a79b4b63140e3f4ab5f662321975679b5f6360b90e2"},
    {file = "zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:fd30d9c67d13d891f2360b2a120186729c111238ac63b43dbd37a5a40670b8ca"},
    {file = "zstandard-0.23.0-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:d20fd853fbb5807c8e84c136c278827b6167ded66c72ec6f9a14b863d809211c"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ed1708dbf4d2e3a1c5c69110ba2b4eb6678262028afd6c6fbcc5a8dac9cda68e"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:be9b5b8659dff1f913039c2feee1aca499cfbc19e98fa12bc85e037c17ec6ca5"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:65308f4b4890aa12d9b6ad9f2844b7ee42c7f7a4fd3390425b242ffc57498f48"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:98da17ce9cbf3bfe4617e836d561e433f871129e3a7ac16d6ef4c680f13a839c"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:8ed7d27cb56b3e058d3cf684d7200703bcae623e1dcc06ed1e18ecda39fee003"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:b69bb4f51daf461b15e7b3db033160937d3ff88303a7bc808c67bbc1eaf98c78"},
    {file = "zstandard-0.23.0-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:034b88913ecc1b097f528e42b539453fa82c3557e414b3de9d5632c80439a473"},
    {file = "zstandard-0.23.0-cp311-cp311-win32.whl", hash = "sha256:f2d4380bf5f62daabd7b751ea2339c1a21d1c9463f1feb7fc2bdcea2c29c3160"},
    {file = "zstandard-0.23.0-cp311-cp311-win_amd64.whl", hash = "sha256:62136da96a973bd2557f06ddd4e8e807f9e13cbb0bfb9cc06cfe6d98ea90dfe0"},
    {file = "zstandard-0.23.0-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:b4567955a6bc1b20e9c31612e615af6b53733491aeaa19a6b3b37f3b65477094"},
    {file = "zstandard-0.23.0-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:1e172f57cd78c20f13a3415cc8dfe24bf388614324d25539146594c16d78fcc8"},
    {file = "zstandard-0.23.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:b0e166f698c5a3e914947388c162be2583e0c638a4703fc6a543e23a88dea3c1"},
    {file = "zstandard-0.23.0-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:12a289832e520c6bd4dcaad68e944b86da3bad0d339ef7989fb7e88f92e96072"},
    {file = "zstandard-0.23.0-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d50d31bfedd53a928fed6707b15a8dbeef011bb6366297cc435accc888b27c20"},
    {file = "zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:72c68dda124a1a138340fb62fa21b9bf4848437d9ca60bd35db36f2d3345f373"},
    {file = "zstandard-0.23.0-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:53dd9d5e3d29f95acd5de6802e909ada8d8d8cfa37a3ac64836f3bc4bc5512db"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:6a41c120c3dbc0d81a8e8adc73312d668cd34acd7725f036992b1b72d22c1772"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:40b33d93c6eddf02d2c19f5773196068d875c41ca25730e8288e9b672897c105"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:9206649ec587e6b02bd124fb7799b86cddec350f6f6c14bc82a2b70183e708ba"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:76e79bc28a65f467e0409098fa2c4376931fd3207fbeb6b956c7c476d53746dd"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:66b689c107857eceabf2cf3d3fc699c3c0fe8ccd18df2219d978c0283e4c508a"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:9c236e635582742fee16603042553d276cca506e824fa2e6489db04039521e90"},
    {file = "zstandard-0.23.0-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:a8fffdbd9d1408006baaf02f1068d7dd1f016c6bcb7538682622c556e7b68e35"},
    {file = "zstandard-0.23.0-cp312-cp312-win32.whl", hash = "sha256:dc1d33abb8a0d754ea4763bad944fd965d3d95b5baef6b121c0c9013eaf1907d"},
    {file = "zstandard-0.23.0-cp312-cp312-win_amd64.whl", hash = "sha256:64585e1dba664dc67c7cdabd56c1e5685233fbb1fc1966cfba2a340ec0dfff7b"},
    {file = "zstandard-0.23.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:576856e8594e6649aee06ddbfc738fec6a834f7c85bf7cadd1c53d4a58186ef9"},
    {file = "zstandard-0.23.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:38302b78a850ff82656beaddeb0bb989a0322a8bbb1bf1ab10c17506681d772a"},
    {file = "zstandard-0.23.0-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d2240ddc86b74966c34554c49d00eaafa8200a18d3a5b6ffbf7da63b11d74ee2"},
    {file = "zstandard-0.23.0-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:2ef230a8fd217a2015bc91b74f6b3b7d6522ba48be29ad4ea0ca3a3775bf7dd5"},
    {file = "zstandard-0.23.0-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:774d45b1fac1461f48698a9d4b5fa19a69d47ece02fa469825b442263f04021f"},
    {file = "zstandard-0.23.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6f77fa49079891a4aab203d0b1744acc85577ed16d767b52fc089d83faf8d8ed"},
    {file = "zstandard-0.23.0-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:ac184f87ff521f4840e6ea0b10c0ec90c6b1dcd0bad2f1e4a9a1b4fa177982ea"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c363b53e257246a954ebc7c488304b5592b9c53fbe74d03bc1c64dda153fb847"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:e7792606d606c8df5277c32ccb58f29b9b8603bf83b48639b7aedf6df4fe8171"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a0817825b900fcd43ac5d05b8b3079937073d2b1ff9cf89427590718b70dd840"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:9da6bc32faac9a293ddfdcb9108d4b20416219461e4ec64dfea8383cac186690"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:fd7699e8fd9969f455ef2926221e0233f81a2542921471382e77a9e2f2b57f4b"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:d477ed829077cd945b01fc3115edd132c47e6540ddcd96ca169facff28173057"},
    {file = "zstandard-0.23.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa6ce8b52c5987b3e34d5674b0ab529a4602b632ebab0a93b07bfb4dfc8f8a33"},
    {file = "zstandard-0.23.0-cp313-cp313-win32.whl", hash = "sha256:a9b07268d0c3ca5c170a385a0ab9fb7fdd9f5fd866be004c4ea39e44edce47dd"},
    {file = "zstandard-0.23.0-cp313-cp313-win_amd64.whl", hash = "sha256:f3513916e8c645d0610815c257cbfd3242adfd5c4cfa78be514e5a3ebb42a41b"},
    {file = "zstandard-0.23.0-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:2ef3775758346d9ac6214123887d25c7061c92afe1f2b354f9388e9e4d48acfc"},
    {file = "zstandard-0.23.0-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:4051e406288b8cdbb993798b9a45c59a4896b6ecee2f875424ec10276a895740"},
    {file = "zstandard-0.23.0-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e2d1a054f8f0a191004675755448d12be47fa9bebbcffa3cdf01db19f2d30a54"},
    {file = "zstandard-0.23.0-cp38-cp38-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:f83fa6cae3fff8e98691248c9320356971b59678a17f20656a9e59cd32cee6d8"},
    {file = "zstandard-0.23.0-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:32ba3b5ccde2d581b1e6aa952c836a6291e8435d788f656fe5976445865ae045"},
    {file = "zstandard-0.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2f146f50723defec2975fb7e388ae3a024eb7151542d1599527ec2aa9cacb152"},
    {file = "zstandard-0.23.0-cp38-cp38-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:1bfe8de1da6d104f15a60d4a8a768288f66aa953bbe00d027398b93fb9680b26"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_1_aarch64.whl", hash = "sha256:29a2bc7c1b09b0af938b7a8343174b987ae021705acabcbae560166567f5a8db"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:61f89436cbfede4bc4e91b4397eaa3e2108ebe96d05e93d6ccc95ab5714be512"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:53ea7cdc96c6eb56e76bb06894bcfb5dfa93b7adcf59d61c6b92674e24e2dd5e"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_2_i686.whl", hash = "sha256:a4ae99c57668ca1e78597d8b06d5af837f377f340f4cce993b551b2d7731778d"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:379b378ae694ba78cef921581ebd420c938936a153ded602c4fea612b7eaa90d"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:50a80baba0285386f97ea36239855f6020ce452456605f262b2d33ac35c7770b"},
    {file = "zstandard-0.23.0-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:61062387ad820c654b6a6b5f0b94484fa19515e0c5116faf29f41a6bc91ded6e"},
    {file = "zstandard-0.23.0-cp38-cp38-win32.whl", hash = "sha256:b8c0bd73aeac689beacd4e7667d48c299f61b959475cdbb91e7d3d88d27c56b9"},
    {file = "zstandard-0.23.0-cp38-cp38-win_amd64.whl", hash = "sha256:a05e6d6218461eb1b4771d973728f0133b2a4613a6779995df557f70794fd60f"},
    {file = "zstandard-0.23.0-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:3aa014d55c3af933c1315eb4bb06dd0459661cc0b15cd61077afa6489bec63bb"},
    {file = "zstandard-0.23.0-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:0a7f0804bb3799414af278e9ad51be25edf67f78f916e08afdb983e74161b916"},
    {file = "zstandard-0.23.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fb2b1ecfef1e67897d336de3a0e3f52478182d6a47eda86cbd42504c5cbd009a"},
    {file = "zstandard-0.23.0-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:837bb6764be6919963ef41235fd56a6486b132ea64afe5fafb4cb279ac44f259"},
    {file = "zstandard-0.23.0-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1516c8c37d3a053b01c1c15b182f3b5f5eef19ced9b930b684a73bad121addf4"},
    {file = "zstandard-0.23.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:48ef6a43b1846f6025dde6ed9fee0c24e1149c1c25f7fb0a0585572b2f3adc58"},
    {file = "zstandard-0.23.0-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:11e3bf3c924853a2d5835b24f03eeba7fc9b07d8ca499e247e06ff5676461a15"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:2fb4535137de7e244c230e24f9d1ec194f61721c86ebea04e1581d9d06ea1269"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:8c24f21fa2af4bb9f2c492a86fe0c34e6d2c63812a839590edaf177b7398f700"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:a8c86881813a78a6f4508ef9daf9d4995b8ac2d147dcb1a450448941398091c9"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:fe3b385d996ee0822fd46528d9f0443b880d4d05528fd26a9119a54ec3f91c69"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:82d17e94d735c99621bf8ebf9995f870a6b3e6d14543b99e201ae046dfe7de70"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:c7c517d74bea1a6afd39aa612fa025e6b8011982a0897768a2f7c8ab4ebb78a2"},
    {file = "zstandard-0.23.0-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:1fd7e0f1cfb70eb2f95a19b472ee7ad6d9a0a992ec0ae53286870c104ca939e5"},
    {file = "zstandard-0.23.0-cp39-cp39-win32.whl", hash = "sha256:43da0f0092281bf501f9c5f6f3b4c975a8a0ea82de49ba3f7100e64d422a1274"},
    {file = "zstandard-0.23.0-cp39-cp39-win_amd64.whl", hash = "sha256:f8346bfa098532bc1fb6c7ef06783e969d87a99dd1d2a5a18a892c1d7a643c58"},
    {file = "zstandard-0.23.0.tar.gz", hash = "sha256:b2d8c62d08e7255f68f7a740bae85b3c9b8e5466baa9cbf7f57f1cde0ac6bc09"},
]

[package.dependencies]
cffi = {version = ">=1.11", markers = "platform_python_implementation == \"PyPy\""}

[package.extras]
cffi = ["cffi (>=1.11)"]

[metadata]
lock-version = "2.0"
python-versions = ">=3.11,<4.0.0"
content-hash = "0f48eaf867d6d63eeea25a7d0f870c231903e4b30149d19298745130e2a56754"



================================================
FILE: memory-v2/pyproject.toml
================================================
[tool.poetry]
name = "memory-v2"
version = "0.0.1"
description = "Memory graph for the Social Media Agent."
authors = ["Brace Sproul"]
readme = "README.md"
license = "MIT"
packages = [{include = "memory_v2"}]

[tool.poetry.dependencies]
python = ">=3.11,<4.0.0"
langmem = ">=0.0.5rc5"
langgraph = ">=0.2.66"
langchain-anthropic = ">=0.3.3"
langgraph-sdk = ">=0.1.51"
python-dotenv = ">=1.0.1"

[tool.poetry.group.dev.dependencies]
mypy = ">=1.11.1"
ruff = ">=0.6.1"

[tool.ruff]
lint.select = [
    "E",    # pycodestyle
    "F",    # pyflakes
    "I",    # isort
    "D",    # pydocstyle
    "D401", # First line should be in imperative mood
    "T201",
    "UP",
]
lint.ignore = [
    "UP006",
    "UP007",
    # We actually do want to import from typing_extensions
    "UP035",
    # Relax the convention by _not_ requiring documentation for every function parameter.
    "D417",
    "E501",
]
[tool.ruff.lint.per-file-ignores]
"tests/*" = ["D", "UP"]
[tool.ruff.lint.pydocstyle]
convention = "google"


================================================
FILE: memory-v2/memory_v2/graph.py
================================================
"""The reflection graph."""

from typing import Any, Dict

from langchain_anthropic import ChatAnthropic
from langgraph.graph import StateGraph
from langgraph.store.base import BaseStore
from langmem.prompts.looping import (
    Prompt,
    create_prompt_optimizer,
)

from memory_v2.state import State


REFLECTIONS_NAMESPACE = ("reflection_rules",)
REFLECTIONS_KEY = "rules"
PROMPT_KEY = "prompt"


async def aget_reflections(store: BaseStore) -> str:
    """Get reflections from the store."""
    reflections = await store.aget(REFLECTIONS_NAMESPACE, REFLECTIONS_KEY)

    if not reflections:
        return "No prompt rules have been created yet."

    ruleset = reflections.value.get(
        PROMPT_KEY, "No prompt rules have been created yet."
    )

    return ruleset


async def aput_reflections(store: BaseStore, reflections: str) -> None:
    """Put reflections in the store."""
    await store.aput(REFLECTIONS_NAMESPACE, REFLECTIONS_KEY, {PROMPT_KEY: reflections})


async def reflection(state: State, store: BaseStore) -> Dict[str, Any]:
    """Process reflection and update rules based on user interaction."""
    model = ChatAnthropic(model="claude-3-5-sonnet-latest", temperature=0)

    current_reflections_prompt = await aget_reflections(store)

    update_instructions = """Analyze the following to determine if rules prompt updates are needed:
1. Current rules prompt (current_prompt)
2. Generated social media post (session)
3. User feedback on the post (feedback)

If the user's feedback explicitly requests changes:
1. Create or update rules that directly address the feedback
2. Keep each rule clear, specific, and concise
3. If a new rule conflicts with an existing one, use the new rule
4. Only add rules that are explicitly mentioned in the user's feedback

Guidelines for updates:
- Do not infer or assume rules beyond what's explicitly stated
- Do not add rules based on implicit feedback
- Do not overgeneralize the feedback
- Combine existing rules if it improves clarity without losing specificity

Output only the updated rules prompt, with no additional context or instructions."""

    feedback = state.user_response

    prompt = Prompt(
        name="Update Prompt",
        prompt=current_reflections_prompt,
        update_instructions=update_instructions,
        feedback=feedback,
    )

    sessions = state.original_post

    optimizer = create_prompt_optimizer(model, kind="metaprompt")

    result = await optimizer(sessions, prompt)

    await aput_reflections(store, result)

    return {}


# Define a new graph
workflow = StateGraph(State)
workflow.add_node("reflection", reflection)
workflow.add_edge("__start__", "reflection")

graph = workflow.compile()
graph.name = "Reflection Graph"



================================================
FILE: memory-v2/memory_v2/state.py
================================================
"""Define the state structures for the agent."""

from __future__ import annotations

from dataclasses import dataclass


@dataclass
class State:
    """The state of the memory graph."""

    original_post: str = ""
    """The original post that the user submitted feedback on"""
    user_response: str = ""
    """The user's feedback on the new post"""



================================================
FILE: scripts/README.md
================================================
# Social Media Agent Scripts

## Setup

First ensure you have all dependencies installed:

```bash
yarn install
```

And your `LANGCHAIN_API_KEY`, `LANGGRAPH_API_URL` environment variables set:

```bash
LANGCHAIN_API_KEY=...
LANGGRAPH_API_URL=...
```

Some scripts will send output to Slack. If you want this output to post to Slack, ensure you have the `SLACK_BOT_OAUTH_TOKEN` and `SLACK_CHANNEL_ID` environment variables set:

```bash
SLACK_BOT_OAUTH_TOKEN=...
SLACK_CHANNEL_ID=...
```

If you don't want to post to Slack, the script will print the output to the console.

## Scripts

### Get Scheduled Runs

This script will fetch all scheduled runs and either send them to Slack or print them to the console.

```bash
yarn get:scheduled_runs
```

### Get all used links

This script will fetch and log all links which are currently scheduled, or interrupted and awaiting human intervention.

```bash
yarn get:used_links
```

### Generate Demo Post

This script will invoke the graph to generate a post. It defaults to a LangChain blog post, and typically used to demonstrate how the Social Media Agent works.

```bash
yarn generate_post
```

### Delete Run(s) & Thread(s)

This script will delete runs and associated threads. It requires setting the run ID(s) and thread ID(s) in the script.

```bash
yarn graph:delete:run_thread
```

### Backfill

This script will backfill your deployment with links. It contains two functions, one for backfilling from Slack, and one for backfilling from a list of links. You'll need to uncomment one/both of the functions to use them.

```bash
yarn graph:backfill
```

### Create Cron

This script will create a cron job to run the `ingest_data` graph.

```bash
yarn cron:create
```

### Delete Cron

This script will delete a cron job.

```bash
yarn cron:delete
```

### List Crons

This script will list all cron jobs.

```bash
yarn cron:list
```



================================================
FILE: scripts/backfill.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";
import {
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
} from "../src/agents/generate-post/constants.js";

/**
 * Performs a manual data backfill operation using LangGraph.
 *
 * This function creates a new thread and initiates a data ingestion run
 * to backfill historical data. It's useful for one-time data imports
 * or catching up on missed data ingestion periods.
 *
 * The default configuration looks back 7 days, but this can be adjusted
 * via the `maxDaysHistory` parameter in the config.
 *
 * @async
 * @returns {Promise<void>} A promise that resolves when the backfill run is created
 * @throws {Error} If there's an issue creating the thread or initiating the run
 *
 * @example
 * ```bash
 * yarn graph:backfill
 * ```
 */
export async function backfill() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const thread = await client.threads.create();
  const res = await client.runs.create(thread.thread_id, "ingest_data", {
    config: {
      configurable: {
        slackChannelId: "ADD_SLACK_CHANNEL_ID_HERE",
        maxDaysHistory: 50, // Or change to desired number of days
        [SKIP_CONTENT_RELEVANCY_CHECK]: true,
        [SKIP_USED_URLS_CHECK]: true,
      },
    },
    input: {},
  });
  console.log("Created run");
  console.log(res);
}

// backfill().catch(console.error);

/**
 * Backfill with links instead of ingesting from Slack.
 */
export async function backfillWithLinks() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const newLinksArr: string[] = [
    // Add your new links here
  ];

  const { thread_id } = await client.threads.create();
  await client.runs.create(thread_id, "ingest_data", {
    input: {
      links: newLinksArr,
    },
    config: {
      configurable: {
        skipIngest: true,
      },
    },
  });
}

// backfillWithLinks().catch(console.error);



================================================
FILE: scripts/checkLanggraphPaths.js
================================================
import fs from "fs";
import path from "path";
import { fileURLToPath } from "url";

// Function to check if a file exists
function fileExists(filePath) {
  return fs.existsSync(filePath);
}

// Function to check if an object is exported from a file
function isObjectExported(filePath, objectName) {
  if (filePath.endsWith(".py")) return true;

  try {
    const fileContent = fs.readFileSync(filePath, "utf8");
    const exportRegex = new RegExp(
      `export\\s+(?:const|let|var)\\s+${objectName}\\s*=|export\\s+\\{[^}]*\\b${objectName}\\b[^}]*\\}`,
    );
    return exportRegex.test(fileContent);
  } catch (error) {
    console.error(`Error reading file ${filePath}: ${error.message}`);
    return false;
  }
}

// Main function to check langgraph.json
function checkLanggraphPaths() {
  const __filename = fileURLToPath(import.meta.url);
  const __dirname = path.dirname(__filename);
  const langgraphPath = path.join(__dirname, "..", "langgraph.json");

  if (!fileExists(langgraphPath)) {
    console.error("langgraph.json not found in the root directory");
    process.exit(1);
  }

  try {
    const langgraphContent = JSON.parse(fs.readFileSync(langgraphPath, "utf8"));
    const graphs = langgraphContent.graphs;

    if (!graphs || typeof graphs !== "object") {
      console.error('Invalid or missing "graphs" object in langgraph.json');
      process.exit(1);
    }

    let hasError = false;

    for (const [key, value] of Object.entries(graphs)) {
      const [filePath, objectName] = value.split(":");
      const fullPath = path.join(__dirname, "..", filePath);

      if (!fileExists(fullPath)) {
        console.error(`File not found: ${fullPath}`);
        hasError = true;
        continue;
      }

      if (!isObjectExported(fullPath, objectName)) {
        console.error(
          `Object "${objectName}" is not exported from ${fullPath}`,
        );
        hasError = true;
      }
    }

    if (hasError) {
      process.exit(1);
    } else {
      console.log(
        "All paths in langgraph.json are valid and objects are exported correctly.",
      );
    }
  } catch (error) {
    console.error(`Error parsing langgraph.json: ${error.message}`);
    process.exit(1);
  }
}

checkLanggraphPaths();



================================================
FILE: scripts/delete-run-thread.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

// Uncomment to delete a single run & thread

// async function deleteRunAndThread() {
//   const threadId = "ADD_THREAD_ID_HERE";
//   const runId = "ADD_RUN_ID_HERE";
//   const client = new Client({
//     apiUrl: process.env.LANGGRAPH_API_URL,
//   });

//   await client.runs.delete(threadId, runId);
//   await client.threads.delete(threadId);
// }

// deleteRunAndThread().catch(console.error);

async function deleteRunsAndThreads() {
  const runAndThreadIds = [
    {
      runId: "ADD_RUN_ID_HERE",
      threadId: "ADD_THREAD_ID_HERE",
    },
    // ...
  ];

  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  await Promise.all(
    runAndThreadIds.map(async ({ runId, threadId }) => {
      try {
        await client.runs.delete(threadId, runId);
      } catch (e) {
        console.error(
          "Failed to delete run",
          runId,
          "from thread",
          threadId,
          e,
        );
      }

      try {
        await client.threads.delete(threadId);
      } catch (e) {
        console.error("Failed to delete thread", threadId, e);
      }
    }),
  );
}

deleteRunsAndThreads().catch(console.error);



================================================
FILE: scripts/generate-post.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";
import {
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
  TEXT_ONLY_MODE,
} from "../src/agents/generate-post/constants.js";

/**
 * Generate a post based on a LangChain blog post.
 * This may be modified to generate posts for other content.
 */
async function invokeGraph() {
  const link = "https://blog.langchain.dev/customers-appfolio/";

  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL || "http://localhost:54367",
  });

  const { thread_id } = await client.threads.create();
  await client.runs.create(thread_id, "generate_post", {
    input: {
      links: [link],
    },
    config: {
      configurable: {
        // By default, the graph will read these values from the environment
        // [TWITTER_USER_ID]: process.env.TWITTER_USER_ID,
        // [LINKEDIN_USER_ID]: process.env.LINKEDIN_USER_ID,
        // This ensures the graph runs in a basic text only mode.
        // If you followed the full setup instructions, you may remove this line.
        [TEXT_ONLY_MODE]: false,
        // These will skip content relevancy checks and used URLs checks
        [SKIP_CONTENT_RELEVANCY_CHECK]: true,
        [SKIP_USED_URLS_CHECK]: true,
      },
    },
  });
}

invokeGraph().catch(console.error);



================================================
FILE: scripts/get-all-used-links.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";
import { extractUrls } from "../src/agents/utils.js";

async function getCurrentInterrupts() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const interrupts = await client.threads.search({
    status: "interrupted",
    limit: 100,
  });

  const links: string[] = interrupts.flatMap(
    (i) => (i.values as Record<string, any>).links,
  );

  return links;
}

async function getScheduledPosts() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const threads = await client.threads.search({
    limit: 300,
    metadata: {
      graph_id: "upload_post",
    },
  });
  const idleAndBusyThreads = threads.filter(
    (t) => t.status === "idle" || t.status === "busy",
  );

  let links: string[] = [];

  for (const { thread_id } of idleAndBusyThreads) {
    const run = await client.runs.list(thread_id);
    const linksFromPost = extractUrls((run[0] as any).kwargs.input.post);
    if (linksFromPost.length > 0) {
      links = links.concat(linksFromPost);
    }
  }

  return links;
}

export async function getAllUsedLinks() {
  const currentInterrupts = await getCurrentInterrupts();
  const scheduledPosts = await getScheduledPosts();
  return [...new Set(currentInterrupts.concat(scheduledPosts))];
}

console.log(await getAllUsedLinks());



================================================
FILE: scripts/get-scheduled-runs.ts
================================================
import "dotenv/config";
import { Client, Run } from "@langchain/langgraph-sdk";
import { SlackClient } from "../src/clients/slack/client.js";
import { format } from "date-fns";
import { toZonedTime } from "date-fns-tz";

type PendingRun = {
  thread_id: string;
  run_id: string;
  post: string;
  image?: {
    imageUrl: string;
    mimeType: string;
  };
  scheduleDate: string;
};

async function getScheduledRuns() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
    // apiUrl: "http://localhost:54367",
  });

  const threads = await client.threads.search({
    metadata: {
      graph_id: "upload_post",
    },
    status: "busy",
  });
  let pendingRuns: PendingRun[] = [];

  for await (const thread of threads) {
    const runs = await client.runs.list(thread.thread_id);
    const run = runs[0] as Run & {
      kwargs: Record<string, any>;
    };
    if (!run) {
      console.warn(`No run found for thread ${thread.thread_id}`);
      continue;
    }
    pendingRuns.push({
      thread_id: thread.thread_id,
      run_id: run.run_id,
      post: run.kwargs.input.post,
      image: run.kwargs.input.image,
      scheduleDate: run.created_at,
    });
  }

  // Sort the pending runs by schedule date
  pendingRuns.sort((a, b) => {
    return (
      new Date(a.scheduleDate).getTime() - new Date(b.scheduleDate).getTime()
    );
  });

  const pendingRunsString = pendingRuns.map(
    (post, index) => `*Post ${index + 1}*:

Scheduled for *${format(toZonedTime(new Date(post.scheduleDate), "America/Los_Angeles"), "MM/dd hh:mm a")} PST*

Post:
\`\`\`
${post.post}
\`\`\`

Image:
\`\`\`
${post.image?.imageUrl}
\`\`\``,
  );

  const slackMessageContent = `Number of scheduled posts: *${pendingRuns.length}*
  
Scheduled posts:

${pendingRunsString.join("\n\n")}`;

  if (process.env.SLACK_CHANNEL_ID && process.env.SLACK_CHANNEL_ID) {
    const slackClient = new SlackClient();

    await slackClient.sendMessage(
      process.env.SLACK_CHANNEL_ID,
      slackMessageContent,
    );
  } else {
    console.log(slackMessageContent);
  }
}

getScheduledRuns().catch(console.error);



================================================
FILE: scripts/reinterrupt.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

async function getInterrupts(client: Client) {
  const interrupts = await client.threads.search({
    status: "interrupted",
    limit: 1000,
  });
  return interrupts;
}

export async function redoInterrupts() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const interrupts = await getInterrupts(client);

  for await (const item of interrupts) {
    const values = item.values as Record<string, any>;

    await client.runs.create(item.thread_id, "generate_post", {
      command: {
        update: {
          ...values,
        },
        goto: "humanNode",
      },
    });
  }
}

// redoInterrupts().catch(console.error);

export async function getAllRuns() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });
  const threads = await client.threads.search({
    status: "interrupted",
    limit: 1000,
  });
  console.log("threads", threads.length);

  for (const { thread_id } of threads) {
    const runs = await client.runs.list(thread_id);
    await Promise.all(runs.map((r) => client.runs.delete(thread_id, r.run_id)));
    await client.threads.delete(thread_id);
  }
}

// getAllRuns().catch(console.error);



================================================
FILE: scripts/crons/create-cron.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";
import {
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
} from "../../src/agents/generate-post/constants.js";

/**
 * Creates a new cron job in LangGraph for data ingestion.
 *
 * This function sets up a daily cron job that runs at midnight (00:00) to ingest data.
 * It uses the LangGraph Client to create a new cron job with specified configuration
 * and then retrieves a list of all existing cron jobs.
 *
 * @async
 * @returns {Promise<void>} A promise that resolves when the cron job is created
 * and the list of crons is retrieved
 * @throws {Error} If there's an issue creating the cron job or retrieving the list
 *
 * @example
 * ```bash
 * yarn cron:create
 * ```
 */
async function createCron() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const res = await client.crons.create("ingest_data", {
    schedule: "0 8 * * *", // Runs at 8:00 AM UTC every day (1AM PST)
    config: {
      configurable: {
        slackChannelId: "ADD_SLACK_CHANNEL_ID_HERE",
        maxDaysHistory: 1,
        [SKIP_CONTENT_RELEVANCY_CHECK]: true,
        [SKIP_USED_URLS_CHECK]: true,
      },
    },
    input: {},
  });
  console.log("\n\nCreated cron\n\n");
  console.dir(res, { depth: null });

  const crons = await client.crons.search();
  console.log("\n\nAll Crons\n\n");
  console.dir(crons, { depth: null });
}

createCron().catch(console.error);



================================================
FILE: scripts/crons/delete-cron.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

/**
 * Deletes a specified cron job from LangGraph.
 *
 * This function connects to the LangGraph API and deletes a cron job with the specified ID.
 * After deletion, it retrieves and displays the updated list of cron jobs.
 *
 * To find available cron IDs that can be deleted, first run the list-crons script:
 *
 * ```bash
 * yarn cron:list
 * ```
 *
 * @async
 * @returns {Promise<void>} A promise that resolves when the cron job is deleted
 * and the updated list is displayed
 * @throws {Error} If there's an issue deleting the cron job or retrieving the list
 *
 * @example
 * ```bash
 * yarn cron:delete
 * ```
 */
async function deleteCron() {
  const cronId = "ADD_CRON_ID_HERE";

  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  await client.crons.delete(cronId);
  console.log("\n\nDeleted cron\n\n");
  const crons = await client.crons.search();
  console.log("\n\nAll Crons\n\n");
  console.dir(crons, { depth: null });
}

deleteCron().catch(console.error);



================================================
FILE: scripts/crons/list-crons.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

/**
 * Retrieves and displays a list of all configured cron jobs from LangGraph.
 *
 * This function connects to the LangGraph API and fetches all existing cron jobs,
 * then logs them to the console for inspection.
 *
 * @async
 * @returns {Promise<void>} A promise that resolves when the cron jobs are retrieved
 * and displayed
 * @throws {Error} If there's an issue connecting to the API or retrieving the cron jobs
 *
 * @example
 * ```bash
 * yarn cron:list
 * ```
 */
async function listCrons() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const crons = await client.crons.search();
  console.log("Crons");
  console.dir(crons, { depth: null });
}

listCrons().catch(console.error);



================================================
FILE: scripts/repurposer/create-cron.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

/**
 * Creates a new cron job in LangGraph for data ingestion.
 *
 * This function sets up a daily cron job that runs at midnight (00:00) to ingest data.
 * It uses the LangGraph Client to create a new cron job with specified configuration
 * and then retrieves a list of all existing cron jobs.
 *
 * @async
 * @returns {Promise<void>} A promise that resolves when the cron job is created
 * and the list of crons is retrieved
 * @throws {Error} If there's an issue creating the cron job or retrieving the list
 *
 * @example
 * ```bash
 * yarn cron:create
 * ```
 */
async function createCron() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL,
  });

  const res = await client.crons.create("ingest_repurposed_data", {
    schedule: "0 0 * * *",
    config: {
      configurable: {
        repurposerSlackChannelId: "ADD_SLACK_CHANNEL_ID_HERE",
        maxDaysHistory: 1,
      },
    },
    input: {},
  });
  console.log("Created cron");
  console.log(res);

  const crons = await client.crons.search();
  console.log("Crons");
  console.log(crons);
}

createCron().catch(console.error);



================================================
FILE: scripts/repurposer/ingest.ts
================================================
import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

async function invokeGraph() {
  const client = new Client({
    apiUrl: process.env.LANGGRAPH_API_URL || "http://localhost:54367",
  });

  const { thread_id } = await client.threads.create();
  await client.runs.create(thread_id, "ingest_repurposed_data", {
    input: {},
    config: {
      configurable: {
        repurposerSlackChannelId: "",
      },
    },
  });
}

invokeGraph().catch(console.error);



================================================
FILE: slack-messaging/README.md
================================================
# LangGraph Application Integration with Slack

Modern AI applications like chatbots and agents communicate through natural language, making messaging platforms like Slack an ideal interface for interacting with them. As these AI assistants take on more complex tasks, users need to engage with them in their native work environments rather than separate web interfaces.

This repository demonstrates how to connect any LangGraph-powered application (chatbot, agent, or other AI system) to Slack, allowing teams to interact with their AI assistants directly in their everyday communication channels. Currently focused on Slack integration, with a straightforward approach that can be adapted for other messaging platforms.

## Quickstart

### Prerequisites

- [LangGraph platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) deployment with a `messages` state key (e.g., a chatbot).

### Flow

The overall concept is simple: Slack routes are added directly to the API server deployed on the LangGraph platform using the custom routes support. The server has two main functions: first, it receives Slack events, packages them into a format that our LangGraph app can understand (chat `messages`), and passes them to our LangGraph app. Second, it receives the LangGraph app's responses, extracts the most recent `message` from the `messages` list, and sends it back to Slack.

<!-- this is outdated; Modal no longer used. Should replace. -->
<!-- ![slack_integration](https://github.com/user-attachments/assets/e73f5121-fed1-4cde-9297-3250ea273e1e) -->

### Quickstart setup

1. Install `uv` (optional) and dependencies.

```shell
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync --dev
```

2. Create a Slack app https://api.slack.com/apps/ and select `From A Manifest`.

3. Copy the below manifest and paste it into the `Manifest` field.

- Replace `your-app-name` with your app's name and `your-app-description` with your app's description.
- Replace `your-langgraph-platform-url` with your LangGraph platform URL (if you're testing locally, you can use something like ngrok for tunneling)
- The scopes gives the app the necessary permissions to read and write messages.
- The events are what we want to receive from Slack.

```JSON
{
    "display_information": {
        "name": "your-app-name"
    },
    "features": {
        "bot_user": {
            "display_name": "your-app-name",
            "always_online": false
        },
        "assistant_view": {
            "assistant_description": "your-app-description"
        }
    },
    "oauth_config": {
        "scopes": {
            "bot": [
                "app_mentions:read",
                "assistant:write",
                "channels:history",
                "channels:join",
                "channels:read",
                "chat:write",
                "groups:history",
                "groups:read",
                "im:history",
                "im:write",
                "mpim:history",
                "im:read",
                "chat:write.public"
            ]
        }
    },
    "settings": {
        "event_subscriptions": {
            "request_url": "your-langgraph-platform-url/events/slack",
            "bot_events": [
                "app_mention",
                "message.channels",
                "message.im",
                "message.mpim",
                "assistant_thread_started"
            ]
        },
        "org_deploy_enabled": false,
        "socket_mode_enabled": false,
        "token_rotation_enabled": false
    }
}
```

4. Got to `OAuth & Permissions` and `Install App to Workspace`.

5. Copy `SLACK_BOT_TOKEN` and `SLACK_SIGNING_SECRET` to the `.env` file:

- `OAuth & Permissions` page will expose the app's `SLACK_BOT_TOKEN` after installation.
- Go to "Basic Information" and get `SLACK_SIGNING_SECRET`.
- `SLACK_BOT_TOKEN` is used to authenticate API calls FROM your bot TO Slack.
- `SLACK_SIGNING_SECRET` is used to verify that incoming requests TO your server are actually FROM Slack.

6. Copy your LangGraph deployment's URL and assistant ID (or graph name) to the `.env` file.

- The `.env.example` file shows the required environment variables.
- Example environment variables:

```shell
# Slack credentials
SLACK_SIGNING_SECRET=
SLACK_BOT_TOKEN=xoxb-...
# SLACK_BOT_USER_ID= (optional)

# LangGraph platform instance you're connecting to
LANGGRAPH_ASSISTANT_ID=
CONFIG= # Optional
```

7. Deploy your application to the LangGraph platform with custom routes.

The Slack routes are added directly to the API server deployed on the LangGraph platform, using the custom routes support as documented at https://langchain-ai.github.io/langgraph/how-tos/http/custom_routes/.

The integration uses the langgraph_sdk and connects to the current platform routes by setting URL to None, which connects to the loopback server. The Slack Bolt SDK is used to register a slack webhook to handle slack events (new messages, @mentions, etc). When an event is received, it creates a run on the current server and passes in a webhook "/webhooks/<thread_id>" that is triggered when the chatbot completes. Since a relative path is provided, the LangGraph platform knows to call the route on this server itself.

8. Add your LangGraph platform URL to `Event Subscriptions` in Slack with `/events/slack` appended.

- E.g., `https://your-langgraph-platform-url/events/slack` as the request URL.
- This is the URL that Slack will send events to.

## `From Scratch` Slack App Setup

You can use this setup to customize your Slack app permissions and event subscriptions.

1. Create a Slack app https://api.slack.com/apps/ and select `From Scratch`.

2. Go to `OAuth & Permissions` and add your desired `Bot Token Scopes`.

- This gives the app the necessary permissions to read and write messages.
- Add scopes for the app's functionality, as an example:

```
# Reading Messages
"app_mentions:read",     # View when the bot is @mentioned
"channels:read",         # View basic channel info and membership
"channels:history",      # View messages in public channels
"groups:read",          # View private channel info and membership
"groups:history",       # View messages in private channels
"im:read",             # View direct message info
"im:history",          # View messages in direct messages
"mpim:history",        # View messages in group direct messages

# Writing Messages
"chat:write",          # Send messages in channels the bot is in
"chat:write.public",   # Send messages in any public channel
"im:write",           # Send direct messages to users

# Special Permissions
"assistant:write",     # Use Slack's built-in AI features
"channels:join",       # Join public channels automatically
```

3. Then, go to `OAuth & Permissions` and `Install App to Workspace`. This will expose the app's `SLACK_BOT_TOKEN`.

4. Go to "Basic Information" and get `SLACK_SIGNING_SECRET`.

5. Copy both `SLACK_BOT_TOKEN` and `SLACK_SIGNING_SECRET` to the `.env` file.

- `SLACK_BOT_TOKEN` is used to authenticate API calls FROM your bot TO Slack.
- `SLACK_SIGNING_SECRET` is used to verify that incoming requests TO your server are actually FROM Slack.

```shell
# .env
SLACK_SIGNING_SECRET=
SLACK_BOT_TOKEN=xoxb-...
```

6. Set up your LangGraph deployment with custom routes support.

```shell
# .env
LANGGRAPH_ASSISTANT_ID=your_assistant_id
CONFIG={"your_config": "here"}
```

7. Deploy your application to the LangGraph platform with custom routes.

The application uses the LangGraph platform's custom routes feature to add Slack integration directly to your deployed application. When deployed, the Slack routes will be available at your LangGraph platform URL.

After deployment, update your Slack app's Event Subscriptions URL to point to your LangGraph platform URL with `/events/slack` appended.

8. In `Event Subscriptions`, add events that you want to receive. As an example:

```
"app_mention",        # Notify when bot is @mentioned
"message.im",         # Notify about direct messages
"message.mpim"        # Notify about group messages
"message.channels",   # Get notified of channel messages
```

9. Chat with the bot in Slack.

- The bot responds if you `@mention` it within a channel of which it is a member.
- You can also DM the bot. You needn't use `@mention`'s in the bot's DMs. It's clear who you are speaking to.

## Customizing the input and output

By default, the bot assums that the LangGraph deployment uses the `messages` state key.

The request to the LangGraph deployment using the LangGraph SDK is made here in `src/langgraph_slack/server.py`:

```
result = await LANGGRAPH_CLIENT.runs.create(
            thread_id=thread_id,
            assistant_id=config.ASSISTANT_ID,
            input={
                "messages": [
                    {
                        "role": "user",
                        "content": _replace_mention(event),
                    }
                ]
            },
```

And you can see that the output, which we send back to Slack, is extracted from the `messages` list here:

```
response_message = state_values["messages"][-1]
```

You can customize either for the specific LangGraph deployment you are using!



================================================
FILE: slack-messaging/.gitignore
================================================
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info
.env

# Virtual environments
.venv
.ipynb_checkpoints/
.python_version



================================================
FILE: slack-messaging/src/langgraph_slack/__init__.py
================================================
import logging
import dotenv

dotenv.load_dotenv()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)



================================================
FILE: slack-messaging/src/langgraph_slack/__main__.py
================================================
import uvicorn

uvicorn.run("langgraph_slack.server:app", host="0.0.0.0", port=8080)



================================================
FILE: slack-messaging/src/langgraph_slack/auth.py
================================================
from langgraph_sdk import Auth


auth = Auth()


@auth.authenticate
async def authenticate(request, path, headers, method):
    user_agent = headers.get(b"user-agent")
    if user_agent and user_agent.startswith(b"Slackbot"):
        return {"identity": "default-user", "permissions": ["read", "write"]}
    return None



================================================
FILE: slack-messaging/src/langgraph_slack/config.py
================================================
from os import environ
import logging

LOGGER = logging.getLogger(__name__)

if DEPLOY_MODAL := environ.get("DEPLOY_MODAL"):
    DEPLOY_MODAL = DEPLOY_MODAL.lower() == "true"
BOT_USER_ID = environ.get("SLACK_BOT_USER_ID")
BOT_TOKEN = environ.get("SLACK_BOT_TOKEN")
if DEPLOY_MODAL:
    if not environ.get("SLACK_BOT_TOKEN"):
        environ["SLACK_BOT_TOKEN"] = "fake-token"
    BOT_USER_ID = BOT_USER_ID or "fake-user-id"
else:
    assert isinstance(BOT_TOKEN, str)
    # APP_TOKEN = environ["SLACK_APP_TOKEN"]


LANGGRAPH_URL = environ.get("LANGGRAPH_URL")
ASSISTANT_ID = environ.get("LANGGRAPH_ASSISTANT_ID", "chat")
CONFIG = environ.get("CONFIG") or "{}"
DEPLOYMENT_URL = environ.get("DEPLOYMENT_URL", "")
SLACK_CHANNEL_ID = environ.get("SLACK_CHANNEL_ID")



================================================
FILE: slack-messaging/src/langgraph_slack/server.py
================================================
import asyncio
import logging
import re
import json
import uuid
from typing import Awaitable, Callable, TypedDict
from contextlib import asynccontextmanager

from fastapi import FastAPI, Request
from langgraph_sdk import get_client
from slack_bolt.adapter.fastapi.async_handler import AsyncSlackRequestHandler
from slack_bolt.async_app import AsyncApp

from langgraph_slack import config

LOGGER = logging.getLogger(__name__)
LANGGRAPH_CLIENT = get_client(url=config.LANGGRAPH_URL)
GRAPH_CONFIG = (
    json.loads(config.CONFIG) if isinstance(config.CONFIG, str) else config.CONFIG
)

USER_NAME_CACHE: dict[str, str] = {}
TASK_QUEUE: asyncio.Queue = asyncio.Queue()


class SlackMessageData(TypedDict):
    user: str
    type: str
    subtype: str | None
    ts: str
    thread_ts: str | None
    client_msg_id: str
    text: str
    team: str
    parent_user_id: str
    blocks: list[dict]
    channel: str
    event_ts: str
    channel_type: str


async def worker():
    LOGGER.info("Background worker started.")
    while True:
        try:
            task = await TASK_QUEUE.get()
            if not task:
                LOGGER.info("Worker received sentinel, exiting.")
                break

            LOGGER.info(f"Worker got a new task: {task}")
            await _process_task(task)
        except Exception as exc:
            LOGGER.exception(f"Error in worker: {exc}")
        finally:
            TASK_QUEUE.task_done()


async def _process_task(task: dict):
    event = task["event"]
    event_type = task["type"]
    if event_type == "slack_message":
        thread_id = _get_thread_id(
            event.get("thread_ts") or event["ts"], event["channel"]
        )
        channel_id = event["channel"]
        # This will connect to the loopback endpoint if not provided.
        webhook = f"{config.DEPLOYMENT_URL}/callbacks/{thread_id}"

        if (await _is_mention(event)) or _is_dm(event):
            text_with_names = await _build_contextual_message(event)
        else:
            LOGGER.info("Skipping non-mention message")
            return

        LOGGER.info(
            f"[{channel_id}].[{thread_id}] sending message to LangGraph: "
            f"with webhook {webhook}: {text_with_names}"
        )

        result = await LANGGRAPH_CLIENT.runs.create(
            thread_id=thread_id,
            assistant_id=config.ASSISTANT_ID,
            input={
                "messages": [
                    {
                        "role": "user",
                        "content": text_with_names,
                    }
                ]
            },
            config=GRAPH_CONFIG,
            metadata={
                "event": "slack",
                "slack_event_type": "message",
                "bot_user_id": config.BOT_USER_ID,
                "slack_user_id": event["user"],
                "channel_id": channel_id,
                "channel": channel_id,
                "thread_ts": event.get("thread_ts"),
                "event_ts": event["ts"],
                "channel_type": event.get("channel_type"),
            },
            multitask_strategy="interrupt",
            if_not_exists="create",
            webhook=webhook,
        )
        LOGGER.info(f"LangGraph run: {result}")

    elif event_type == "callback":
        LOGGER.info(f"Processing LangGraph callback: {event['thread_id']}")
        state_values = event["values"]
        response_message = state_values["messages"][-1]
        thread_ts = event["metadata"].get("thread_ts") or event["metadata"].get(
            "event_ts"
        )
        channel_id = event["metadata"].get("channel") or config.SLACK_CHANNEL_ID
        if not channel_id:
            raise ValueError(
                "Channel ID not found in event metadata and not set in environment"
            )

        await APP_HANDLER.app.client.chat_postMessage(
            channel=channel_id,
            thread_ts=thread_ts,
            text=_clean_markdown(_get_text(response_message["content"])),
            metadata={
                "event_type": "webhook",
                "event_payload": {"thread_id": event["thread_id"]},
            },
        )
        LOGGER.info(
            f"[{channel_id}].[{thread_ts}] sent message to Slack for callback {event['thread_id']}"
        )
    else:
        raise ValueError(f"Unknown event type: {event_type}")


async def handle_message(event: SlackMessageData, say: Callable, ack: Callable):
    LOGGER.info("Enqueuing handle_message task...")
    nouser = not event.get("user")
    ismention = await _is_mention(event)
    userisbot = event.get("bot_id") == config.BOT_USER_ID
    isdm = _is_dm(event)
    if nouser or userisbot or not (ismention or isdm):
        LOGGER.info(f"Ignoring message not directed at the bot: {event}")
        return

    TASK_QUEUE.put_nowait({"type": "slack_message", "event": event})
    await ack()


async def just_ack(ack: Callable[..., Awaitable], event):
    LOGGER.info(f"Acknowledging {event.get('type')} event")
    await ack()


APP_HANDLER = AsyncSlackRequestHandler(AsyncApp(logger=LOGGER))
MENTION_REGEX = re.compile(r"<@([A-Z0-9]+)>")
USER_ID_PATTERN = re.compile(rf"<@{config.BOT_USER_ID}>")
APP_HANDLER.app.event("message")(ack=just_ack, lazy=[handle_message])
APP_HANDLER.app.event("app_mention")(
    ack=just_ack,
    lazy=[],
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    LOGGER.info("App is starting up. Creating background worker...")
    loop = asyncio.get_running_loop()
    loop.create_task(worker())
    yield
    LOGGER.info("App is shutting down. Stopping background worker...")
    TASK_QUEUE.put_nowait(None)


APP = FastAPI(lifespan=lifespan)


@APP.post("/events/slack")
async def slack_endpoint(req: Request):
    return await APP_HANDLER.handle(req)


def _get_text(content: str | list[dict]):
    if isinstance(content, str):
        return content
    else:
        return "".join([block["text"] for block in content if block["type"] == "text"])


def _clean_markdown(text: str) -> str:
    text = re.sub(r"^```[^\n]*\n", "```\n", text, flags=re.MULTILINE)
    text = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", r"<\2|\1>", text)
    text = re.sub(r"\*\*([^*]+)\*\*", r"*\1*", text)
    text = re.sub(r"(?<!\*)\*([^*]+)\*(?!\*)", r"_\1_", text)
    text = re.sub(r"_([^_]+)_", r"_\1_", text)
    text = re.sub(r"^\s*[-*]\s", "â€¢ ", text, flags=re.MULTILINE)
    return text


@APP.post("/callbacks/{thread_id}")
async def webhook_callback(req: Request):
    body = await req.json()
    LOGGER.info(
        f"Received webhook callback for {req.path_params['thread_id']}/{body['thread_id']}"
    )
    TASK_QUEUE.put_nowait({"type": "callback", "event": body})
    return {"status": "success"}


async def _is_mention(event: SlackMessageData):
    global USER_ID_PATTERN
    if not config.BOT_USER_ID or config.BOT_USER_ID == "fake-user-id":
        config.BOT_USER_ID = (await APP_HANDLER.app.client.auth_test())["user_id"]
        USER_ID_PATTERN = re.compile(rf"<@{config.BOT_USER_ID}>")
    matches = re.search(USER_ID_PATTERN, event["text"])
    return bool(matches)


def _get_thread_id(thread_ts: str, channel: str) -> str:
    return str(uuid.uuid5(uuid.NAMESPACE_DNS, f"SLACK:{thread_ts}-{channel}"))


def _is_dm(event: SlackMessageData):
    if channel_type := event.get("channel_type"):
        return channel_type == "im"
    return False


async def _fetch_thread_history(
    channel_id: str, thread_ts: str
) -> list[SlackMessageData]:
    """
    Fetch all messages in a Slack thread, following pagination if needed.
    """
    LOGGER.info(
        f"Fetching thread history for channel={channel_id}, thread_ts={thread_ts}"
    )
    all_messages = []
    cursor = None

    while True:
        try:
            if cursor:
                response = await APP_HANDLER.app.client.conversations_replies(
                    channel=channel_id,
                    ts=thread_ts,
                    inclusive=True,
                    limit=150,
                    cursor=cursor,
                )
            else:
                response = await APP_HANDLER.app.client.conversations_replies(
                    channel=channel_id,
                    ts=thread_ts,
                    inclusive=True,
                    limit=150,
                )
            all_messages.extend(response["messages"])
            if not response.get("has_more"):
                break
            cursor = response["response_metadata"]["next_cursor"]
        except Exception as exc:
            LOGGER.exception(f"Error fetching thread messages: {exc}")
            break

    return all_messages


async def _fetch_user_names(user_ids: set[str]) -> dict[str, str]:
    """Fetch and cache Slack display names for user IDs."""
    uncached_ids = [uid for uid in user_ids if uid not in USER_NAME_CACHE]
    if uncached_ids:
        tasks = [APP_HANDLER.app.client.users_info(user=uid) for uid in uncached_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for uid, result in zip(uncached_ids, results):
            if isinstance(result, Exception):
                LOGGER.warning(f"Failed to fetch user info for {uid}: {result}")
                continue
            user_obj = result.get("user", {})
            profile = user_obj.get("profile", {})
            display_name = (
                profile.get("display_name") or profile.get("real_name") or uid
            )
            USER_NAME_CACHE[uid] = display_name
    return {uid: USER_NAME_CACHE[uid] for uid in user_ids if uid in USER_NAME_CACHE}


async def _build_contextual_message(event: SlackMessageData) -> str:
    """Build a message with thread context, using display names for all users."""
    thread_ts = event.get("thread_ts") or event["ts"]
    channel_id = event["channel"]

    history = await _fetch_thread_history(channel_id, thread_ts)
    included = []
    for msg in reversed(history):
        if msg.get("bot_id") == config.BOT_USER_ID:
            break
        included.append(msg)

    all_user_ids = set()
    for msg in included:
        all_user_ids.add(msg.get("user", "unknown"))
        all_user_ids.update(MENTION_REGEX.findall(msg["text"]))

    all_user_ids.add(event["user"])
    all_user_ids.update(MENTION_REGEX.findall(event["text"]))

    user_names = await _fetch_user_names(all_user_ids)

    def format_message(msg: SlackMessageData) -> str:
        text = msg["text"]
        user_id = msg.get("user", "unknown")

        def repl(match: re.Match) -> str:
            uid = match.group(1)
            return user_names.get(uid, uid)

        replaced_text = MENTION_REGEX.sub(repl, text)
        speaker_name = user_names.get(user_id, user_id)

        return (
            f'<slackMessage user="{speaker_name}">' f"{replaced_text}" "</slackMessage>"
        )

    context_parts = [format_message(msg) for msg in reversed(included)]
    new_message = context_parts[-1]
    preceding_context = "\n".join(context_parts[:-1])

    contextual_message = (
        (("Preceding context:\n" + preceding_context) if preceding_context else "")
        + "\n\nNew message:\n"
        + new_message
    )
    return contextual_message


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("langgraph_slack.server:APP", host="0.0.0.0", port=8080)



================================================
FILE: src/agents/should-exclude.ts
================================================
import { useLangChainPrompts } from "./utils.js";

export const LANGCHAIN_DOMAINS = [
  "langchain.com",
  "langchain.dev",
  "langchain-ai.github.io",
];

export function shouldExcludeGeneralContent(url: string): boolean {
  // Do not exclude any content if USE_LANGCHAIN_PROMPTS is not set to true.
  if (!useLangChainPrompts()) {
    return false;
  }

  // We don't want to generate posts on LangChain website content.
  if (LANGCHAIN_DOMAINS.some((lcUrl) => url.includes(lcUrl))) {
    return true;
  }

  return false;
}

export function shouldExcludeGitHubContent(link: string): boolean {
  // Do not exclude any content if USE_LANGCHAIN_PROMPTS is not set to true.
  if (!useLangChainPrompts()) {
    return false;
  }

  const langChainGitHubOrg = "github.com/langchain-ai/";
  // Do not generate posts on LangChain repos.
  return link.includes(langChainGitHubOrg);
}

export function shouldExcludeYouTubeContent(channelName: string): boolean {
  // Do not exclude any content if USE_LANGCHAIN_PROMPTS is not set to true.
  if (!useLangChainPrompts()) {
    return false;
  }

  return channelName.toLowerCase() === "langchain";
}

export function shouldExcludeTweetContent(externalUrls: string[]): boolean {
  // Do not exclude any content if USE_LANGCHAIN_PROMPTS is not set to true.
  if (!useLangChainPrompts()) {
    return false;
  }

  // If there are no external URLs, then we should exclude the tweet. Return true.
  return externalUrls.length === 0;
}



================================================
FILE: src/agents/types.ts
================================================
export type DateType = Date | "p1" | "p2" | "p3" | "r1" | "r2" | "r3";

export type Image = { imageUrl: string; mimeType: string };

export type AdditionalContext = {
  /**
   * The string content from the link.
   */
  content: string;
  /**
   * The link from which the content was extracted.
   */
  link: string;
};

export type RepurposedPost = {
  /**
   * The content of the specific post.
   */
  content: string;
  /**
   * The index of the post in the series.
   */
  index: number;
};



================================================
FILE: src/agents/utils.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import * as cheerio from "cheerio";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
  TEXT_ONLY_MODE,
} from "./generate-post/constants.js";
import { Image } from "./types.js";
import { traceable } from "langsmith/traceable";

export const BLACKLISTED_MIME_TYPES = [
  "image/svg+xml",
  "image/x-icon",
  "image/bmp",
  "text/",
];
export const BLACKLISTED_GENERAL_URLS = ["vimeo.com"];

/**
 * Extracts URLs from Slack-style message text containing links in the format:
 * <display_text|https://example.com> or <https://example.com>
 * @param text The message text to extract URLs from
 * @returns Array of URLs found in the text
 */
export function extractUrlsFromSlackText(text: string): string[] {
  const regex = /<(?:([^|>]*)\|)?([^>]+)>/g;
  const matches = [...text.matchAll(regex)];
  return matches.map((match) => match[2]);
}

/**
 * Checks if a string ends with a file extension (e.g., 'file.md', 'doc.txt')
 * @param str The string to check
 * @returns boolean indicating if the string ends with a file extension
 */
export function hasFileExtension(str: string): boolean {
  return /\.[a-zA-Z0-9]+$/.test(str);
}

/**
 * Extracts the tweet ID from a Twitter URL
 * @param url The Twitter URL (can be string or URL object)
 * @returns The tweet ID
 * @throws Error if the tweet ID cannot be extracted
 */
export function extractTweetId(url: string | URL): string | undefined {
  const pathname = url instanceof URL ? url.pathname : new URL(url).pathname;
  const tweetId = pathname.match(/\/status\/(\d+)/)?.[1];
  return tweetId;
}

/**
 * Extracts all URLs from a given string
 * @param text The string to extract URLs from
 * @returns Array of URLs found in the text
 */
export function extractUrls(text: string): string[] {
  const markdownLinkRegex = /\[([^\]]+)\]\((https?:\/\/[^\s)]+)\)/g;
  const urls = new Set<string>();

  // First replace all markdown links with spaces to avoid double-matching
  const processedText = text.replace(markdownLinkRegex, (match, _, url) => {
    urls.add(url);
    return " ".repeat(match.length); // Replace with spaces to preserve string length
  });

  // Then look for any remaining plain URLs in the text
  const plainUrlRegex = /https?:\/\/[^\s<\]]+(?:[^<.,:;"'\]\s)]|(?=\s|$))/g;
  const plainUrls = processedText.match(plainUrlRegex) || [];
  plainUrls.forEach((url) => urls.add(url));

  return Array.from(urls);
}

/**
 * Removes all URLs from a given string
 * @param text The string to remove URLs from
 * @returns The input string with all URLs removed
 */
export function removeUrls(text: string): string {
  const urlRegex = /(https?:\/\/[^\s<]+[^<.,:;"')\]\s])/g;
  return text.replace(urlRegex, "").replace(/\s+/g, " ").trim();
}

/**
 * Fetches and extracts the main text content from a webpage by removing common non-content elements.
 *
 * @param {string} url - The URL of the webpage to fetch and extract text from.
 * @returns {Promise<string | undefined>} The cleaned text content of the webpage, or undefined if an error occurs.
 *
 * @throws {TypeError} When the provided URL is invalid.
 * @throws {Error} When there's a network error during the request.
 *
 * @example
 * ```typescript
 * const text = await getPageText('https://example.com');
 * if (text) {
 *   console.log('Page content:', text);
 * } else {
 *   console.log('Failed to fetch page content');
 * }
 * ```
 */
export async function getPageText(url: string): Promise<string | undefined> {
  try {
    // Validate URL
    new URL(url); // Will throw if URL is invalid

    const response = await fetch(url, {
      headers: {
        "User-Agent":
          "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
      },
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const html = await response.text();
    const $ = cheerio.load(html);

    // Remove unwanted elements
    $("script").remove();
    $("style").remove();
    $("head").remove();
    $("nav").remove();
    $("footer").remove();
    $("header").remove();

    // Extract image information
    const images = $("img")
      .map((_, img) => {
        const alt = $(img).attr("alt") || "";
        const src = $(img).attr("src") || "";
        return `[Image: ${alt}](${src})`;
      })
      .get();

    // Get text content and clean it up
    const text = $("body")
      .text()
      .replace(/\s+/g, " ")
      .replace(/\n+/g, " ")
      .trim();

    // Combine text and image information
    return `${text}\n\n${images.join("\n")}`;
  } catch (error) {
    if (error instanceof Error) {
      // Handle specific error types
      if (error.message.includes("HTTP error")) {
        console.error(`Network error: ${error.message}`);
        return undefined;
      } else if (error instanceof TypeError) {
        console.error(`Invalid URL: ${error.message}`);
        return undefined;
      }

      console.error(`Unknown error: ${error.message}`);
      return undefined;
    }

    console.error("Unknown error:", error);
    return undefined;
  }
}

/**
 * Checks if a given string is a valid URL
 *
 * @param {string} str - The string to check
 * @returns {boolean} True if the string is a valid URL, false otherwise
 *
 * @example
 * ```typescript
 * isValidUrl('https://example.com'); // returns true
 * isValidUrl('not-a-url'); // returns false
 * isValidUrl('http://localhost:3000'); // returns true
 * ```
 */
export function isValidUrl(str: string): boolean {
  if (!str || typeof str !== "string") {
    return false;
  }

  try {
    new URL(str);
    return true;
  } catch (error: any) {
    if (error?.message === "Invalid URL" || error instanceof TypeError) {
      return false;
    }

    // Re-throw unexpected errors
    throw error;
  }
}

/**
 * Fetches an image from a URL and converts it to a base64 string
 *
 * @param {string} imageUrl - The URL of the image to fetch
 * @returns {Promise<string>} A Promise that resolves to the base64 string of the image
 * @throws {Error} When the URL is invalid or the fetch request fails
 *
 * @example
 * ```typescript
 * const base64String = await imageUrlToBase64('https://example.com/image.jpg');
 * console.log(base64String); // data:image/jpeg;base64,/9j/4AAQSkZJRg...
 * ```
 */
export async function imageUrlToBuffer(imageUrl: string): Promise<{
  buffer: Buffer;
  contentType: string;
}> {
  if (!isValidUrl(imageUrl)) {
    throw new Error("Invalid image URL provided");
  }

  const response = await fetch(imageUrl);
  if (!response.ok) {
    throw new Error(`Failed to fetch image: ${response.statusText}`);
  }

  const imageBuffer = Buffer.from(await response.arrayBuffer());
  const contentType = response.headers.get("content-type") || "image/jpeg";

  return {
    buffer: imageBuffer,
    contentType,
  };
}

/**
 * Extracts the MIME type from a base64 string.
 *
 * @param {string} base64String - The base64 string to extract MIME type from
 * @returns {string | null} The MIME type if found, null otherwise
 *
 * @throws {Error} When the input string is not a valid base64 string
 *
 * @example
 * ```typescript
 * const mimeType = extractMimeTypeFromBase64('data:image/jpeg;base64,/9j/4AAQSkZJRg...');
 * console.log(mimeType); // 'image/jpeg'
 * ```
 */
export function extractMimeTypeFromBase64(base64String: string): string | null {
  try {
    // Check if the string is empty or not a string
    if (!base64String || typeof base64String !== "string") {
      throw new Error("Invalid input: base64String must be a non-empty string");
    }

    // Check if it's a data URL
    if (base64String.startsWith("data:")) {
      const matches = base64String.match(
        /^data:([a-zA-Z0-9]+\/[a-zA-Z0-9-.+]+);base64,/,
      );
      if (matches && matches.length > 1) {
        return matches[1];
      }
    }

    // If it's just a base64 string without data URL prefix, try to detect common file signatures
    const decodedString = Buffer.from(base64String, "base64")
      .toString("hex")
      .toLowerCase();

    // Common file signatures (magic numbers)
    const signatures: { [key: string]: string } = {
      ffd8ff: "image/jpeg",
      "89504e47": "image/png",
      "47494638": "image/gif",
      "25504446": "application/pdf",
      "504b0304": "application/zip",
    };

    for (const [signature, mimeType] of Object.entries(signatures)) {
      if (decodedString.startsWith(signature)) {
        return mimeType;
      }
    }

    return null;
  } catch (error) {
    if (error instanceof Error) {
      throw new Error(`Failed to extract MIME type: ${error.message}`);
    }
    throw new Error("Failed to extract MIME type: Unknown error");
  }
}

/**
 * Processes an image input which can be a URL, base64 string, or "remove" command
 * @param imageInput The image input string (URL, base64, or "remove")
 * @returns Object containing the public image URL and MIME type, or undefined if image should be removed
 */
export async function processImageInput(
  imageInput: string,
): Promise<Image | "remove" | undefined> {
  if (imageInput.toLowerCase() === "remove" || !imageInput) {
    return "remove";
  }

  if (isValidUrl(imageInput)) {
    const { contentType } = await imageUrlToBuffer(imageInput);

    if (BLACKLISTED_MIME_TYPES.find((mt) => contentType.startsWith(mt))) {
      return undefined;
    }

    return {
      imageUrl: imageInput,
      mimeType: contentType,
    };
  }

  return undefined;
}

// Regex to match markdown image syntax: ![alt text](url)
const MARKDOWN_IMAGE_REGEX = /!\[([^\]]*)\]\(([^)]+)\)/g;
// Regex to match HTML img tags with src attribute (handles both single and double quotes)
const HTML_IMG_REGEX = /<img[^>]+src=["']([^"'>]+)["']/g;

/**
 * Extracts all image URLs from a markdown string, including both markdown syntax and HTML img tags
 * @param text The markdown text to search
 * @returns Array of all matched image URLs
 */
export function extractAllImageUrlsFromMarkdown(text: string): string[] {
  const urls: string[] = [];
  let match;

  // Reset regex states
  MARKDOWN_IMAGE_REGEX.lastIndex = 0;
  HTML_IMG_REGEX.lastIndex = 0;

  // Extract markdown style images
  while ((match = MARKDOWN_IMAGE_REGEX.exec(text)) !== null) {
    urls.push(match[2]);
  }

  // Extract HTML img tags
  while ((match = HTML_IMG_REGEX.exec(text)) !== null) {
    urls.push(match[1]);
  }

  return urls;
}

const BLACKLISTED_IMAGE_URL_ENDINGS = [".svg", ".ico", ".bmp"];
const BLACKLISTED_IMAGE_URLS = ["img.shields.io", "contrib.rocks"];

export function filterUnwantedImageUrls(urls: string[]): string[] {
  return urls.filter(
    (url) =>
      !BLACKLISTED_IMAGE_URL_ENDINGS.find((ending) => url?.endsWith(ending)) &&
      !BLACKLISTED_IMAGE_URLS.find((blacklistedUrl) =>
        url.includes(blacklistedUrl),
      ) &&
      isValidUrl(url),
  );
}

/**
 * The type of a URL. One of "github", "youtube", "general"
 * `undefined` if the URL type could not be determined
 */
export type UrlType =
  | "github"
  | "youtube"
  | "general"
  | "twitter"
  | "reddit"
  | "luma"
  | undefined;

export function getUrlType(url: string): UrlType {
  let parsedUrl: URL | undefined = undefined;
  try {
    const formattedUrl = url.startsWith("http") ? url : `https://${url}`;
    parsedUrl = new URL(formattedUrl);
  } catch (e) {
    console.error("Failed to parse URL:", e);
    return undefined;
  }

  if (
    parsedUrl.hostname.includes("github") &&
    // github.io sites should be considered general URLs
    !parsedUrl.hostname.includes("github.io")
  ) {
    return "github";
  }

  if (
    parsedUrl.hostname.includes("youtube") ||
    parsedUrl.hostname.includes("youtu.be")
  ) {
    return "youtube";
  }

  if (
    parsedUrl.hostname.includes("twitter") ||
    parsedUrl.hostname.includes("x.com")
  ) {
    return "twitter";
  }

  if (
    parsedUrl.hostname.includes("reddit") ||
    parsedUrl.hostname.includes("np.reddit") ||
    parsedUrl.hostname.includes("redd.it")
  ) {
    return "reddit";
  }

  if (parsedUrl.host === "lu.ma") {
    return "luma";
  }

  return "general";
}

/**
 * Extracts the MIME type from a URL based on its file extension or path
 * @param url The URL to extract MIME type from
 * @returns The MIME type (e.g., 'image/jpeg', 'image/png') or undefined if not determinable
 */
export function getMimeTypeFromUrl(url: string): string | undefined {
  try {
    // Handle encoded URLs (like in Substack CDN URLs)
    const decodedUrl = decodeURIComponent(url);

    // Try to find the last occurrence of a file extension
    const extensionMatch = decodedUrl.match(/\.([^./\\?#]+)(?:[?#].*)?$/i);

    if (!extensionMatch) {
      return undefined;
    }

    const extension = extensionMatch[1].toLowerCase();

    // Map common image extensions to MIME types
    const mimeTypeMap: Record<string, string> = {
      jpg: "image/jpeg",
      jpeg: "image/jpeg",
      png: "image/png",
      gif: "image/gif",
      webp: "image/webp",
      svg: "image/svg+xml",
      ico: "image/x-icon",
      bmp: "image/bmp",
    };

    return mimeTypeMap[extension];
  } catch (error) {
    console.error("Error extracting MIME type from URL:", error);
    return undefined;
  }
}

/**
 * Removes query parameters from a URL while preserving the base URL and path
 * @param url The URL to remove query parameters from
 * @returns The URL without query parameters
 *
 * @example
 * ```typescript
 * removeQueryParams('https://example.com/path?query=123'); // returns 'https://example.com/path'
 * removeQueryParams('https://example.com'); // returns 'https://example.com'
 * ```
 */
export function removeQueryParams(url: string): string {
  try {
    const urlObj = new URL(url);
    return `${urlObj.protocol}//${urlObj.host}${urlObj.pathname}`;
  } catch (error) {
    // If URL parsing fails, return the original string
    return url;
  }
}

/**
 * Splits an array into smaller chunks of a specified size
 * @param {T[]} arr - The array to be chunked
 * @param {number} size - The size of each chunk
 * @returns {T[][]} An array of chunks, where each chunk is an array of size elements (except possibly the last chunk which may be smaller)
 * @template T - The type of elements in the array
 */
export function chunkArray<T>(arr: T[], size: number): T[][] {
  if (!arr.length) return [];
  return Array.from({ length: Math.ceil(arr.length / size) }, (_, i) =>
    arr.slice(i * size, i * size + size),
  );
}

export function isTextOnly(config?: LangGraphRunnableConfig): boolean {
  const textOnlyModeConfig = config?.configurable?.[TEXT_ONLY_MODE];
  const isTextOnlyMode =
    textOnlyModeConfig != null
      ? textOnlyModeConfig
      : process.env.TEXT_ONLY_MODE === "true";
  return isTextOnlyMode;
}

export function shouldPostToLinkedInOrg(
  config?: LangGraphRunnableConfig,
): boolean {
  const postToOrgConfig = config?.configurable?.[POST_TO_LINKEDIN_ORGANIZATION];
  const postToOrg =
    postToOrgConfig != null
      ? postToOrgConfig
      : process.env.POST_TO_LINKEDIN_ORGANIZATION === "true";
  return postToOrg;
}

export async function sleep(ms: number) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

/**
 * Calculates delay times for processing a list of URLs to prevent rate limiting.
 * Each URL gets a minimum 30-second delay from the previous URL's processing time.
 * Twitter URLs receive an additional 30-second delay due to stricter rate limits.
 *
 * @param links - Array of URLs to process
 * @param baseDelaySeconds - The base delay time in seconds (default: 30)
 * @returns Array of objects containing the original URL and its calculated delay time
 * @example
 * // For URLs: ['https://example.com', 'https://twitter.com/user', 'https://github.com']
 * // Returns:
 * // [
 * //   { link: 'https://example.com', afterSeconds: 0 },
 * //   { link: 'https://twitter.com/user', afterSeconds: 60 }, // 30 (base) + 30 (twitter)
 * //   { link: 'https://github.com', afterSeconds: 60 } // 30 * 2 (third position)
 * // ]
 */
export function getAfterSecondsFromLinks(
  links: string[],
  options?: {
    baseDelaySeconds?: number;
  },
): {
  link: string;
  afterSeconds: number;
}[] {
  const baseDelaySeconds =
    options?.baseDelaySeconds != null ? options.baseDelaySeconds : 30;
  return links.map((link, index) => {
    const isTwitterUrl = getUrlType(link) === "twitter";
    const additionalDelay = isTwitterUrl ? baseDelaySeconds : 0;
    const afterSeconds = index * baseDelaySeconds + additionalDelay;
    return {
      link,
      afterSeconds,
    };
  });
}

/**
 * Filters an array of URLs by type. Excludes any Twitter & Reddit URLs, then returns the filtered list joined by two newlines.
 * This is used when getting the list of links which may be included in the post content.
 *
 * @param links - Array of URLs to filter
 * @returns A joined string of the filtered URLs
 */
export function filterLinksForPostContent(links: string[]): string {
  const blacklistedTypes = ["twitter", "reddit"];
  const filteredLinks = links.filter(
    (l) => !blacklistedTypes.includes(getUrlType(l) ?? ""),
  );
  return filteredLinks.join("\n\n");
}

/**
 * Capitalizes the first letter of a string.
 */
export function capitalize(string: string): string {
  return string.charAt(0).toUpperCase() + string.slice(1);
}

/**
 * Returns true if LangChain prompts should be used
 *
 * @returns {boolean} True if LangChain prompts should be used
 */
export function useLangChainPrompts(): boolean {
  return process.env.USE_LANGCHAIN_PROMPTS === "true";
}

/**
 * Returns true if Arcade authentication should be used
 *
 * @returns {boolean} True if Arcade authentication should be used
 */
export function useArcadeAuth(): boolean {
  return process.env.USE_ARCADE_AUTH === "true";
}

/**
 * Returns true if Twitter API-only mode should be used
 *
 * @returns {boolean} True if Twitter API-only mode should be used
 */
export function useTwitterApiOnly(): boolean {
  return process.env.USE_TWITTER_API_ONLY === "true";
}

function skipContentRelevancyCheckFunc(
  configurable?: Record<string, unknown>,
): boolean {
  const skipRelevancyCheck = configurable?.[SKIP_CONTENT_RELEVANCY_CHECK];
  return !!(
    skipRelevancyCheck ?? process.env.SKIP_CONTENT_RELEVANCY_CHECK === "true"
  );
}

/**
 * Returns true if content relevancy verification should be skipped
 *
 * @param configurable - Optional configuration object
 * @returns {Promise<boolean>} True if content relevancy verification should be skipped
 */
export const skipContentRelevancyCheck = traceable(
  skipContentRelevancyCheckFunc,
  { name: "skipContentRelevancyCheck" },
);

function skipUsedUrlsCheckFunc(
  configurable?: Record<string, unknown>,
): boolean {
  const skipUsedUrlsCheck = configurable?.[SKIP_USED_URLS_CHECK];
  return !!(skipUsedUrlsCheck ?? process.env.SKIP_USED_URLS_CHECK === "true");
}

/**
 * Returns true if used URLs check should be skipped
 *
 * @param configurable - Optional configuration object
 * @returns {Promise<boolean>} True if used URLs check should be skipped
 */
export const skipUsedUrlsCheck = traceable(skipUsedUrlsCheckFunc, {
  name: "skipUsedUrlsCheck",
});



================================================
FILE: src/agents/curate-data/constants.ts
================================================
export const NUM_POSTS_PER_SUBREDDIT = "num_posts_per_subreddit";



================================================
FILE: src/agents/curate-data/index.ts
================================================
import { Annotation, END, Send, START, StateGraph } from "@langchain/langgraph";
import {
  CurateDataAnnotation,
  CurateDataConfigurableAnnotation,
  CurateDataState,
} from "./state.js";
import { ingestData } from "./nodes/ingest-data.js";
import { verifyGitHubWrapper } from "./nodes/verify-github-wrapper.js";
import { verifyRedditWrapper } from "./nodes/verify-reddit-wrapper.js";
import { verifyGeneralContent } from "../shared/nodes/verify-general.js";
import { VerifyContentAnnotation } from "../shared/shared-state.js";
import { validateBulkTweets } from "./nodes/validate-bulk-tweets.js";
import { formatData } from "./nodes/format-data.js";
import { groupTweetsByContent } from "./nodes/tweets/group-tweets-by-content.js";
import { reflectOnTweetGroups } from "./nodes/tweets/reflect-tweet-groups.js";
import { reGroupTweets } from "./nodes/tweets/re-group-tweets.js";
import { generatePostsSubgraph } from "./nodes/generate-posts-subgraph.js";
import { extractAINewsletterContent } from "./nodes/extract-ai-newsletter-content.js";
import { useLangChainPrompts } from "../utils.js";

function generatePostOrContinue(
  _state: CurateDataState,
): "generatePostsSubgraph" | "verifyBulkTweets" {
  if (useLangChainPrompts()) {
    return "generatePostsSubgraph";
  }
  return "verifyBulkTweets";
}

function verifyContentWrapper(state: CurateDataState): Send[] {
  const generalSends = state.generalUrls.map((post) => {
    return new Send("verifyGeneralContent", {
      link: post,
    });
  });

  const githubSends = state.rawTrendingRepos?.length
    ? [
        new Send("verifyGitHubContent", {
          rawTrendingRepos: state.rawTrendingRepos,
        }),
      ]
    : [];

  const redditSends = state.rawRedditPosts?.length
    ? [
        new Send("verifyRedditPost", {
          rawRedditPosts: state.rawRedditPosts,
        }),
      ]
    : [];

  const twitterSends = state.validatedTweets?.length
    ? [
        new Send("groupTweetsByContent", {
          validatedTweets: state.validatedTweets,
        }),
      ]
    : [];

  return [...generalSends, ...githubSends, ...redditSends, ...twitterSends];
}

function reGroupOrContinue(
  state: CurateDataState,
): "reGroupTweets" | "formatData" {
  if (state.similarGroupIndices && state.similarGroupIndices.length > 0) {
    return "reGroupTweets";
  }
  return "formatData";
}

const curateDataWorkflow = new StateGraph(
  { stateSchema: CurateDataAnnotation, input: Annotation.Root({}) },
  CurateDataConfigurableAnnotation,
)
  .addNode("ingestData", ingestData)
  .addNode("extractAINewsletterContent", extractAINewsletterContent)
  .addNode("verifyGitHubContent", verifyGitHubWrapper)
  .addNode("verifyRedditPost", verifyRedditWrapper)
  .addNode("verifyGeneralContent", verifyGeneralContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyBulkTweets", validateBulkTweets)
  .addNode("groupTweetsByContent", groupTweetsByContent)
  .addNode("reflectOnTweetGroups", reflectOnTweetGroups)
  .addNode("reGroupTweets", reGroupTweets)

  .addNode("generatePostsSubgraph", generatePostsSubgraph)

  .addNode("formatData", formatData)
  .addEdge(START, "ingestData")
  .addConditionalEdges("ingestData", generatePostOrContinue, [
    "generatePostsSubgraph",
    "verifyBulkTweets",
  ])
  .addEdge("verifyBulkTweets", "extractAINewsletterContent")

  .addConditionalEdges("extractAINewsletterContent", verifyContentWrapper, [
    "verifyGeneralContent",
    "verifyGitHubContent",
    "verifyRedditPost",
    "groupTweetsByContent",
  ])
  // If generatePostsSubgraph is called, we should end.
  .addEdge("generatePostsSubgraph", END)
  .addEdge("verifyGeneralContent", "formatData")
  .addEdge("verifyGitHubContent", "formatData")
  .addEdge("verifyRedditPost", "formatData")
  .addEdge("groupTweetsByContent", "reflectOnTweetGroups")
  .addConditionalEdges("reflectOnTweetGroups", reGroupOrContinue, [
    "reGroupTweets",
    "formatData",
  ])
  .addEdge("reGroupTweets", "formatData")
  .addEdge("formatData", END);

export const curateDataGraph = curateDataWorkflow.compile();
curateDataGraph.name = "Curate Data Graph";



================================================
FILE: src/agents/curate-data/state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import {
  CuratedData,
  GitHubTrendingData,
  ThreadRunId,
  TweetsGroupedByContent,
} from "./types.js";
import { TweetV2 } from "twitter-api-v2";
import { SimpleRedditPostWithComments } from "../../clients/reddit/types.js";
import { RedditPostsWithExternalData } from "../verify-reddit-post/types.js";
import { NUM_POSTS_PER_SUBREDDIT } from "./constants.js";
import { Source } from "../supervisor/types.js";
import { VerifyLinksResultAnnotation } from "../verify-links/verify-links-state.js";

export const CurateDataAnnotation = Annotation.Root({
  ...VerifyLinksResultAnnotation.spec,
  /**
   * The final data object to be returned.
   */
  curatedData: Annotation<CuratedData>,
  /**
   * Collection of saved tweets from a Twitter list.
   * Each tweet contains metadata like ID, creation time, text content, and media references.
   */
  rawTweets: Annotation<TweetV2[]>,
  /**
   * A list of validated tweets.
   */
  validatedTweets: Annotation<TweetV2[]>,
  /**
   * Tweets which have been grouped by their external URLs.
   * Each group contains a list of tweets which reference the same external URL.
   */
  tweetsGroupedByContent: Annotation<TweetsGroupedByContent[]>,
  /**
   * Array of indices of similar groups of tweets to re-evaluate the grouping of.
   */
  similarGroupIndices: Annotation<number[]>,

  /**
   * List of trending GitHub repository names/paths.
   */
  rawTrendingRepos: Annotation<string[]>,
  /**
   * A list of trending GitHub repositories & README contents which have been
   * validated.
   */
  githubTrendingData: Annotation<GitHubTrendingData[]>,

  /**
   * A list of new AI Newsletter posts.
   */
  aiNewsPosts: Annotation<string[]>,
  /**
   * Collection of saved Reddit posts and their associated comments.
   * Each post contains the original content and relevant discussion threads.
   */
  rawRedditPosts: Annotation<SimpleRedditPostWithComments[]>,
  /**
   * A list of verified Reddit posts.
   */
  redditPosts: Annotation<RedditPostsWithExternalData[]>,
  /**
   * The thread & run IDs for runs kicked off after curating data.
   */
  threadRunIds: Annotation<ThreadRunId[]>,
  /**
   * General URLs to scrape content from.
   */
  generalUrls: Annotation<string[]>,
});

export const CurateDataConfigurableAnnotation = Annotation.Root({
  /**
   * The sources to ingest from.
   */
  sources: Annotation<Source[]>,
  /**
   * The number of posts to fetch per subreddit when ingesting Reddit posts.
   */
  [NUM_POSTS_PER_SUBREDDIT]: Annotation<number | undefined>(),
});

export type CurateDataState = typeof CurateDataAnnotation.State;
export type CurateDataConfigurable =
  typeof CurateDataConfigurableAnnotation.State;



================================================
FILE: src/agents/curate-data/types.ts
================================================
import { TweetV2 } from "twitter-api-v2";
import { SimpleRedditPostWithComments } from "../../clients/reddit/types.js";

export type TweetV2WithURLs = TweetV2 & {
  external_urls: string[];
};

export type GitHubTrendingData = {
  repoURL: string;
  pageContent: string;
};

export type TweetsGroupedByContent = {
  explanation: string;
  tweets: TweetV2WithURLs[];
};

export type ThreadRunId = { thread_id: string; run_id: string };

export type CuratedData = {
  /**
   * The tweets grouped by content.
   */
  tweetsGroupedByContent?: TweetsGroupedByContent[];
  /**
   * If reports were curated, they will be included here.
   */
  redditPosts?: SimpleRedditPostWithComments[];
  /**
   * The general content scraped from URLs
   */
  generalContents?: {
    pageContent: string;
    relevantLinks: string[];
  }[];
  /**
   * The GitHub trending data.
   */
  githubTrendingData?: GitHubTrendingData[];
};



================================================
FILE: src/agents/curate-data/loaders/ai-news-blog.ts
================================================
import { traceable } from "langsmith/traceable";
import { parseStringPromise } from "xml2js";

const AI_NEWS_BLOG_RSS_URL = "https://buttondown.com/ainews/rss";

interface RSSItem {
  title: string[];
  link: string[];
  pubDate: string[];
  guid: string[];
}

interface RSSFeed {
  rss: {
    channel: [
      {
        item: RSSItem[];
      },
    ];
  };
}

/**
 * Loads the latest posts from the AI News Blog RSS feed.
 *
 * @returns {Promise<string[]>} Array of links
 */
async function aiNewsBlogLoaderFunc(): Promise<string[]> {
  const lastCheckTime = new Date(new Date().getTime() - 96 * 60 * 60 * 1000); // 24 hours ago

  try {
    // Fetch the RSS feed
    const response = await fetch(AI_NEWS_BLOG_RSS_URL);
    if (!response.ok) {
      throw new Error(`Failed to fetch RSS feed: ${response.statusText}`);
    }

    // Get the text content
    const xmlContent = await response.text();
    // Parse the XML content
    const parsedFeed = (await parseStringPromise(xmlContent)) as RSSFeed;

    // Get all items from the feed
    const items = parsedFeed.rss.channel[0].item;

    // Filter for only new items
    const filteredItems = items.filter((item) => {
      const pubDate = new Date(item.pubDate[0]);
      return pubDate > lastCheckTime;
    });

    // Return array of links
    return filteredItems.map((item) => item.link[0]);
  } catch (error) {
    console.error("Error fetching RSS feed:", error);
    throw error;
  }
}

export const aiNewsBlogLoader = traceable(aiNewsBlogLoaderFunc, {
  name: "ai-news-loader",
});



================================================
FILE: src/agents/curate-data/loaders/latent-space.ts
================================================
import * as cheerio from "cheerio";
import {
  getLatentSpaceLinks,
  putLatentSpaceLinks,
} from "../utils/stores/latent-space-links.js";
import { BaseStore } from "@langchain/langgraph";
import { getUniqueArrayItems } from "../utils/get-unique-array.js";
import { traceable } from "langsmith/traceable";

async function latentSpaceLoaderFunc(store: BaseStore | undefined) {
  const siteMapUrl = "https://www.latent.space/sitemap/2025";

  const links = await fetch(siteMapUrl)
    .then((response) => response.text())
    .then((html) => {
      const $ = cheerio.load(html);

      const links = $(".sitemap-link")
        .map((_, element) => $(element).attr("href"))
        .get();

      return links;
    });

  const processedLinks = await getLatentSpaceLinks(store);
  const uniqueLinks = getUniqueArrayItems(processedLinks, links);
  const allLinks = Array.from(new Set([...processedLinks, ...uniqueLinks]));

  await putLatentSpaceLinks(allLinks, store);

  return uniqueLinks;
}

export const latentSpaceLoader = traceable(latentSpaceLoaderFunc, {
  name: "latent-space-loader",
});



================================================
FILE: src/agents/curate-data/loaders/reddit.ts
================================================
import { BaseStore } from "@langchain/langgraph";
import { RedditClient } from "../../../clients/reddit/client.js";
import {
  getRedditPostIds,
  putRedditPostIds,
} from "../utils/stores/reddit-post-ids.js";
import { getUniqueArrayItems } from "../utils/get-unique-array.js";
import { SimpleRedditPostWithComments } from "../../../clients/reddit/types.js";
import { traceable } from "langsmith/traceable";
import { isValidUrl } from "../../utils.js";

async function getRedditPostsFunc(
  store: BaseStore | undefined,
): Promise<SimpleRedditPostWithComments[]> {
  const client = await RedditClient.fromUserless();
  const topPosts = await client.getTopPosts("LocalLLaMA", { limit: 15 });
  const data: SimpleRedditPostWithComments[] = [];
  for (const post of topPosts) {
    const comments = await client.getPostComments(post.id, {
      limit: 10, // default
      depth: 3, // default
    });
    data.push({
      post: client.simplifyPost(post),
      comments: comments.map(client.simplifyComment),
    });
  }

  const processedPostIds = await getRedditPostIds(store);
  const postIds = data.map((post) => post.post.id);
  const uniquePostIds = getUniqueArrayItems(processedPostIds, postIds);
  const allPostIds = Array.from(
    new Set([...processedPostIds, ...uniquePostIds]),
  );

  await putRedditPostIds(allPostIds, store);

  return data.filter((post) => uniquePostIds.includes(post.post.id));
}

export const getRedditPosts = traceable(getRedditPostsFunc, {
  name: "reddit-loader",
});

const filterRedditPosts = (
  post: SimpleRedditPostWithComments,
  uniquePostIds: string[],
) => {
  return (
    uniquePostIds.includes(post.post.id) &&
    isValidUrl(post.post.url) &&
    !post.post.url.endsWith(".png") &&
    !post.post.url.endsWith(".jpg") &&
    !post.post.url.endsWith(".jpeg") &&
    !post.post.url.endsWith(".gif")
  );
};

async function getLangChainRedditPostsFunc(
  store: BaseStore | undefined,
  numPostsPerSubreddit = 25,
) {
  const client = await RedditClient.fromUserless();
  const newPostsLangChain = await client.getNewPosts("LangChain", {
    limit: numPostsPerSubreddit,
  });
  const newPostsLangGraph = await client.getNewPosts("LangGraph", {
    limit: numPostsPerSubreddit,
  });

  const allNewPosts = [...newPostsLangChain, ...newPostsLangGraph];

  const data: SimpleRedditPostWithComments[] = [];
  for (const post of allNewPosts) {
    const comments = await client.getPostComments(post.id, {
      limit: 10, // default
      depth: 3, // default
    });
    data.push({
      post: client.simplifyPost(post),
      comments: comments.map(client.simplifyComment),
    });
  }

  const processedPostIds = await getRedditPostIds(store);
  const postIds = data.map((post) => post.post.id);
  const uniquePostIds = getUniqueArrayItems(processedPostIds, postIds);
  return data.filter((post) => filterRedditPosts(post, uniquePostIds));
}

export const getLangChainRedditPosts = traceable(getLangChainRedditPostsFunc, {
  name: "reddit-loader-langchain",
});



================================================
FILE: src/agents/curate-data/loaders/twitter.ts
================================================
import { TweetV2, TweetV2ListTweetsPaginator } from "twitter-api-v2";
import { TwitterClient } from "../../../clients/twitter/client.js";
import { createdAtAfter } from "../utils/created-at-after.js";
import { BaseStore } from "@langchain/langgraph";
import {
  getLastIngestedTweetId,
  putLastIngestedTweetId,
} from "../utils/stores/twitter.js";
import { traceable } from "langsmith/traceable";

const LIST_ID = "1585430245762441216";

/**
 * Fetches tweets from a Twitter list, handling pagination and error handling.
 * If an error occurs, the function logs the error and returns undefined.
 * @param client
 * @param paginationToken
 * @returns {Promise<TweetV2ListTweetsPaginator | undefined>}
 */
async function fetchListTweetsWrapper(
  client: TwitterClient,
  paginationToken: string | undefined,
): Promise<TweetV2ListTweetsPaginator | undefined> {
  try {
    return await client.getListTweets(LIST_ID, {
      maxResults: 100,
      paginationToken,
    });
  } catch (error) {
    console.error(`Error fetching tweets: ${error}`);
    return undefined;
  }
}

async function twitterLoaderFunc(): Promise<TweetV2[]> {
  const client = TwitterClient.fromBasicTwitterAuth();

  // Initialize variables for pagination
  let paginationToken: string | undefined;
  let allTweets: TweetV2[] = [];
  let requestCount = 0;
  const maxRequests = 5;

  // Calculate 24 hours ago once, using the provided current time
  const oneDayAgo = new Date(new Date().getTime() - 24 * 60 * 60 * 1000);
  const oneDayAgoUTC = new Date(oneDayAgo.toISOString());

  while (requestCount < maxRequests) {
    requestCount += 1;

    const tweets = await fetchListTweetsWrapper(client, paginationToken);
    if (tweets === undefined || !tweets?.data?.data?.length) {
      // No more tweets to fetch
      break;
    }

    const filteredTweets: TweetV2[] = tweets?.data.data.filter(
      (tweet) =>
        tweet.created_at && createdAtAfter(tweet.created_at, oneDayAgoUTC),
    );

    allTweets = [...allTweets, ...filteredTweets];

    // Get the next pagination token
    paginationToken = tweets.data.meta?.next_token;
    // If no more tweets or we got tweets older than 24 hours, stop paginating
    if (
      !paginationToken ||
      tweets.data.data.some(
        (tweet) =>
          tweet.created_at && !createdAtAfter(tweet.created_at, oneDayAgoUTC),
      )
    ) {
      break;
    }
  }

  return allTweets;
}

export const twitterLoader = traceable(twitterLoaderFunc, {
  name: "twitter-loader",
});

async function twitterLoaderWithLangChainFunc(store: BaseStore | undefined) {
  const lastIngestedTweetId = await getLastIngestedTweetId(store);
  const client = TwitterClient.fromBasicTwitterAuth();
  // Don't return tweets from the LangChain account, or active LangChain employees since these
  // are likely to have already been sent to the agent/are duplicates.
  const excludeUsers =
    "-from:LangChainAI -from:hwchase17 -from:bracesproul -from:Hacubu";
  const query = `@LangChainAI -is:reply -is:retweet -is:quote has:links ${excludeUsers}`;
  const langchainTweets = await client.searchTweets(query, {
    maxResults: 60, // Twitter API v2 limits to 60 req/15 min,
    sinceId: lastIngestedTweetId || undefined,
  });
  const tweets = langchainTweets.data.data;

  if (!tweets) {
    throw new Error("No tweets found");
  }

  const mostRecentTweetId = tweets
    .sort((a, b) => {
      if (!a.created_at || !b.created_at) {
        return 0;
      }
      return (
        new Date(b.created_at).getTime() - new Date(a.created_at).getTime()
      );
    })
    .map((tweet) => tweet.id)[0];

  if (mostRecentTweetId) {
    await putLastIngestedTweetId(mostRecentTweetId, store);
  }

  return tweets;
}

export const twitterLoaderWithLangChain = traceable(
  twitterLoaderWithLangChainFunc,
  { name: "twitter-loader-langchain" },
);



================================================
FILE: src/agents/curate-data/loaders/github/langchain.ts
================================================
import { BaseStore } from "@langchain/langgraph";
import { getGitHubRepoURLs } from "../../utils/stores/github-repos.js";
import { Octokit } from "@octokit/rest";
import { sleep } from "../../../utils.js";
import { traceable } from "langsmith/traceable";

function getOctokit() {
  const token = process.env.GITHUB_TOKEN;
  if (!token) {
    throw new Error("GITHUB_TOKEN environment variable is required");
  }

  const octokit = new Octokit({
    auth: token,
  });

  return octokit;
}

const JS_LANGCHAIN_PACKAGES = [
  "@langchain/langgraph",
  "@langchain/core",
  "@langchain/community",
  "@langchain/openai",
  "@langchain/anthropic",
];

const PY_LANGCHAIN_PACKAGES = [
  "langgraph",
  "langchain-core",
  "langchain-community",
  "langchain-openai",
  "langchain-anthropic",
];

const JS_PATH_QUERY = "filename:package.json";
const PY_PATH_QUERY_REQUIREMENTS = "filename:requirements.txt";
const PY_PATH_QUERY_PYPROJECT = "filename:pyproject.toml";

const NOT_LANGCHAIN_ORG_QUERY = "NOT org:langchain-ai";

const FULL_PY_QUERY = `${PY_PATH_QUERY_REQUIREMENTS} OR ${PY_PATH_QUERY_PYPROJECT} ${NOT_LANGCHAIN_ORG_QUERY}`;
const FULL_JS_QUERY = `${JS_LANGCHAIN_PACKAGES.join(" OR ")} ${JS_PATH_QUERY} ${NOT_LANGCHAIN_ORG_QUERY}`;

async function checkJSDependencies(
  octokit: Octokit,
  owner: string,
  repo: string,
  path: string,
): Promise<boolean> {
  try {
    const { data } = await octokit.repos.getContent({
      owner,
      repo,
      path,
    });

    if (!("content" in data)) {
      return false;
    }

    const content = Buffer.from(data.content, "base64").toString();
    const packageJson = JSON.parse(content);
    const dependencies = {
      ...packageJson.dependencies,
      ...packageJson.devDependencies,
    };

    const langchainDeps = Object.keys(dependencies).filter((dep) =>
      JS_LANGCHAIN_PACKAGES.some((jsDep) => jsDep === dep),
    );

    return langchainDeps.length > 0;
  } catch (error) {
    return false;
  }
}

async function checkPythonDependencies(
  octokit: Octokit,
  owner: string,
  repo: string,
  path: string,
): Promise<boolean> {
  try {
    const { data } = await octokit.repos.getContent({
      owner,
      repo,
      path,
    });

    if (!("content" in data)) {
      return false;
    }

    if (path.endsWith(".txt")) {
      const content = Buffer.from(data.content, "base64").toString();
      return PY_LANGCHAIN_PACKAGES.some(
        (pkg) =>
          content.includes(pkg) || new RegExp(`${pkg}[-~=]+`).test(content),
      );
    }

    // Assume it's a pyproject.toml file
    if (path.endsWith(".toml")) {
      const content = Buffer.from(data.content, "base64").toString();
      return PY_LANGCHAIN_PACKAGES.some(
        (pkg) =>
          content.includes(pkg) || new RegExp(`${pkg}[-~=]+`).test(content),
      );
    }

    return false;
  } catch (error) {
    return false;
  }
}

async function langchainDependencyReposLoaderFunc(
  store: BaseStore | undefined,
) {
  const octokit = getOctokit();
  const processedRepos = await getGitHubRepoURLs(store);
  const newRepoURLs: string[] = [];
  let page = 0;
  const maxAttempts = 1;

  // Fetch PY repos first
  while (newRepoURLs.length < 5 || page <= maxAttempts) {
    page += 1;

    try {
      const results = await Promise.allSettled(
        PY_LANGCHAIN_PACKAGES.map((pkg) =>
          octokit.search.code({
            q: `${pkg} ${FULL_PY_QUERY}`,
            sort: "indexed",
            order: "desc",
            per_page: 25,
            page,
          }),
        ),
      );

      let incompleteResults = false;
      const filteredResults = results
        .filter((result) => result.status === "fulfilled")
        .map((result) => result.value);

      for (const { data } of filteredResults) {
        console.log(`Found ${data.total_count} Python repos`);
        // Only set to true. This is to prevent one query setting it to false when another one returns incomplete results
        if (data.incomplete_results) {
          incompleteResults = data.incomplete_results;
        }

        if (data.total_count === 0) {
          break;
        }

        for (const repo of data.items) {
          const repoUrl = repo.repository.html_url;
          if (
            processedRepos.includes(repoUrl) ||
            !repo.repository.owner ||
            newRepoURLs.some((r) => r === repoUrl)
          ) {
            if (data.incomplete_results) {
              continue;
            } else {
              break;
            }
          }

          const hasDeps = await checkPythonDependencies(
            octokit,
            repo.repository.owner.login,
            repo.repository.name,
            repo.path,
          );

          if (hasDeps) {
            newRepoURLs.push(repoUrl);
            if (newRepoURLs.length >= 5) break;
          }

          // Sleep for 30s due to rate limits of 10 req/min.
          console.log("Sleeping...");
          await sleep(30000);
        }
      }

      if (newRepoURLs.length >= 5 || !incompleteResults) break;
    } catch (error) {
      console.error("Failed to fetch repos:", error);
      break;
    }
  }

  // Reset page counter and search for JS repos
  page = 0;
  while (newRepoURLs.length < 10 || page <= maxAttempts) {
    page += 1;

    try {
      const results = await Promise.allSettled(
        JS_LANGCHAIN_PACKAGES.map((pkg) =>
          octokit.search.code({
            q: `${pkg} ${FULL_JS_QUERY}`,
            sort: "indexed",
            order: "desc",
            per_page: 25,
            page,
          }),
        ),
      );

      let incompleteResults = false;
      const filteredResults = results
        .filter((result) => result.status === "fulfilled")
        .map((result) => result.value);

      for (const { data } of filteredResults) {
        console.log(`Found ${data.total_count} JS repos`);
        // Only set to true. This is to prevent one query setting it to false when another one returns incomplete results
        if (data.incomplete_results) {
          incompleteResults = data.incomplete_results;
        }

        if (data.total_count === 0) {
          break;
        }

        for (const repo of data.items) {
          const repoUrl = repo.repository.html_url;
          if (
            processedRepos.includes(repoUrl) ||
            !repo.repository.owner ||
            newRepoURLs.some((r) => r === repoUrl)
          ) {
            if (data.incomplete_results) {
              continue;
            } else {
              break;
            }
          }

          const hasDeps = await checkJSDependencies(
            octokit,
            repo.repository.owner.login,
            repo.repository.name,
            repo.path,
          );

          if (hasDeps) {
            newRepoURLs.push(repoUrl);
            if (newRepoURLs.length >= 5) break;
          }

          // Sleep for 30s due to rate limits of 10 req/min.
          console.log("Sleeping...");
          await sleep(30000);
        }
      }

      if (newRepoURLs.length >= 25 || !incompleteResults) break;
    } catch (error) {
      console.error("Failed to fetch repos:", error);
      break;
    }
  }

  return newRepoURLs.slice(0, 25);
}

export const langchainDependencyReposLoader = traceable(
  langchainDependencyReposLoaderFunc,
  { name: "github-loader-langchain" },
);



================================================
FILE: src/agents/curate-data/loaders/github/trending.ts
================================================
import { BaseStore } from "@langchain/langgraph";
import {
  getGitHubRepoURLs,
  putGitHubRepoURLs,
} from "../../utils/stores/github-repos.js";
import { getUniqueArrayItems } from "../../utils/get-unique-array.js";
import * as cheerio from "cheerio";
import { traceable } from "langsmith/traceable";

const TYPESCRIPT_TRENDING_URL =
  "https://github.com/trending/typescript?since=daily";
const PYTHON_TRENDING_URL = "https://github.com/trending/python?since=daily";

// Check github dependabot for depending on langchain
// Check for github langchain tags
async function githubTrendingLoaderFunc(store: BaseStore | undefined) {
  const fetchRepos = async (url: string) => {
    const response = await fetch(url);
    const html = await response.text();
    const $ = cheerio.load(html);

    return $("h2.h3.lh-condensed")
      .map((_, element) => {
        const repoPath = $(element).find("a").attr("href");
        return repoPath ? `https://github.com${repoPath}` : null;
      })
      .get()
      .filter((url): url is string => url !== null);
  };

  const [pythonRepos, typescriptRepos] = await Promise.all([
    fetchRepos(PYTHON_TRENDING_URL),
    fetchRepos(TYPESCRIPT_TRENDING_URL),
  ]);

  const processedRepos = await getGitHubRepoURLs(store);
  const uniqueRepos = getUniqueArrayItems(processedRepos, [
    ...pythonRepos,
    ...typescriptRepos,
  ]);
  const allRepos = Array.from(new Set([...processedRepos, ...uniqueRepos]));

  await putGitHubRepoURLs(allRepos, store);

  return uniqueRepos;
}

export const githubTrendingLoader = traceable(githubTrendingLoaderFunc, {
  name: "github-loader",
});



================================================
FILE: src/agents/curate-data/loaders/tests/ai-news-blog.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { aiNewsBlogLoader } from "../ai-news-blog.js";

test("aiNewsBlogLoader", async () => {
  const results = await aiNewsBlogLoader();
  console.log(results);
  expect(results.length).toBeGreaterThan(0);
});



================================================
FILE: src/agents/curate-data/loaders/tests/github.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { InMemoryStore } from "@langchain/langgraph";
import { langchainDependencyReposLoader } from "../github/langchain.js";
import { githubTrendingLoader } from "../github/trending.js";

test("githubTrendingLoader", async () => {
  const store = new InMemoryStore();
  const config = {
    store,
  };
  const results = await githubTrendingLoader(config.store);
  console.log("\n\nTEST COMPLETED\n\n");
  console.log("results.length", results);
  expect(results.length).toBeGreaterThan(0);

  // This should return 0 results due to all the links being in the store
  const results2 = await githubTrendingLoader(config.store);
  console.log("\n\nTEST COMPLETED\n\n");
  console.log("results2.length", results2);
  expect(results2.length).toBe(0);
});

test("langchainDependencyReposLoader", async () => {
  const store = new InMemoryStore();
  const config = {
    store,
  };
  const results = await langchainDependencyReposLoader(config.store);
  console.log("\n\nTEST COMPLETED\n\n");
  console.log("results.length", results.length);
  console.log(results);
  expect(results.length).toBe(10);
}, 240000); // 4 minutes since there's 30/s delays after each 5 requests



================================================
FILE: src/agents/curate-data/loaders/tests/latent-space.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { latentSpaceLoader } from "../latent-space.js";
import { InMemoryStore } from "@langchain/langgraph";
import { putLatentSpaceLinks } from "../../utils/stores/latent-space-links.js";

test("Latent space loader", async () => {
  const store = new InMemoryStore();
  const config = {
    store,
  };
  // Seed the store with some existing values
  await putLatentSpaceLinks(
    [
      "https://www.latent.space/p/2024-simonw",
      "https://www.latent.space/p/o1-skill-issue",
      "https://www.latent.space/p/exa",
    ],
    config.store,
  );
  const data = await latentSpaceLoader(config.store);
  expect(data.length).toBeGreaterThan(1);
});



================================================
FILE: src/agents/curate-data/loaders/tests/reddit.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { getRedditPosts } from "../reddit.js";
import { InMemoryStore } from "@langchain/langgraph";

test("getRedditPosts", async () => {
  const store = new InMemoryStore();
  const config = {
    store,
  };
  const results = await getRedditPosts(config.store);
  console.log("\n\nTEST COMPLETED\n\n");
  console.log("results.length", results.length);
  expect(results.length).toBeGreaterThan(0);
});



================================================
FILE: src/agents/curate-data/loaders/tests/twitter.int.test.ts
================================================
import fs from "fs/promises";
import { test, expect } from "@jest/globals";
import { twitterLoader } from "../twitter.js";

test("twitterLoader", async () => {
  const resultsPath =
    "/Users/bracesproul/code/lang-chain-ai/projects/social-media-agent/src/agents/curate-reports/nodes/tests/data/tweets-3.json";
  const results = await twitterLoader();
  console.log("\n\nTEST COMPLETED\n\n");
  console.log("results.length", results.length);
  await fs.writeFile(resultsPath, JSON.stringify(results));
  expect(results.length).toBeGreaterThan(0);
});



================================================
FILE: src/agents/curate-data/nodes/extract-ai-newsletter-content.ts
================================================
import { FireCrawlLoader } from "@langchain/community/document_loaders/web/firecrawl";
import { CurateDataState } from "../state.js";
import { extractTweetId, extractUrls, getUrlType, sleep } from "../../utils.js";
import { RedditPostsWithExternalData } from "../../verify-reddit-post/types.js";
import { TwitterClient } from "../../../clients/twitter/client.js";
import { TweetV2 } from "twitter-api-v2";
import { RedditClient } from "../../../clients/reddit/client.js";
import { SimpleRedditPostWithComments } from "../../../clients/reddit/types.js";

function checkTwitterURLExists(url: string, validatedTweets: TweetV2[]) {
  const tweetId = extractTweetId(url);

  return validatedTweets.some((tweet) => tweet.id === tweetId);
}

function checkRedditURLExists(
  url: string,
  redditPosts: RedditPostsWithExternalData[],
): boolean {
  return redditPosts.some((post) => post.post.url === url);
}

// MUST BE CALLED BEFORE GENERATING TWITTER GROUPS, AND BEFORE VERIFYING REDDIT POSTS
export async function extractAINewsletterContent(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  const relevantTwitterURLs: string[] = [];
  const relevantRedditURLs: string[] = [];
  const generalURLs: string[] = [];

  for await (const url of state.aiNewsPosts) {
    const loader = new FireCrawlLoader({
      url,
      mode: "scrape",
      params: {
        formats: ["markdown"],
      },
    });

    const docs = await loader.load();

    const docsText = docs.map((d) => d.pageContent).join("\n");

    const urls: string[] = [];
    // Content between these sections are from the AI Twitter Recap
    if (
      docsText.includes("AI Twitter Recap") &&
      docsText.includes("AI Reddit Recap")
    ) {
      const twitterRecap = docsText
        .split("AI Twitter Recap")[1]
        .split("AI Reddit Recap")[0];
      if (twitterRecap.length > 0) {
        const twitterURLs = extractUrls(twitterRecap);
        urls.push(...twitterURLs);
      }
    }

    // Content between these sections are from the AI Reddit Recap
    if (
      docsText.includes("AI Reddit Recap") &&
      docsText.includes("AI Discord Recap")
    ) {
      const redditRecap = docsText
        .split("AI Reddit Recap")[1]
        .split("AI Discord Recap")[0];
      if (redditRecap.length > 0) {
        const redditURLs = extractUrls(redditRecap);
        urls.push(...redditURLs);
      }
    }

    const twitterURLsInText = urls
      .filter((url) => getUrlType(url) === "twitter")
      .filter((url) => !checkTwitterURLExists(url, state.validatedTweets));
    relevantTwitterURLs.push(...twitterURLsInText);

    const redditURLsInText = urls
      // Do not include media links
      .filter((url) => getUrlType(url) === "reddit" && !url.includes("/media"))
      .filter((url) => !checkRedditURLExists(url, state.redditPosts));
    relevantRedditURLs.push(...redditURLsInText);

    generalURLs.push(
      ...urls.filter((url) =>
        ["general", "github", "youtube"].includes(getUrlType(url) || ""),
      ),
    );
  }

  const tweets: TweetV2[] = [];
  const twitterClient = TwitterClient.fromBasicTwitterAuth();
  for await (const twitterURL of relevantTwitterURLs) {
    // Sleep for 30/s between requests to avoid rate limits
    await sleep(30000);
    const tweetId = extractTweetId(twitterURL);
    if (!tweetId) {
      console.warn("Failed to extract tweet ID from URL:", twitterURL);
      continue;
    }
    try {
      const tweetContent = await twitterClient.getTweet(tweetId);
      tweets.push(tweetContent.data);
    } catch (e) {
      console.error("Failed to fetch tweet:", e);
      // Break on the first twitter error, as it's likely due to rate limits.
      break;
    }
  }

  const redditPosts: SimpleRedditPostWithComments[] = [];
  const client = await RedditClient.fromUserless();
  for await (const redditURL of relevantRedditURLs) {
    const postAndComments = await client.getSimplePostAndComments(redditURL);
    redditPosts.push(postAndComments);
  }

  return {
    validatedTweets: [...state.validatedTweets, ...tweets],
    rawRedditPosts: [...state.rawRedditPosts, ...redditPosts],
    generalUrls: [...state.generalUrls, ...generalURLs],
  };
}



================================================
FILE: src/agents/curate-data/nodes/format-data.ts
================================================
import { CurateDataState } from "../state.js";

export async function formatData(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  return {
    curatedData: {
      tweetsGroupedByContent: state.tweetsGroupedByContent,
      redditPosts: state.redditPosts,
      generalContents: state.pageContents?.map((pc, idx) => ({
        pageContent: pc,
        relevantLinks: (state.relevantLinks?.[idx] || []) as string[],
      })),
      githubTrendingData: state.githubTrendingData,
    },
  };
}



================================================
FILE: src/agents/curate-data/nodes/generate-posts-subgraph.ts
================================================
import { Client } from "@langchain/langgraph-sdk";
import { CurateDataState } from "../state.js";
import { getTweetLink } from "../../../clients/twitter/utils.js";
import { POST_TO_LINKEDIN_ORGANIZATION } from "../../generate-post/constants.js";
import {
  getAfterSecondsFromLinks,
  shouldPostToLinkedInOrg,
} from "../../utils.js";
import { BaseStore, LangGraphRunnableConfig } from "@langchain/langgraph";
import {
  getGitHubRepoURLs,
  putGitHubRepoURLs,
} from "../utils/stores/github-repos.js";
import {
  getRedditPostIds,
  putRedditPostIds,
} from "../utils/stores/reddit-post-ids.js";
import { getTweetIds, putTweetIds } from "../utils/stores/twitter.js";
import { SlackClient } from "../../../clients/slack/client.js";
import { ThreadRunId } from "../types.js";

async function saveIngestedData(
  state: CurateDataState,
  store: BaseStore | undefined,
) {
  const [existingGitHubRepoURLs, redditPostIds, existingTweetIds] =
    await Promise.all([
      getGitHubRepoURLs(store),
      getRedditPostIds(store),
      getTweetIds(store),
    ]);

  const newGitHubRepoURLs = new Set([
    ...existingGitHubRepoURLs,
    ...state.rawTrendingRepos,
  ]);
  const newRedditPostIds = new Set([
    ...redditPostIds,
    ...state.rawRedditPosts.map((p) => p.post.id),
  ]);
  const newTweetIds = new Set([
    ...existingTweetIds,
    ...state.rawTweets.map((t) => t.id),
  ]);

  await Promise.all([
    putGitHubRepoURLs(Array.from(newGitHubRepoURLs), store),
    putRedditPostIds(Array.from(newRedditPostIds), store),
    putTweetIds(Array.from(newTweetIds), store),
  ]);
}

async function sendSlackNotification(
  state: CurateDataState,
  config: LangGraphRunnableConfig,
) {
  if (!process.env.SLACK_CHANNEL_ID || !process.env.SLACK_TOKEN) {
    return;
  }

  const slackClient = new SlackClient({
    token: process.env.SLACK_TOKEN,
  });

  try {
    await saveIngestedData(state, config.store);
    if (slackClient) {
      await slackClient.sendMessage(
        process.env.SLACK_CHANNEL_ID,
        `âœ… INGESTED DATA SAVED SUCCESSFULLY âœ…

Number of tweets: *${state.rawTweets.length}*
Number of repos: *${state.rawTrendingRepos.length}*
Number of reddit posts: *${state.rawRedditPosts.length}*
Run ID: *${config.configurable?.run_id || "not found"}*
Thread ID: *${config.configurable?.thread_id || "not found"}*
      `,
      );
    }
  } catch (error: any) {
    console.warn("Error saving ingested data", error);
    if (slackClient) {
      const errMessage = "message" in error ? error.message : String(error);

      await slackClient.sendMessage(
        process.env.SLACK_CHANNEL_ID,
        `FAILED TO SAVE INGESTED DATA: ${errMessage}
  
Run ID: *${config.configurable?.run_id || "not found"}*
Thread ID: *${config.configurable?.thread_id || "not found"}*
      `,
      );
    }
  }
}

function getAfterSeconds(state: CurateDataState) {
  const twitterURLs = state.rawTweets.flatMap((t) =>
    t.author_id ? [getTweetLink(t.author_id, t.id)] : [],
  );
  const redditURLs = state.rawRedditPosts.map((p) => p.post.url);
  const afterSecondsList = getAfterSecondsFromLinks(
    [...twitterURLs, ...redditURLs, ...state.rawTrendingRepos],
    {
      baseDelaySeconds: 60,
    },
  );

  return afterSecondsList;
}

export async function generatePostsSubgraph(
  state: CurateDataState,
  config: LangGraphRunnableConfig,
): Promise<Partial<CurateDataState>> {
  const postToLinkedInOrg = shouldPostToLinkedInOrg(config);

  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const afterSecondsList = getAfterSeconds(state);

  const threadRunIds: ThreadRunId[] = [];

  for (const { link, afterSeconds } of afterSecondsList) {
    const { thread_id } = await client.threads.create();
    const { run_id } = await client.runs.create(thread_id, "generate_post", {
      input: {
        links: [link],
      },
      config: {
        configurable: {
          [POST_TO_LINKEDIN_ORGANIZATION]: postToLinkedInOrg,
          origin: "curate-data",
        },
      },
      afterSeconds,
    });
    threadRunIds.push({ thread_id, run_id });
  }

  await sendSlackNotification(state, config);

  return {
    threadRunIds,
  };
}



================================================
FILE: src/agents/curate-data/nodes/ingest-data.ts
================================================
import { CurateDataConfigurable, CurateDataState } from "../state.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { aiNewsBlogLoader } from "../loaders/ai-news-blog.js";
import {
  twitterLoader,
  twitterLoaderWithLangChain,
} from "../loaders/twitter.js";
import { getLangChainRedditPosts, getRedditPosts } from "../loaders/reddit.js";
import { githubTrendingLoader } from "../loaders/github/trending.js";
import { TweetV2 } from "twitter-api-v2";
import { latentSpaceLoader } from "../loaders/latent-space.js";
import { SimpleRedditPostWithComments } from "../../../clients/reddit/types.js";
import { langchainDependencyReposLoader } from "../loaders/github/langchain.js";
import { useLangChainPrompts } from "../../utils.js";
import { NUM_POSTS_PER_SUBREDDIT } from "../constants.js";

export async function ingestData(
  _state: CurateDataState,
  config: LangGraphRunnableConfig,
): Promise<Partial<CurateDataState>> {
  const sources = (config.configurable as CurateDataConfigurable | undefined)
    ?.sources;
  if (!sources) {
    throw new Error("No sources provided");
  }
  if (!process.env.PORT) {
    throw new Error("PORT environment variable not set");
  }

  let tweets: TweetV2[] = [];
  let trendingRepos: string[] = [];
  let latentSpacePosts: string[] = [];
  let aiNewsPosts: string[] = [];
  let redditPosts: SimpleRedditPostWithComments[] = [];

  if (useLangChainPrompts()) {
    if (sources.includes("twitter")) {
      try {
        tweets = await twitterLoaderWithLangChain(config.store);
      } catch (e) {
        console.error("Failed to load tweets with LangChain prompts", e);
      }
    }
    if (sources.includes("github")) {
      try {
        trendingRepos = await langchainDependencyReposLoader(config.store);
      } catch (e) {
        console.error(
          "Failed to load trending repos with LangChain prompts",
          e,
        );
      }
    }
    if (sources.includes("reddit")) {
      try {
        redditPosts = await getLangChainRedditPosts(
          config.store,
          config.configurable?.[NUM_POSTS_PER_SUBREDDIT],
        );
      } catch (e) {
        console.error("Failed to load Reddit posts with LangChain prompts", e);
      }
    }

    // Latent space and AI news are not high signal for LangChain. Return early in this case.
    return {
      rawTweets: tweets,
      rawTrendingRepos: trendingRepos,
      rawRedditPosts: redditPosts,
    };
  }

  if (sources.includes("twitter")) {
    try {
      tweets = await twitterLoader();
    } catch (e) {
      console.error("Failed to load tweets", e);
    }
  }
  if (sources.includes("github")) {
    try {
      trendingRepos = await githubTrendingLoader(config.store);
    } catch (e) {
      console.error("Failed to load trending repos", e);
    }
  }
  if (sources.includes("reddit")) {
    try {
      redditPosts = await getRedditPosts(config.store);
    } catch (e) {
      console.error("Failed to load Reddit posts", e);
    }
  }
  if (sources.includes("latent_space")) {
    latentSpacePosts = await latentSpaceLoader(config.store);
  }
  if (sources.includes("ai_news")) {
    aiNewsPosts = await aiNewsBlogLoader();
  }

  return {
    rawTweets: tweets,
    rawTrendingRepos: trendingRepos,
    generalUrls: latentSpacePosts,
    aiNewsPosts,
    rawRedditPosts: redditPosts,
  };
}



================================================
FILE: src/agents/curate-data/nodes/validate-bulk-tweets.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { CurateDataState } from "../state.js";
import { z } from "zod";
import { chunkArray } from "../../utils.js";
import { TweetV2 } from "twitter-api-v2";

const EXAMPLES = `<example index="0">
    <example-tweet>
      RT @arjunkhemani: .@naval: Looking for truth is the opposite of looking for social approval.\n\nâ€œIâ€™m deeply suspicious of groups of people coâ€¦
    </example-tweet>

    <scratchpad>
      This tweet is not relevant because it has no mentions of AI, or valuable content for learning.
    </scratchpad>
    is_relevant: false
  </example>

  <example index="0">
    <example-tweet>
      Blog post for TransformerÂ²: Self-Adaptive LLMs\n\nhttps://t.co/AyeFdqEKsd\n\nEventually, neural network weights should be as adaptive as the Octopus ðŸ™\nhttps://t.co/me7urXJ6BS
    </example-tweet>

    <scratchpad>
      This tweet is relevant because it is about AI, and contains links to blog posts which are most likely about AI.
    </scratchpad>
    is_relevant: true
  </example>

  <example index="0">
    <example-tweet>
      @karpathy @martin_casado Sir, how do I convince my talented ex-big tech SDE peers to use LLMs more for coding\n\nalmost all of them cite privacy/security concerns or hallucinations
    </example-tweet>

    <scratchpad>
      This tweet is not relevant because it does not have enough AI content for learning, but rather it's presenting a question about AI.
    </scratchpad>
    is_relevant: false
  </example>

  <example index="0">
    <example-tweet>
      Aligning Instruction Tuning with Pre-training\n\nDetermines differences between pretraining corpus and SFT corpus and generates instruction data for the difference set. Evaluations on three fully\nopen LLMs across eight benchmarks demonstrate\nconsistent performance improvements. https://t.co/1jJxiv5q2T
    </example-tweet>

    <scratchpad>
      This tweet is relevant because it appears to be referencing a research paper on AI.
    </scratchpad>
    is_relevant: true
  </example>

  <example index="0">
    <example-tweet>
      Btw, your docs are likely AI generated, GAIA is not about environmental and sustainability at all ðŸ¤£
    </example-tweet>

    <scratchpad>
      This tweet is not relevant. Although it does mention AI, the tweet itself has no content for learning, or writing educational AI content. Instead it's dissing someone's (alleged) poor documentation.
    </scratchpad>
    is_relevant: false
  </example>

  <example index="0">
    <example-tweet>
      Prompt Engineers at Work ðŸ°ðŸ‘·ðŸŽ¨\n\nExclusive merch only available for the PromptLayer team... but good news is that we are hiring! https://t.co/X9aJO95RQp
    </example-tweet>

    <scratchpad>
      This tweet is not relevant because it is promoting a non-software product.
    </scratchpad>
    is_relevant: false
  </example>`;

const VALIDATE_BULK_TWEETS_PROMPT = `You are an AI assistant tasked with curating a dataset of tweets about AI. Your job is to review a series of tweets and determine which ones are relevant to AI, LLMs, and related topics. The tweets are provided from a 'Twitter List' of users who primarily tweet about AI.

Here are the rules for determining whether a tweet is relevant:
1. The tweet discusses AI, LLMs, or anything interesting related to AI.
2. The tweet is a retweet of content about AI, LLMs, or anything interesting related to AI.
3. The tweet mentions a research paper on AI, LLMs, or related topics.
4. The tweet is about a new product, tool, or service related to AI, LLMs, or anything interesting related to AI.
5. The tweet references a blog post, video, or other content related to AI, LLMs, or anything interesting related to AI.

Additionally, ensure that the tweet has enough content to be interesting and engaging. Avoid short tweets that merely reference AI without providing substantial information that could be used to create longer content or posts about AI.
The tweets you do approve will be used to create educational content about AI, so ensure the tweets approved are high quality, engaging, and informative.

You will be provided with a list of tweets, each associated with an index number. Your task is to analyze these tweets and identify which ones are relevant according to the rules above.

Use the following examples to guide your analysis:
<analysis-examples>
${EXAMPLES}
</analysis-examples>

Here are the tweets to analyze:
<tweets>
{TWEETS}
</tweets>

Use a scratchpad to analyze each tweet independently. In your scratchpad, briefly explain why each tweet is relevant or not relevant based on the rules provided. Then, create a list of the index numbers for the relevant tweets.
Remember, we only want the highest quality AI tweets, so you should lean towards NOT including a tweet unless it is clearly highly relevant, and would be useful when writing educational content about AI.

<scratchpad>
[Analyze each tweet here, explaining your reasoning]
</scratchpad>

After your analysis, provide your final answer to the 'answer' tool.
Remember, there will be times when all of the tweets are NOT relevant. In this case, do not be worried and simply answer with an empty array.
I won't be upset with you if you don't find any relevant tweets, however I WILL be upset if you mark tweets as relevant which are NOT actually relevant.

Begin!`;

const answerSchema = z
  .object({
    answer: z
      .array(z.number())
      .describe("The index numbers of the relevant tweets."),
  })
  .describe("Your final answer to what tweets are relevant.");

function formatTweets(tweets: TweetV2[]): string {
  return tweets
    .map((t, index) => {
      const fullText = t.note_tweet?.text || t.text || "";
      return `<tweet index="${index}">\n${fullText}\n</tweet>`;
    })
    .join("\n");
}

/**
 * Validates a batch of tweets to determine which ones are relevant to AI-related topics.
 * This function processes tweets in chunks of 25 and uses Claude 3 Sonnet to analyze each tweet
 * against a set of predefined relevance criteria.
 *
 * The relevance criteria includes:
 * - Discussions about AI, LLMs, or AI-related topics
 * - Retweets of AI-related content
 * - Mentions of AI research papers
 * - Information about AI products, tools, or services
 * - References to AI-related blog posts, videos, or other content
 *
 * @param {CurateDataState} state - The current state containing tweets to be validated
 * @returns {Promise<Partial<CurateDataState>>} A promise that resolves to an updated state
 *                                                 containing only the relevant tweets
 */
export async function validateBulkTweets(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  }).withStructuredOutput(answerSchema, { name: "answer" });

  // Chunk the tweets into groups of 25
  const chunkedTweets = chunkArray(state.rawTweets, 25);
  const allRelevantTweets: TweetV2[] = [];

  for (const chunk of chunkedTweets) {
    const formattedPrompt = VALIDATE_BULK_TWEETS_PROMPT.replace(
      "{TWEETS}",
      formatTweets(chunk),
    );

    const { answer } = await model.invoke([["user", formattedPrompt]]);

    const answerSet = new Set(answer);
    const relevantTweets = chunk.filter((_, index) => answerSet.has(index));
    if (relevantTweets.length !== answer.length) {
      console.warn(
        `Expected ${answer.length} relevant tweets, but found ${relevantTweets.length}`,
      );
    }
    allRelevantTweets.push(...relevantTweets);
  }

  return {
    validatedTweets: allRelevantTweets,
  };
}



================================================
FILE: src/agents/curate-data/nodes/verify-github-wrapper.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { CurateDataState } from "../state.js";
import { GitHubTrendingData } from "../types.js";
import { verifyGitHubContent } from "../../shared/nodes/verify-github.js";

export async function verifyGitHubWrapper(
  state: CurateDataState,
  config: LangGraphRunnableConfig,
): Promise<Partial<CurateDataState>> {
  const verifiedRepoData: GitHubTrendingData[] = [];

  // Iterate over each raw GitHub repo & verify + extract page contents
  for await (const repoURL of state.rawTrendingRepos) {
    const results = await verifyGitHubContent(
      {
        link: repoURL,
      },
      config,
    );

    if (
      results.relevantLinks &&
      results.relevantLinks.length > 0 &&
      results.pageContents &&
      results.pageContents.length > 0
    ) {
      verifiedRepoData.push({
        repoURL,
        pageContent: results.pageContents[0], // Take first page content, as there should only be one
      });
    }
  }

  return {
    githubTrendingData: verifiedRepoData,
  };
}



================================================
FILE: src/agents/curate-data/nodes/verify-reddit-wrapper.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { CurateDataState } from "../state.js";
import { verifyRedditPostGraph } from "../../verify-reddit-post/verify-reddit-post-graph.js";
import { RedditPostsWithExternalData } from "../../verify-reddit-post/types.js";
import { VerifyRedditPostConfigurable } from "../../verify-reddit-post/verify-reddit-post-state.js";

export async function verifyRedditWrapper(
  state: CurateDataState,
  config: LangGraphRunnableConfig<VerifyRedditPostConfigurable>,
): Promise<Partial<CurateDataState>> {
  const verifiedRedditPosts: RedditPostsWithExternalData[] = [];

  for (const post of state.rawRedditPosts) {
    try {
      const result = await verifyRedditPostGraph.invoke(
        {
          redditPost: post,
        },
        config,
      );

      if (
        result.relevantLinks &&
        result.relevantLinks.length > 0 &&
        result.pageContents &&
        result.pageContents.length > 0
      ) {
        verifiedRedditPosts.push({
          ...post,
          externalData: result.pageContents.map((pageContent, idx) => ({
            pageContent,
            url: result.relevantLinks?.[idx] || "",
          })),
        });
      }
    } catch (e) {
      console.error("Failed to verify Reddit post", e);
    }
  }

  return {
    redditPosts: verifiedRedditPosts,
  };
}



================================================
FILE: src/agents/curate-data/nodes/tweets/group-tweets-by-content.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { CurateDataState } from "../../state.js";
import { GROUP_BY_CONTENT_CRITERIA } from "./prompts.js";

const GROUP_BY_CONTENT_PROMPT = `You're an advanced AI software engineer who's working on curating education content about AI.
You're given a dump of Tweets about AI, LLMs, or related software. Your task is to carefully inspect each and every tweet the user provides, thinking about the meaning, context, and significance of each tweet.
Once you're done carefully reading over every tweet, you should group tweets which are talking about the same topic together.

Use the following criteria to group tweets:
${GROUP_BY_CONTENT_CRITERIA}

Ensure you are careful in doing this, as each group will be used to write a specific piece of educational content on the topic of the group.
This means if you create poorly grouped content, the educational content will not be high quality, which we DO NOT WANT!

Think slowly, and carefully. When you are done you should provide your group identifying each tweet by the 'index' property given to you. Ensure you format your response as follows:
The entire response should be wrapped inside <answer> tags.
Each subgroup should be wrapped inside <group> tags.
Inside each <group> tag, you should have an <explanation> tag containing a short explanation of the topic of the group.
Additionally, include the <tweet-index-list> tag, which should contain a comma-separated list of the indices of the tweets in the group, using the same indices provided to you in the original <tweet> list.

Follow the formatting instructions carefully, and ensure all of your XML tags are properly closed and balanced.`;

/**
 * Parse the LLM generation to extract groups containing explanations and tweet indices.
 * Each group should contain an explanation and a list of tweet indices.
 * @param generation The text generation to parse
 * @returns Array of objects containing explanation and tweet indices, or empty array if parsing fails
 */
function parseGeneration(
  generation: string,
): Array<{ explanation: string; tweetIndices: number[] }> {
  try {
    const groups = generation.match(/<group>([\s\S]*?)<\/group>/g) || [];
    return groups.map((group) => {
      const explanationMatch = group.match(
        /<explanation>([\s\S]*?)<\/explanation>/,
      );
      const indicesMatch = group.match(
        /<tweet-index-list>([\s\S]*?)<\/tweet-index-list>/,
      );

      if (!explanationMatch || !indicesMatch) {
        throw new Error("Missing required tags in group");
      }

      const explanation = explanationMatch[1].trim();
      const tweetIndices = indicesMatch[1]
        .split(",")
        .map((index) => parseInt(index.trim()))
        .filter((index) => !isNaN(index));

      return { explanation, tweetIndices };
    });
  } catch (error) {
    console.warn(
      "Could not parse groups from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
      "\nError:",
      error,
    );
    return [];
  }
}

export async function groupTweetsByContent(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  const model = new ChatOpenAI({ model: "o1", streaming: false });

  const formattedUserPrompt = `Here are the tweets you should inspect, and group:
<all-tweets>
${state.validatedTweets
  .map((twt, index) => {
    const tweetText = twt.note_tweet?.text || twt.text || "";
    if (!tweetText) return "";
    return `<tweet index="${index}">
${tweetText}
</tweet>`;
  })
  .join("\n")}
</all-tweets>`;

  const result = await model.invoke([
    ["system", GROUP_BY_CONTENT_PROMPT],
    ["user", formattedUserPrompt],
  ]);

  const parsedGenerations = parseGeneration(result.content as string);

  if (!parsedGenerations.length) {
    console.warn("No groups found in generation");
    return {};
  }

  // Convert the parsed generations into the expected TweetsGroupedByContent format
  const tweetsGroupedByContent = parsedGenerations.map((group) => ({
    explanation: group.explanation,
    tweets: group.tweetIndices.map((index) => {
      const tweet = state.validatedTweets[index];
      // Extract URLs from tweet entities if they exist
      const external_urls =
        tweet.entities?.urls?.map((url) => url.expanded_url) || [];
      return {
        ...tweet,
        external_urls,
      };
    }),
  }));

  return {
    tweetsGroupedByContent,
  };
}



================================================
FILE: src/agents/curate-data/nodes/tweets/prompts.ts
================================================
export const GROUP_BY_CONTENT_CRITERIA = `<grouping-criteria>
- Tweets discussing a new model, benchmark, tool, product or other released by an individual or organization
- Tweets discussing a new UI/UX pattern for AI applications
- Tweets discussing pitfalls of specific prompting strategies when working with LLMs
- General news or updates about AI
- You should try to group your tweets into fine-grained topics, to avoid grouping unrelated tweets into the same group.
</grouping-criteria>

<grouping-rules>
Tweets which discuss/reference the same model, benchmark, product, tool, etc should be grouped together. Ensure you do not group unrelated tweets together, unless you believe they are relevant to each others subjects.
Remember, you are allowed to put the same tweet into multiple groups, if you think they're relevant to each other. This should be used if a single tweet is relevant to multiple topics.
If you think a tweet is talking about a model, benchmark, tool, product or other, do your very best to identify what exactly it is, and group it accordingly.
</grouping-rules>`;



================================================
FILE: src/agents/curate-data/nodes/tweets/re-group-tweets.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { CurateDataState } from "../../state.js";
import { GROUP_BY_CONTENT_CRITERIA } from "./prompts.js";
import { TweetsGroupedByContent, TweetV2WithURLs } from "../../types.js";

const RE_GROUP_TWEETS = `You're an advanced AI software engineer who's working on curating education content about AI. A previous step your colleague took was to group tweets about AI, LLMs, and related software into unique topics. After this, a third party identified potential issues with some of the groups.
These groups which will be provided to you MAY need to be combined, or separated. Your task is to carefully review each group, and the tweets within each group. After this, inspect them all in the context of the entire list of groups passed to you.
Then, either re-group the tweets, combine groups, or do nothing if you deem the groups to be fine as is.

Use the following criteria to group tweets:
${GROUP_BY_CONTENT_CRITERIA}

Think carefully before you make a decision. Always provide a detailed explanation of your decision, and why you think it is appropriate.

If you deem no changes to be necessary, stop here and simply respond with "No changes needed".

If you do decide to re-group the tweets, you MUST respond with ALL of the tweets in new, existing, or updated groups. Any tweets which are NOT included in your response WILL be IGNORED, and left out (this is NOT what we want, since these tweets have already been flagged as relevant).

You should respond in the following format:
The entire response should be wrapped inside <answer> tags.
Each subgroup should be wrapped inside <group> tags.
Inside each <group> tag, you should have an <explanation> tag containing a short explanation of the topic of the group. Do not reference the old explanations, or describe why the group was split/joined. ONLY include the detailed explanation of the tweets in the group.
Additionally, include the <tweet-index-list> tag, which should contain a comma-separated list of the indices of the tweets to include in the group.
These indices should be formatted as group:tweet_index, where group is the index of the group in the original list, and tweet_index is the index of the tweet in the original group.

Follow the formatting instructions carefully, and ensure all of your XML tags are properly closed and balanced.

Begin!`;

function formatTweetsInGroup(tweets: TweetV2WithURLs[]): string {
  const text = tweets
    .map((twt, index) => {
      const tweetText = twt.note_tweet?.text || twt.text || "";
      return `<tweet index="${index}">\n${tweetText}\n</tweet>`;
    })
    .join("\n");

  return `<tweets-in-group>
${text}
</tweets-in-group>`;
}

function formatGroupsPrompt(tweetsGroupedByContent: TweetsGroupedByContent[]) {
  return `<groups-to-inspect>
${tweetsGroupedByContent
  .map(
    (group, index) => `<group index="${index}">
<explanation>${group.explanation}</explanation>
${formatTweetsInGroup(group.tweets)}
</group>`,
  )
  .join("\n")}
</groups-to-inspect>`;
}

function splitGroups(
  tweetsGroupedByContent: TweetsGroupedByContent[],
  similarGroupIndices: number[],
): {
  include: TweetsGroupedByContent[];
  review: TweetsGroupedByContent[];
} {
  const include: TweetsGroupedByContent[] = [];
  const review: TweetsGroupedByContent[] = [];

  tweetsGroupedByContent.forEach((group, index) => {
    if (similarGroupIndices.includes(index)) {
      review.push(group);
    } else {
      include.push(group);
    }
  });

  return {
    include,
    review,
  };
}

/**
 * Parse the LLM generation to extract groups containing explanations and tweet indices.
 * Each group should contain an explanation and a list of tweet indices.
 * @param generation The text generation to parse
 * @returns Array of objects containing explanation and tweet indices, or empty array if parsing fails
 */
function parseGeneration(generation: string): Array<{
  explanation: string;
  tweetIndices: { groupIndex: number; tweetIndex: number }[];
}> {
  try {
    const groups = generation.match(/<group>([\s\S]*?)<\/group>/g) || [];
    return groups.map((group) => {
      const explanationMatch = group.match(
        /<explanation>([\s\S]*?)<\/explanation>/,
      );
      const indicesMatch = group.match(
        /<tweet-index-list>([\s\S]*?)<\/tweet-index-list>/,
      );

      if (!explanationMatch || !indicesMatch) {
        throw new Error("Missing required tags in group");
      }

      const explanation = explanationMatch[1].trim();
      const tweetIndices = indicesMatch[1].split(",").map((index) => {
        const [groupIdx, tweetIdx] = index
          .trim()
          .split(":")
          .map((num) => parseInt(num.trim()));
        if (isNaN(groupIdx) || isNaN(tweetIdx)) {
          throw new Error(`Invalid index format: ${index}`);
        }
        return { groupIndex: groupIdx, tweetIndex: tweetIdx };
      });

      return { explanation, tweetIndices };
    });
  } catch (error) {
    console.warn(
      "Could not parse groups from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
      "\nError:",
      error,
    );
    return [];
  }
}

export async function reGroupTweets(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  if (state.similarGroupIndices.length === 0) {
    console.warn("No similar groups found. Continuing...");
    return {};
  }

  const model = new ChatOpenAI({ model: "o1", streaming: false });

  const { include, review } = splitGroups(
    state.tweetsGroupedByContent,
    state.similarGroupIndices,
  );

  const formattedUserPrompt = `Here are the groups you should inspect:
${formatGroupsPrompt(review)}`;

  const result = await model.invoke([
    ["system", RE_GROUP_TWEETS],
    ["user", formattedUserPrompt],
  ]);

  const parsedResult = parseGeneration(result.content as string);

  const newGroups: TweetsGroupedByContent[] = parsedResult.map((group) => {
    const tweets = group.tweetIndices.flatMap((index) => {
      if (!review[index.groupIndex]) {
        console.warn("Invalid tweet index:", index);
        return [];
      }
      return review[index.groupIndex].tweets[index.tweetIndex];
    });
    return {
      explanation: group.explanation,
      tweets,
    };
  });

  return {
    tweetsGroupedByContent: [...include, ...newGroups],
    similarGroupIndices: [],
  };
}



================================================
FILE: src/agents/curate-data/nodes/tweets/reflect-tweet-groups.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { CurateDataState } from "../../state.js";
import { GROUP_BY_CONTENT_CRITERIA } from "./prompts.js";
import { TweetV2WithURLs, TweetsGroupedByContent } from "../../types.js";

const REFLECT_ON_GROUPS_PROMPT = `You're an advanced AI software engineer who's working on curating education content about AI. Your colleague has taken a large dataset of tweets about AI, LLMs, and related software, and grouped them into a set of unique topics.

He was instructed to group them based on the following criteria:
${GROUP_BY_CONTENT_CRITERIA}

In doing this, he wrote descriptions of each group. Your task is to take a look at all of the descriptions, and the tweets which belong to each group. After inspecting each group, make a decision on if any of the groups should be combined, or updated.
This should be based off of whether or not you think the same model, benchmark, product, tool, etc is being referenced in multiple groups, or if the groups are really different topics altogether.

Your colleague will provide you with the group descriptions, and indices for each group. Carefully review each individually, and then in the context of the entire group, and make your decision.

Remember, the groups provided to you do NOT necessarily have issues, or duplicates, but you should still take a careful look at each group, and make a decision on if you think the groups should be combined, or updated.
> IMPORTANT: You will NOT be penalized for saying no groups are similar.

Think carefully before you make a decision. Always provide a detailed explanation of your decision, and why you think it is appropriate.

You should format your response as follows:
The entire response should be wrapped inside <answer> tags.
Within the <answer> tag, you should have a <similar-groups> tag, containing a comma-separated list of the indices of the groups that you think are similar, and should have a deeper look into.
Ensure you always wrap your tags with proper opening and closing tags. Do not prefix or suffix your response with any other dialog or text.

Follow the instructions carefully. Begin!`;

/**
 * Parse the LLM generation to extract similar group indices from the response
 * @param generation The text generation to parse
 * @returns Object containing array of similar group indices, or empty array if parsing fails
 */
function parseGeneration(generation: string): {
  similarGroupIndices: number[];
} {
  try {
    const answerMatch = generation.match(/<answer>([\s\S]*?)<\/answer>/);
    if (!answerMatch) {
      throw new Error("Missing answer tags in generation");
    }

    const similarGroupsMatch = answerMatch[1].match(
      /<similar-groups>([\s\S]*?)<\/similar-groups>/,
    );
    if (!similarGroupsMatch) {
      throw new Error("Missing similar-groups tags in generation");
    }

    const indices = similarGroupsMatch[1]
      .split(",")
      .map((index) => parseInt(index.trim()))
      .filter((index) => !isNaN(index));

    return { similarGroupIndices: indices };
  } catch (error) {
    console.warn(
      "Could not parse groups from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
      "\nError:",
      error,
    );
    return { similarGroupIndices: [] };
  }
}

function formatTweetsInGroup(tweets: TweetV2WithURLs[]): string {
  return tweets
    .map((twt) => {
      const text = twt.note_tweet?.text || twt.text || "";
      return `<tweet>\n${text}\n</tweet>`;
    })
    .join("\n");
}

function formatUserPrompt(
  tweetsGroupedByContent: TweetsGroupedByContent[],
): string {
  return `<all-groups>
${tweetsGroupedByContent
  .map((group, index) => {
    return `<group index="${index}">
<description>
${group.explanation}
</description>
<tweets>
${formatTweetsInGroup(group.tweets)}
</tweets>
</group>`;
  })
  .join("\n")}
</all-groups>`;
}

export async function reflectOnTweetGroups(
  state: CurateDataState,
): Promise<Partial<CurateDataState>> {
  const model = new ChatOpenAI({ model: "o1", streaming: false });

  const formattedUserPrompt = `Hi! Here are all of the groups I put together:
${formatUserPrompt(state.tweetsGroupedByContent)}

Thanks for your help!`;

  const result = await model.invoke([
    ["system", REFLECT_ON_GROUPS_PROMPT],
    ["user", formattedUserPrompt],
  ]);

  const parsedResult = parseGeneration(result.content as string);

  if (parsedResult.similarGroupIndices.length === 0) {
    return {};
  }

  return {
    similarGroupIndices: parsedResult.similarGroupIndices,
  };
}



================================================
FILE: src/agents/curate-data/nodes/tweets/tests/group-by-content.int.test.ts
================================================
import fs from "fs";
import * as ls from "langsmith/jest";
import { SimpleEvaluator } from "langsmith/jest";
import { TweetV2 } from "twitter-api-v2";
import { groupTweetsByContent } from "../group-tweets-by-content.js";
import { TweetsGroupedByContent } from "../../../types.js";
import { formatInTimeZone } from "date-fns-tz";

const tweetEvaluator: SimpleEvaluator = () => {
  return {
    key: "tweet_generation",
    score: 1,
  };
};

function loadTweets(): TweetV2[] {
  const tweets = JSON.parse(
    fs.readFileSync(
      // "src/agents/curate-data/nodes/tweets/tests/data/relevant-tweets/relevant-01-17-2025-12-10.json",
      // "src/agents/curate-data/nodes/tweets/tests/data/relevant-tweets/relevant-01-19-2025-12-59.json",
      "src/agents/curate-data/nodes/tweets/tests/data/relevant-tweets/relevant-01-20-2025-11-28.json",
      "utf-8",
    ),
  );
  return tweets;
}

async function saveTweets(tweets: TweetsGroupedByContent[]) {
  const currentDateUTC = new Date().toISOString();
  const formattedPSTDate = formatInTimeZone(
    currentDateUTC,
    "America/Los_Angeles",
    "MM-dd-yyyy-HH-mm",
  );
  await fs.promises.writeFile(
    `src/agents/curate-data/nodes/tweets/tests/data/grouped-by-llms/${formattedPSTDate}.json`,
    JSON.stringify(tweets),
  );
}

const validatedTweets = loadTweets();

ls.describe("SMA - Curate Data - Group By Content", () => {
  ls.test(
    "Can group tweets",
    {
      inputs: { validatedTweets },
      expected: {},
    },
    async ({ inputs }) => {
      console.log(
        "Starting test with",
        inputs.validatedTweets.length,
        "tweets",
      );

      const result = await groupTweetsByContent(inputs as any);

      if (!result.tweetsGroupedByContent?.length) {
        console.log("No tweets were found that are relevant to AI");
        return result;
      }

      await saveTweets(result.tweetsGroupedByContent || []);

      await ls.expect(result).evaluatedBy(tweetEvaluator).toBe(1);
      return result;
    },
  );
});



================================================
FILE: src/agents/curate-data/nodes/tweets/tests/re-group-reflect.int.test.ts
================================================
import * as ls from "langsmith/jest";
import fs from "fs";
import { TweetsGroupedByContent } from "../../../types.js";
import { formatInTimeZone } from "date-fns-tz";
import { reflectOnTweetGroups } from "../reflect-tweet-groups.js";
import { reGroupTweets } from "../re-group-tweets.js";

const tweetEvaluator: ls.SimpleEvaluator = () => {
  return {
    key: "tweet_generation",
    score: 1,
  };
};

function loadTweets(): TweetsGroupedByContent[] {
  const tweets = JSON.parse(
    fs.readFileSync(
      // "src/agents/curate-data/nodes/tweets/tests/data/grouped-by-llms/01-19-2025-13-36.json",
      // "src/agents/curate-data/nodes/tweets/tests/data/grouped-by-llms/01-19-2025-13-46.json",
      "src/agents/curate-data/nodes/tweets/tests/data/grouped-by-llms/01-20-2025-13-00.json",
      "utf-8",
    ),
  );
  return tweets;
}

async function saveTweets(tweets: TweetsGroupedByContent[]) {
  const currentDateUTC = new Date().toISOString();
  const formattedPSTDate = formatInTimeZone(
    currentDateUTC,
    "America/Los_Angeles",
    "MM-dd-yyyy-HH-mm",
  );
  await fs.promises.writeFile(
    `src/agents/curate-data/nodes/tweets/tests/data/grouped-by-llms/re-grouped/${formattedPSTDate}.json`,
    JSON.stringify(tweets),
  );
}

const tweetsGroupedByContent = loadTweets();

ls.describe("SMA - Curate Data - Reflect and Re-group", () => {
  ls.test(
    "Can reflect and re-group tweets",
    {
      inputs: { tweetsGroupedByContent },
      expected: {},
    },
    async ({ inputs }) => {
      console.log(
        "Starting test with",
        inputs.tweetsGroupedByContent.length,
        "tweet groups",
      );

      const reflectionResult = await reflectOnTweetGroups(inputs as any);

      if (!reflectionResult.similarGroupIndices?.length) {
        console.log("No groups found needing reflection");
        return reflectionResult;
      }
      console.log(
        "reflectionResult.similarGroupIndices\n------------\n",
        reflectionResult.similarGroupIndices,
        "\n------------\n",
      );

      const reGroupResult = await reGroupTweets({
        tweetsGroupedByContent: inputs.tweetsGroupedByContent,
        similarGroupIndices: reflectionResult.similarGroupIndices,
      } as any);

      console.log("reGroupResult\n------------\n");
      console.dir(reGroupResult, { depth: null });
      console.log("\n------------\n");

      await saveTweets(reGroupResult.tweetsGroupedByContent || []);

      await ls.expect(reGroupResult).evaluatedBy(tweetEvaluator).toBe(1);
      return reGroupResult;
    },
  );
});



================================================
FILE: src/agents/curate-data/nodes/tweets/tests/validate-bulk-tweets.int.test.ts
================================================
import fs from "fs";
import * as ls from "langsmith/jest";
import { validateBulkTweets } from "../../validate-bulk-tweets.js";
import { SimpleEvaluator } from "langsmith/jest";
import { formatInTimeZone } from "date-fns-tz";
import { TweetV2 } from "twitter-api-v2";

const tweetEvaluator: SimpleEvaluator = () => {
  return {
    key: "tweet_generation",
    score: 1,
  };
};

function loadTweets(): TweetV2[] {
  const tweets = JSON.parse(
    fs.readFileSync(
      // "src/agents/curate-data/nodes/tweets/tests/data/tweets.json",
      // "src/agents/curate-data/nodes/tweets/tests/data/tweets-2.json",
      "src/agents/curate-data/nodes/tweets/tests/data/tweets-3.json",
      "utf-8",
    ),
  );
  return tweets;
}

function saveRelevantTweets(tweets: TweetV2[]): void {
  try {
    const currentDateUTC = new Date().toISOString();
    const formattedPSTDate = formatInTimeZone(
      currentDateUTC,
      "America/Los_Angeles",
      "MM-dd-yyyy-HH-mm",
    );
    fs.writeFileSync(
      `src/agents/curate-data/nodes/tweets/tests/data/relevant-tweets/relevant-${formattedPSTDate}.json`,
      JSON.stringify(tweets),
    );
  } catch (e) {
    console.error("Failed to save relevant tweets:", e);
    console.log("Tweets:", tweets);
  }
}

const rawTweets = loadTweets();

ls.describe("SMA - Curate Data - Validate Bulk Tweets", () => {
  ls.test(
    "Can validate tweets",
    // You can pass an "iterations" parameter or other LS config here if desired
    {
      inputs: { rawTweets },
      expected: {},
    },
    async ({ inputs }) => {
      console.log("Starting test with", inputs.rawTweets.length, "tweets");
      const result = await validateBulkTweets(inputs as any);
      if (result.validatedTweets?.length === 0) {
        console.log("No tweets were found that are relevant to AI");
        return result;
      }

      saveRelevantTweets(result.validatedTweets || []);
      await ls.expect(result).evaluatedBy(tweetEvaluator).toBe(1);
      return result;
    },
  );
});



================================================
FILE: src/agents/curate-data/tests/e2e.int.test.ts
================================================
import { InMemoryStore, MemorySaver } from "@langchain/langgraph";
import * as ls from "langsmith/jest";
import { SimpleEvaluator } from "langsmith/jest";
import { curateDataGraph } from "../index.js";
import { Client } from "@langchain/langgraph-sdk";
import { NUM_POSTS_PER_SUBREDDIT } from "../constants.js";
import { CurateDataState } from "../state.js";

const e2eEvaluator: SimpleEvaluator = () => {
  return {
    key: "successful_run",
    score: 1,
  };
};

ls.describe("SMA - Curate Data - E2E", () => {
  ls.test.only(
    "Can run end-to-end for Reddit posts",
    {
      inputs: {},
      expected: {},
    },
    async () => {
      const client = new Client({
        apiUrl: `http://localhost:${process.env.PORT || "2024"}`,
      });

      console.log("Before invoking curateDataGraph");
      const { thread_id } = await client.threads.create();
      const result = await client.runs.wait(thread_id, "curate_data", {
        input: {},
        config: {
          configurable: {
            sources: ["reddit"],
            [NUM_POSTS_PER_SUBREDDIT]: 2,
          },
        },
      });
      console.log("After invoking curateDataGraph");
      console.log(
        "result.\nresult.rawRedditPosts.length:",
        (result as CurateDataState).rawRedditPosts.length,
      );

      const { threadRunIds } = result as CurateDataState;

      console.log("Waiting for all generate post results");
      const allGeneratePostResults = await Promise.allSettled(
        threadRunIds.map(async ({ thread_id, run_id }) => {
          const result = await client.runs.join(thread_id, run_id);
          console.log("Got generate post result:\n", result);
          return result;
        }),
      );

      await ls.expect(allGeneratePostResults).evaluatedBy(e2eEvaluator).toBe(1);
      return result;
    },
    240000, // 4 minutes
  );

  ls.test(
    "Can run end-to-end for GitHub repos",
    {
      inputs: {},
      expected: {},
    },
    async () => {
      const store = new InMemoryStore();
      const checkpointer = new MemorySaver();

      curateDataGraph.store = store;
      curateDataGraph.checkpointer = checkpointer;

      const result = await curateDataGraph.invoke(
        {},
        {
          configurable: {
            sources: ["github"],
            [NUM_POSTS_PER_SUBREDDIT]: undefined,
          },
        },
      );

      await ls.expect(result).evaluatedBy(e2eEvaluator).toBe(1);
      return result;
    },
    240000, // 4 minutes
  );

  ls.test(
    "Can run end-to-end for Twitter posts",
    {
      inputs: {},
      expected: {},
    },
    async () => {
      const store = new InMemoryStore();
      const checkpointer = new MemorySaver();

      curateDataGraph.store = store;
      curateDataGraph.checkpointer = checkpointer;

      const result = await curateDataGraph.invoke(
        {},
        {
          configurable: {
            sources: ["twitter"],
            [NUM_POSTS_PER_SUBREDDIT]: undefined,
          },
        },
      );

      await ls.expect(result).evaluatedBy(e2eEvaluator).toBe(1);
      return result;
    },
    240000, // 4 minutes
  );
});



================================================
FILE: src/agents/curate-data/utils/created-at-after.ts
================================================
/**
 * Returns true if the createdAt date is after a reference date
 * @param createdAt The createdAt date in ISO 8601 format
 * @param referenceDate The reference date to compare against
 * @returns true if the createdAt date is after a reference date, false otherwise
 */
export function createdAtAfter(createdAt: string, referenceDate: Date) {
  return new Date(createdAt).getTime() > referenceDate.getTime();
}



================================================
FILE: src/agents/curate-data/utils/get-unique-array.ts
================================================
export function getUniqueArrayItems<T>(existing: T[], toAdd: T[]): T[] {
  const existingSet = new Set(existing);
  return toAdd.filter((item) => !existingSet.has(item));
}



================================================
FILE: src/agents/curate-data/utils/stores/github-repos.ts
================================================
import { BaseStore } from "@langchain/langgraph";

export const NAMESPACE = ["saved_data", "github_repos"];
export const KEY = "urls";
export const OBJECT_KEY = "data";

export async function putGitHubRepoURLs(
  repoUrls: string[],
  store: BaseStore | undefined,
) {
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, KEY, {
    [OBJECT_KEY]: repoUrls,
  });
}

export async function getGitHubRepoURLs(
  store: BaseStore | undefined,
): Promise<string[]> {
  if (!store) {
    throw new Error("No store provided");
  }
  const repoUrls = await store.get(NAMESPACE, KEY);
  if (!repoUrls) {
    return [];
  }
  return repoUrls.value?.[OBJECT_KEY] || [];
}



================================================
FILE: src/agents/curate-data/utils/stores/latent-space-links.ts
================================================
import { BaseStore } from "@langchain/langgraph";

export const NAMESPACE = ["saved_data", "latent_space"];
export const KEY = "links";
export const OBJECT_KEY = "data";

export async function putLatentSpaceLinks(
  links: string[],
  store: BaseStore | undefined,
) {
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, KEY, {
    [OBJECT_KEY]: links,
  });
}

export async function getLatentSpaceLinks(
  store: BaseStore | undefined,
): Promise<string[]> {
  if (!store) {
    throw new Error("No store provided");
  }
  const links = await store.get(NAMESPACE, KEY);
  if (!links) {
    return [];
  }
  return links.value?.[OBJECT_KEY] || [];
}



================================================
FILE: src/agents/curate-data/utils/stores/reddit-post-ids.ts
================================================
import { BaseStore } from "@langchain/langgraph";

export const NAMESPACE = ["saved_data", "reddit"];
export const KEY = "post_ids";
export const OBJECT_KEY = "data";

export async function putRedditPostIds(
  postIds: string[],
  store: BaseStore | undefined,
) {
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, KEY, {
    [OBJECT_KEY]: postIds,
  });
}

export async function getRedditPostIds(
  store: BaseStore | undefined,
): Promise<string[]> {
  if (!store) {
    throw new Error("No store provided");
  }
  const savedPostIds = await store.get(NAMESPACE, KEY);
  if (!savedPostIds) {
    return [];
  }
  return savedPostIds.value?.[OBJECT_KEY] || [];
}



================================================
FILE: src/agents/curate-data/utils/stores/twitter.ts
================================================
import { BaseStore } from "@langchain/langgraph";

export const NAMESPACE = ["saved_data", "twitter"];
export const IDS_KEY = "ids";
export const IDS_OBJECT_KEY = "data";

export const LAST_INGESTED_ID_KEY = "last_ingested_id";
export const LAST_INGESTED_ID_OBJECT_KEY = "data";

export async function putTweetIds(
  tweetIds: string[],
  store: BaseStore | undefined,
) {
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, IDS_KEY, {
    [IDS_OBJECT_KEY]: tweetIds,
  });
}

export async function getTweetIds(
  store: BaseStore | undefined,
): Promise<string[]> {
  if (!store) {
    throw new Error("No store provided");
  }
  const tweetIds = await store.get(NAMESPACE, IDS_KEY);
  if (!tweetIds) {
    return [];
  }
  return tweetIds.value?.[IDS_OBJECT_KEY] || [];
}

export async function putLastIngestedTweetId(
  id: string,
  store: BaseStore | undefined,
) {
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, LAST_INGESTED_ID_KEY, {
    [LAST_INGESTED_ID_OBJECT_KEY]: id,
  });
}

export async function getLastIngestedTweetId(
  store: BaseStore | undefined,
): Promise<string> {
  if (!store) {
    throw new Error("No store provided");
  }
  const idData = await store.get(NAMESPACE, LAST_INGESTED_ID_KEY);
  if (!idData) {
    return "";
  }
  return idData.value?.[LAST_INGESTED_ID_OBJECT_KEY] || "";
}



================================================
FILE: src/agents/curated-post-interrupt/index.ts
================================================
import { END, START, StateGraph } from "@langchain/langgraph";
import {
  CuratedPostInterruptAnnotation,
  CuratedPostInterruptConfigurableAnnotation,
  CuratedPostInterruptState,
  CuratedPostInterruptUpdate,
} from "./types.js";
import { updateScheduledDate } from "../shared/nodes/update-scheduled-date.js";
import { humanNode } from "../shared/nodes/generate-post/human-node.js";
import { schedulePost } from "../shared/nodes/generate-post/schedule-post.js";
import { rewritePost } from "../shared/nodes/generate-post/rewrite-post.js";

function rewriteOrEndConditionalEdge(
  state: CuratedPostInterruptState,
):
  | "rewritePost"
  | "schedulePost"
  | "humanNode"
  | "updateScheduleDate"
  | typeof END {
  if (!state.next) {
    return END;
  }

  if (state.next === "unknownResponse") {
    // If the user's response is unknown, we should route back to the human node.
    return "humanNode";
  }
  return state.next;
}

const workflow = new StateGraph(
  CuratedPostInterruptAnnotation,
  CuratedPostInterruptConfigurableAnnotation,
)
  // Interrupts the node for human in the loop.
  .addNode(
    "humanNode",
    humanNode<CuratedPostInterruptState, CuratedPostInterruptUpdate>,
  )
  // Schedules the post for Twitter/LinkedIn.
  .addNode(
    "schedulePost",
    schedulePost<CuratedPostInterruptState, CuratedPostInterruptUpdate>,
  )
  // Rewrite a post based on the user's response.
  .addNode(
    "rewritePost",
    rewritePost<CuratedPostInterruptState, CuratedPostInterruptUpdate>,
  )
  // Updated the scheduled date from the natural language response from the user.
  .addNode("updateScheduleDate", updateScheduledDate)
  .addEdge(START, "humanNode")
  .addConditionalEdges("humanNode", rewriteOrEndConditionalEdge, [
    "rewritePost",
    "schedulePost",
    "updateScheduleDate",
    "humanNode",
    END,
  ])
  // Always route back to `humanNode` if the post was re-written or date was updated.
  .addEdge("rewritePost", "humanNode")
  .addEdge("updateScheduleDate", "humanNode")

  // Always end after scheduling the post.
  .addEdge("schedulePost", END);

export const curatedPostInterruptGraph = workflow.compile();
curatedPostInterruptGraph.name = "Curated Post Interrupt Graph";



================================================
FILE: src/agents/curated-post-interrupt/types.ts
================================================
import { Annotation, END } from "@langchain/langgraph";
import { IngestDataAnnotation } from "../ingest-data/ingest-data-state.js";
import { DateType } from "../types.js";
import { VerifyLinksResultAnnotation } from "../verify-links/verify-links-state.js";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  TEXT_ONLY_MODE,
} from "../generate-post/constants.js";
import { ComplexPost } from "../shared/nodes/generate-post/types.js";

export const CuratedPostInterruptAnnotation = Annotation.Root({
  /**
   * The links to use to generate a post.
   */
  links: Annotation<string[]>,
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  report: IngestDataAnnotation.spec.report,
  ...VerifyLinksResultAnnotation.spec,
  /**
   * The generated post for LinkedIn/Twitter.
   */
  post: Annotation<string>,
  /**
   * The complex post, if the user decides to split the URL from the main body.
   *
   * TODO: Refactor the post/complexPost state interfaces to use a single shared interface
   * which includes images too.
   * Tracking issue: https://github.com/langchain-ai/social-media-agent/issues/144
   */
  complexPost: Annotation<ComplexPost | undefined>,
  /**
   * The date to schedule the post for.
   */
  scheduleDate: Annotation<DateType>,
  /**
   * Response from the user for the post. Typically used to request
   * changes to be made to the post.
   */
  userResponse: Annotation<string | undefined>,
  /**
   * The node to execute next.
   */
  next: Annotation<
    | "schedulePost"
    | "rewritePost"
    | "unknownResponse"
    | "updateScheduleDate"
    | typeof END
    | undefined
  >,
  /**
   * The image to attach to the post, and the MIME type.
   */
  image: Annotation<
    | {
        imageUrl: string;
        mimeType: string;
      }
    | undefined
  >,
  /**
   * The number of times the post has been condensed. We should stop condensing after
   * 3 times to prevent an infinite loop.
   */
  condenseCount: Annotation<number>({
    reducer: (_state, update) => update,
    default: () => 0,
  }),
});

export type CuratedPostInterruptState =
  typeof CuratedPostInterruptAnnotation.State;
export type CuratedPostInterruptUpdate =
  typeof CuratedPostInterruptAnnotation.Update;

export const CuratedPostInterruptConfigurableAnnotation = Annotation.Root({
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
  /**
   * Whether or not to use text only mode throughout the graph.
   * If true, it will not try to extract, validate, or upload images.
   * Additionally, it will not be able to handle validating YouTube videos.
   * @default false
   */
  [TEXT_ONLY_MODE]: Annotation<boolean | undefined>({
    reducer: (_state, update) => update,
    default: () => false,
  }),
});



================================================
FILE: src/agents/find-images/find-images-graph.ts
================================================
import { Annotation, END, START, StateGraph } from "@langchain/langgraph";
import { findImages } from "./nodes/find-images.js";
import { validateImages } from "./nodes/validate-images.js";
import { reRankImages } from "./nodes/re-rank-images.js";
import { VerifyLinksResultAnnotation } from "../verify-links/verify-links-state.js";

export const FindImagesAnnotation = Annotation.Root({
  ...VerifyLinksResultAnnotation.spec,
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  report: Annotation<string>,
  /**
   * The generated post for LinkedIn/Twitter.
   */
  post: Annotation<string>,
});

function validateImagesOrEnd(state: typeof FindImagesAnnotation.State) {
  if (state.imageOptions?.length) {
    return "validateImages";
  }
  return END;
}

const findImagesWorkflow = new StateGraph(FindImagesAnnotation)
  .addNode("findImages", findImages)
  .addNode("validateImages", validateImages)
  .addNode("reRankImages", reRankImages)

  .addEdge(START, "findImages")

  .addConditionalEdges("findImages", validateImagesOrEnd, [
    "validateImages",
    END,
  ])

  .addEdge("validateImages", "reRankImages")

  .addEdge("reRankImages", END);

export const findImagesGraph = findImagesWorkflow.compile();
findImagesGraph.name = "Find Images Graph";



================================================
FILE: src/agents/find-images/screenshot.ts
================================================
import { fileTypeFromBuffer } from "file-type";
import type { BrowserContextOptions, PageScreenshotOptions } from "playwright";
import {
  getRepoContents,
  getFileContents,
} from "../../utils/github-repo-contents.js";
import { takeScreenshot } from "../../utils/screenshot.js";
import { createSupabaseClient } from "../../utils/supabase.js";
import {
  GITHUB_SCREENSHOT_OPTIONS,
  GITHUB_BROWSER_CONTEXT_OPTIONS,
} from "../generate-post/constants.js";
import { getUrlType } from "../utils.js";

/**
 * Take a screenshot of a URL and upload it to Supabase.
 * @param url The URL to take a screenshot of
 * @returns {Promise<string | undefined>} A public URL to the screenshot or undefined if the screenshot could not be taken
 */
export async function takeScreenshotAndUpload(
  url: string,
): Promise<string | undefined> {
  const screenshotUrl = await getUrlForScreenshot(url);
  const urlType = getUrlType(url);
  if (!screenshotUrl) {
    console.warn("No screenshot URL found for", url);
    return undefined;
  }

  const supabase = createSupabaseClient();

  let screenshotOptions: PageScreenshotOptions = {};
  let browserContextOptions: BrowserContextOptions = {};
  if (urlType === "github") {
    // We want to clip GitHub screenshots to only include the README contents.
    screenshotOptions = GITHUB_SCREENSHOT_OPTIONS;
    browserContextOptions = GITHUB_BROWSER_CONTEXT_OPTIONS;
  }

  try {
    const screenshotBuffer = await takeScreenshot(screenshotUrl, {
      screenshotOptions,
      browserContextOptions,
    });
    const urlHostName = new URL(screenshotUrl).hostname;

    // Detect the file type from the buffer
    const type = await fileTypeFromBuffer(screenshotBuffer);
    if (!type || !type.mime.startsWith("image/")) {
      throw new Error("Invalid image file");
    }

    const extension = type.mime.split("/")[1];
    const fileName = `screenshot-${urlHostName}-${Date.now()}.${extension}`;

    const { data, error } = await supabase.storage
      .from("images")
      .upload(fileName, screenshotBuffer, {
        contentType: type.mime,
        duplex: "half",
        upsert: false,
      });

    if (error) {
      console.error("Supabase upload error details:", {
        message: error.message,
        name: error.name,
        stack: error.stack,
      });
      throw error;
    }

    const {
      data: { publicUrl },
    } = supabase.storage.from("images").getPublicUrl(data.path);

    return publicUrl;
  } catch (error) {
    console.error("Error taking and uploading screenshot:", error);
    throw error;
  }
}

/**
 * Gets the URL for a screenshot given the base URL. Mainly used to either avoid
 * taking a screenshot of a YouTube video, or getting the proper URL for GitHub repos.
 * @param url The URL to take a screenshot of
 * @returns {Promise<string | undefined>} A public URL to use to take the screenshot or undefined if the URL is not supported
 */
async function getUrlForScreenshot(url: string): Promise<string | undefined> {
  const urlType = getUrlType(url);
  // Do not attempt to take a screenshot of YouTube URLs (should get thumbnail instead)
  // or undefined types as those are not supported
  if (!urlType || urlType === "youtube") return undefined;

  if (urlType === "github") {
    const repoContents = await getRepoContents(url);
    const readmePath = repoContents.find(
      (c) =>
        c.name.toLowerCase() === "readme.md" ||
        c.name.toLowerCase() === "readme",
    )?.path;
    // Fallback to root of repo if no README is found.
    if (!readmePath) {
      return url;
    }
    const readmeContents = await getFileContents(url, readmePath);
    // HTML URLs are the public human-readable URL.
    return readmeContents.html_url || url;
  }
  return url;
}



================================================
FILE: src/agents/find-images/nodes/find-images.ts
================================================
import * as path from "path";
import { getFileContents } from "../../../utils/github-repo-contents.js";
import {
  filterUnwantedImageUrls,
  extractAllImageUrlsFromMarkdown,
  isValidUrl,
  getUrlType,
} from "../../utils.js";
import { takeScreenshotAndUpload } from "../screenshot.js";
import { FindImagesAnnotation } from "../find-images-graph.js";
import { validate } from "uuid";

function checkIsGitHubImageUrl(url: string): boolean {
  if (
    url?.startsWith("https://github.com/user-attachments/assets") ||
    url?.includes("githubusercontent.com/")
  ) {
    return true;
  }
  try {
    const parsedUrl = new URL(url);
    const pathname = parsedUrl.pathname;
    const split = pathname.split("/");
    const lastEle = split[split.length - 1];
    const thirdToLastEle = split[split.length - 3];
    if (thirdToLastEle === "assets" && validate(lastEle)) {
      return true;
    }
    return false;
  } catch (_) {
    return false;
  }
}

export async function findImages(state: typeof FindImagesAnnotation.State) {
  const { pageContents, imageOptions, relevantLinks } = state;
  const link = relevantLinks?.[0] || undefined;
  if (!link || !relevantLinks?.length) {
    console.warn("No relevant links passed to findImages.");
    return {};
  }
  const imageUrls = new Set<string>();
  const gitHubSubLinks = relevantLinks.filter(
    (rl) => getUrlType(rl) === "github" && rl !== link,
  );

  let screenshotUrl: string | undefined;
  if (!["youtube", "twitter"].includes(getUrlType(link) || "")) {
    screenshotUrl = await takeScreenshotAndUpload(link);
    if (screenshotUrl) {
      imageUrls.add(screenshotUrl);
    }
  }

  // Take screenshots of all GitHub links (excluding parent link)
  if (gitHubSubLinks.length) {
    for await (const ghLink of gitHubSubLinks) {
      const ghScreenshotUrl = await takeScreenshotAndUpload(ghLink);
      if (ghScreenshotUrl) {
        imageUrls.add(ghScreenshotUrl);
      }
    }
  }

  if (imageOptions?.length) {
    imageOptions.forEach((urlOrPathname) => {
      imageUrls.add(urlOrPathname);
    });
  }

  if (pageContents && pageContents.length) {
    const allImageUrls = filterUnwantedImageUrls(
      pageContents.flatMap(extractAllImageUrlsFromMarkdown),
    );

    for await (const urlOrPathname of allImageUrls) {
      if (isValidUrl(urlOrPathname)) {
        if (getUrlType(urlOrPathname) !== "github") {
          imageUrls.add(urlOrPathname);
        } else {
          // If a full github URL. extract the file name from the path. to do this, extract the path after `blob/<branch>`
          const filePath = urlOrPathname.match(/blob\/[^/]+\/(.+)/)?.[1];
          if (!filePath) {
            if (!checkIsGitHubImageUrl(urlOrPathname)) {
              console.warn(
                "Could not extract file path from URL",
                urlOrPathname,
              );
            } else {
              imageUrls.add(urlOrPathname);
            }
            continue;
          }

          const getContents = await getFileContents(urlOrPathname, filePath);
          if (getContents.download_url) {
            imageUrls.add(getContents.download_url);
          }
        }

        continue;
      }

      // We have to assume the path is from the relevant link.
      const fullUrl = new URL(link);
      if (getUrlType(link) === "github") {
        const parsedPathname = path.normalize(urlOrPathname);
        const getContents = await getFileContents(link, parsedPathname);
        imageUrls.add(getContents.download_url || fullUrl.href);
      } else {
        fullUrl.pathname = path.join(fullUrl.pathname, urlOrPathname);
        imageUrls.add(fullUrl.href);
      }
    }
  } else {
    throw new Error("No page content or images found");
  }

  return {
    imageOptions: Array.from(imageUrls),
  };
}



================================================
FILE: src/agents/find-images/nodes/re-rank-images.ts
================================================
import { ChatVertexAI } from "@langchain/google-vertexai-web";
import { FindImagesAnnotation } from "../find-images-graph.js";
import { chunkArray } from "../../utils.js";
import { getImageMessageContents } from "../../../utils/image-message.js";

const RE_RANK_IMAGES_PROMPT = `You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for your company's LinkedIn and Twitter pages.

You're writing a post, and in doing so you've found a series of images that you think will help make the post more engaging.

Your task is to re-rank these images in order of which you think is the most engaging and best for the post.

Here is the marketing report the post was generated based on:
<report>
{REPORT}
</report>

And here's the actual post:
<post>
{POST}
</post>

Now, given this context, re-rank the images in order of most relevant to least relevant.

Provide your response in the following format:
1. <analysis> tag: Briefly explain your thought process for each image, referencing specific elements from the post and report and why each image is or isn't as relevant as others.
2. <reranked-indices> tag: List the indices of the relevant images in order of most relevant to least relevant, separated by commas.

Example: You're given 5 images, and deem that the relevancy order is [2, 0, 1, 4, 3], then you would respond as follows:
<answer>
<analysis>
- Image 2 is (explanation here)
- Image 0 is (explanation here)
- Image 1 is (explanation here)
- Image 4 is (explanation here)
- Image 3 is (explanation here)
</analysis>
<reranked-indices>
2, 0, 1, 4, 3
</reranked-indices>
</answer>

Ensure you ALWAYS WRAP your analysis and relevant indices inside the <analysis> and <reranked-indices> tags, respectively. Do not only prefix, but ensure they are wrapped completely.

Provide your complete response within <answer> tags.`;

export function parseResult(result: string): number[] {
  const match = result.match(
    /<reranked-indices>\s*([\d,\s]*?)\s*<\/reranked-indices>/s,
  );
  if (!match) return [];

  return match[1]
    .split(",")
    .map((s) => s.trim())
    .filter((s) => s.length > 0)
    .map(Number)
    .filter((n) => !isNaN(n));
}

export async function reRankImages(state: typeof FindImagesAnnotation.State) {
  // No need to re-rank if less than 2 images
  if (state.imageOptions && state.imageOptions.length < 2) {
    return {
      imageOptions: state.imageOptions,
    };
  }

  const model = new ChatVertexAI({
    model: "gemini-2.0-flash",
    temperature: 0,
  });

  // Split images into chunks of 5
  const imageChunks = chunkArray(state.imageOptions || [], 5);
  let reRankedIndices: number[] = [];
  let baseIndex = 0;

  const formattedSystemPrompt = RE_RANK_IMAGES_PROMPT.replace(
    "{POST}",
    state.post,
  ).replace("{REPORT}", state.report);

  // Process each chunk
  for (const imageChunk of imageChunks) {
    const imageMessages = await getImageMessageContents(imageChunk, baseIndex);

    if (!imageMessages.length) {
      continue;
    }

    try {
      const response = await model.invoke([
        {
          role: "system",
          content: formattedSystemPrompt,
        },
        {
          role: "user",
          content: imageMessages,
        },
      ]);

      const chunkAnalysis = parseResult(response.content as string);
      // Convert chunk indices to global indices and add to our list of re-ranked indices
      const globalIndices = chunkAnalysis.map((index) => index + baseIndex);
      reRankedIndices = [...reRankedIndices, ...globalIndices];
    } catch (error) {
      console.error(
        `Failed to re-rank images.\nImage URLs: ${imageMessages
          .filter((m) => m.fileUri)
          .map((m) => m.fileUri)
          .join(", ")}\n\nError:`,
        error,
      );
      // Add all indices from the failed chunk to allIrrelevantIndices
      const failedChunkIndices = Array.from(
        { length: imageChunk.length },
        (_, i) => i + baseIndex,
      );
      reRankedIndices = [...reRankedIndices, ...failedChunkIndices];
    }

    baseIndex += imageChunk.length;
  }

  if (reRankedIndices.length !== state.imageOptions?.length) {
    console.warn(
      "Re-ranked indices length does not match image options length. Returning original image options.",
    );
    return {
      imageOptions: state.imageOptions,
    };
  }

  const imageOptionsInOrder = reRankedIndices.map(
    (index) => state.imageOptions?.[index],
  );

  return {
    imageOptions: imageOptionsInOrder,
  };
}



================================================
FILE: src/agents/find-images/nodes/validate-images.ts
================================================
import { ChatVertexAI } from "@langchain/google-vertexai-web";
import { FindImagesAnnotation } from "../find-images-graph.js";
import { chunkArray, imageUrlToBuffer, isValidUrl } from "../../utils.js";
import { getImageMessageContents } from "../../../utils/image-message.js";

const VALIDATE_IMAGES_PROMPT = `You are an advanced AI assistant tasked with validating image options for a social media post.
Your goal is to identify which images from a given set are relevant to the post, based on the content of the post and an associated marketing report.

First, carefully read and analyze the following social media post:

<post>
{POST}
</post>

Now, review the marketing report that was used to generate this post:

<report>
{REPORT}
</report>

To determine which images are relevant, consider the following criteria:
1. Does the image directly illustrate a key point or theme from the post?
2. Does the image represent any products, services, or concepts mentioned in either the post or the report?

You should NEVER include images which are:
- Logos, icons, or profile pictures (unless it is a LangChain/LangGraph/LangSmith logo).
- Personal, or non-essential images from a business perspective.
- Small, low-resolution images. These are likely accidentally included in the post and should be excluded.

You will be presented with a list of image options. Your task is to identify which of these images are relevant to the post based on the criteria above.

Provide your response in the following format:
1. <analysis> tag: Briefly explain your thought process for each image, referencing specific elements from the post and report.
2. <relevant_indices> tag: List the indices of the relevant images, starting from 0, separated by commas.

Ensure you ALWAYS WRAP your analysis and relevant indices inside the <analysis> and <relevant_indices> tags, respectively. Do not only prefix, but ensure they are wrapped completely.

Remember to carefully consider each image in relation to both the post content and the marketing report.
Be thorough in your analysis, but focus on the most important factors that determine relevance.
If an image is borderline, err on the side of inclusion.

Provide your complete response within <answer> tags.
`;

export function parseResult(result: string): number[] {
  const match = result.match(
    /<relevant_indices>\s*([\d,\s]*?)\s*<\/relevant_indices>/s,
  );
  if (!match) return [];

  return match[1]
    .split(",")
    .map((s) => s.trim())
    .filter((s) => s.length > 0)
    .map(Number)
    .filter((n) => !isNaN(n));
}

const YOUTUBE_THUMBNAIL_URL = "https://i.ytimg.com/";

function removeProtectedUrls(imageOptions: string[]): string[] {
  return imageOptions.filter(
    (fileUri) =>
      (!process.env.SUPABASE_URL ||
        !fileUri.startsWith(process.env.SUPABASE_URL)) &&
      !fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );
}

function getProtectedUrls(imageOptions: string[]): string[] {
  return imageOptions.filter(
    (fileUri) =>
      (process.env.SUPABASE_URL &&
        fileUri.startsWith(process.env.SUPABASE_URL)) ||
      fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );
}

async function filterImageUrls(imageOptions: string[]): Promise<{
  imageOptions: string[];
  returnEarly: boolean;
}> {
  const imagesWithoutProtected = imageOptions?.length
    ? removeProtectedUrls(imageOptions)
    : [];

  if (!imagesWithoutProtected?.length) {
    return {
      imageOptions,
      returnEarly: true,
    };
  }

  const validImageUrlPromises = imagesWithoutProtected.filter(
    async (imgUrl) => {
      if (!isValidUrl(imgUrl)) return false;

      try {
        // Use this as a way to validate the image exists
        const { contentType } = await imageUrlToBuffer(imgUrl);
        if (contentType.startsWith("image/")) {
          return true;
        }
      } catch (_) {
        // no-op
      }
      return false;
    },
  );

  const validImageUrls = await Promise.all(validImageUrlPromises);
  if (!validImageUrls.length) {
    const protectedImageUrls = imageOptions?.length
      ? getProtectedUrls(imageOptions)
      : [];
    return {
      imageOptions: [...protectedImageUrls],
      returnEarly: true,
    };
  }

  return {
    imageOptions: validImageUrls,
    returnEarly: false,
  };
}

export async function validateImages(
  state: typeof FindImagesAnnotation.State,
): Promise<{
  imageOptions: string[] | undefined;
}> {
  const { imageOptions, report, post } = state;

  const model = new ChatVertexAI({
    model: "gemini-2.0-flash",
    temperature: 0,
  });

  const { imageOptions: imagesWithoutProtected, returnEarly } =
    await filterImageUrls(imageOptions ?? []);

  if (returnEarly || !imagesWithoutProtected?.length) {
    return {
      imageOptions: imagesWithoutProtected,
    };
  }

  // Split images into chunks of 10
  const imageChunks = chunkArray(imagesWithoutProtected, 10);
  let allIrrelevantIndices: number[] = [];
  let baseIndex = 0;

  const formattedSystemPrompt = VALIDATE_IMAGES_PROMPT.replace(
    "{POST}",
    post,
  ).replace("{REPORT}", report);

  // Process each chunk
  for (const imageChunk of imageChunks) {
    const imageMessages = await getImageMessageContents(imageChunk, baseIndex);

    if (!imageMessages.length) {
      continue;
    }

    try {
      const response = await model.invoke([
        {
          role: "system",
          content: formattedSystemPrompt,
        },
        {
          role: "user",
          content: imageMessages,
        },
      ]);

      const chunkAnalysis = parseResult(response.content as string);
      // Convert chunk indices to global indices and add to our list of relevant indices
      const globalIndices = chunkAnalysis.map((index) => index + baseIndex);
      allIrrelevantIndices = [...allIrrelevantIndices, ...globalIndices];
    } catch (error) {
      console.error(
        `Failed to validate images.\nImage URLs: ${imageMessages
          .filter((m) => m.fileUri)
          .map((m) => m.fileUri)
          .join(", ")}\n\nError:`,
        error,
      );
      // Add all indices from the failed chunk to allIrrelevantIndices
      const failedChunkIndices = Array.from(
        { length: imageChunk.length },
        (_, i) => i + baseIndex,
      );
      allIrrelevantIndices = [...allIrrelevantIndices, ...failedChunkIndices];
    }

    baseIndex += imageChunk.length;
  }

  const protectedUrls = imageOptions?.filter(
    (fileUri) =>
      (process.env.SUPABASE_URL &&
        fileUri.startsWith(process.env.SUPABASE_URL)) ||
      fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );

  // Keep only the relevant images (those whose indices are in allIrrelevantIndices)
  return {
    imageOptions: [
      ...(protectedUrls || []),
      ...(imagesWithoutProtected || []).filter((_, index) =>
        allIrrelevantIndices.includes(index),
      ),
    ],
  };
}



================================================
FILE: src/agents/generate-post/constants.ts
================================================
import type { BrowserContextOptions, PageScreenshotOptions } from "playwright";

export const ALLOWED_DAYS = [
  "Monday",
  "Tuesday",
  "Wednesday",
  "Thursday",
  "Friday",
  "Saturday",
  "Sunday",
];

export const ALLOWED_TIMES = [
  "8:00 AM",
  "8:10 AM",
  "8:20 AM",
  "8:30 AM",
  "8:40 AM",
  "8:50 AM",
  "9:00 AM",
  "9:10 AM",
  "9:20 AM",
  "9:30 AM",
  "9:40 AM",
  "9:50 AM",
  "10:00 AM",
  "10:10 AM",
  "10:20 AM",
  "10:30 AM",
  "10:40 AM",
  "10:50 AM",
  "11:00 AM",
  "11:10 AM",
  "11:20 AM",
  "11:30 AM",
  "11:40 AM",
  "11:50 AM",
  "12:00 PM",
  "12:10 PM",
  "12:20 PM",
  "12:30 PM",
  "12:40 PM",
  "12:50 PM",
  "1:00 PM",
  "1:10 PM",
  "1:20 PM",
  "1:30 PM",
  "1:40 PM",
  "1:50 PM",
  "2:00 PM",
  "2:10 PM",
  "2:20 PM",
  "2:30 PM",
  "2:40 PM",
  "2:50 PM",
  "3:00 PM",
  "3:10 PM",
  "3:20 PM",
  "3:30 PM",
  "3:40 PM",
  "3:50 PM",
  "4:00 PM",
  "4:10 PM",
  "4:20 PM",
  "4:30 PM",
  "4:40 PM",
  "4:50 PM",
  "5:00 PM",
];

export const GITHUB_SCREENSHOT_OPTIONS: PageScreenshotOptions = {
  clip: {
    width: 1200,
    height: 1500,
    x: 525,
    y: 350,
  },
};
export const GITHUB_BROWSER_CONTEXT_OPTIONS: BrowserContextOptions = {
  viewport: {
    width: 1920,
    height: 1500,
  },
};

// Configurable keys
// LinkedIn
export const LINKEDIN_PERSON_URN = "linkedInPersonUrn";
export const LINKEDIN_ORGANIZATION_ID = "linkedInOrganizationId";
export const LINKEDIN_ACCESS_TOKEN = "linkedInAccessToken";
export const POST_TO_LINKEDIN_ORGANIZATION = "postToLinkedInOrganization";
export const LINKEDIN_USER_ID = "linkedInUserId";
// Twitter
export const TWITTER_USER_ID = "twitterUserId";
export const TWITTER_TOKEN = "twitterToken";
export const TWITTER_TOKEN_SECRET = "twitterTokenSecret";
export const INGEST_TWITTER_USERNAME = "ingestTwitterUsername";
// Simplified text only mode
export const TEXT_ONLY_MODE = "textOnlyMode";

export const SKIP_CONTENT_RELEVANCY_CHECK = "skipContentRelevancyCheck";

export const SKIP_USED_URLS_CHECK = "skipUsedUrlsCheck";



================================================
FILE: src/agents/generate-post/generate-post-graph.ts
================================================
import {
  END,
  LangGraphRunnableConfig,
  START,
  StateGraph,
} from "@langchain/langgraph";
import {
  GeneratePostAnnotation,
  GeneratePostConfigurableAnnotation,
  GeneratePostInputAnnotation,
  GeneratePostState,
  GeneratePostUpdate,
} from "./generate-post-state.js";
import { generateContentReport } from "./nodes/generate-report/index.js";
import { generatePost } from "./nodes/generate-post/index.js";
import { condensePost } from "./nodes/condense-post.js";
import { isTextOnly, removeUrls, shouldPostToLinkedInOrg } from "../utils.js";
import { verifyLinksGraph } from "../verify-links/verify-links-graph.js";
import { authSocialsPassthrough } from "./nodes/auth-socials.js";
import { findImagesGraph } from "../find-images/find-images-graph.js";
import { updateScheduledDate } from "../shared/nodes/update-scheduled-date.js";
import { getSavedUrls } from "../shared/stores/post-subject-urls.js";
import { humanNode } from "../shared/nodes/generate-post/human-node.js";
import { schedulePost } from "../shared/nodes/generate-post/schedule-post.js";
import { rewritePost } from "../shared/nodes/generate-post/rewrite-post.js";
import { Client } from "@langchain/langgraph-sdk";
import { POST_TO_LINKEDIN_ORGANIZATION } from "./constants.js";
import { rewritePostWithSplitUrl } from "./nodes/rewrite-with-split-url.js";

function routeAfterGeneratingReport(
  state: GeneratePostState,
): "generatePost" | typeof END {
  if (state.report) {
    return "generatePost";
  }
  return END;
}

function rewriteOrEndConditionalEdge(
  state: GeneratePostState,
):
  | "rewritePost"
  | "schedulePost"
  | "updateScheduleDate"
  | "humanNode"
  | "rewriteWithSplitUrl"
  | typeof END {
  if (state.next) {
    if (state.next === "unknownResponse") {
      // If the user's response is unknown, we should route back to the human node.
      return "humanNode";
    }
    return state.next;
  }
  return END;
}

async function condenseOrHumanConditionalEdge(
  state: GeneratePostState,
  config: LangGraphRunnableConfig,
): Promise<"condensePost" | "humanNode" | "findImagesSubGraph" | typeof END> {
  const cleanedPost = removeUrls(state.post || "");
  if (cleanedPost.length > 280 && state.condenseCount <= 3) {
    return "condensePost";
  }

  const isTextOnlyMode = isTextOnly(config);
  if (isTextOnlyMode) {
    return routeToCuratedInterruptOrContinue(state, config);
  }
  return "findImagesSubGraph";
}

/**
 * Checks if any of the URLs in the array are already stored in the post subject URLs store.
 * If so, it returns `true` and the post should NOT be generated.
 * @param urls The array of URLs to check. These are all of the input & extracted relevant URLs.
 * @param config The LangGraph config to get the store from.
 * @returns {Promise<boolean>} `true` if any of the URLs in the array are already stored in the post subject URLs store. `false` otherwise.
 */
async function checkIfUrlsArePreviouslyUsed(
  urls: string[],
  config: LangGraphRunnableConfig,
) {
  const existingUrls = await getSavedUrls(config);
  return urls.some((url) =>
    existingUrls.some((existingUrl) => url === existingUrl),
  );
}

async function generateReportOrEndConditionalEdge(
  state: GeneratePostState,
  config: LangGraphRunnableConfig,
): Promise<"generateContentReport" | typeof END> {
  const urlsAlreadyUsed = await checkIfUrlsArePreviouslyUsed(
    [...(state.relevantLinks ?? []), ...state.links],
    config,
  );

  // End early if the URLs have already been used, or if there are no
  // page contents extracted from any of the URLs.
  if (urlsAlreadyUsed || !state.pageContents?.length) {
    return END;
  }

  return "generateContentReport";
}

/**
 * If the 'origin' is set to 'curate-data' we need to route to a new graph 'curated_data_interrupt'
 * for the interrupt, so that users can separate curated posts from posts ingested from Slack in the
 * Agent Inbox.
 */
async function routeToCuratedInterruptOrContinue(
  state: GeneratePostState,
  config: LangGraphRunnableConfig,
): Promise<"humanNode" | typeof END> {
  if (config.configurable?.origin === "curate-data") {
    const postToLinkedInOrg = shouldPostToLinkedInOrg(config);
    const client = new Client({
      apiUrl: `http://localhost:${process.env.PORT}`,
    });

    const { thread_id } = await client.threads.create();
    await client.runs.create(thread_id, "curated_post_interrupt", {
      input: state,
      config: {
        configurable: {
          [POST_TO_LINKEDIN_ORGANIZATION]: postToLinkedInOrg,
        },
      },
    });

    return END;
  }

  return "humanNode";
}

const generatePostBuilder = new StateGraph(
  { stateSchema: GeneratePostAnnotation, input: GeneratePostInputAnnotation },
  GeneratePostConfigurableAnnotation,
)
  .addNode("authSocialsPassthrough", authSocialsPassthrough)

  .addNode("verifyLinksSubGraph", verifyLinksGraph)

  // Generates a Tweet/LinkedIn post based on the report content.
  .addNode("generatePost", generatePost)
  // Attempt to condense the post if it's too long.
  .addNode("condensePost", condensePost)
  // Interrupts the node for human in the loop.
  .addNode("humanNode", humanNode<GeneratePostState, GeneratePostUpdate>)
  // Schedules the post for Twitter/LinkedIn.
  .addNode("schedulePost", schedulePost<GeneratePostState, GeneratePostUpdate>)
  // Rewrite a post based on the user's response.
  .addNode("rewritePost", rewritePost<GeneratePostState, GeneratePostUpdate>)
  // Generates a report on the content.
  .addNode("generateContentReport", generateContentReport)
  // Finds images in the content.
  .addNode("findImagesSubGraph", findImagesGraph)
  // Updated the scheduled date from the natural language response from the user.
  .addNode("updateScheduleDate", updateScheduledDate)
  // Rewrite the post splitting the URL from the main body of the tweet
  .addNode("rewriteWithSplitUrl", rewritePostWithSplitUrl)

  // Start node
  .addEdge(START, "authSocialsPassthrough")
  .addEdge("authSocialsPassthrough", "verifyLinksSubGraph")

  // After verifying the different content types, we should generate a report on them.
  .addConditionalEdges(
    "verifyLinksSubGraph",
    generateReportOrEndConditionalEdge,
    ["generateContentReport", END],
  )

  // Once generating a report, we should confirm the report exists (meaning the content is relevant).
  .addConditionalEdges("generateContentReport", routeAfterGeneratingReport, [
    "generatePost",
    END,
  ])

  // After generating the post for the first time, check if it's too long,
  // and if so, condense it. Otherwise, route to the human node.
  .addConditionalEdges("generatePost", condenseOrHumanConditionalEdge, [
    "condensePost",
    "findImagesSubGraph",
    "humanNode",
    END,
  ])
  // After condensing the post, we should verify again that the content is below the character limit.
  // Once the post is below the character limit, we can find & filter images. This needs to happen after the post
  // has been generated because the image validator requires the post content.
  .addConditionalEdges("condensePost", condenseOrHumanConditionalEdge, [
    "condensePost",
    "findImagesSubGraph",
    "humanNode",
    END,
  ])

  // After finding images, we are done and can interrupt for the human to respond.
  .addConditionalEdges(
    "findImagesSubGraph",
    routeToCuratedInterruptOrContinue,
    ["humanNode", END],
  )

  // Always route back to `humanNode` if the post was re-written or date was updated.
  .addEdge("rewritePost", "humanNode")
  .addEdge("updateScheduleDate", "humanNode")
  .addEdge("rewriteWithSplitUrl", "humanNode")

  // If the schedule post is successful, end the graph.
  .addConditionalEdges("humanNode", rewriteOrEndConditionalEdge, [
    "rewritePost",
    "schedulePost",
    "updateScheduleDate",
    "humanNode",
    "rewriteWithSplitUrl",
    END,
  ])
  // Always end after scheduling the post.
  .addEdge("schedulePost", END);

export const generatePostGraph = generatePostBuilder.compile();

generatePostGraph.name = "Generate Post Subgraph";



================================================
FILE: src/agents/generate-post/generate-post-state.ts
================================================
import { Annotation, END } from "@langchain/langgraph";
import { IngestDataAnnotation } from "../ingest-data/ingest-data-state.js";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
  TEXT_ONLY_MODE,
} from "./constants.js";
import { DateType } from "../types.js";
import { VerifyLinksResultAnnotation } from "../verify-links/verify-links-state.js";
import { ComplexPost } from "../shared/nodes/generate-post/types.js";

export type LangChainProduct = "langchain" | "langgraph" | "langsmith";

export type YouTubeVideoSummary = {
  /**
   * The link to the YouTube video the summary is for.
   */
  link: string;
  /**
   * The summary of the video.
   */
  summary: string;
};

export const GeneratePostAnnotation = Annotation.Root({
  /**
   * The links to use to generate a post.
   */
  links: Annotation<string[]>,
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  report: IngestDataAnnotation.spec.report,
  ...VerifyLinksResultAnnotation.spec,
  /**
   * The generated post for LinkedIn/Twitter.
   */
  post: Annotation<string>,
  /**
   * The complex post, if the user decides to split the URL from the main body.
   *
   * TODO: Refactor the post/complexPost state interfaces to use a single shared interface
   * which includes images too.
   * Tracking issue: https://github.com/langchain-ai/social-media-agent/issues/144
   */
  complexPost: Annotation<ComplexPost | undefined>,
  /**
   * The date to schedule the post for.
   */
  scheduleDate: Annotation<DateType>,
  /**
   * Response from the user for the post. Typically used to request
   * changes to be made to the post.
   */
  userResponse: Annotation<string | undefined>,
  /**
   * The node to execute next.
   */
  next: Annotation<
    | "schedulePost"
    | "rewritePost"
    | "updateScheduleDate"
    | "unknownResponse"
    | "rewriteWithSplitUrl"
    | typeof END
    | undefined
  >,
  /**
   * The image to attach to the post, and the MIME type.
   */
  image: Annotation<
    | {
        imageUrl: string;
        mimeType: string;
      }
    | undefined
  >,
  /**
   * The number of times the post has been condensed. We should stop condensing after
   * 3 times to prevent an infinite loop.
   */
  condenseCount: Annotation<number>({
    reducer: (_state, update) => update,
    default: () => 0,
  }),
});

export type GeneratePostState = typeof GeneratePostAnnotation.State;
export type GeneratePostUpdate = typeof GeneratePostAnnotation.Update;

export const GeneratePostInputAnnotation = Annotation.Root({
  /**
   * The links to use to generate a post.
   */
  links: Annotation<string[]>,
});

export const GeneratePostConfigurableAnnotation = Annotation.Root({
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
  /**
   * Whether or not to use text only mode throughout the graph.
   * If true, it will not try to extract, validate, or upload images.
   * Additionally, it will not be able to handle validating YouTube videos.
   * @default false
   */
  [TEXT_ONLY_MODE]: Annotation<boolean | undefined>({
    reducer: (_state, update) => update,
    default: () => false,
  }),
  /**
   * The original graph that started the "generate-post" graph
   * run. Undefined if the graph was started directly.
   */
  origin: Annotation<string | undefined>,
  /**
   * Whether or not to skip the content relevancy check.
   */
  [SKIP_CONTENT_RELEVANCY_CHECK]: Annotation<boolean | undefined>(),
  /**
   * Whether or not to skip the used URLs check. This will also
   * skip saving the URLs in the store.
   */
  [SKIP_USED_URLS_CHECK]: Annotation<boolean | undefined>(),
});

export const BASE_GENERATE_POST_CONFIG: typeof GeneratePostConfigurableAnnotation.State =
  {
    [POST_TO_LINKEDIN_ORGANIZATION]: undefined,
    [TEXT_ONLY_MODE]: false,
    origin: undefined,
    [SKIP_CONTENT_RELEVANCY_CHECK]: undefined,
    [SKIP_USED_URLS_CHECK]: undefined,
  };



================================================
FILE: src/agents/generate-post/nodes/auth-socials.ts
================================================
import { interrupt, LangGraphRunnableConfig } from "@langchain/langgraph";
import { GeneratePostAnnotation } from "../generate-post-state.js";
import { getLinkedInAuthOrInterrupt } from "../../shared/auth/linkedin.js";
import { getTwitterAuthOrInterrupt } from "../../shared/auth/twitter.js";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import { shouldPostToLinkedInOrg } from "../../utils.js";

export async function authSocialsPassthrough(
  _state: typeof GeneratePostAnnotation.State,
  config: LangGraphRunnableConfig,
) {
  let linkedInHumanInterrupt: HumanInterrupt | undefined = undefined;
  const linkedInUserId = process.env.LINKEDIN_USER_ID;
  if (linkedInUserId) {
    const postToLinkedInOrg = shouldPostToLinkedInOrg(config);
    linkedInHumanInterrupt = await getLinkedInAuthOrInterrupt({
      linkedInUserId,
      returnInterrupt: true,
      postToOrg: postToLinkedInOrg,
    });
  }

  let twitterHumanInterrupt: HumanInterrupt | undefined = undefined;
  const twitterUserId = process.env.TWITTER_USER_ID;
  if (twitterUserId) {
    twitterHumanInterrupt = await getTwitterAuthOrInterrupt({
      twitterUserId,
      returnInterrupt: true,
    });
  }

  if (!twitterHumanInterrupt && !linkedInHumanInterrupt) {
    // Use has already authorized. Return early
    return {};
  }

  const combinedArgs = {
    ...twitterHumanInterrupt?.action_request.args,
    ...linkedInHumanInterrupt?.action_request.args,
  };

  const description = `# Authorization Required

Please visit the following URL(s) to authorize your social media accounts:

${combinedArgs.authorizeTwitterURL ? `Twitter: ${combinedArgs.authorizeTwitterURL}` : ""}
${combinedArgs.authorizeLinkedInURL ? `LinkedIn: ${combinedArgs.authorizeLinkedInURL}` : ""}
${combinedArgs.authorizationDocs ? `LinkedIn Authorization Docs: ${combinedArgs.authorizationDocs}` : ""}

Once done, please 'accept' this interrupt event.`;

  const interruptEvent: HumanInterrupt = {
    description,
    action_request: {
      action: "Authorize Social Media Accounts",
      args: combinedArgs,
    },
    config: {
      allow_accept: true,
      allow_ignore: true,
      allow_respond: false,
      allow_edit: false,
    },
  };

  const interruptRes = interrupt<HumanInterrupt[], HumanResponse[]>([
    interruptEvent,
  ])[0];

  if (interruptRes.type === "ignore") {
    // Throw an error to end the graph.
    throw new Error("Authorization denied by user.");
  }

  return {};
}



================================================
FILE: src/agents/generate-post/nodes/condense-post.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { GeneratePostAnnotation } from "../generate-post-state.js";
import { parseGeneration } from "./generate-post/utils.js";
import { filterLinksForPostContent, removeUrls } from "../../utils.js";
import {
  REFLECTIONS_PROMPT,
  getReflectionsPrompt,
} from "../../../utils/reflections.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { getPrompts } from "../prompts/index.js";

const CONDENSE_POST_PROMPT = `You're a highly skilled marketer at LangChain, working on crafting thoughtful and engaging content for LangChain's LinkedIn and Twitter pages.
You wrote a post for the LangChain LinkedIn and Twitter pages, however it's a bit too long for Twitter, and thus needs to be condensed.

You wrote this marketing report on the content which you used to write the original post:
<report>
{report}
</report>

Here are the relevant links used to create the report. At least ONE should be included in the condensed post.
The links do NOT contribute to the post's length. They are temporarily removed from the post before the length is calculated, and re-added afterwards.
<links>
{links}
</links>

You should not be worried by the length of the link, as that will be shortened before posting. Only focus on condensing the length of the post content itself.

Here are the rules and structure you used to write the original post, which you should use when condensing the post now:
<rules-and-structure>

${getPrompts().postStructureInstructions}

<rules>
${getPrompts().postContentRules}
</rules>

{reflectionsPrompt}

</rules-and-structure>

Given the marketing report, link, rules and structure, please condense the post down to roughly 280 characters (not including the link). The original post was {originalPostLength} characters long.
Ensure you keep the same structure, and do not omit any crucial content outright.

Follow this flow to rewrite the post in a condensed format:

<rewriting-flow>
1. Carefully read over the report, original post provided by the user below, the rules and structure.
2. Write down your thoughts about where and how you can condense the post inside <thinking> tags. This should contain details you think will help make the post more engaging, snippets you think can be condensed, etc. This should be the first text you write.
3. Using all the context provided to you above, the original post, and your thoughts, rewrite the post in a condensed format inside <post> tags. This should be the last text you write.
</rewriting-flow>

Follow all rules and instructions outlined above. The user message below will provide the original post. Remember to have fun while rewriting it! Go!`;

/**
 * Attempts to condense a post if the original generation is longer than 300 characters.
 * @param state The state of the graph
 * @returns A partial state of the graph
 */
export async function condensePost(
  state: typeof GeneratePostAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof GeneratePostAnnotation.State>> {
  if (!state.post) {
    throw new Error("No post found");
  }
  if (!state.report) {
    throw new Error("No report found");
  }
  if (!state.relevantLinks?.length) {
    throw new Error("No relevant links found");
  }

  const originalPostLength = removeUrls(state.post || "").length.toString();

  const reflections = await getReflectionsPrompt(config);
  const reflectionsPrompt = REFLECTIONS_PROMPT.replace(
    "{reflections}",
    reflections,
  );

  const formattedSystemPrompt = CONDENSE_POST_PROMPT.replace(
    "{report}",
    state.report,
  )
    .replace("{links}", filterLinksForPostContent(state.relevantLinks))
    .replace("{originalPostLength}", originalPostLength)
    .replace("{reflectionsPrompt}", reflectionsPrompt);

  const condensePostModel = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0.5,
  });

  const userMessageContent = `Here is the original post:\n\n${state.post}`;

  const condensePostResponse = await condensePostModel.invoke([
    {
      role: "system",
      content: formattedSystemPrompt,
    },
    {
      role: "user",
      content: userMessageContent,
    },
  ]);

  return {
    post: parseGeneration(condensePostResponse.content as string),
    condenseCount: state.condenseCount + 1,
  };
}



================================================
FILE: src/agents/generate-post/nodes/rewrite-with-split-url.ts
================================================
import { z } from "zod";
import {
  GeneratePostState,
  GeneratePostUpdate,
} from "../generate-post-state.js";
import { ChatAnthropic } from "@langchain/anthropic";

const postSchema = z.object({
  main_post: z
    .string()
    .describe(
      "The main content of the post. Should NOT include the URL, but it should include a concise callout indicating the URL is in the reply.",
    ),
  reply_post: z
    .string()
    .describe(
      "The reply to the Tweet. This should contain a very concise callout (e.g. 'Check out the repo here:'), and the URL.",
    ),
});

const REWRITE_WITH_SPLIT_URL_PROMPT = `You're an advanced AI marketer who has been tasked with splitting a social media post into two unique posts:
1. The first is the main body of the post. You should NOT make any changes to the input, EXCEPT for replacing the callout URL at the bottom, with a message indicating the URL is in the reply.
You may include an emoji to indicate the URL is in the reply.
Example:
"Repo link in reply ðŸ‘‡"
or
"Video link in reply ðŸ§µ"
2. The second is the reply post. This should contain a very concise callout (e.g. 'Check out the repo here:'), and the URL.
Example:
"Checkout the repo here: https://github.com/bracesproul/langchain-ai"

Given the following post:
{POST}

Please split it into the two unique posts. Ensure the ONLY modification you make is to the callout & URL at the end of the post.`;

export async function rewritePostWithSplitUrl(
  state: GeneratePostState,
): Promise<GeneratePostUpdate> {
  const postModel = new ChatAnthropic({
    model: "claude-3-7-sonnet-latest",
    temperature: 0,
  }).bindTools(
    [
      {
        name: "rewrite_post",
        description:
          "Rewrite the post with the split URL from the main post content to the reply",
        schema: postSchema,
      },
    ],
    {
      tool_choice: "rewrite_post",
    },
  );

  const formattedPrompt = REWRITE_WITH_SPLIT_URL_PROMPT.replace(
    "{POST}",
    state.post || "",
  );

  const result = await postModel.invoke([
    {
      role: "user",
      content: formattedPrompt,
    },
  ]);

  const rewrittenPost = result.tool_calls?.[0].args as
    | z.infer<typeof postSchema>
    | undefined;

  return {
    complexPost: rewrittenPost,
  };
}



================================================
FILE: src/agents/generate-post/nodes/generate-post/index.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { GeneratePostAnnotation } from "../../generate-post-state.js";
import { ChatAnthropic } from "@langchain/anthropic";
import { GENERATE_POST_PROMPT } from "./prompts.js";
import { formatPrompt, parseGeneration } from "./utils.js";
import { ALLOWED_TIMES } from "../../constants.js";
import {
  getReflectionsPrompt,
  REFLECTIONS_PROMPT,
} from "../../../../utils/reflections.js";
import { getNextSaturdayDate } from "../../../../utils/date.js";

export async function generatePost(
  state: typeof GeneratePostAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof GeneratePostAnnotation.State>> {
  if (!state.report) {
    throw new Error("No report found");
  }
  if (!state.relevantLinks?.length) {
    throw new Error("No relevant links found");
  }
  const postModel = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0.5,
  });

  const prompt = formatPrompt(state.report, state.relevantLinks);

  const reflections = await getReflectionsPrompt(config);
  const reflectionsPrompt = REFLECTIONS_PROMPT.replace(
    "{reflections}",
    reflections,
  );

  const generatePostPrompt = GENERATE_POST_PROMPT.replace(
    "{reflectionsPrompt}",
    reflectionsPrompt,
  );

  const postResponse = await postModel.invoke([
    {
      role: "system",
      content: generatePostPrompt,
    },
    {
      role: "user",
      content: prompt,
    },
  ]);

  // Randomly select a time from the allowed times
  const [postHour, postMinute] = ALLOWED_TIMES[
    Math.floor(Math.random() * ALLOWED_TIMES.length)
  ]
    .split(" ")[0]
    .split(":");
  const postDate = getNextSaturdayDate(Number(postHour), Number(postMinute));

  return {
    post: parseGeneration(postResponse.content as string),
    scheduleDate: postDate,
  };
}



================================================
FILE: src/agents/generate-post/nodes/generate-post/prompts.ts
================================================
import { getPrompts } from "../../prompts/index.js";

export const GENERATE_POST_PROMPT = `You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for the LinkedIn and Twitter pages.
You've been provided with a report on some content that you need to turn into a LinkedIn/Twitter post. The same post will be used for both platforms.
Your coworker has already taken the time to write a detailed marketing report on this content for you, so please take your time and read it carefully.

The following are examples of LinkedIn/Twitter posts on third-party content that have done well, and you should use them as style inspiration for your post:
<examples>
${getPrompts().tweetExamples}
</examples>

Now that you've seen some examples, lets's cover the structure of the LinkedIn/Twitter post you should follow.
${getPrompts().postStructureInstructions}

This structure should ALWAYS be followed. And remember, the shorter and more engaging the post, the better (your yearly bonus depends on this!!).

Here are a set of rules and guidelines you should strictly follow when creating the LinkedIn/Twitter post:
<rules>
${getPrompts().postContentRules}
</rules>

{reflectionsPrompt}

Lastly, you should follow the process below when writing the LinkedIn/Twitter post:
<writing-process>
Step 1. First, read over the marketing report VERY thoroughly.
Step 2. Take notes, and write down your thoughts about the report after reading it carefully. This should include details you think will help make the post more engaging, and your initial thoughts about what to focus the post on, the style, etc. This should be the first text you write. Wrap the notes and thoughts inside a "<thinking>" tag.
Step 3. Lastly, write the LinkedIn/Twitter post. Use the notes and thoughts you wrote down in the previous step to help you write the post. This should be the last text you write. Wrap your report inside a "<post>" tag. Ensure you write only ONE post for both LinkedIn and Twitter.
</writing-process>

Given these examples, rules, and the content provided by the user, curate a LinkedIn/Twitter post that is engaging and follows the structure of the examples provided.`;



================================================
FILE: src/agents/generate-post/nodes/generate-post/utils.ts
================================================
import { filterLinksForPostContent } from "../../../utils.js";

/**
 * Parse the LLM generation to extract the report from inside the <report> tag.
 * If the report can not be parsed, the original generation is returned.
 * @param generation The text generation to parse
 * @returns The parsed generation, or the unmodified generation if it cannot be parsed
 */
export function parseGeneration(generation: string): string {
  const reportMatch = generation.match(/<post>([\s\S]*?)<\/post>/);
  if (!reportMatch) {
    console.warn(
      "Could not parse post from generation:\nSTART OF POST GENERATION\n\n",
      generation,
      "\n\nEND OF POST GENERATION",
    );
  }
  return reportMatch ? reportMatch[1].trim() : generation;
}

export function formatPrompt(report: string, relevantLinks: string[]): string {
  return `Here is the report I wrote on the content I'd like promoted by LangChain:
<report>
${report}
</report>

Here are the relevant links used to create the report.
You should remove tracking query parameters from the link, if present.
If you are unsure whether a link's parameters are tracking, do not remove them. It's better to have a link with tracking parameters than a broken link.
The links do NOT contribute to the post's length. They are temporarily removed from the post before the length is calculated, and re-added afterwards.
<links>
${filterLinksForPostContent(relevantLinks)}
</links>`;
}



================================================
FILE: src/agents/generate-post/nodes/generate-report/index.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { GeneratePostAnnotation } from "../../generate-post-state.js";
import { ChatAnthropic } from "@langchain/anthropic";
import { GENERATE_REPORT_PROMPT } from "./prompts.js";

/**
 * Parse the LLM generation to extract the report from inside the <report> tag.
 * If the report can not be parsed, the original generation is returned.
 * @param generation The text generation to parse
 * @returns The parsed generation, or the unmodified generation if it cannot be parsed
 */
function parseGeneration(generation: string): string {
  const reportMatch = generation.match(/<report>([\s\S]*?)<\/report>/);
  if (!reportMatch) {
    console.warn(
      "Could not parse report from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
    );
  }
  return reportMatch ? reportMatch[1].trim() : generation;
}

const formatReportPrompt = (pageContents: string[]): string => {
  return `The following text contains summaries, or entire pages from the content I submitted to you. Please review the content and generate a report on it.
${pageContents.map((content, index) => `<Content index={${index + 1}}>\n${content}\n</Content>`).join("\n\n")}`;
};

export async function generateContentReport(
  state: typeof GeneratePostAnnotation.State,
  _config: LangGraphRunnableConfig,
): Promise<Partial<typeof GeneratePostAnnotation.State>> {
  if (!state.pageContents?.length) {
    throw new Error(
      "No page contents found. pageContents must be defined to generate a content report.",
    );
  }

  const reportModel = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  });

  const result = await reportModel.invoke([
    {
      role: "system",
      content: GENERATE_REPORT_PROMPT,
    },
    {
      role: "user",
      content: formatReportPrompt(state.pageContents),
    },
  ]);

  return {
    report: parseGeneration(result.content as string),
  };
}



================================================
FILE: src/agents/generate-post/nodes/generate-report/prompts.ts
================================================
import { getPrompts } from "../../prompts/index.js";

const STRUCTURE_GUIDELINES = `<part key="1">
This is the introduction and summary of the content. This must include key details such as:
- the name of the content/product/service.
- what the content/product/service does, and/or the problems it solves.
- unique selling points or interesting facts about the content.
- a high level summary of the content/product/service.

Ensure this is section packed with details and engaging.
</part>

<part key="2">
This section should focus on how the content implements, or related to any of the business context outlined above. It should include:
- key details about how it relates to the context.
- any product(s) or service(s) used in the content.
- why the content is relevant to the business context.
- how the content is used, implemented, or related.
- why these products are important to the application.
</part>

<part key="3">
This section should cover any additional details about the content that the first two parts missed. It should include:
- a detailed technical overview of the content.
- interesting facts about the content.
- any other relevant information that may be engaging to readers.

This is the section where you should include any relevant parts of the content which you were unable to include in the first two sections.
Ensure you do NOT leave out any relevant details in the report. You want your report to be extensive and detailed. Remember, it's better to overdo it than underdo it.
</part>`;

const REPORT_RULES = `- Focus on the subject of the content, and how it uses or relates to the business context outlined above.
- The final Tweet/LinkedIn post will be developer focused, so ensure the report is VERY technical and detailed.
- You should include ALL relevant details in the report, because doing this will help the final post be more informed, relevant and engaging.
- Include any relevant links found in the content in the report. These will be useful for readers to learn more about the content.
- Include details about what the product does, what problem it solves, and how it works. If the content is not about a product, you should focus on what the content is about instead of making it product focused.
- Use proper markdown styling when formatting the marketing report.
- Generate the report in English, even if the content submitted is not in English.`;

export const GENERATE_REPORT_PROMPT = `You are a highly regarded marketing employee.
You have been tasked with writing a marketing report on content submitted to you from a third party which uses your products.
This marketing report will then be used to craft Tweets and LinkedIn posts promoting the content and your products.

${getPrompts().businessContext}

The marketing report should follow the following structure guidelines. It will be made up of three main sections outlined below:
<structure-guidelines>
${STRUCTURE_GUIDELINES}
</structure-guidelines>

Follow these rules and guidelines when generating the report:
<rules>
${REPORT_RULES}
<rules>

Lastly, you should use the following process when writing the report:
<writing-process>
- First, read over the content VERY thoroughly.
- Take notes, and write down your thoughts about the content after reading it carefully. These should be interesting insights or facts which you think you'll need later on when writing the final report. This should be the first text you write. ALWAYS perform this step first, and wrap the notes and thoughts inside opening and closing "<thinking>" tags.
- Finally, write the report. Use the notes and thoughts you wrote down in the previous step to help you write the report. This should be the last text you write. Wrap your report inside "<report>" tags. Ensure you ALWAYS WRAP your report inside the "<report>" tags, with an opening and closing tag.
</writing-process>

Do not include any personal opinions or biases in the report. Stick to the facts and technical details.
Your response should ONLY include the marketing report, and no other text.
Remember, the more detailed and engaging the report, the better!!
Finally, remember to have fun!

Given these instructions, examine the users input closely, and generate a detailed and thoughtful marketing report on it.`;



================================================
FILE: src/agents/generate-post/prompts/examples.ts
================================================
export const EXAMPLES = [
  `Agent Laboratory

An approach that leverages LLM agents capable of completing the entire research process.

Main findings:

1) Agents driven by o1-preview resulted in the best research outcomes

2) Generated machine learning code can achieve state-of-the-art performance compared to existing methods

3) Human feedback further improves the quality of research

4) Agent laboratory significantly reduces research expenses`,
  `rStar-Math helps small language models rival or even surpass OpenAI o1 on math reasoning.

How do they achieve this?

rStar-Math uses a math policy SLM for test-time search guided by an SLM-based process reward model.

What's new in rStar-Math?

- a code-augmented CoT data synthesis method involving MCTS to generate step-by-step verified reasoning trajectories which is used to train the policy SLM

- an SLM-based process reward model that reliably predicts a reward label for each math reasoning step. This leads to a more effective process preference model (PPM).

- a self-evolution recipe where the policy SLM and PPM are iteratively evolved to improve math reasoning.

Putting it together:

They first curate a dataset of 747k math word problems from publicly available sources. In each round (for four rounds), they use the latest policy model and PPM to perform MCTS, generating increasingly high-quality training data to train a stronger policy model and PPM for the next round.

Results:

On the MATH benchmark, rStar-Math improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%.

My thoughts:

The iterative self-evolution deep thinking process combined with small language models is an interesting development because there is not much evidence that these SLMs can generate high-quality and reliable training data. However, this work shows that SLMs with extensive MCTS rollouts can lead to the self-generation of high-quality training data for frontier-level math reasoning.`,
  `ðŸŒ²The promise of dynamic few-shot prompting

After sharing AppFolio's story of putting an agent in production, 
@jobergum
 noted a key part of the story: how dynamic few shot prompting greatly improved their performance

This is how it works:

You collect a set of example inputs and example outputs. This set grows to be rather large. Rather than put all of them as few shot examples in the prompt, you dynamically select the 'k' most relevant ones based on the user query/state

Few shot examples in general can help give the LLM examples of what to do. We've found that this works particularly well in classification, extraction, and tone

AppFolio isn't the only one who has found this! Dosu also found this (for a classification task): https://blog.langchain.dev/dosu-langsmith-no-prompt-eng/

See AppFolio's story here: https://blog.langchain.dev/customers-appfolio/

We have built in support for this in LangSmith - datasets aren't just for evals, they should also be used to improve your application! https://docs.smith.langchain.com/evaluation/how_to_guides/index_datasets_for_dynamic_few_shot_example_selection`,
  `Ever struggled to understand how users use your product? 

I just built an open source implementation of Anthropic's internal clustering algorithm - CLIO.

With Gemini Flash, you can generate human readable labels which are clustered and grouped together to spot usage patterns.

Read more to find out how it works`,
  `RAG isn't just embeddings. It's a complex system that needs constant refinement.

Start with synthetic data. Use both full-text and vector search. Implement clear user feedback. Cluster topics. Monitor 
constantly.

The real work begins when you have enough data to truly optimize.

https://jxnl.co/writing/2024/05/22/systematically-improving-your-rag/`,
  `Anthropic's Contextual Retrieval technique is a game-changer for RAG systems, addressing the pesky problem of lost context when chunking documents. This instructor implementation takes it a step further by adding async processing for improved efficiency. 

As someone who's spent way too much time tinkering with RAG, I'm excited to see how this approach could supercharge retrieval performance. If you've ever struggled with RAG systems spitting out nonsensical answers, this article is for you. 

Read on to learn how to level up your RAG game

https://python.useinstructor.com/blog/2024/09/26/implementing-anthropics-contextual-retrieval-with-async-processing/`,
  `Most teams get RAG wrong. They obsess over generation before nailing search.

The secret? Start with synthetic data. Focus on retrieval. Build a continuous improvement loop.

It's not about perfection. It's about creating a learning system that compounds over time.

https://jxnl.co/writing/2024/08/19/rag-flywheel/`,
  `Introducing Llama 3.3 â€“ a new 70B model that delivers the performance of our 405B model but is easier & more cost-efficient to run. By leveraging the latest advancements in post-training techniques including online preference optimization, this model improves core performance at a significantly lower cost, making it even more accessible to the entire open source community ðŸ”¥

https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct`,
  `The new Gemini 2.0 Flash Thinking model (Gemini version of GPT o1 that takes a while to think before responding) is very nice and fast and now available to try on Google AI Studio ðŸ§‘â€ðŸ³ðŸ‘.

The prominent and pleasant surprise here is that unlike o1 the reasoning traces of the model are shown. As a user I personally really like this because the reasoning itself is interesting to see and read - the models actively think through different possibilities, ideas, debate themselves, etc., it's part of the value add. The case against showing these is typically a concern of someone collecting the reasoning traces and training to imitate them on top of a different base model, to gain reasoning ability possibly and to some extent.`,
  `RAG Check

Multimodal RAG will be explored extensively in 2025. 

However, there is not much literature on measuring hallucination in multimodal RAG systems which is very common.

This work presents a novel framework to evaluate the reliability of multimodal RAG systems. 

They train two models to measure relevancy and correctness from a ChatGPT-derived database and human evaluator samples. 

Both models achieve 88% accuracy on test data.

I don't think there is a lot of good literature on evaluating multimodal RAG systems so this looks like a paper to bookmark.`,
  `We've been busy this year!  Here's 60 of our biggest AI announcements in 2024!

Very proud of everyone who worked on all of these and I'm excited we're able to bring our research to life in products and features for our users. ðŸŽ‰

Looking forward to 2025!`,
  `Just when you thought it was over...  weâ€™re introducing Gemini 2.0 Flash Thinking, a new experimental model that unlocks stronger reasoning capabilities and shows its thoughts.

The model plans (with thoughts visible), can solve complex problems with Flash speeds, and more`,
  `Introducing the Gemini for Academic Research program, created to help researchers using Gemini models accelerate their research agenda. ðŸ§ªðŸ¥¼

We are supporting researches exploring evaluations,  benchmarks, embodiment, science, and more! Apply today:`,
  `Really enjoyed "Things we learned about LLMs in 2024" by 
@simonw
, especially this analogy between today's datacenter buildout and the 19th century railway boom. The parallels are striking. https://simonwillison.net/2024/Dec/31/llms-in-2024/#the-environmental-impact-got-much-much-worse`,
  `Experimentation mindset is the key to AI success, not having all the answers.

Define metrics that matter, prioritize experiments, and redefine success as learning. Knowledge sharing is crucial.

Remove barriers to learning, not just run more experiments. Build team capability and improve incrementally.

https://skylarbpayne.com/posts/experimentation-mindset`,
  `One of my favorite applications of LLMs is reading books together. I want to ask questions or hear generated discussion (NotebookLM style) while it is automatically conditioned on the surrounding content. If Amazon or so built a Kindle AI reader that â€œjust worksâ€ imo it would be a huge hit.

For now, it is possible to kind of hack it with a bunch of script. Possibly someone already tried to build a very nice AI-native reader app and I missed it.`,
];



================================================
FILE: src/agents/generate-post/prompts/index.ts
================================================
import {
  BUSINESS_CONTEXT as LANGCHAIN_BUSINESS_CONTEXT,
  TWEET_EXAMPLES as LANGCHAIN_TWEET_EXAMPLES,
  POST_STRUCTURE_INSTRUCTIONS as LANGCHAIN_POST_STRUCTURE_INSTRUCTIONS,
  POST_CONTENT_RULES as LANGCHAIN_POST_CONTENT_RULES,
  CONTENT_VALIDATION_PROMPT as LANGCHAIN_CONTENT_VALIDATION_PROMPT,
} from "./prompts.langchain.js";
import { EXAMPLES } from "./examples.js";
import { useLangChainPrompts } from "../../utils.js";

export const TWEET_EXAMPLES = EXAMPLES.map(
  (example, index) => `<example index="${index}">\n${example}\n</example>`,
).join("\n");

/**
 * This prompt details the structure the post should follow.
 * Updating this will change the sections and structure of the post.
 * If you want to make changes to how the post is structured, you
 * should update this prompt, along with the `EXAMPLES` list.
 */
export const POST_STRUCTURE_INSTRUCTIONS = `<section key="1">
The first part should be the introduction or hook. This should be short and to the point, ideally no more than 5 words. If necessary, you can include one to two emojis in the header, however this is not required. You should not include emojis if the post is more casual, however if you're making an announcement, you should include an emoji.
</section>

<section key="2">
This section will contain the main content of the post. The post body should contain a concise, high-level overview of the content/product/service/findings outlined in the marketing report.
It should focus on what the content does, shows off, or the problem it solves.
This may include some technical details if the marketing report is very technical, however you should keep in mind your audience is not all advanced developers, so do not make it overly technical.
Ensure this section is short, no more than 3 (short) sentences. Optionally, if the content is very technical, you may include bullet points covering the main technical aspects of the content to make it more engaging and easier to follow.
Remember, the content/product/service/findings outlined in the marketing report is the main focus of this post.
</section>

<section key="3">
The final section of the post should contain a call to action. This should contain a few words that encourage the reader to click the link to the content being promoted.
Optionally, you can include an emoji here.
Ensure you do not make this section more than 3-6 words.
</section>`;

/**
 * This prompt is used when generating, condensing, and re-writing posts.
 * You should make this prompt very specific to the type of content you
 * want included/focused on in the posts.
 */
export const POST_CONTENT_RULES = `- Focus your post on what the content covers, aims to achieve, or the findings of the marketing report. This should be concise and high level.
- Do not make the post over technical as some of our audience may not be advanced developers, but ensure it is technical enough to engage developers.
- Keep posts short, concise and engaging
- Limit the use of emojis to the post header, and optionally in the call to action.
- NEVER use hashtags in the post.
- ALWAYS use present tense to make announcements feel immediate (e.g., "Microsoft just launched..." instead of "Microsoft launches...").
- ALWAYS include the link to the content being promoted in the call to action section of the post.
- You're acting as a human, posting for other humans. Keep your tone casual and friendly. Don't make it too formal or too consistent with the tone.`;

/**
 * This should contain "business content" into the type of content you care
 * about, and want to post/focus your posts on. This prompt is used widely
 * throughout the agent in steps such as content validation, and post generation.
 * It should be generalized to the type of content you care about, or if using
 * for a business, it should contain details about your products/offerings/business.
 */
export const BUSINESS_CONTEXT = `
Here is some context about the types of content you should be interested in prompting:
<business-context>
- AI applications. You care greatly about all new and novel ways people are using AI to solve problems.
- UI/UX for AI. You are interested in how people are designing UI/UXs for AI applications.
- New AI/LLM research. You want your followers to always be up to date with the latest in AI research.
- Agents. You find agents very interesting and want to always be up to date with the latest in agent implementations and systems.
- Multi-modal AI. You're deeply invested in how multi-modal LLMs can be used in AI applications.
- Generative UI. You're interested in how developers are using generative UI to enhance their applications.
- Development software for building AI applications.
- Open source AI/LLM projects, tools, frameworks, etc.
</business-context>`;

/**
 * A prompt to be used in conjunction with the business context prompt when
 * validating content for social media posts. This prompt should outline the
 * rules for what content should be approved/rejected.
 */
export const CONTENT_VALIDATION_PROMPT = `This content will be used to generate engaging, informative and educational social media posts.
The following are rules to follow when determining whether or not to approve content as valid, or not:
<validation-rules>
- The content may be about a new product, tool, service, or similar.
- The content is a blog post, or similar content of which, the topic is AI, which can likely be used to generate a high quality social media post.
- The goal of the final social media post should be to educate your users, or to inform them about new content, products, services, or findings about AI.
- You should NOT approve content from users who are requesting help, giving feedback, or otherwise not clearly about software for AI.
- You only want to approve content which can be used as marketing material, or other content to promote the content above.
</validation-rules>`;

export function getPrompts() {
  // NOTE: you should likely not have this set, unless you want to use the LangChain prompts
  if (useLangChainPrompts()) {
    return {
      businessContext: LANGCHAIN_BUSINESS_CONTEXT,
      tweetExamples: LANGCHAIN_TWEET_EXAMPLES,
      postStructureInstructions: LANGCHAIN_POST_STRUCTURE_INSTRUCTIONS,
      postContentRules: LANGCHAIN_POST_CONTENT_RULES,
      contentValidationPrompt: LANGCHAIN_CONTENT_VALIDATION_PROMPT,
    };
  }

  return {
    businessContext: BUSINESS_CONTEXT,
    tweetExamples: TWEET_EXAMPLES,
    postStructureInstructions: POST_STRUCTURE_INSTRUCTIONS,
    postContentRules: POST_CONTENT_RULES,
    contentValidationPrompt: CONTENT_VALIDATION_PROMPT,
  };
}



================================================
FILE: src/agents/generate-post/prompts/prompts.langchain.ts
================================================
import { LANGCHAIN_DOMAINS } from "../../should-exclude.js";

export const BUSINESS_CONTEXT = `
Here is some context about the different LangChain products and services:
<business-context>
- **LangChain** - the main open source libraries developers use for building AI applications. These are open source Python/JavaScript/TypeScript libraries.
- **LangGraph** - an open source library for building agentic AI applications. This is a Python/JavaScript/TypeScript library.
  LangChain also offers a hosted cloud platform called 'LangGraph Cloud' or 'LangGraph Platform' which developers can use to host their LangGraph applications in production.
- **LangSmith** - this is LangChain's SaaS product for building AI applications. It offers solutions for evaluating AI systems, observability, datasets and testing.
</business-context>`;

export const CONTENT_VALIDATION_PROMPT = `This content will be used to generate engaging, informative and educational social media posts.
The following are rules to follow when determining whether or not to approve content as valid, or not:
<validation-rules>
- The content should be about a new product, tool, service, or similar.
- You should NOT approve content from users who are requesting help, giving feedback, or otherwise not clearly about software which uses the LangChain ecosystem.
- You should NOT approve content that is showing an error, or specific problematic issue with one of LangChain's products or services.
- The content should be about AI, or software related to AI/LLMs in some way. LangChain is an AI software development company, so you should NOT approve content from users which are not at least somewhat related to AI.
- Content which includes LangChain's products or services but is not focused on LangChain's products or services should be approved. As LangChain's products or services plays a part in the content, you should approve it.
- The content must include some mention or usage of at least one of LangChain's products and services, or LangGraph's products and services. The following is a full list of LangChain products/services you should likely approve if mentioned:
  - LangChain
  - LangGraph
  - LangSmith
  - Prompt Hub
  - Social Media Agent
  - Executive AI Assistant/Email Assistant
  - Chat LangChain
  - Open Canvas
- If the content outlines how it uses LangChain's products in the making of it, but LangChain is not the main focus, you should approve it.
- We want to promote all content/products/services if they use LangChain's products to make them.
</validation-rules>`;

export const TWEET_EXAMPLES = `<example index="1">
Podcastfy.ai ðŸŽ™ï¸ðŸ¤–

An Open Source API alternative to NotebookLM's podcast product

Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI

https://podcastfy.ai
</example>

<example index="2">
ðŸ§±Complex SQL Joins with LangGraph and Waii

Waii is a toolkit that provides text-to-SQL and text-to-chart capabilities

This post focuses on Waii's approach to handling complex joins in databases, doing so within LangGraph

https://waii.com
</example>

<example index="3">
ðŸŒ Build agents that can interact with any website

Check out this video by @DendriteSystems showing how to build an agent that can interact with websites just like a human would!

This video demonstrates a workflow that:

- Finds competitors on Product Hunt and Hacker News
- Drafts an email about new competitors
- Sends the email via Outlook

ðŸ“º Video: https://youtube.com/watch?v=BGvqeRB4Jpk
ðŸ§  Repo: https://github.com/dendrite-systems/dendrite-examples
</example>

<example index="4">
ðŸš€RepoGPT: AI-Powered GitHub Assistant 

RepoGPT is an open-source, AI-powered assistant

Chat with your repositories using natural language to get insights, generate documentation, or receive code suggestions

https://repogpt.com
</example>

<example index="5">
âœˆï¸AI Travel Agent

This is one of the most comprehensive examples we've seen of a LangGraph agent. It's specifically designed to be a real world practical use case

Features
- Stateful Interactions
- Human-in-the-Loop
- Dynamic LLMs
- Email Automation

https://github.com/nirbar1985/ai-travel-agent
</example>`;

export const POST_STRUCTURE_INSTRUCTIONS = `The post should have three main sections, outlined below:
<structure-instructions>

<section key="1">
The first part of the post is the header. This should be very short, no more than 5 words, and should include one to two emojis, and the name of the content provided. If the marketing report does not specify a name, you should get creative and come up with a catchy title for it.
</section>

<section key="2">
This section will contain the main content of the post. The post body should contain a concise, high-level overview of the content/product/service outlines in the marketing report.
It should focus on what the content does, or the problem it solves. Also include details on how the content implements LangChain's product(s) and why these products are important to the application.
Ensure this is short, no more than 3 sentences. Optionally, if the content is very technical, you may include bullet points covering the main technical aspects of the content.
You should NOT make the main focus of this on LangChain, but instead on the content itself. Remember, the content/product/service outlined in the marketing report is the main focus of this post.
</section>

<section key="3">
The final section of the post should contain a call to action. This should be a short sentence that encourages the reader to click the link to the content being promoted. Optionally, you can include an emoji here.
</section>

</structure-instructions>`;

export const POST_CONTENT_RULES = `- Focus your post on what the content covers, aims to achieve, and how it uses LangChain's product(s) to do that. This should be concise and high level.
- Do not make the post over technical as some of our audience may not be advanced developers, but ensure it is technical enough to engage developers.
- Keep posts short, concise and engaging
- Limit the use of emojis to the post header, and optionally in the call to action.
- NEVER use hashtags in the post.
- ALWAYS use present tense to make announcements feel immediate (e.g., "Microsoft just launched..." instead of "Microsoft launches...").
- ALWAYS include at least one link to the content being promoted in the call to action section of the post.
- If the call to action links to a domain owned by LangChain (${LANGCHAIN_DOMAINS.map((domain) => `"${domain}"`).join(", ")}), use first person pronouns (e.g., "We released...", "Our latest...") since you're posting from the LangChain account about LangChain's own content.`;



================================================
FILE: src/agents/generate-report/index.ts
================================================
import { END, START, StateGraph } from "@langchain/langgraph";
import { GenerateReportAnnotation } from "./state.js";
import { generateReport } from "./nodes/generate-report.js";
import { extractKeyDetails } from "./nodes/extract-key-details.js";

const generateReportWorkflow = new StateGraph(GenerateReportAnnotation)
  .addNode("extractKeyDetails", extractKeyDetails)
  .addNode("generateReport", generateReport)
  .addEdge(START, "extractKeyDetails")
  .addEdge("extractKeyDetails", "generateReport")
  .addEdge("generateReport", END);

export const generateReportGraph = generateReportWorkflow.compile();
generateReportGraph.name = "Generate Report Graph";



================================================
FILE: src/agents/generate-report/prompts.ts
================================================
import { getPrompts } from "../generate-post/prompts/index.js";

export const EXTRACT_KEY_DETAILS_PROMPT = `You are a highly skilled marketing analyst. You've been tasked with extracting key details from the content submitted to you.

You should focus on technical details, new findings, new features, and other interesting information about the content.
These details will be used in a report generated after this, so ensure the details you extract are relevant and accurate. Do NOT make up details, or make assumptions.

You should first read the entire content carefully, then do the following:

1. Ask yourself what the content is about, and why it matters.
2. Ensure each key detail is unique from the rest of the list. You should group together similar details you find throughout the content.
3. With this in mind, think about ALL of the key details from the content. Remember: NO DETAIL IS TOO SMALL, and NO DETAIL IS TOO LARGE. It's better to overdo it than underdo it.
4. Finally, extract the key details from the content, and respond with them.

Your response should be in proper markdown format, and should ONLY include the key details from the content, and no other dialog.
Think carefully and slowly. Begin!`;

const REPORT_RULES = `- Focus on the subject of the content, and how it uses or relates to the business context outlined above.
- The final Tweet/LinkedIn post will be developer focused, so ensure the report is VERY technical and detailed.
- You should include ALL relevant details in the report, because doing this will help the final post be more informed, relevant and engaging.
- Include any relevant links found in the content in the report. These will be useful for readers to learn more about the content.
- Include details about what the product does, what problem it solves, and how it works. If the content is not about a product, you should focus on what the content is about instead of making it product focused.
- Use proper markdown styling when formatting the marketing report.
- Generate the report in English, even if the content submitted is not in English.`;

const STRUCTURE_GUIDELINES = `<part key="1">
This is the introduction and summary of the content.
It should contain:
- A high level summary of the content provided.
- All of the key details from the content, along with any additional context you're able to find which was not present in the key details.
  - Each key detail should be explained and detailed, ensuring the reader of this report will be able to gain a deep understanding of each key detail.
- Unique selling points, or interesting facts about the content.

Ensure this is section packed with details and engaging. Do NOT make anything up, or make assumptions. Everything you say should be able to be referenced and in the original context.
</part>

<part key="2">
The second part of the marketing report should focus on the key details. Go into even deeper detail, without making anything up, or making assumptions.
Reference specific parts of the content when elaborating on the key details.
</part>

<part key="3">
This section should focus on how the content implements, or related to any of the "business context" outlined above. It should include:
- Specific details about how it relates to the context.
- Specifics on each product(s) or service(s) used in the content.
- Why the content is relevant to the "business context".
</part>

<part key="4">
This section should cover any additional details about the content that the first three parts missed. It should include:
- A detailed technical overview of the content.
- Interesting facts about the content.
- Any other relevant information that may be engaging to readers.

This is the section where you should include any relevant parts of the content which you were unable to include in the first three sections.
Ensure you do NOT leave out any relevant details in the report. You want your report to be extensive and detailed. Remember, it's better to overdo it than underdo it.
</part>`;

export const GENERATE_REPORT_PROMPT_O1 = `You are a highly regarded marketing employee.
You have been tasked with writing a marketing report on content submitted to you from a third party which uses your products.
This marketing report will then be used to craft Tweets and LinkedIn posts promoting the content and your products.

${getPrompts().businessContext}

The marketing report should follow the following structure guidelines. It will be made up of three main sections outlined below:
<structure-guidelines>
${STRUCTURE_GUIDELINES}
</structure-guidelines>

Follow these rules and guidelines when generating the report:
<rules>
${REPORT_RULES}
<rules>

You also identified the following key details from the content:
<key-details>
{keyDetails}
</key-details>

When writing the report, you should make an emphasis on these details. But remember, these details may not include all of the key details from the content, so ensure you do NOT ONLY focus on these, but also do your own research to find other key details from the content.

Lastly, you should use the following process when writing the report:
<writing-process>
- First, read over the content VERY thoroughly.
- Finally, write the report. Use the notes and thoughts you wrote down in the previous step to help you write the report. This should be the last text you write. Wrap your report inside "<report>" tags. Ensure you ALWAYS WRAP your report inside the "<report>" tags, with an opening and closing tag.
</writing-process>

Do not include any personal opinions or biases in the report. Stick to the facts and technical details.
Your response should ONLY include the marketing report, wrapped in "<report>" tags, and no other text.
Remember, the more detailed and engaging the report, the better!!
Finally, remember to have fun!

Given these instructions, examine the users input closely, and generate a detailed and thoughtful marketing report on it.`;



================================================
FILE: src/agents/generate-report/state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { TweetsGroupedByContent } from "../curate-data/types.js";
import { VerifyLinksResultAnnotation } from "../verify-links/verify-links-state.js";

export const GenerateReportAnnotation = Annotation.Root({
  ...VerifyLinksResultAnnotation.spec,
  tweetGroup: Annotation<TweetsGroupedByContent>,
  keyReportDetails: Annotation<string>,
  /**
   * Must be an array even though it will only contain a single report.
   * This is due to its usage in a subgraph, and the shared key is `reports`.
   */
  reports: Annotation<
    Array<{
      report: string;
      keyDetails: string;
    }>
  >({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
});

export type GenerateReportState = typeof GenerateReportAnnotation.State;



================================================
FILE: src/agents/generate-report/utils.ts
================================================
import { BaseMessageLike } from "@langchain/core/messages";

export function formatImageMessages(imageOptions: string[]): BaseMessageLike {
  return {
    role: "user",
    content: [
      {
        type: "text",
        text: "The following are images you should use as context when extracting key details. All of the following images were extracted from the content you are also provided with.",
      },
      ...imageOptions.map((url) => ({
        type: "image_url",
        image_url: {
          url,
        },
      })),
    ],
  };
}



================================================
FILE: src/agents/generate-report/nodes/extract-key-details.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { GenerateReportState } from "../state.js";
import { EXTRACT_KEY_DETAILS_PROMPT } from "../prompts.js";
import { TweetsGroupedByContent } from "../../curate-data/types.js";
import { formatImageMessages } from "../utils.js";

const formatKeyDetailsPrompt = (
  pageContents: string[],
  tweetGroup: TweetsGroupedByContent | undefined,
): string => {
  let tweetGroupText = "";
  if (tweetGroup) {
    tweetGroupText = `Here is a group of tweets I extracted which are relevant to this content:
<tweet-group>
${tweetGroup.explanation}

${tweetGroup.tweets
  .map((tweet) => {
    const tweetText = tweet.note_tweet?.text || tweet.text || "";
    return `<tweet>\n${tweetText}\n</tweet>`;
  })
  .join("\n\n")}
</tweet-group>
`;
  }

  const pageContentsText =
    pageContents.length > 0
      ? pageContents
          .map(
            (content, index) =>
              `<Content index={${index + 1}}>\n${content}\n</Content>`,
          )
          .join("\n\n")
      : "";

  if (pageContentsText.length > 0) {
    return `The following text contains summaries, or entire pages from the content I submitted to you. Please review the content and extract ALL of the key details from it.
${pageContentsText}

${tweetGroupText}`;
  }

  return tweetGroupText;
};

export async function extractKeyDetails(
  state: GenerateReportState,
): Promise<Partial<GenerateReportState>> {
  if (!state.pageContents?.length && !state.tweetGroup) {
    throw new Error(
      "Missing page contents and tweet group. One of these must be defined to extract key details.",
    );
  }
  const keyDetailsPrompt = formatKeyDetailsPrompt(
    state.pageContents || [],
    state.tweetGroup,
  );

  const model = new ChatOpenAI({
    model: "o1",
    streaming: false,
  });

  const imageMessage = state.imageOptions?.length
    ? formatImageMessages(state.imageOptions)
    : undefined;

  const keyDetailsRes = await model.invoke([
    {
      role: "system",
      content: EXTRACT_KEY_DETAILS_PROMPT,
    },
    ...(imageMessage ? [imageMessage] : []),
    {
      role: "user",
      content: keyDetailsPrompt,
    },
  ]);

  return {
    keyReportDetails: keyDetailsRes.content as string,
  };
}



================================================
FILE: src/agents/generate-report/nodes/generate-report.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { GenerateReportState } from "../state.js";
import { GENERATE_REPORT_PROMPT_O1 } from "../prompts.js";
import { TweetsGroupedByContent } from "../../curate-data/types.js";
import { formatImageMessages } from "../utils.js";

interface FormatReportPromptParams {
  pageContents?: string[];
  tweetGroup?: TweetsGroupedByContent;
}

const formatReportPrompt = ({
  pageContents,
  tweetGroup,
}: FormatReportPromptParams): string => {
  let pageContentsText = "";

  if (pageContents?.length) {
    pageContentsText = `<page-contents>
<context>
The following text contains summaries, or entire pages from the content I submitted to you.
Please review the content and generate a report on it
</context>
${pageContents.map((content, index) => `<content index={${index + 1}}>\n${content}\n</content>`).join("\n")}
</page-contents>`;
  }

  let tweetGroupText = "";

  if (tweetGroup) {
    tweetGroupText = `${pageContentsText.length > 0 ? "\n\n" : ""}<tweet-group>
<context>
The following contains a series of tweets I grouped together based on the topic of the tweets.
Here is the explanation I wrote for this group:
${tweetGroup.explanation}
Please review the content${pageContentsText.length > 0 ? ", along with the page contents above," : ""} and generate a report on it
</context>
${tweetGroup.tweets
  .map((tweet, index) => {
    const tweetText = tweet.note_tweet?.text || tweet.text || "";
    return `<tweet index={${index + 1}}>\n${tweetText}\n</tweet>`;
  })
  .join("\n")}
</tweet-group>`;
  }

  return `${pageContentsText}${tweetGroupText}`;
};

/**
 * Parse the LLM generation to extract the report from inside the <report> tag.
 * If the report can not be parsed, the original generation is returned.
 * @param generation The text generation to parse
 * @returns The parsed generation, or the unmodified generation if it cannot be parsed
 */
function parseGeneration(generation: string): string {
  const reportMatch = generation.match(/<report>([\s\S]*?)<\/report>/);
  if (!reportMatch) {
    console.warn(
      "Could not parse report from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
    );
  }
  return reportMatch ? reportMatch[1].trim() : generation;
}

export async function generateReport(
  state: GenerateReportState,
): Promise<Partial<GenerateReportState>> {
  if (!state.pageContents?.length && !state.tweetGroup) {
    throw new Error(
      "No page contents found. pageContents must be defined to generate a report.",
    );
  }
  const prompt = formatReportPrompt({
    pageContents: state.pageContents,
    tweetGroup: state.tweetGroup,
  });

  const reportO1Model = new ChatOpenAI({
    model: "o1",
    streaming: false,
  });

  const formattedReportPrompt = GENERATE_REPORT_PROMPT_O1.replace(
    "{keyDetails}",
    state.keyReportDetails,
  );

  const imageMessage = state.imageOptions?.length
    ? formatImageMessages(state.imageOptions)
    : undefined;

  const report = await reportO1Model.invoke([
    {
      role: "system",
      content: formattedReportPrompt,
    },
    ...(imageMessage ? [imageMessage] : []),
    {
      role: "user",
      content: prompt,
    },
  ]);

  return {
    reports: [
      {
        report: parseGeneration(report.content as string),
        keyDetails: state.keyReportDetails,
      },
    ],
  };
}



================================================
FILE: src/agents/generate-thread/index.ts
================================================
import { END, START, StateGraph } from "@langchain/langgraph";
import { generateThreadPlan } from "./nodes/generate-thread-plan.js";
import { generateThreadPosts } from "./nodes/generate-thread-posts.js";
import { GenerateThreadAnnotation, GenerateThreadState } from "./state.js";
import { humanNode } from "./nodes/human-node/index.js";
import { updateScheduledDate } from "../shared/nodes/update-scheduled-date.js";
import { rewriteThread } from "./nodes/rewrite-thread.js";
import { scheduleThread } from "./nodes/schedule-thread.js";

function rewriteOrEndConditionalEdge(
  state: GenerateThreadState,
):
  | "rewriteThread"
  | "scheduleThread"
  | "updateScheduleDate"
  | "humanNode"
  | typeof END {
  if (state.next) {
    if (state.next === "unknownResponse") {
      // If the user's response is unknown, we should route back to the human node.
      return "humanNode";
    } else if (state.next === "rewritePost") {
      return "rewriteThread";
    } else if (state.next === "schedulePost") {
      return "scheduleThread";
    }

    return state.next;
  }
  return END;
}

const generateThreadWorkflow = new StateGraph(GenerateThreadAnnotation)
  .addNode("generateThreadPlan", generateThreadPlan)
  .addNode("generateThreadPosts", generateThreadPosts)
  .addNode("humanNode", humanNode)
  // Updated the scheduled date from the natural language response from the user.
  .addNode("updateScheduleDate", updateScheduledDate)
  .addNode("scheduleThread", scheduleThread)
  .addNode("rewriteThread", rewriteThread)
  .addEdge(START, "generateThreadPlan")
  .addEdge("generateThreadPlan", "generateThreadPosts")
  .addEdge("generateThreadPosts", "humanNode")
  .addConditionalEdges("humanNode", rewriteOrEndConditionalEdge, [
    "rewriteThread",
    "scheduleThread",
    "updateScheduleDate",
    "humanNode",
    END,
  ])
  .addEdge("rewriteThread", "humanNode")
  .addEdge("updateScheduleDate", "humanNode")
  .addEdge("scheduleThread", END);

export const generateThreadGraph = generateThreadWorkflow.compile();
generateThreadGraph.name = "Generate Thread Graph";



================================================
FILE: src/agents/generate-thread/state.ts
================================================
import { Annotation, END } from "@langchain/langgraph";
import { ThreadPost } from "./types.js";
import { DateType } from "../types.js";

export const GenerateThreadAnnotation = Annotation.Root({
  /**
   * The reports to use for generating the thread.
   */
  reports: Annotation<string[]>,
  /**
   * The total number of posts to generate.
   */
  totalPosts: Annotation<number>,
  /**
   * The plan generated for the thread.
   */
  threadPlan: Annotation<string>,
  /**
   * The posts generated for the thread.
   */
  threadPosts: Annotation<ThreadPost[]>,
  /**
   * The date to schedule the post for.
   */
  scheduleDate: Annotation<DateType>,
  /**
   * Response from the user for the post. Typically used to request
   * changes to be made to the post.
   */
  userResponse: Annotation<string | undefined>,
  /**
   * The node to execute next.
   */
  next: Annotation<
    | "schedulePost"
    | "rewritePost"
    | "updateScheduleDate"
    | "unknownResponse"
    | typeof END
    | undefined
  >,

  /**
   * The image to attach to the post, and the MIME type.
   */
  image: Annotation<
    | {
        imageUrl: string;
        mimeType: string;
      }
    | undefined
  >,
});

export type GenerateThreadState = typeof GenerateThreadAnnotation.State;



================================================
FILE: src/agents/generate-thread/types.ts
================================================
export type ThreadPost = {
  text: string;
  index: number;
  imageUrls?: string[];
};



================================================
FILE: src/agents/generate-thread/utils.ts
================================================
import { EXAMPLES } from "../generate-post/prompts/examples.js";

export function formatReportsForPrompt(reports: string[]): string {
  return reports
    .map((r, index) => `<report index="${index}">\n${r}\n</report>`)
    .join("\n");
}

export function formatBodyPostsForPrompt(posts: string[]): string {
  if (posts.length === 0) {
    return "You have not generated any body posts yet, only the introduction.";
  }
  const postsString = posts
    .map((p, index) => `<post index="${index}">\n${p}\n</post>`)
    .join("\n");
  return `Here are the body posts you have generated so far:
<body-posts>
${postsString}
</body-posts>`;
}

export function formatAllPostsForPrompt(posts: string[]): string {
  return posts
    .map((p, index) => `<post index="${index}">\n${p}\n</post>`)
    .join("\n");
}

export function formatTweetExamplesForPrompt(): string {
  return EXAMPLES.map(
    (ex, index) => `<example index="${index}">\n${ex}\n</example>`,
  ).join("\n");
}

export function parseTweetGeneration(tweet: string): string {
  const tweetMatch = tweet.match(/<tweet>([\s\S]*?)<\/tweet>/);
  if (!tweetMatch) {
    console.warn(
      "Could not parse tweet from generation:\nSTART OF TWEET\n\n",
      tweet,
      "\n\nEND OF TWEET",
    );
  }
  return tweetMatch?.[1] ? tweetMatch[1].trim() : tweet;
}



================================================
FILE: src/agents/generate-thread/nodes/generate-thread-plan.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { formatReportsForPrompt } from "../utils.js";
import { GenerateThreadState } from "../state.js";

const PROMPT = `You're an expert in social media and marketing. Your newest assignment is to create a detailed outline/plan for a Twitter thread.
You're given a single/series of marketing reports on the subject of the thread.

Your task is to analyze the reports, and create a detailed plan for each post in the thread.

Here is some background on the type of posts you've written in the past, and context about your page & audience:
-  The thread should be informative, interesting and engaging.
- You tweet about technical topics, mainly about AI. This means you want your posts to include relevant technical information, but not too much to bore users.
- You write threads which users will actually read and enjoy. Your users finish reading your threads knowing more about the topic, and inspired to take action and learn more, thanks to the posts in the thread.
- Your target audience is AI enthusiasts, software developers, and data scientists.
- Your tone is causal, friendly, engaging and informative. You sound like a friendly AI expert.
- You're a 'thought leader' in the AI space, and want to share your knowledge and insights with your audience.

Your thread should contain between 3 and 10 posts. Each post should be between 150-300 words. Ensure you kep this in mind when planning your posts.

Your plan should include the following sections:

1. Introduction. This is the beginning of your plan, and will act as a high level overview of the rest of your plan.
- A title. This should be short, and convey the main idea of the thread.
- A short introduction. This should concisely explain what the thread will cover, and why you think your audience will care.
- A list of key points to be made in the thread.
- A short, high level summary of each post you plan to write for the thread. This should be short and to the point. Do not make this more than 2-3 short sentences.
  Additionally, ensure you include the TOTAL NUMBER OF POSTS you plan to write in the thread inside <total-posts> opening & closing tags.
  Example: <total-posts>4</total-posts>

2. Body. This is the main body of your plan. It should include detailed information on each post you outlined in the introduction.
Focus on key points made in the report, specific technical details, or other relevant and interesting facts.
Each section of the body should cover one post in the thread. Each section should include the following:
  1. Objective (what does this post specifically aim to convey?)
  2. Key points (bullet them out)
  3. Source/citations (where it comes from in the original reports)
  4. Intended transitions (optional - how this post connects to the next one)

3. Conclusion. This is the final section of your plan. You should summarize the main takeaways from the thread, and include any final notes you want to make about the thread.

The user will provide you with the reports on the subject of the thread.

Ensure you do NOT write the actual posts in this plan. For now, you're only planning the posts. Ensure you respond ONLY with the plan. Do NOT include any prefixing or suffixing dialog.`;

function parseTotalPosts(generation: string): number | undefined {
  const reportMatch = generation.match(
    /<total-posts>([\s\S]*?)<\/total-posts>/,
  );
  if (!reportMatch) {
    console.warn(
      "Could not parse report from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
    );
  }

  const totalPosts = reportMatch ? reportMatch[1].trim() : generation;
  if (!totalPosts) {
    return undefined;
  }

  // Check totalPosts is a number
  if (isNaN(parseInt(totalPosts))) {
    console.warn(
      "Could not parse total posts from generation:\nSTART OF GENERATION\n\n",
      generation,
      "\n\nEND OF GENERATION",
    );
    return undefined;
  }

  return Number(totalPosts);
}

export async function generateThreadPlan(
  state: GenerateThreadState,
): Promise<Partial<GenerateThreadState>> {
  const model = new ChatOpenAI({
    model: "o1",
    streaming: false,
  });

  const userMessage = `Here are the report(s) you should use to plan the thread:

<all-reports>
${formatReportsForPrompt(state.reports)}
</all-reports>`;

  const response = await model.invoke([
    ["system", PROMPT],
    ["user", userMessage],
  ]);

  const threadPlan = response.content as string;
  const totalPosts = parseTotalPosts(threadPlan);
  if (totalPosts === undefined) {
    // TODO: Make this pass to an LLM and have the LLM extract the number.
    throw new Error("Could not parse total posts from generation");
  }

  return {
    threadPlan,
    totalPosts,
  };
}



================================================
FILE: src/agents/generate-thread/nodes/generate-thread-posts.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import {
  formatAllPostsForPrompt,
  formatBodyPostsForPrompt,
  formatReportsForPrompt,
  formatTweetExamplesForPrompt,
  parseTweetGeneration,
} from "../utils.js";
import { GenerateThreadState } from "../state.js";

const STYLE_RULES = `- Ensure it's engaging and interesting.
- Keep it under 280 characters to fit in a single Tweet.
- Avoid marketing jargon or language. Don't sound like a salesperson, AI, or corporate employee.
- Do NOT use hashtags or emojis.
- Avoid the following and similar phrases, and tactics to get clicks:
  - "let's jump into this ðŸ§µ"
  - "read more below"
  - "here's what you need to know"
  - "the numbers are mind-blowing"
  - "The roadmap gets even wilder"
  - "game changing"
  Your facts and numbers should speak for themselves. Including these phrases or similar would make me want to click AWAY since it sounds like a sales pitch, or someone is grifting.
- Your post should sound like it was written by a human.`;

/**
 * Prompt for generating the initial post for the thread.
 */
const FIRST_POST_PROMPT = `You are an expert in social media and marketing, tasked with writing the first post for a Twitter thread. Your goal is to create an engaging, informative, and interesting post that will hook users and get them excited to read the rest of the thread.

Follow these style rules when crafting your initial tweet:
<rules-and-guidelines>
${STYLE_RULES}
- Provide a high-level overview that excites users to read the rest of the posts, rather than covering all points in the plan.
</rules-and-guidelines>

Here are examples of tweets that sound like they were written by humans.
Even though some might be much longer than 280 characters, you should ignore the length and instead focus on the content, wording and tone.
Your main goal is to SOUND like a human, who writes casual and informative tweets:
<tweet-examples>
${formatTweetExamplesForPrompt()}
</tweet-examples>

Here is the plan for the thread:
<thread-plan>
{THREAD_PLAN}
</thread-plan>

And here are the marketing reports to inform your post:
<marketing-reports>
{MARKETING_REPORTS}
</marketing-reports>

Now, follow these steps to create your tweet:

1. Carefully read through the thread plan and marketing reports.
2. Identify the key points or themes that would be most engaging to potential readers.
3. Think about how you can introduce these points in a way that piques curiosity without giving everything away.
4. Craft a tweet that adheres to the guidelines provided earlier, using the information you've gathered.
5. Review your tweet to ensure it's under 280 characters and sounds like it was written by a human.

Once you've completed these steps, provide your tweet inside <tweet> tags. Do not include any explanation or commentary outside of the tweet itself. ALWAYS WRAP your tweet inside opening and closing <tweet> tags.`;

/**
 * Prompt for generating followup posts.
 */
const FOLLOWING_POST_PROMPTS = `You are an expert in social media and marketing, tasked with writing a post for a Twitter thread. Your goal is to create an engaging, informative, and interesting post that will make users want to keep reading the thread.

You've been tasked with writing the body posts for a thread, following the plan provided to you. You're currently writing {CURRENT_POST_NUMBER} in the thread out of {TOTAL_POSTS} (including the first introduction post).

Follow these style rules when crafting your tweet:
<rules-and-guidelines>
${STYLE_RULES}
</rules-and-guidelines>

Here are examples of tweets that sound like they were written by humans.
Even though some might be much longer than 280 characters, you should ignore the length and instead focus on the content, wording and tone.
Your main goal is to SOUND like a human, who writes casual and informative tweets:
<tweet-examples>
${formatTweetExamplesForPrompt()}
</tweet-examples>

Here is the introduction post you already wrote for the thread:
<introduction-post>
{INTRODUCTION_POST}
</introduction-post>

{BODY_POSTS}

Here is the plan for the thread. Ensure you examine closely the plan for the post you're about to write, the post you wrote previously, and if applicable, the post you'll write next.
This is important to keep the entire thread coherent and engaging:
<thread-plan>
{THREAD_PLAN}
</thread-plan>

Now, follow these steps to create your tweet:

1. Carefully read through the thread plan.
2. Identify the key points or themes that would be most engaging to potential readers.
3. Think about how you can introduce these points in a way that piques curiosity without giving everything away.
4. Craft a tweet that adheres to the guidelines provided earlier, using the information you've gathered.
5. Review your tweet to ensure it's under 280 characters and sounds like it was written by a human.

Once you've completed these steps, provide your tweet inside <tweet> tags. Do not include any explanation or commentary outside of the tweet itself. ALWAYS WRAP your tweet inside opening and closing <tweet> tags.`;

/**
 * Prompt for the final post in the thread.
 */
const FINAL_POST_PROMPT = `You are an expert in social media and marketing, tasked with writing the LAST post in a Twitter thread.
Your goal is to write a post which cleanly wraps up the thread.

Follow these style rules when crafting your concluding tweet:
<rules-and-guidelines>
${STYLE_RULES}
- Ensure this tweet wraps up the entire thread.
</rules-and-guidelines>

Here are examples of tweets that sound like they were written by humans.
Even though some might be much longer than 280 characters, you should ignore the length and instead focus on the content, wording and tone.
Your main goal is to SOUND like a human, who writes casual and informative tweets:
<tweet-examples>
${formatTweetExamplesForPrompt()}
</tweet-examples>

Here are all of the posts you've written so far:
<thread-posts>
{THREAD_POSTS}
</thread-posts>

You also wrote this plan for the thread. Ensure you examine the plan closely for the conclusion you're about to write, and all of the posts you wrote previously.
This is important to keep the entire thread coherent and engaging:
<thread-plan>
{THREAD_PLAN}
</thread-plan>

Now, follow these steps to create your tweet:

1. Carefully read through the thread plan.
2. Identify the key points or themes that would be most engaging to potential readers.
3. Think about how you can introduce these points in a way that piques curiosity without giving everything away.
4. Craft a tweet that adheres to the guidelines provided earlier, using the information you've gathered.
5. Review your tweet to ensure it's under 280 characters and sounds like it was written by a human.

Once you've completed these steps, provide your tweet inside <tweet> tags. Do not include any explanation or commentary outside of the tweet itself. ALWAYS WRAP your tweet inside opening and closing <tweet> tags.`;

export async function generateThreadPosts(
  state: GenerateThreadState,
): Promise<Partial<GenerateThreadState>> {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0, // TODO: Eval different temperatures
  });

  const formattedFirstPostPrompt = FIRST_POST_PROMPT.replace(
    "{THREAD_PLAN}",
    state.threadPlan,
  ).replace("{MARKETING_REPORTS}", formatReportsForPrompt(state.reports));
  const firstPostGeneration = await model.invoke([
    ["user", formattedFirstPostPrompt],
  ]);
  const firstPost = parseTweetGeneration(firstPostGeneration.content as string);

  const bodyPosts: string[] = [];

  // Subtract 2 because we already have the first post, and we generate the final post at the end
  for (let i = 0; i < state.totalPosts - 2; i += 1) {
    const formattedFollowingPostPrompt = FOLLOWING_POST_PROMPTS.replace(
      "{CURRENT_POST_NUMBER}",
      i === 0 ? "the first post" : `post no. ${i + 1}`,
    )
      .replace("{TOTAL_POSTS}", state.totalPosts.toString())
      .replace("{INTRODUCTION_POST}", firstPost)
      .replace("{BODY_POSTS}", formatBodyPostsForPrompt(bodyPosts))
      .replace("{THREAD_PLAN}", state.threadPlan);

    const bodyPost = (
      await model.invoke([["user", formattedFollowingPostPrompt]])
    ).content as string;

    bodyPosts.push(parseTweetGeneration(bodyPost));
  }

  const formattedFinalPostPrompt = FINAL_POST_PROMPT.replace(
    "{THREAD_PLAN}",
    state.threadPlan,
  ).replace(
    "{THREAD_POSTS}",
    formatAllPostsForPrompt([firstPost, ...bodyPosts]),
  );

  const finalPostGeneration = await model.invoke([
    ["user", formattedFinalPostPrompt],
  ]);
  const finalPost = parseTweetGeneration(finalPostGeneration.content as string);

  const allPosts = [firstPost, ...bodyPosts, finalPost];
  return {
    threadPosts: allPosts.map((post, index) => ({
      text: post,
      index,
    })),
  };
}



================================================
FILE: src/agents/generate-thread/nodes/rewrite-thread.ts
================================================
import { z } from "zod";
import { GenerateThreadState } from "../state.js";
import { ChatAnthropic } from "@langchain/anthropic";
import {
  getThreadReflections,
  THREAD_REFLECTIONS_PROMPT,
  THREAD_RULESET_KEY,
} from "../../../utils/reflections.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const REWRITE_THREAD_PROMPT = `<context>
You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for your LinkedIn and Twitter pages.
You wrote a thread for your LinkedIn and Twitter pages, however your boss has asked for some changes to be made before it can be published.
You're also be provided with the original plan for the thread which you wrote.
Use this plan to guide your decisions, however ensure you weigh the user's requests above the plan if they contradict each other.
</context>

<original-thread>
{originalThread}
</original-thread>

{reflectionsPrompt}

<original-thread-plan>
{originalThreadPlan}
</original-thread-plan>

<instructions>
Listen to your boss closely, and make the necessary changes to the thread.
Ensure you keep the reflections above in mind when making the changes.
You should ONLY update the posts which the user has requested to be updated.
If it is not clear which posts should be updated, you should do your best to determine which posts they intend to update.
You should respond with ALL of the posts, including updated and unchanged posts.
</instructions>`;

const schema = z
  .object({
    threadPosts: z
      .array(
        z.object({
          index: z.number().describe("The index of the post in the thread."),
          text: z
            .string()
            .describe("The text content of the individual post in the thread."),
        }),
      )
      .describe("The list of updated thread posts."),
  })
  .describe("The updated thread posts.");

export async function rewriteThread(
  state: GenerateThreadState,
  config: LangGraphRunnableConfig,
): Promise<Partial<GenerateThreadState>> {
  if (!state.threadPosts?.length) {
    throw new Error("No thread found. Can not rewrite thread without posts.");
  }
  if (!state.userResponse) {
    throw new Error(
      "No user response found. Can not rewrite thread without a response.",
    );
  }

  const rewriteThreadModel = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  }).withStructuredOutput(schema, {
    name: "rewriteThreadPosts",
  });

  const threadReflections = await getThreadReflections(config);
  let threadReflectionsPrompt = "";
  if (
    threadReflections?.value?.[THREAD_RULESET_KEY]?.length &&
    Array.isArray(threadReflections?.value?.[THREAD_RULESET_KEY])
  ) {
    const rulesetString = `- ${threadReflections.value[THREAD_RULESET_KEY].join("\n- ")}`;
    threadReflectionsPrompt = THREAD_REFLECTIONS_PROMPT.replace(
      "{reflections}",
      rulesetString,
    );
  }

  const formattedSystemPrompt = REWRITE_THREAD_PROMPT.replace(
    "{originalThread}",
    state.threadPosts
      .map((p) => `<post index="${p.index}">\n${p.text}\n</post>`)
      .join("\n"),
  )
    .replace("{originalThreadPlan}", state.threadPlan)
    .replace("{reflections}", threadReflectionsPrompt);

  const revisedThreadResponse = await rewriteThreadModel.invoke([
    {
      role: "system",
      content: formattedSystemPrompt,
    },
    {
      role: "user",
      content: state.userResponse,
    },
  ]);

  return {
    threadPosts: revisedThreadResponse.threadPosts,
    next: undefined,
    userResponse: undefined,
  };
}



================================================
FILE: src/agents/generate-thread/nodes/schedule-thread.ts
================================================
import { GenerateThreadState } from "../state.js";

export async function scheduleThread(state: GenerateThreadState) {
  throw new Error("Not implemented" + state);
}



================================================
FILE: src/agents/generate-thread/nodes/human-node/index.ts
================================================
import { END, LangGraphRunnableConfig, interrupt } from "@langchain/langgraph";
import { ThreadPost } from "../../types.js";
import { GenerateThreadState } from "../../state.js";
import {
  getNextSaturdayDate,
  parseDateResponse,
} from "../../../../utils/date.js";
import { formatInTimeZone } from "date-fns-tz";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import { processImageInput } from "../../../utils.js";
import { routeResponse } from "../../../shared/nodes/route-response.js";

interface ConstructDescriptionArgs {
  unknownResponseDescription: string;
  threadPosts: ThreadPost[];
  imageOptions?: string[];
}

function constructDescription({
  unknownResponseDescription,
  imageOptions,
  threadPosts,
}: ConstructDescriptionArgs): string {
  const imageOptionsText = imageOptions?.length
    ? `## Image Options\n\nThe following image options are available. Select one by copying and pasting the URL into the 'image' field.\n\n${imageOptions.map((url) => `URL: ${url}\nImage: <details><summary>Click to view image</summary>\n\n![](${url})\n</details>\n`).join("\n")}`
    : "";

  const unknownResponseString = unknownResponseDescription
    ? `${unknownResponseDescription}\n\n`
    : "";

  const imageInstructionsString = imageOptions?.length
    ? `If you wish to attach an image to the post, please add a public image URL.

You may remove the image by setting the 'image' field to 'remove', or by removing all text from the field
To replace the image, simply add a new public image URL to the field.

MIME types will be automatically extracted from the image.
Supported image types: \`image/jpeg\` | \`image/gif\` | \`image/png\` | \`image/webp\``
    : "No image options available.";

  return `${unknownResponseString}# Schedule post
  
### Post:
\`\`\`
${threadPosts.map((p) => `${p.index}\n${p.text}`).join("\n---\n")}
\`\`\`

${imageOptionsText}

## Instructions

There are a few different actions which can be taken:\n
- **Edit**: If the post is edited and submitted, it will be scheduled for Twitter/LinkedIn.
- **Response**: If a response is sent, it will be sent to a router which can be routed to either
  1. A node which will be used to rewrite the post. Please note, the response will be used as the 'user' message in an LLM call to rewrite the post, so ensure your response is properly formatted.
  2. A node which will be used to update the scheduled date for the post.
  If an unknown/invalid response is sent, nothing will happen, and it will be routed back to the human node.
- **Accept**: If 'accept' is selected, the post will be scheduled for Twitter/LinkedIn.
- **Ignore**: If 'ignore' is selected, this post will not be scheduled, and the thread will end.

## Additional Instructions

### Schedule Date

The date the post will be scheduled for may be edited, but it must follow the format 'MM/dd/yyyy hh:mm a z'. Example: '12/25/2024 10:00 AM PST', _OR_ you can use a priority level:
- **P1**: Saturday/Sunday between 8:00 AM and 10:00 AM PST.
- **P2**: Friday/Monday between 8:00 AM and 10:00 AM PST _OR_ Saturday/Sunday between 11:30 AM and 1:00 PM PST.
- **P3**: Saturday/Sunday between 1:00 PM and 5:00 PM PST.

### Image

${imageInstructionsString}
`;
}

const getUnknownResponseDescription = (state: GenerateThreadState) => {
  if (state.next === "unknownResponse" && state.userResponse) {
    return `# <div style="color: red;">UNKNOWN/INVALID RESPONSE RECEIVED: '${state.userResponse}'</div>

<div style="color: red;">Please respond with either a request to update/rewrite the post, or a valid priority level or a date to schedule the post.</div>

<div style="color: red;">See the \`Schedule Date\`, or \`Instructions\` sections for more information.</div>

<hr />`;
  }
  return "";
};

function extractThreadPostsFromArgs(
  args: Record<string, string>,
): ThreadPost[] {
  const posts: ThreadPost[] = [];

  // Find all keys that match post_X pattern
  Object.entries(args).forEach(([key, value]) => {
    const match = key.match(/^post_(\d+)$/);
    if (match) {
      const index = parseInt(match[1], 10);
      posts.push({
        text: value,
        index,
      });
    }
  });

  // Sort posts by index to maintain order
  return posts.sort((a, b) => a.index - b.index);
}

export async function humanNode(
  state: GenerateThreadState,
  _config: LangGraphRunnableConfig,
): Promise<Partial<GenerateThreadState>> {
  if (!state.threadPosts.length) {
    throw new Error("No thread found");
  }

  const unknownResponseDescription = getUnknownResponseDescription(state);
  const defaultDate = state.scheduleDate || getNextSaturdayDate();
  let defaultDateString = "";
  if (
    typeof state.scheduleDate === "string" &&
    ["p1", "p2", "p3"].includes(state.scheduleDate)
  ) {
    defaultDateString = state.scheduleDate as string;
  } else {
    defaultDateString = formatInTimeZone(
      defaultDate,
      "America/Los_Angeles",
      "MM/dd/yyyy hh:mm a z",
    );
  }

  const interruptValue: HumanInterrupt = {
    action_request: {
      action: "Schedule Twitter/LinkedIn thread",
      args: {
        ...Object.fromEntries(
          state.threadPosts.map((p) => [`post_${p.index}`, p.text]),
        ),
        date: defaultDateString,
        // Do not provide an image field if the mode is text only
        image: state.image?.imageUrl ?? "",
      },
    },
    config: {
      allow_accept: true,
      allow_edit: true,
      allow_ignore: true,
      allow_respond: true,
    },
    description: constructDescription({
      threadPosts: state.threadPosts,
      unknownResponseDescription,
    }),
  };

  const response = interrupt<HumanInterrupt[], HumanResponse[]>([
    interruptValue,
  ])[0];

  if (!["edit", "ignore", "accept", "response"].includes(response.type)) {
    throw new Error(
      `Unexpected response type: ${response.type}. Must be "edit", "ignore", "accept", or "response".`,
    );
  }
  if (response.type === "ignore") {
    return {
      next: END,
    };
  }
  if (!response.args) {
    throw new Error(
      `Unexpected response args: ${response.args}. Must be defined.`,
    );
  }

  if (response.type === "response") {
    if (typeof response.args !== "string") {
      throw new Error("Response args must be a string.");
    }

    const { route } = await routeResponse({
      post: state.threadPosts.map((p) => p.text).join("\n"),
      dateOrPriority: defaultDateString,
      userResponse: response.args,
    });

    if (route === "rewrite_post") {
      return {
        userResponse: response.args,
        next: "rewritePost",
      };
    }
    if (route === "update_date") {
      return {
        userResponse: response.args,
        next: "updateScheduleDate",
      };
    }

    return {
      userResponse: response.args,
      next: "unknownResponse",
    };
  }

  if (typeof response.args !== "object") {
    throw new Error(
      `Unexpected response args type: ${typeof response.args}. Must be an object.`,
    );
  }
  if (!("args" in response.args)) {
    throw new Error(
      `Unexpected response args value: ${response.args}. Must be defined.`,
    );
  }

  const castArgs = response.args.args as unknown as Record<string, string>;
  const threadPosts = extractThreadPostsFromArgs(castArgs);
  if (!threadPosts.length) {
    throw new Error("No thread posts found");
  }

  const postDateString = castArgs.date || defaultDateString;
  const postDate = parseDateResponse(postDateString);
  if (!postDate) {
    // TODO: Handle invalid dates better
    throw new Error(
      `Invalid date provided. Expected format: 'MM/dd/yyyy hh:mm a z' or 'P1'/'P2'/'P3'. Received: '${postDateString}'`,
    );
  }

  let imageState: { imageUrl: string; mimeType: string } | undefined =
    undefined;
  const processedImage = await processImageInput(castArgs.image);
  if (processedImage && processedImage !== "remove") {
    imageState = processedImage;
  } else if (processedImage === "remove") {
    imageState = undefined;
  } else {
    imageState = state.image;
  }

  return {
    next: "schedulePost",
    scheduleDate: postDate,
    threadPosts,
    // TODO: Update so if the mime type is blacklisted, it re-routes to human node with an error message.
    image: imageState,
    userResponse: undefined,
  };
}



================================================
FILE: src/agents/ingest-data/ingest-data-graph.ts
================================================
import {
  END,
  LangGraphRunnableConfig,
  START,
  StateGraph,
} from "@langchain/langgraph";
import {
  IngestDataConfigurableAnnotation,
  IngestDataAnnotation,
} from "./ingest-data-state.js";
import { ingestSlackData } from "./nodes/ingest-slack.js";
import { Client } from "@langchain/langgraph-sdk";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
  TEXT_ONLY_MODE,
} from "../generate-post/constants.js";
import {
  getAfterSecondsFromLinks,
  isTextOnly,
  shouldPostToLinkedInOrg,
  skipContentRelevancyCheck,
  skipUsedUrlsCheck,
} from "../utils.js";

async function generatePostFromMessages(
  state: typeof IngestDataAnnotation.State,
  config: LangGraphRunnableConfig,
) {
  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const linkAndDelay = getAfterSecondsFromLinks(state.links);
  const isTextOnlyMode = isTextOnly(config);
  const postToLinkedInOrg = shouldPostToLinkedInOrg(config);
  const shouldSkipContentRelevancyCheck = await skipContentRelevancyCheck(
    config?.configurable,
  );
  const shouldSkipUsedUrlsCheck = await skipUsedUrlsCheck(config?.configurable);

  for await (const { link, afterSeconds } of linkAndDelay) {
    const thread = await client.threads.create();
    await client.runs.create(thread.thread_id, "generate_post", {
      input: {
        links: [link],
      },
      config: {
        configurable: {
          [POST_TO_LINKEDIN_ORGANIZATION]: postToLinkedInOrg,
          [TEXT_ONLY_MODE]: isTextOnlyMode,
          [SKIP_CONTENT_RELEVANCY_CHECK]: shouldSkipContentRelevancyCheck,
          [SKIP_USED_URLS_CHECK]: shouldSkipUsedUrlsCheck,
        },
      },
      afterSeconds,
    });
  }
  return {};
}

const builder = new StateGraph(
  IngestDataAnnotation,
  IngestDataConfigurableAnnotation,
)
  // Ingests posts from Slack channel.
  .addNode("ingestSlackData", ingestSlackData)
  // Subgraph which is invoked once for each message.
  // This subgraph will verify content is relevant to
  // LangChain, generate a report on the content, and
  // finally generate and schedule a post.
  .addNode("generatePostGraph", generatePostFromMessages)
  // Start node
  .addEdge(START, "ingestSlackData")
  // After ingesting data, route to the subgraph for each message.
  .addEdge("ingestSlackData", "generatePostGraph")
  // Finish after generating the Twitter post.
  .addEdge("generatePostGraph", END);

export const graph = builder.compile();

graph.name = "Social Media Agent";



================================================
FILE: src/agents/ingest-data/ingest-data-state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { SimpleSlackMessage } from "../../clients/slack/client.js";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  SKIP_CONTENT_RELEVANCY_CHECK,
  SKIP_USED_URLS_CHECK,
  TEXT_ONLY_MODE,
} from "../generate-post/constants.js";

export type LangChainProduct = "langchain" | "langgraph" | "langsmith";
export type SimpleSlackMessageWithLinks = SimpleSlackMessage & {
  links: string[];
};

export const IngestDataAnnotation = Annotation.Root({
  /**
   * The links to content to use for generating posts.
   */
  links: Annotation<string[]>,
  /**
   * A report generated on the content. Will be used in the main
   * graph when generating the post about this content.
   */
  report: Annotation<string>,
  /**
   * The content of the linkedin post.
   */
  linkedinPost: Annotation<string>,
  /**
   * The content of the tweet.
   */
  twitterPost: Annotation<string>,
});

export const IngestDataConfigurableAnnotation = Annotation.Root({
  maxMessages: Annotation<number>({
    reducer: (_state, update) => update,
    default: () => 100,
  }),
  /**
   * The maximum number of days to go back when ingesting
   * messages from Slack.
   */
  maxDaysHistory: Annotation<number>,
  slackChannelId: Annotation<string>,
  /**
   * Whether or not to skip ingesting messages from Slack.
   * This will throw an error if slack messages are not
   * pre-provided in state.
   */
  skipIngest: Annotation<boolean | undefined>,
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
  /**
   * Whether or not to use text only mode throughout the graph.
   * If true, it will not try to extract, validate, or upload images.
   * Additionally, it will not be able to handle validating YouTube videos.
   * @default false
   */
  [TEXT_ONLY_MODE]: Annotation<boolean | undefined>({
    reducer: (_state, update) => update,
    default: () => false,
  }),
  /**
   * Whether or not to skip content verification.
   * If true, it will not attempt to verify the content from the link provided.
   * @default undefined
   */
  [SKIP_CONTENT_RELEVANCY_CHECK]: Annotation<boolean | undefined>(),
  /**
   * Whether or not to skip the used URLs check. This will also
   * skip saving the URLs in the store.
   */
  [SKIP_USED_URLS_CHECK]: Annotation<boolean | undefined>(),
});



================================================
FILE: src/agents/ingest-data/nodes/ingest-slack.ts
================================================
import { IngestDataAnnotation } from "../ingest-data-state.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { SlackClient } from "../../../clients/slack/client.js";
import { extractUrlsFromSlackText } from "../../utils.js";

const getChannelIdFromConfig = async (
  config: LangGraphRunnableConfig,
): Promise<string | undefined> => {
  return config.configurable?.slackChannelId;
};

export async function ingestSlackData(
  state: typeof IngestDataAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof IngestDataAnnotation.State>> {
  if (config.configurable?.skipIngest) {
    if (state.links.length === 0) {
      throw new Error("Can not skip ingest with no links");
    }
    return {};
  }

  const channelId = await getChannelIdFromConfig(config);
  if (!channelId) {
    throw new Error("Channel ID not found");
  }

  const client = new SlackClient();
  const recentMessages = await client.getChannelMessages(channelId, {
    maxMessages: config.configurable?.maxMessages,
    maxHoursHistory: config.configurable?.maxDaysHistory
      ? 24 * config.configurable?.maxDaysHistory
      : undefined,
  });

  const links = recentMessages.flatMap((msg) => {
    const links = extractUrlsFromSlackText(msg.text);
    if (!links.length) {
      return [];
    }
    return links;
  });

  return {
    links,
  };
}



================================================
FILE: src/agents/ingest-data/nodes/ingest-twitter.ts
================================================
import { IngestDataAnnotation } from "../ingest-data-state.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import Arcade from "@arcadeai/arcadejs";
import { getArcadeTwitterAuthOrInterrupt } from "../../shared/auth/twitter.js";
import { INGEST_TWITTER_USERNAME } from "../../generate-post/constants.js";

type TweetResult = {
  author_id: string;
  author_name: string;
  author_username: string;
  edit_history_tweet_ids: string[];
  id: string;
  text: string;
  tweet_url: string;
};

/**
 * Ingests Twitter data into the graph.
 *
 * This function will ingest tweets by a username.
 *
 * @param state The current state of the graph.
 * @param config The configuration for the ingest operation.
 * @returns A partial update to the graph state with the ingested tweets.
 */
export async function ingestTweets(
  state: typeof IngestDataAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof IngestDataAnnotation.State>> {
  if (config.configurable?.skipIngest) {
    if (state.links.length === 0) {
      throw new Error("Can not skip ingest with no links");
    }
    return {};
  }
  const twitterUserId = process.env.TWITTER_USER_ID;
  if (!twitterUserId) {
    throw new Error("Twitter user ID not found in configurable fields.");
  }

  const username = config.configurable?.[INGEST_TWITTER_USERNAME] as
    | string
    | undefined;
  if (!username) {
    throw new Error("Twitter username not found in configurable fields.");
  }

  const arcade = new Arcade({
    apiKey: process.env.ARCADE_API_KEY,
  });
  await getArcadeTwitterAuthOrInterrupt(twitterUserId, arcade);

  let links: string[] = [];
  const result = await arcade.tools.execute({
    tool_name: "X.SearchRecentTweetsByUsername",
    input: {
      username,
      // (integer, optional, Defaults to 10) The maximum number of results to return. Cannot be less than 10.
      // 15 since the rate limit is 15 req/15 min
      max_results: 15,
    },
    user_id: twitterUserId,
  });

  const castValue = result.output?.value as { data: TweetResult[] | undefined };
  if (castValue && castValue.data) {
    links = castValue.data.map((t) => t.tweet_url);
  }

  return {
    links,
  };
}



================================================
FILE: src/agents/ingest-repurposed-data/constants.ts
================================================
export const DEFAULT_POST_QUANTITY = 3;



================================================
FILE: src/agents/ingest-repurposed-data/index.ts
================================================
import {
  END,
  LangGraphRunnableConfig,
  START,
  StateGraph,
} from "@langchain/langgraph";
import { ingestSlackMessages } from "./nodes/ingest-slack.js";
import { Client } from "@langchain/langgraph-sdk";
import { POST_TO_LINKEDIN_ORGANIZATION } from "../generate-post/constants.js";
import { shouldPostToLinkedInOrg } from "../utils.js";
import {
  IngestRepurposedDataAnnotation,
  IngestRepurposedDataConfigurableAnnotation,
  IngestRepurposedDataState,
} from "./types.js";
import { extract } from "./nodes/extract.js";

async function generatePostsFromMessages(
  state: IngestRepurposedDataState,
  config: LangGraphRunnableConfig,
) {
  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const postToLinkedInOrg = shouldPostToLinkedInOrg(config);

  for await (const content of state.contents) {
    const thread = await client.threads.create();
    await client.runs.create(thread.thread_id, "repurposer", {
      input: {
        originalLink: content.originalLink,
        contextLinks: content.additionalContextLinks,
        quantity: content.quantity,
      },
      config: {
        configurable: {
          [POST_TO_LINKEDIN_ORGANIZATION]: postToLinkedInOrg,
        },
      },
    });
  }
  return {};
}

function ingestSlackMessagesOrSkip(
  state: IngestRepurposedDataState,
): "extract" | "ingestSlackMessages" {
  if (state.messages.length > 0) {
    return "extract";
  }
  return "ingestSlackMessages";
}

const builder = new StateGraph(
  IngestRepurposedDataAnnotation,
  IngestRepurposedDataConfigurableAnnotation,
)
  // Ingests posts from Slack channel.
  .addNode("ingestSlackMessages", ingestSlackMessages)
  // A node which extracts the links and other data from the slack messages
  .addNode("extract", extract)
  // Subgraph which is invoked once for each message.
  // This subgraph will verify content is relevant to
  // LangChain, generate a report on the content, and
  // finally generate and schedule the specified number of posts.
  .addNode("generatePostsGraph", generatePostsFromMessages)
  // Start node
  .addConditionalEdges(START, ingestSlackMessagesOrSkip, [
    "ingestSlackMessages",
    "extract",
  ])
  // After ingesting the messages, send them to the extract function to extract the links and other data
  .addEdge("ingestSlackMessages", "extract")
  // After extracting the data, route to the subgraph for each message.
  .addEdge("extract", "generatePostsGraph")
  // Finish after kicking off the subgraph for each message.
  .addEdge("generatePostsGraph", END);

export const graph = builder.compile();

graph.name = "Ingest Repurposed Data Graph";



================================================
FILE: src/agents/ingest-repurposed-data/types.ts
================================================
import { Annotation, MessagesAnnotation } from "@langchain/langgraph";
import { SimpleSlackMessage } from "../../clients/slack/client.js";
import { POST_TO_LINKEDIN_ORGANIZATION } from "../generate-post/constants.js";

export type RepurposedContent = {
  originalLink: string;
  additionalContextLinks?: string[];
  quantity: number;
  attachmentUrls: string[] | undefined;
};

export const IngestRepurposedDataAnnotation = Annotation.Root({
  /**
   * The message that triggered the repurposer. Must be of type list, but it will only
   * ever contain a single message from the user.
   */
  messages: MessagesAnnotation.spec["messages"],
  /**
   * The contents to use for generating repurposed posts.
   */
  contents: Annotation<RepurposedContent[]>,
  /**
   * The Slack messages ingested.
   */
  slackMessages: Annotation<SimpleSlackMessage[]>,
});

export type IngestRepurposedDataState =
  typeof IngestRepurposedDataAnnotation.State;
export type IngestRepurposedDataUpdate =
  typeof IngestRepurposedDataAnnotation.Update;

export const IngestRepurposedDataConfigurableAnnotation = Annotation.Root({
  /**
   * The ID of the slack channel to use when ingesting data.
   */
  repurposerSlackChannelId: Annotation<string | undefined>,
  /**
   * Whether or not to skip ingesting messages from Slack.
   * This will throw an error if slack messages are not
   * pre-provided in state.
   */
  skipIngest: Annotation<boolean | undefined>,
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
});



================================================
FILE: src/agents/ingest-repurposed-data/nodes/extract.ts
================================================
import { z } from "zod";
import { IngestRepurposedDataState, RepurposedContent } from "../types.js";
import { ChatAnthropic } from "@langchain/anthropic";
import { isValidUrl } from "../../utils.js";
import { traceable } from "langsmith/traceable";
import { DEFAULT_POST_QUANTITY } from "../constants.js";
import { MessageContentText } from "@langchain/core/messages";
import { getPublicFileUrls } from "../../../clients/slack/utils.js";

const EXTRACT_CONTENT_PROMPT = `You're a helpful AI assistant, tasked with extracting content from a Slack message.

<context>
The Slack message will contain link(s) to content which our company (LangChain) wants to post about.
To generate a post, we need at a minimum, one link to the original content.
Additionally, users can send followup links to be used as "additional contexts".
Lastly, they can provide a "quantity" number, indicating how many posts to generate. They do not need to supply this, as we'll set the default to 2.
</context>

<extraction-fields-context>
The content you need to extract is as follows:
- "original_link" This is the main link to use to generate content. If there is only one link provided, it will be used as the original link.
- "additional_contents" These are any followup links provided by the user. This is not required and can be left blank.
- "quantity" This is the number of posts to generate. If not provided, we'll default to 2.
</extraction-fields-context>

<instructions>
- Examine the message closely, and determine which links should be extracted and set in the proper fields.
- If no links are provided, you should set the "original_link" field to "NO_LINKS_PROVIDED"
</instructions>

Here is the Slack message:
<slack-message>
{SLACK_MESSAGE}
</slack-message>
`;

const extractionSchema = z.object({
  original_link: z
    .string()
    .describe(
      "The main link to use to generate content. Set to 'NO_LINKS_PROVIDED' if none provided.",
    ),
  additional_contents: z
    .array(z.string())
    .describe("Any followup links provided by the user.")
    .optional(),
  quantity: z
    .number()
    .describe(
      "The number of posts to generate. Defaults to 2 if no quantity specified in the message.",
    )
    .default(DEFAULT_POST_QUANTITY),
});

async function extractContentsFunc(
  messageText: string,
): Promise<Omit<RepurposedContent, "attachmentUrls"> | undefined> {
  const model = new ChatAnthropic({
    model: "claude-3-7-sonnet-latest",
    temperature: 0,
  }).bindTools(
    [
      {
        name: "extract_content",
        description: "Extract content from a Slack message.",
        schema: extractionSchema,
      },
    ],
    {
      tool_choice: "extract_content",
    },
  );

  const formattedPrompt = EXTRACT_CONTENT_PROMPT.replace(
    "{SLACK_MESSAGE}",
    messageText,
  );

  const result = await model.invoke(formattedPrompt);
  const args = result.tool_calls?.[0]?.args as z.infer<typeof extractionSchema>;

  if (!isValidUrl(args?.original_link)) {
    return undefined;
  }

  return {
    originalLink: args?.original_link,
    additionalContextLinks: args?.additional_contents,
    quantity: args?.quantity ?? DEFAULT_POST_QUANTITY,
  };
}

const extractContents = traceable(extractContentsFunc, {
  name: "extract_contents",
});

export async function extract(
  state: IngestRepurposedDataState,
): Promise<Partial<IngestRepurposedDataState>> {
  const extractedContents: RepurposedContent[] = [];

  for await (const message of state.slackMessages) {
    const messageText = message.text;
    const [content, attachmentUrls] = await Promise.all([
      extractContents(messageText),
      getPublicFileUrls(message.fileIds),
    ]);
    if (content) {
      extractedContents.push({ ...content, attachmentUrls });
    }
  }

  for await (const message of state.messages) {
    const messageText =
      typeof message.content === "string"
        ? message.content
        : message.content
            .filter((c) => c.type === "text" && "text" in c)
            .map((c: MessageContentText) => c.text)
            .join(" ");
    const content = await extractContents(messageText);
    if (content) {
      // TODO: Update Slack message handler to include fileIds
      extractedContents.push({ ...content, attachmentUrls: undefined });
    }
  }

  return {
    contents: extractedContents,
  };
}



================================================
FILE: src/agents/ingest-repurposed-data/nodes/ingest-slack.ts
================================================
import { IngestRepurposedDataState } from "../types.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { SlackClient } from "../../../clients/slack/client.js";

const getChannelIdFromConfig = async (
  config: LangGraphRunnableConfig,
): Promise<string | undefined> => {
  if (config.configurable?.repurposerSlackChannelId) {
    return config.configurable?.repurposerSlackChannelId;
  }

  throw new Error("Repurposer Slack channel ID not found in config.");
};

export async function ingestSlackMessages(
  state: IngestRepurposedDataState,
  config: LangGraphRunnableConfig,
): Promise<Partial<IngestRepurposedDataState>> {
  if (config.configurable?.skipIngest) {
    if (state.contents.length === 0) {
      throw new Error("Can not skip ingest with no links");
    }
    return {};
  }

  const channelId = await getChannelIdFromConfig(config);
  if (!channelId) {
    throw new Error("Channel ID not found");
  }

  const client = new SlackClient();
  const recentMessages = await client.getChannelMessages(channelId);

  return {
    slackMessages: recentMessages,
  };
}



================================================
FILE: src/agents/reflection/index.ts
================================================
import {
  Annotation,
  END,
  LangGraphRunnableConfig,
  START,
  StateGraph,
} from "@langchain/langgraph";
import {
  getReflectionsPrompt,
  putReflectionsPrompt,
} from "../../utils/reflections.js";
import { Client } from "@langchain/langgraph-sdk";

const ReflectionAnnotation = Annotation.Root({
  /**
   * The original post before edits were made.
   */
  originalPost: Annotation<string>,
  /**
   * The post after edits have been made.
   */
  newPost: Annotation<string>,
  /**
   * The user's response to the interrupt event
   * which triggered the reflection.
   */
  userResponse: Annotation<string>,
});

const UPDATE_INSTRUCTIONS = `Analyze the following to determine if rules prompt updates are needed:
1. Current rules prompt (current_prompt)
2. Generated social media post (session)
3. User feedback on the post (feedback)

If the user's feedback explicitly requests changes:
1. Create or update rules that directly address the feedback
2. Keep each rule clear, specific, and concise
3. If a new rule conflicts with an existing one, use the new rule
4. Only add rules that are explicitly mentioned in the user's feedback

Guidelines for updates:
- Do not infer or assume rules beyond what's explicitly stated
- Do not add rules based on implicit feedback
- Do not overgeneralize the feedback
- Combine existing rules if it improves clarity without losing specificity

Output only the updated rules prompt, with no additional context or instructions.`;

const WHEN_TO_UPDATE_INSTRUCTIONS = `You should update the prompt if the user's feedback is explicit, and can be generalized to apply to all future social media posts.
You should not update the prompt if the user's feedback does not explicitly request changes, or if the changes are not clear and specific enough to be applied consistently.`;

async function reflection(
  state: typeof ReflectionAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof ReflectionAnnotation.State>> {
  if (!config.store) {
    throw new Error("No store provided");
  }

  const langMemClient = new Client({
    apiUrl:
      "https://langmem-v0-544fccf4898a5e3c87bdca29b5f9ab21.us.langgraph.app",
    apiKey: process.env.LANGCHAIN_API_KEY,
  });

  const existingRules = await getReflectionsPrompt(config);

  const conversation = [{ role: "assistant", content: state.originalPost }];
  const feedback = {
    user_feedback: state.userResponse,
  };
  const threads = [[conversation, feedback]];

  const result = await langMemClient.runs.wait(null, "optimize_prompts", {
    input: {
      prompts: [
        {
          name: "Update Prompt",
          prompt: existingRules,
          when_to_update: WHEN_TO_UPDATE_INSTRUCTIONS,
          update_instructions: UPDATE_INSTRUCTIONS,
        },
      ],
      threads,
    },
    config: {
      configurable: { model: "claude-3-5-sonnet-latest", kind: "metaprompt" },
    },
  });

  const updated = (result as Record<string, any>).updated_prompts[0].prompt;

  await putReflectionsPrompt(config, updated);

  return {};
}

const reflectionWorkflow = new StateGraph(ReflectionAnnotation)
  .addNode("reflection", reflection)
  .addEdge(START, "reflection")
  .addEdge("reflection", END);

export const reflectionGraph = reflectionWorkflow.compile();
reflectionGraph.name = "Reflection Graph";



================================================
FILE: src/agents/reflection/prompts.ts
================================================
export const REFLECTION_PROMPT = `You are an AI assistant tasked with analyzing social media post revisions and user feedback to determine if a new rule should be created for future post modifications.
Your goal is to identify patterns in the changes requested by the user and decide if these changes should be applied automatically in the future.

You will be given three pieces of information:

1. The original social media post:
<original_post>
{ORIGINAL_POST}
</original_post>

2. The revised post:
<new_post>
{NEW_POST}
</new_post>

3. The user's response to the revision:
<user_response>
{USER_RESPONSE}
</user_response>

Carefully analyze these three elements, paying attention to the following:
1. What specific changes were made between the original and new post?
2. How did the user respond to these changes?
3. Is there a clear pattern or preference expressed by the user?
4. Could this preference be generalized into a rule for future posts?

Based on your analysis, decide if a new rule should be created. Consider the following:
1. Is the change specific enough to be applied consistently?
2. Would applying this change automatically improve future posts?
3. Is there any potential downside to always making this change?

If you determine that a new rule should be created, formulate it clearly and concisely. The rule should be specific enough to be applied consistently but general enough to cover similar situations in the future.
You should not be generating a rule which is specific to this post, like business logic. The rule, if created, should be applicable to any future post.

Provide your analysis and decision in the following format:

<analysis>
[Your detailed analysis of the changes and user response]
</analysis>

<decision>
[Your decision on whether a new rule should be created, along with your reasoning]
</decision>

If applicable, call the 'new_rule' tool to create the new rule. If no new rule is needed, simply write "No new rule required."

Remember to be thorough in your analysis, clear in your decision-making, and precise in your rule formulation if one is needed.`;

export const UPDATE_RULES_PROMPT = `You are an AI assistant tasked with updating a ruleset based on the addition of a new rule. Your goal is to analyze the new rule in relation to the existing rules and provide an updated ruleset.

First, review the existing rules:
<existing_rules>
{EXISTING_RULES}
</existing_rules>

Now, consider the new rule:
<new_rule>
{NEW_RULE}
</new_rule>

Analyze the new rule in relation to the existing rules by considering the following:
1. Can this rule be combined with existing rules to cover similar situations?
2. Has this rule already been covered by existing rules?
3. Does this rule conflict with existing rules?

Follow these guidelines when updating the ruleset:
1. If the new rule conflicts with an existing rule, remove the existing conflicting rule and prioritize the new rule.
2. If the new rule is already covered by an existing rule, remove the new rule or combine them.
3. If the new rule can be combined with existing rules, combine them to cover similar situations.

Before providing the updated ruleset, use a <scratchpad> to think through your analysis and decision-making process. Consider each existing rule in relation to the new rule, and explain your reasoning for any changes you plan to make.

After your analysis, provide the updated ruleset in the following format:
<updated_ruleset>
1. [First updated or new rule]
2. [Second updated or new rule]
...
n. [Last updated or new rule]
</updated_ruleset>

Following the updated ruleset, provide a brief explanation of the changes made and the reasoning behind them in <explanation> tags.`;



================================================
FILE: src/agents/repurposer/index.ts
================================================
import { END, START, StateGraph } from "@langchain/langgraph";
import {
  RepurposerGraphAnnotation,
  RepurposerInputAnnotation,
  RepurposerConfigurableAnnotation,
} from "./types.js";
import { extractContent } from "./nodes/extract-content/index.js";
import { generateCampaignPlan } from "./nodes/generate-campaign-plan.js";
import { generatePosts } from "./nodes/generate-posts.js";
import { generateReportGraph } from "../generate-report/index.js";
import { validateImages } from "./nodes/validate-images.js";
import { startInterruptGraphRuns } from "./nodes/start-interrupt-graph.js";

const repurposerBuilder = new StateGraph(
  {
    stateSchema: RepurposerGraphAnnotation,
    input: RepurposerInputAnnotation,
  },
  RepurposerConfigurableAnnotation,
)
  .addNode("extractContent", extractContent)
  .addNode("validateImages", validateImages)
  .addNode("generateReport", generateReportGraph)
  .addNode("generateCampaignPlan", generateCampaignPlan)
  .addNode("generatePosts", generatePosts)
  .addNode("startInterruptGraphRuns", startInterruptGraphRuns)

  .addEdge(START, "extractContent")
  .addEdge("extractContent", "validateImages")
  .addEdge("validateImages", "generateReport")
  .addEdge("generateReport", "generateCampaignPlan")
  .addEdge("generateCampaignPlan", "generatePosts")
  .addEdge("generatePosts", "startInterruptGraphRuns")
  .addEdge("startInterruptGraphRuns", END);

export const repurposerGraph = repurposerBuilder.compile();

repurposerGraph.name = "Repurposer Graph";



================================================
FILE: src/agents/repurposer/types.ts
================================================
import { Annotation, END } from "@langchain/langgraph";
import { AdditionalContext, DateType, RepurposedPost } from "../types.js";
import { DEFAULT_POST_QUANTITY } from "../ingest-repurposed-data/constants.js";
import { POST_TO_LINKEDIN_ORGANIZATION } from "../generate-post/constants.js";

export type Image = { imageUrl: string; mimeType: string; index: number };

export const RepurposerGraphAnnotation = Annotation.Root({
  /**
   * The link to the original post/content the new campaign is based on.
   */
  originalLink: Annotation<string>,
  /**
   * The original content input as a string. Contains all extracted/scraped
   * content from the original link.
   */
  originalContent: Annotation<string>,
  /**
   * The links to use to generate a series of posts.
   */
  contextLinks: Annotation<string[] | undefined>,
  /**
   * The additional context to use for generating posts.
   */
  additionalContexts: Annotation<AdditionalContext[] | undefined>,
  /**
   * The pageContents field is required as it's the input to the generateReportGraph.
   * It will contain a string, combining the above originalContent and additionalContexts.
   */
  pageContents: Annotation<string[]>,
  /**
   * The quantity of posts to generate.
   */
  quantity: Annotation<number>({
    reducer: (_state, update) => update,
    default: () => DEFAULT_POST_QUANTITY,
  }),
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  reports: Annotation<
    Array<{
      report: string;
      keyDetails: string;
    }>
  >({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  /**
   * The image options extracted from the original/additional contexts.
   */
  imageOptions: Annotation<string[]>(),
  /**
   * The generated campaign plan to generate posts from.
   */
  campaignPlan: Annotation<string>,
  /**
   * The generated posts for LinkedIn/Twitter.
   */
  posts: Annotation<RepurposedPost[]>,
  /**
   * A human response if the user submitted feedback after the interrupt.
   */
  userResponse: Annotation<string | undefined>(),
  /**
   * The next node to execute.
   */
  next: Annotation<
    "rewritePosts" | "schedulePosts" | "unknownResponse" | typeof END
  >(),
  /**
   * The images to use for the posts. Each 'index' field in the images corresponds to the 'index' field in the posts.
   */
  images: Annotation<Image[]>({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  /**
   * The date to schedule the posts for. Only one priority level can be specified.
   * If a date is specified, every post will be posted on that date.
   * (this is only intended to be used for testing/single posts)
   */
  scheduleDate: Annotation<DateType | undefined>(),
  /**
   * The number of weeks between each post.
   */
  numWeeksBetween: Annotation<number>({
    reducer: (_state, update) => update,
    default: () => 1,
  }),
});

export type RepurposerState = typeof RepurposerGraphAnnotation.State;
export type RepurposerUpdate = typeof RepurposerGraphAnnotation.Update;

export const RepurposerInputAnnotation = Annotation.Root({
  /**
   * The link to the original post/content the new campaign is based on.
   */
  originalLink: Annotation<string>,
  /**
   * The links to additional contexts to use for generating the new posts.
   */
  contextLinks: Annotation<string[] | undefined>,
  /**
   * The quantity of posts to generate.
   */
  quantity: Annotation<number>,
});

export const RepurposerConfigurableAnnotation = Annotation.Root({
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
});



================================================
FILE: src/agents/repurposer/utils.ts
================================================
export function formatReportForPrompt(report: {
  report: string;
  keyDetails: string;
}): string {
  return `<key-details>\n${report.keyDetails}\n</key-details>\n\n<report>\n${report.report}\n</report>`;
}



================================================
FILE: src/agents/repurposer/nodes/generate-campaign-plan.ts
================================================
import { ChatOpenAI } from "@langchain/openai";
import { RepurposerState } from "../types.js";
import { formatReportForPrompt } from "../utils.js";

const GENERATE_CAMPAIGN_PLAN_PROMPT = `You're a highly skilled marketing professional, working on crafting a thoughtful and detailed marketing campaign plan for a new series of posts for your Twitter and LinkedIn pages.

You originally publicized this content on your socials (this could be your blog, Twitter/LinkedIn page, or a combination). Here is the content you already published:

For this new marketing campaign, you wrote a highly detailed marketing report on the content, using the original content, and some additional resources. This report contains key details extracted from the original, and new context.

Your task is to write a detailed marketing campaign plan for {NUM_POSTS} new {POST_OR_POSTS}.
For each new post you intend to write, ensure it does not simply repeat the original content, but instead provides new context from the report, focusing on areas which were not originally covered.

You should respond in the following format with the marketing campaign plan:
<response-format-instructions>
For each post in the campaign, ensure you include at least one key point from the marketing report to be the subject of the post.
You are allowed to include multiple key points, but be aware that too many key points may feel repetitive, or too dense for the readers to easily grasp.
It is important to remember that if generating more than one post, the campaign should flow seamlessly between each post, without repeating or duplicating content.

Ensure each post in the campaign is wrapped in '<post>' tags. This MUST have opening and closing tags.
Inside these '<post>' tags, it should have three sub-tags:
1. "<header>" This should be a short header, one to two sentence summary of what the post should cover.
2. "<details>" This should also be a short summary, referencing key detail(s) from the marketing report this post will be focused on.
3. "<index>" This should be the index of the post, it should be an ordered list starting from '1'.

For example:
<post>
<header>
some header text here
</header>
<details>
some details text here
</details>
<index>
1
</index>
</post>

<post>
<header>
some header text here
</header>
<details>
some details text here
</details>
<index>
2
</index>
</post>

<post>
<header>
some header text here
</header>
<details>
some details text here
</details>
<index>
3
</index>
</post>
</response-format-instructions>

<original-content>
{ORIGINAL_CONTENT}
</original-content>

<new-marketing-report>
{NEW_MARKETING_REPORT}
</new-marketing-report>`;

export async function generateCampaignPlan(
  state: RepurposerState,
): Promise<Partial<RepurposerState>> {
  const model = new ChatOpenAI({
    model: "o1",
    streaming: false,
  });

  const formattedUserPrompt = GENERATE_CAMPAIGN_PLAN_PROMPT.replace(
    `{NUM_POSTS}`,
    state.quantity.toString(),
  )
    .replace("{POST_OR_POSTS}", state.quantity === 1 ? "post" : "posts")
    .replace("{ORIGINAL_CONTENT}", state.originalContent)
    .replace("{NEW_MARKETING_REPORT}", formatReportForPrompt(state.reports[0]));

  const response = await model.invoke(formattedUserPrompt);

  return {
    campaignPlan: response.content as string,
  };
}



================================================
FILE: src/agents/repurposer/nodes/generate-posts.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { RepurposerState } from "../types.js";
import { z } from "zod";
import { formatReportForPrompt } from "../utils.js";

export const POST_STRUCTURE_INSTRUCTIONS = `The post should have three main sections, outlined below:
<structure-instructions>

<section key="1">
The first part of the post is the header. This should be very short, no more than 5 words, and should include one to two emojis, and the name of the content provided. If the marketing report does not specify a name, you should get creative and come up with a catchy title for it.
</section>

<section key="2">
This section will contain the main content of the post. The post body should contain a concise, high-level overview of the key point from the marketing report which was selected to be the focus of this post.
It should focus on what the content does, or the problem it solves. Also include details on how the content implements LangChain's product(s) and why these products are important to the application.
Ensure this is short, no more than 3 sentences. Optionally, if the content is very technical, you may include bullet points covering the main technical aspects of the content.
</section>

<section key="3">
The final section of the post should contain a call to action. This should be a short sentence that encourages the reader to click the link to the content being promoted. Optionally, you can include an emoji here.
</section>

</structure-instructions>`;

export const GENERATE_POST_PROMPT = `You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for LinkedIn and Twitter pages.
You've been provided with a detailed marketing campaign report on content which you have already publicized, but want to renew interest in, and repurpose.

The following are examples of LinkedIn/Twitter posts that have done well, and you should use them as style inspiration for your post:
<examples>
${getPrompts().tweetExamples}
</examples>

Now that you've seen some examples, lets's cover the structure of each LinkedIn/Twitter post you will write:
${POST_STRUCTURE_INSTRUCTIONS}

This structure should ALWAYS be followed. And remember, the shorter and more engaging the post, the better (your yearly bonus depends on this!!).

Here are a set of rules and guidelines you should strictly follow when creating each LinkedIn/Twitter post:
<rules>
${getPrompts().postContentRules}
</rules>

Ensure you read over each plan item (or the only one one, depending on how many posts it requests you write) carefully, and ensure your post(s) flow well together.

Lastly, you should follow the process below when writing the LinkedIn/Twitter {POST_OR_POSTS}:
<writing-process>
Step 1. First, read over the marketing report VERY thoroughly. (the user will provide this)
Step 2. Once finished reading the marketing report, inspect the post campaign plan (the user will also provide this) so you have a deep understanding of each post you will need to write.
Step 3. Lastly, write {NUM_POSTS} LinkedIn/Twitter {POST_OR_POSTS}. Use all of the information provided above to help you write the post. Ensure you write only ONE post for both LinkedIn and Twitter.
</writing-process>

Given these examples, rules, and the content provided by the user, curate a LinkedIn/Twitter post that is engaging and follows the structure of the examples provided.
Ensure you do NOT make information up, or make assumptions about the content. Base your {POST_OR_POSTS} on the content and facts provided above.`;

const USER_PROMPT = `Hello. Please write {NUM_POSTS} new {POST_OR_POSTS} for my LinkedIn and Twitter pages.

Here is the marketing report you should use as the master context for all of the LinkedIn/Twitter posts you will write:
<report>
{MARKETING_REPORT}
</report>

Here is the plan for the new marketing campaign. You should generate {NUM_POSTS} new {POST_OR_POSTS} based on this plan:
<plan>
{CAMPAIGN_PLAN}
</plan>

Take in all of the provided context, and write {NUM_POSTS} new {POST_OR_POSTS} I, and you, would be happy with!`;

function formatUserPrompt(report: string, plan: string, numPosts: number) {
  const postOrPosts = numPosts === 1 ? "post" : "posts";
  return USER_PROMPT.replaceAll("{NUM_POSTS}", numPosts.toString())
    .replaceAll("{POST_OR_POSTS}", postOrPosts)
    .replace("{MARKETING_REPORT}", report)
    .replace("{CAMPAIGN_PLAN}", plan);
}

export async function generatePosts(
  state: RepurposerState,
): Promise<Partial<RepurposerState>> {
  const { quantity: numPosts } = state;
  const postOrPosts = numPosts === 1 ? "post" : "posts";

  const postsSchema = z.object({
    posts: z
      .array(
        z
          .string()
          .describe(
            "The individual post for the LinkedIn/Twitter page. Ensure this post follows all of the instructions outlined in the prompt, and uses the context provided via the marketing report and post campaign plan.",
          ),
      )
      .describe(
        `The LinkedIn/Twitter  ${postOrPosts} you should write.${numPosts > 1 ? " Ensure each post is in the proper order they should be published in." : ""}`,
      ),
  });

  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0.5,
  }).bindTools([
    {
      name: `write_${postOrPosts}`,
      description: `Write ${numPosts} LinkedIn/Twitter ${postOrPosts} based on the marketing report and post campaign plan provided.`,
      schema: postsSchema,
    },
  ]);

  const formattedSystemPrompt = GENERATE_POST_PROMPT.replaceAll(
    "{NUM_POSTS}",
    numPosts.toString(),
  ).replaceAll("{POST_OR_POSTS}", postOrPosts);

  const userPrompt = formatUserPrompt(
    formatReportForPrompt(state.reports[0]),
    state.campaignPlan,
    numPosts,
  );

  const result = await model.invoke([
    {
      role: "system",
      content: formattedSystemPrompt,
    },
    {
      role: "user",
      content: userPrompt,
    },
  ]);

  return {
    posts: (result.tool_calls?.[0].args?.posts as string[]).map(
      (content, index) => ({
        content,
        index,
      }),
    ),
  };
}



================================================
FILE: src/agents/repurposer/nodes/start-interrupt-graph.ts
================================================
import { Client } from "@langchain/langgraph-sdk";
import { RepurposerState, RepurposerUpdate } from "../types.js";

export async function startInterruptGraphRuns(
  state: RepurposerState,
): Promise<RepurposerUpdate> {
  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const runsPromise = await Promise.all(
    state.posts.map(async (post, index) => {
      const image = state.images.find((i) => i.index === index);
      const { thread_id } = await client.threads.create();
      await client.runs.create(thread_id, "repurposer_post_interrupt", {
        input: {
          post: post.content,
          image,
          originalLink: state.originalLink,
          originalContent: state.originalContent,
          contextLinks: state.contextLinks,
          additionalContexts: state.additionalContexts,
          reports: state.reports,
          imageOptions: state.imageOptions,
          posts: state.posts,
          campaignPlan: state.campaignPlan,
        },
      });
    }),
  );

  await Promise.all(runsPromise);

  return {};
}



================================================
FILE: src/agents/repurposer/nodes/validate-images.ts
================================================
import { traceable } from "langsmith/traceable";
import { ChatVertexAI } from "@langchain/google-vertexai-web";
import { chunkArray, imageUrlToBuffer, isValidUrl } from "../../utils.js";
import { RepurposerState } from "../types.js";
import { getImageMessageContents } from "../../../utils/image-message.js";

const VALIDATE_IMAGES_PROMPT = `You are an advanced AI assistant tasked with validating image options to be included in context when generating a marketing report for a social media campaign.
Your task is to identify which images extracted from a blog/webpage are relevant to the marketing campaign, and will be useful when writing a marketing report.

First, carefully read and analyze the full text content extracted from the blog/webpage:

<page-contents>
{PAGE_CONTENTS}
</page-contents>

To determine which images are relevant, consider the following criteria:
1. Does the image directly illustrate a key point or theme from the webpage?
2. Does the image represent any products, services, or concepts mentioned throughout the webpage?
3. Is the image highly relevant, or a technical illustration? 

You should NEVER include images which are:
- Only containing logos, icons, or profile pictures.
- Personal, or non-essential images from a business perspective.
- Small, low-resolution images. These are likely accidentally included in the marketing campaign and should be excluded.

You will be presented with a list of image options. Your task is to identify which of these images are relevant to the marketing campaign based on the criteria above.

Provide your response in the following format:
1. <analysis> tag: Briefly explain your thought process for each image, referencing specific elements from the webpage.
2. <relevant_indices> tag: List the indices of the relevant images, starting from 0, separated by commas.

Ensure you ALWAYS WRAP your analysis and relevant indices inside the <analysis> and <relevant_indices> tags, respectively. Do not only prefix, but ensure they are wrapped completely.

Remember to carefully consider each image in relation to the webpage content.
Be thorough in your analysis, but focus on the most important factors that determine relevance.
If an image is borderline, err on the side of inclusion.

Provide your complete response within <answer> tags.
`;

export function parseResult(result: string): number[] {
  if (result.includes("<relevant_indices>")) {
    let relevantIndicesText = "";
    if (result.includes("</relevant_indices>")) {
      relevantIndicesText = result
        .split("<relevant_indices>")[1]
        ?.split("</relevant_indices>")[0]
        ?.replaceAll(" ", "")
        .replaceAll("\n", "");
    } else if (result.includes("</answer>")) {
      relevantIndicesText = result
        .split("<relevant_indices>")[1]
        ?.split("</answer>")[0]
        ?.replaceAll(" ", "")
        .replaceAll("\n", "");
    }

    if (relevantIndicesText?.length) {
      if (!relevantIndicesText.includes(",")) {
        // Add a comma so the code below which parses the string into an array will work.
        relevantIndicesText += ",";
      }

      const indices = relevantIndicesText
        .split(",")
        .map((s) => s.trim())
        .filter((s) => s.length > 0)
        .map(Number);
      if (indices.length && indices.every((n) => !isNaN(n))) {
        return indices;
      }
    }
  }

  const match = result.match(
    /<relevant_indices>\s*([\d,\s]*?)\s*<\/relevant_indices>/s,
  );
  if (!match) return [];

  return match[1]
    .split(",")
    .map((s) => s.trim())
    .filter((s) => s.length > 0)
    .map(Number)
    .filter((n) => !isNaN(n));
}

const YOUTUBE_THUMBNAIL_URL = "https://i.ytimg.com/";

function removeProtectedUrls(imageOptions: string[]): string[] {
  return imageOptions.filter(
    (fileUri) =>
      (!process.env.SUPABASE_URL ||
        !fileUri.startsWith(process.env.SUPABASE_URL)) &&
      !fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );
}

function getProtectedUrls(imageOptions: string[]): string[] {
  return imageOptions.filter(
    (fileUri) =>
      (process.env.SUPABASE_URL &&
        fileUri.startsWith(process.env.SUPABASE_URL)) ||
      fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );
}

async function filterImageUrls(imageOptions: string[]): Promise<{
  imageOptions: string[];
  returnEarly: boolean;
}> {
  const imagesWithoutProtected = imageOptions?.length
    ? removeProtectedUrls(imageOptions)
    : [];

  if (!imagesWithoutProtected?.length) {
    return {
      imageOptions,
      returnEarly: true,
    };
  }

  const validImageUrlPromises = imagesWithoutProtected.filter(
    async (imgUrl) => {
      if (!isValidUrl(imgUrl)) return false;

      try {
        // Use this as a way to validate the image exists
        const { contentType } = await imageUrlToBuffer(imgUrl);
        if (contentType.startsWith("image/")) {
          return true;
        }
      } catch (_) {
        // no-op
      }
      return false;
    },
  );

  const validImageUrls = await Promise.all(validImageUrlPromises);
  if (!validImageUrls.length) {
    const protectedImageUrls = imageOptions?.length
      ? getProtectedUrls(imageOptions)
      : [];
    return {
      imageOptions: [...protectedImageUrls],
      returnEarly: true,
    };
  }

  return {
    imageOptions: validImageUrls,
    returnEarly: false,
  };
}

const extractIndicesFromText = traceable(
  (text: string): number[] => {
    const chunkAnalysis = parseResult(text);
    return chunkAnalysis;
  },
  {
    name: "extractIndicesFromText",
  },
);

export async function validateImages(state: RepurposerState): Promise<{
  imageOptions: string[] | undefined;
}> {
  const { imageOptions, originalContent } = state;

  const model = new ChatVertexAI({
    model: "gemini-2.0-pro-exp-02-05",
    temperature: 0,
  });

  const { imageOptions: imagesWithoutProtected, returnEarly } =
    await filterImageUrls(imageOptions ?? []);

  if (returnEarly || !imagesWithoutProtected?.length) {
    return {
      imageOptions: imagesWithoutProtected,
    };
  }

  // Split images into chunks of 10
  const imageChunks = chunkArray(imagesWithoutProtected, 10);
  let allRelevantIndices: number[] = [];
  let baseIndex = 0;

  const formattedSystemPrompt = VALIDATE_IMAGES_PROMPT.replace(
    "{PAGE_CONTENTS}",
    originalContent,
  );

  // Process each chunk
  for (const imageChunk of imageChunks) {
    const imageMessages = await getImageMessageContents(imageChunk, baseIndex);

    if (!imageMessages.length) {
      continue;
    }

    try {
      const response = await model.invoke([
        {
          role: "system",
          content: formattedSystemPrompt,
        },
        {
          role: "user",
          content: imageMessages,
        },
      ]);

      allRelevantIndices = [
        ...allRelevantIndices,
        ...(await extractIndicesFromText(response.content as string)),
      ];
    } catch (error) {
      console.error(
        `Failed to validate images.\nImage URLs: ${imageMessages
          .filter((m) => m.fileUri)
          .map((m) => m.fileUri)
          .join(", ")}\n\nError:`,
        error,
      );
    }

    baseIndex += imageChunk.length;
  }

  const protectedUrls = imageOptions?.filter(
    (fileUri) =>
      (process.env.SUPABASE_URL &&
        fileUri.startsWith(process.env.SUPABASE_URL)) ||
      fileUri.startsWith(YOUTUBE_THUMBNAIL_URL),
  );

  // Keep only the relevant images (those whose indices are in allRelevantIndices)
  return {
    imageOptions: [
      ...(protectedUrls || []),
      ...(imagesWithoutProtected || []).filter((_, index) =>
        allRelevantIndices.some((i) => i === index),
      ),
    ],
  };
}



================================================
FILE: src/agents/repurposer/nodes/extract-content/get-url-contents.ts
================================================
import { TweetV2, TweetV2SingleResult } from "twitter-api-v2";
import { getTwitterClient } from "../../../../clients/twitter/client.js";
import {
  extractTweetId,
  extractAllImageUrlsFromMarkdown,
  getUrlType,
} from "../../../utils.js";
import { FireCrawlLoader } from "@langchain/community/document_loaders/web/firecrawl";
import {
  getFullThreadText,
  getMediaUrls,
  resolveAndReplaceTweetTextLinks,
} from "../../../../clients/twitter/utils.js";
import { getVideoSummary } from "../../../shared/youtube/video-summary.js";
import { getImagesFromFireCrawlMetadata } from "../../../../utils/firecrawl.js";

async function getGeneralContent(url: string): Promise<{
  contents: string;
  imageUrls: string[];
}> {
  try {
    const loader = new FireCrawlLoader({
      url,
      mode: "scrape",
      params: {
        formats: ["markdown"],
      },
    });

    const docs = await loader.load();

    const metadataImageUrls = docs.flatMap(
      (d) => getImagesFromFireCrawlMetadata(d.metadata) || [],
    );
    const imageUrlsFromText = extractAllImageUrlsFromMarkdown(
      docs[0].pageContent,
    );

    return {
      contents: `<webpage-content url="${url}">\n${docs[0].pageContent}\n</webpage-content>`,
      imageUrls: Array.from(
        new Set([...metadataImageUrls, ...imageUrlsFromText]),
      ),
    };
  } catch (e) {
    throw new Error(`Failed to fetch content from ${url}.` + e);
  }
}

async function getYouTubeContent(url: string): Promise<{
  contents: string;
  imageUrls: string[];
}> {
  try {
    const { summary, thumbnail } = await getVideoSummary(url);
    return {
      contents: `<youtube-video-summary>\n${summary}</youtube-video-summary>`,
      imageUrls: thumbnail ? [thumbnail] : [],
    };
  } catch (e) {
    throw new Error(`Failed to get YouTube summary for URL ${url}` + e);
  }
}

async function getTwitterContent(url: string): Promise<{
  contents: string;
  imageUrls: string[];
}> {
  const tweetId = extractTweetId(url);
  if (!tweetId) {
    throw new Error("Failed to extract tweet ID from link:" + url);
  }

  const twitterClient = await getTwitterClient();
  let tweetContent: TweetV2SingleResult | undefined;

  try {
    tweetContent = await twitterClient.getTweet(tweetId);
    if (!tweetContent) {
      throw new Error("No tweet content returned from Twitter API.");
    }
  } catch (e: any) {
    throw new Error(`Failed to get tweet content from ${url}.` + e);
  }

  const threadReplies: TweetV2[] = [];
  if (tweetContent.includes?.users?.length) {
    const username = tweetContent.includes?.users[0]?.username;
    threadReplies.push(
      ...(await twitterClient.getThreadReplies(tweetId, username)),
    );
  }

  const mediaUrls = await getMediaUrls(tweetContent, threadReplies);
  const tweetContentText = getFullThreadText(tweetContent, threadReplies);

  const { content, externalUrls } =
    await resolveAndReplaceTweetTextLinks(tweetContentText);

  const externalUrlPromises = externalUrls.map(async (url) => {
    const type = getUrlType(url);

    try {
      if (type === "general") {
        return getGeneralContent(url);
      } else if (type === "youtube") {
        return getYouTubeContent(url);
      }
    } catch (e) {
      console.error(`Failed to get content from ${url} extracted in Tweet.`, e);
    }

    return {
      contents: "",
      imageUrls: [],
    };
  });
  const externalUrlsContent = await Promise.all(externalUrlPromises);

  return {
    contents: `<twitter-thread>
  <post>
    ${content}
  </post>
  <external-urls-content>
    ${externalUrlsContent.map((c, idx) => `<external-content index="${idx}">\n${c.contents}\n</external-content>`).join("\n")}
  </external-urls-content>
</twitter-thread>`,
    imageUrls: mediaUrls,
  };
}

/**
 * Extracts the contents from a given URL. This can be either a blog post, tweet, or YouTube video.
 */
export async function getUrlContents(url: string): Promise<{
  contents: string;
  imageUrls: string[];
}> {
  const type = getUrlType(url);

  if (type === "general" || type === "luma") {
    return getGeneralContent(url);
  } else if (type === "youtube") {
    return getYouTubeContent(url);
  } else if (type === "twitter") {
    return getTwitterContent(url);
  }

  throw new Error(`Unsupported URL type: ${type}`);
}



================================================
FILE: src/agents/repurposer/nodes/extract-content/index.ts
================================================
import { RepurposerState } from "../../types.js";
import { getUrlContents } from "./get-url-contents.js";

export async function extractContent(
  state: RepurposerState,
): Promise<Partial<RepurposerState>> {
  const { contents: originalContent, imageUrls } = await getUrlContents(
    state.originalLink,
  );
  const originalContentPrompt = `Here is the original content. This content is the basis of the new marketing campaign. This post has already been shared, so use this as a base for the new campaign building on top of this post:
  
<original-post-content>
${originalContent}
</original-post-content>`;

  if (!state.contextLinks?.length) {
    return {
      pageContents: [originalContentPrompt],
      imageOptions: imageUrls,
      originalContent,
    };
  }

  const additionalContextPromises = state.contextLinks.map(async (link) => {
    const { contents, imageUrls } = await getUrlContents(link);
    return {
      content: contents,
      link,
      imageUrls,
    };
  });
  const additionalContexts = await Promise.all(additionalContextPromises);

  const masterPageContent = `${originalContentPrompt}

Here is additional related context you should use in the new marketing campaign. This context has not been released yet, so use this as the new context for this marketing campaign:
<additional-contexts>
${additionalContexts
  .map(
    ({ content, link }, index) => `<context link="${link}" index="${index}">
${content}
</context>`,
  )
  .join("\n")}
</additional-contexts>`;

  return {
    pageContents: [masterPageContent],
    imageOptions: [
      ...imageUrls,
      ...additionalContexts.flatMap((c) => c.imageUrls || []),
    ],
    originalContent,
    additionalContexts: additionalContexts.map((c) => ({
      content: c.content,
      link: c.link,
    })),
  };
}



================================================
FILE: src/agents/repurposer/tests/graph.int.test.ts
================================================
import * as ls from "langsmith/jest";
import { SimpleEvaluator } from "langsmith/jest";
import { repurposerGraph } from "../index.js";

const tweetEvaluator: SimpleEvaluator = () => {
  return {
    key: "content_extraction",
    score: 1,
  };
};

ls.describe("SMA - Repurposer", () => {
  ls.test(
    "Can extract content",
    {
      inputs: {
        originalLink: "https://x.com/LangChainAI/status/1857117443065540707",
        quantity: 3,
      },
      expected: {},
    },
    async ({ inputs }) => {
      const result = await repurposerGraph.nodes.extractContent.invoke(
        inputs as any,
      );

      console.log("Result\n");
      console.dir(result, { depth: null });

      await ls.expect(result).evaluatedBy(tweetEvaluator).toBe(1);
      return result;
    },
  );
});



================================================
FILE: src/agents/repurposer/tests/images.test.ts
================================================
import { test, expect } from "@jest/globals";
import { parseResult } from "../nodes/validate-images.js";

test("Can extract indices from a string", () => {
  const str =
    '```\n<answer>\n1.  <analysis>\n    Image 16: This image is a bar graph showing the "buzziest AI agent applications," with Cursor, Perplexity, and Replit as the top three. This directly relates to the section "Agent success stories: Cursor steals the spotlight" in the text, making it highly relevant.\n    Image 17: This image is a decorative graphic and doesn\'t provide any specific information related to the content of the webpage. It should be excluded.\n    Image 18: This image, similar to image 17, is a decorative graphic and doesn\'t offer any relevant information. It should also be excluded.\n\n2.  <relevant_indices>\n    16\n</answer>\n```\n';

  const indices = parseResult(str);
  expect(indices).toEqual([16]);
});



================================================
FILE: src/agents/repurposer-post-interrupt/index.ts
================================================
import { END, START, StateGraph } from "@langchain/langgraph";
import {
  RepurposerPostInterruptAnnotation,
  RepurposerPostInterruptConfigurableAnnotation,
  RepurposerPostInterruptState,
} from "./types.js";
import { updateScheduledDate } from "../shared/nodes/update-scheduled-date.js";
import { schedulePost } from "../shared/nodes/generate-post/schedule-post.js";
import { rewritePost } from "./nodes/rewrite-posts.js";
import { humanNode } from "./nodes/human-node/index.js";

function rewriteOrEndConditionalEdge(
  state: RepurposerPostInterruptState,
): "rewritePost" | "schedulePost" | "humanNode" | typeof END {
  if (!state.next) {
    return END;
  }

  if (state.next === "unknownResponse") {
    // If the user's response is unknown, we should route back to the human node.
    return "humanNode";
  }
  return state.next;
}

const workflow = new StateGraph(
  RepurposerPostInterruptAnnotation,
  RepurposerPostInterruptConfigurableAnnotation,
)
  // Interrupts the node for human in the loop.
  .addNode("humanNode", humanNode)
  // Schedules the post for Twitter/LinkedIn.
  .addNode("schedulePost", schedulePost)
  // Rewrite a post based on the user's response.
  .addNode("rewritePost", rewritePost)
  // Updated the scheduled date from the natural language response from the user.
  .addNode("updateScheduleDate", updateScheduledDate)
  .addEdge(START, "humanNode")
  .addConditionalEdges("humanNode", rewriteOrEndConditionalEdge, [
    "rewritePost",
    "schedulePost",
    "updateScheduleDate",
    "humanNode",
    END,
  ])
  // Always route back to `humanNode` if the post was re-written or date was updated.
  .addEdge("rewritePost", "humanNode")
  .addEdge("updateScheduleDate", "humanNode")

  // Always end after scheduling the post.
  .addEdge("schedulePost", END);

export const repurposerPostInterruptGraph = workflow.compile();
repurposerPostInterruptGraph.name = "Repurposer Post Interrupt Graph";



================================================
FILE: src/agents/repurposer-post-interrupt/types.ts
================================================
import { Annotation, END } from "@langchain/langgraph";
import {
  AdditionalContext,
  DateType,
  Image,
  RepurposedPost,
} from "../types.js";
import { POST_TO_LINKEDIN_ORGANIZATION } from "../generate-post/constants.js";

export const RepurposerPostInterruptAnnotation = Annotation.Root({
  /**
   * The link to the original post/content the new campaign is based on.
   */
  originalLink: Annotation<string>,
  /**
   * The original content input as a string. Contains all extracted/scraped
   * content from the original link.
   */
  originalContent: Annotation<string>,
  /**
   * The links to use to generate a series of posts.
   */
  contextLinks: Annotation<string[] | undefined>,
  /**
   * The additional context to use for generating posts.
   */
  additionalContexts: Annotation<AdditionalContext[] | undefined>,
  /**
   * The pageContents field is required as it's the input to the generateReportGraph.
   * It will contain a string, combining the above originalContent and additionalContexts.
   */
  pageContents: Annotation<string[]>,
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  reports: Annotation<
    Array<{
      report: string;
      keyDetails: string;
    }>
  >({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  /**
   * The image options extracted from the original/additional contexts.
   */
  imageOptions: Annotation<string[]>(),
  /**
   * The generated campaign plan to generate posts from.
   */
  campaignPlan: Annotation<string>,
  /**
   * The generated posts for LinkedIn/Twitter.
   */
  posts: Annotation<RepurposedPost[]>,
  /**
   * The generated post for LinkedIn/Twitter.
   */
  post: Annotation<string>,
  /**
   * A human response if the user submitted feedback after the interrupt.
   */
  userResponse: Annotation<string | undefined>,
  /**
   * The next node to execute.
   */
  next: Annotation<
    "rewritePost" | "schedulePost" | "unknownResponse" | typeof END
  >(),
  /**
   * The image to use for the post.
   */
  image: Annotation<Image | undefined>,
  /**
   * The date to schedule the posts for. Only one priority level can be specified.
   * If a date is specified, every post will be posted on that date.
   * (this is only intended to be used for testing/single posts)
   */
  scheduleDate: Annotation<DateType | undefined>(),
});

export type RepurposerPostInterruptState =
  typeof RepurposerPostInterruptAnnotation.State;
export type RepurposerPostInterruptUpdate =
  typeof RepurposerPostInterruptAnnotation.Update;

export const RepurposerPostInterruptConfigurableAnnotation = Annotation.Root({
  /**
   * Whether to post to the LinkedIn organization or the user's profile.
   * If true, [LINKEDIN_ORGANIZATION_ID] is required.
   */
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
});



================================================
FILE: src/agents/repurposer-post-interrupt/nodes/rewrite-posts.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";
import { formatReportForPrompt } from "../../repurposer/utils.js";
import {
  RepurposerPostInterruptState,
  RepurposerPostInterruptUpdate,
} from "../types.js";

const REWRITE_POST_PROMPT = `<context>
You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for your LinkedIn and Twitter pages.
You wrote a series of posts for a marketing campaign for your LinkedIn and Twitter pages, however your boss has asked for some changes to one of the posts to be made before they can be published.
You're provided with the original campaign plan, and marketing report used to initially write the collection of posts.
Use this plan to guide your decisions, however ensure you weigh your boss's requests above the plan if they contradict each other.
</context>

<original-posts>
{ORIGINAL_POSTS}
</original-posts>

<original-campaign-plan>
{ORIGINAL_CAMPAIGN_PLAN}
</original-campaign-plan>

<marketing-report>
{MARKETING_REPORT}
</marketing-report>

<post-to-update>
{POST_TO_UPDATE}
</post-to-update>

<instructions>
Listen to your boss closely, and make the necessary changes to the post.
You should ONLY update the post inside the <post-to-update> tag.
Ensure you keep the other posts in this campaign in mind when making updates, as we do not want to duplicate information, unless your boss explicitly requests it.
Do NOT make any changes other than those requested by your boss.
Do not let your boss down, ensure your updates are clean, and fulfill his requests.
</instructions>`;

const updatePostsPrompt = z.object({
  post: z.string().describe("The updated post content."),
});

export async function rewritePost(
  state: RepurposerPostInterruptState,
): Promise<RepurposerPostInterruptUpdate> {
  if (!state.userResponse) {
    throw new Error("Can not rewrite posts without user response");
  }

  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  }).bindTools(
    [
      {
        name: "update_post",
        schema: updatePostsPrompt,
        description: "Update the post based on the user's requests.",
      },
    ],
    {
      tool_choice: "update_post",
    },
  );

  const formattedPrompt = REWRITE_POST_PROMPT.replace(
    "{POST_TO_UPDATE}",
    state.post,
  )
    .replace(
      "{ORIGINAL_POSTS}",
      state.posts
        .map((p) => `<post index="${p.index}">\n${p.content}\n</post>`)
        .join("\n"),
    )
    .replace("{ORIGINAL_CAMPAIGN_PLAN}", state.campaignPlan)
    .replace("{MARKETING_REPORT}", formatReportForPrompt(state.reports[0]));

  const response = await model.invoke([
    {
      role: "system",
      content: formattedPrompt,
    },
    {
      role: "user",
      content: state.userResponse,
    },
  ]);

  const toolCall = response.tool_calls?.[0].args as z.infer<
    typeof updatePostsPrompt
  >;

  return {
    next: undefined,
    userResponse: undefined,
    post: toolCall.post,
  };
}



================================================
FILE: src/agents/repurposer-post-interrupt/nodes/human-node/index.ts
================================================
import {
  RepurposerPostInterruptState,
  RepurposerPostInterruptUpdate,
} from "../../types.js";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import { END, interrupt } from "@langchain/langgraph";
import { parseDateResponse, PRIORITY_LEVELS } from "../../../../utils/date.js";
import {
  constructDescription,
  getUnknownResponseDescription,
} from "./utils.js";
import { routeResponse } from "./router.js";
import { formatInTimeZone } from "date-fns-tz";
import { processImageInput } from "../../../utils.js";
import { DateType, Image } from "../../../types.js";

export async function humanNode(
  state: RepurposerPostInterruptState,
): Promise<RepurposerPostInterruptUpdate> {
  if (!state.post) {
    throw new Error("No post found");
  }

  let defaultDateString = "p1";
  if (
    typeof state.scheduleDate === "string" &&
    PRIORITY_LEVELS.includes(state.scheduleDate)
  ) {
    defaultDateString = state.scheduleDate as string;
  } else if (state.scheduleDate && typeof state.scheduleDate === "object") {
    defaultDateString = formatInTimeZone(
      state.scheduleDate,
      "America/Los_Angeles",
      "MM/dd/yyyy hh:mm a z",
    );
  }

  const interruptValue: HumanInterrupt = {
    action_request: {
      action: "Schedule Repurposed Post",
      args: {
        date: defaultDateString,
        post: state.post,
        image: state.image?.imageUrl ?? "",
      },
    },
    config: {
      allow_accept: true,
      allow_edit: true,
      allow_ignore: true,
      allow_respond: true,
    },
    description: constructDescription({
      state,
      unknownResponseDescription: getUnknownResponseDescription(state),
    }),
  };

  const response = interrupt<HumanInterrupt[], HumanResponse[]>([
    interruptValue,
  ])[0];

  if (!["edit", "ignore", "accept", "response"].includes(response.type)) {
    throw new Error(
      `Unexpected response type: ${response.type}. Must be "edit", "ignore", "accept", or "response".`,
    );
  }
  if (response.type === "ignore") {
    return {
      next: END,
    };
  }
  if (!response.args) {
    throw new Error(
      `Unexpected response args: ${response.args}. Must be defined.`,
    );
  }

  if (response.type === "response") {
    if (typeof response.args !== "string") {
      throw new Error("Response args must be a string.");
    }

    const { route } = await routeResponse(state.post, response.args);

    if (route === "rewrite_post") {
      return {
        userResponse: response.args,
        next: "rewritePost",
      };
    }

    return {
      userResponse: response.args,
      next: "unknownResponse",
    };
  }

  if (typeof response.args !== "object") {
    throw new Error(
      `Unexpected response args type: ${typeof response.args}. Must be an object.`,
    );
  }
  if (!("args" in response.args)) {
    throw new Error(
      `Unexpected response args value: ${response.args}. Must be defined.`,
    );
  }

  const castArgs = response.args.args as unknown as Record<string, string>;
  const post = castArgs.post;
  if (!post) {
    throw new Error("No post found");
  }

  let imageState: Image | undefined = undefined;
  const processedImage = await processImageInput(castArgs.image);
  if (processedImage && processedImage !== "remove") {
    imageState = processedImage;
  } else if (processedImage === "remove") {
    imageState = undefined;
  } else {
    imageState = state.image;
  }

  const postDateString = castArgs.date;
  let postDate: DateType | undefined;
  if (postDateString) {
    postDate = parseDateResponse(postDateString);
    if (!postDate) {
      throw new Error(
        "Invalid date provided.\n\n" +
          "Expected format: 'MM/dd/yyyy hh:mm a z' or 'P1'/'P2'/'P3'/'R1'/'R2'/'R3' or leave empty to post now.\n\n" +
          `Received: '${postDateString}'`,
      );
    }
  }

  return {
    next: "schedulePost",
    scheduleDate: postDate,
    post,
    image: imageState,
    userResponse: undefined,
  };
}



================================================
FILE: src/agents/repurposer-post-interrupt/nodes/human-node/router.ts
================================================
import { z } from "zod";
import { ChatAnthropic } from "@langchain/anthropic";

const ROUTE_POST_PROMPT = `You're an advanced AI assistant, tasked with routing a user's response.
The only route which can be taken is 'rewrite_post'. If the user is not asking to rewrite a post, then choose the 'unknown_response' route.

Here's the post the user is responding to:
<post>
{POST}
</post>

Here's the user's response:
<user-response>
{USER_RESPONSE}
</user-response>

Please examine the {POST_OR_POSTS} and determine which route to take.
`;

const routeResponseSchema = z.object({
  route: z.enum(["rewrite_post", "unknown_response"]),
});

export async function routeResponse(
  post: string,
  userResponse: string,
): Promise<z.infer<typeof routeResponseSchema>> {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  }).bindTools(
    [
      {
        name: "route_response",
        description: "Route the user's response to the appropriate route.",
        schema: routeResponseSchema,
      },
    ],
    {
      tool_choice: "route_response",
    },
  );

  const formattedPrompt = ROUTE_POST_PROMPT.replace("{POST}", post).replace(
    "{USER_RESPONSE}",
    userResponse,
  );

  const response = await model.invoke(formattedPrompt);

  return response.tool_calls?.[0].args as z.infer<typeof routeResponseSchema>;
}



================================================
FILE: src/agents/repurposer-post-interrupt/nodes/human-node/utils.ts
================================================
import { RepurposerPostInterruptState } from "../../types.js";

export function getUnknownResponseDescription(
  state: RepurposerPostInterruptState,
) {
  if (state.next === "unknownResponse" && state.userResponse) {
    return `# <div style="color: red;">UNKNOWN/INVALID RESPONSE RECEIVED: '${state.userResponse}'</div>

<div style="color: red;">Please respond with either request to rewrite the post(s). Other responses are <strong>not supported</strong>.</div>

<div style="color: red;">See the \`Instructions\` sections for more information.</div>

<hr />`;
  }
  return "";
}

function formatImageDescriptions(imageOptions: string[]): {
  imageOptionsText: string;
  imageInstructionsString: string;
} {
  const imageOptionsText = imageOptions?.length
    ? `## Image Options
    
The following image options are available. Select one by copying and pasting the URL into the 'New Images' field, separated by double commas (e.g \`example.com,,other-example.com\`).
    
${imageOptions
  .map(
    (url) => `URL: ${url}
Image: <details><summary>Click to view image</summary>

![](${url})
</details>
`,
  )
  .join("\n")}`
    : "";

  const imageInstructionsString = imageOptions?.length
    ? `If you wish to attach an image to the post, please add a public image URL.

You may remove the image by setting the 'image' field to 'remove', or by removing all text from the field
To replace the image, simply add a new public image URL to the field.

MIME types will be automatically extracted from the image.
Supported image types: \`image/jpeg\` | \`image/gif\` | \`image/png\` | \`image/webp\``
    : "No image options available.";

  return {
    imageOptionsText,
    imageInstructionsString,
  };
}

export function constructDescription({
  state,
  unknownResponseDescription,
}: {
  state: RepurposerPostInterruptState;
  unknownResponseDescription: string;
}): string {
  const {
    originalLink,
    originalContent,
    contextLinks,
    additionalContexts,
    reports,
    imageOptions,
    posts,
    post,
  } = state;

  const unknownResponseString = unknownResponseDescription
    ? `${unknownResponseDescription}\n\n`
    : "";

  const { imageOptionsText, imageInstructionsString } =
    formatImageDescriptions(imageOptions);

  return `${unknownResponseString}# Schedule Repurposed Post

## Post

${post}

## All Generated Posts (for reference)

${posts.map((p) => `### Post ${p.index + 1}:\n\`\`\`${p.content}\n\`\`\``).join("\n\n")}

## Original Link

${originalLink}

<details><summary>Original Link Contents</summary>

\`\`\`
${originalContent}
\`\`\`

</details>
  
## Additional Contexts

${contextLinks?.length ? `- ${contextLinks.join("\n- ")}` : "No additional context available."}

${
  additionalContexts?.length
    ? `<details><summary>Additional Contexts Contents</summary>

${additionalContexts.map(
  (c, idx) => `<details><summary>${idx + 1} - ${c.link}</summary>

\`\`\`
${c.content}
\`\`\`

</details>
`,
)}

</details>`
    : ""
}

${imageOptionsText}

## Instructions

There are a few different actions which can be taken:\n
- **Edit**: If the post is edited and submitted, it will be scheduled for Twitter/LinkedIn.
- **Response**: If a response is sent, it will be sent to a router which can be routed to either
  1. A node which will be used to rewrite the post. Please note, the response will be used as the 'user' message in an LLM call to rewrite the post, so ensure your response is properly formatted.
  2. A node which will be used to update the scheduled date for the post.
  If an unknown/invalid response is sent, nothing will happen, and it will be routed back to the human node.
- **Accept**: If 'accept' is selected, the post will be scheduled for Twitter/LinkedIn.
- **Ignore**: If 'ignore' is selected, this post will not be scheduled, and the thread will end.

## Additional Instructions

### Schedule Date

The date the post will be scheduled for may be edited, but it must follow the format 'MM/dd/yyyy hh:mm a z'. Example: '12/25/2024 10:00 AM PST', _OR_ you can use a priority level:
- **R1**: TODO: WHAT SCHEDULE DATE FOR R1
- **R2**: TODO: WHAT SCHEDULE DATE FOR R2
- **R3**: TODO: WHAT SCHEDULE DATE FOR R3

### Image

${imageInstructionsString}

## Report

Here is the report that was generated for the posts

<details><summary>Expand Report</summary>

#### Key Details

${reports[0].keyDetails}

<hr/>

${reports[0].report}
</details>
`;
}



================================================
FILE: src/agents/shared/shared-state.ts
================================================
import { Annotation } from "@langchain/langgraph";

export const VerifyContentAnnotation = Annotation.Root({
  /**
   * The link to the content to verify.
   */
  link: Annotation<string>,
});



================================================
FILE: src/agents/shared/auth/linkedin.ts
================================================
import {
  interrupt,
  // @ts-expect-error - The type is used in the JSDoc comment, but not defined in the code.
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  type NodeInterrupt,
} from "@langchain/langgraph";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import Arcade from "@arcadeai/arcadejs";
import { useArcadeAuth } from "../../utils.js";

const LINKEDIN_AUTHORIZATION_DOCS_URL =
  "https://github.com/langchain-ai/social-media-agent?tab=readme-ov-file#setup";

/**
 * Checks LinkedIn authorization status and triggers an interrupt if authorization is needed.
 * This function verifies the presence of required LinkedIn credentials (access token and either person URN or organization ID).
 * If credentials are missing, it will interrupt the flow to request user authorization.
 *
 * @param linkedInUserId - The user ID for LinkedIn authorization
 * @param config - Configuration object containing LinkedIn credentials and other settings
 * @param options - Optional configuration for interrupt handling
 * @param options.returnInterrupt - If true, returns the interrupt object instead of throwing it
 * @returns A promise that resolves to a HumanInterrupt object if returnInterrupt is true and authorization is needed,
 *          otherwise resolves to undefined
 * @throws {NodeInterrupt} When authorization is needed and returnInterrupt is false
 * @throws {Error} When user denies authorization by ignoring the interrupt
 *
 * @example
 * ```typescript
 * await getLinkedInAuthOrInterrupt("user123", config);
 * ```
 */
export async function getBasicLinkedInAuthOrInterrupt(fields?: {
  linkedInUserId?: string;
  returnInterrupt?: boolean;
}) {
  const { accessToken, personUrn, organizationId } = {
    accessToken: process.env.LINKEDIN_ACCESS_TOKEN,
    organizationId: process.env.LINKEDIN_ORGANIZATION_ID,
    personUrn: process.env.LINKEDIN_PERSON_URN,
  };

  if (!accessToken || (!personUrn && !organizationId)) {
    const description = `# Authorization Required
    
${fields?.linkedInUserId ? `Missing LinkedIn authorization for user: ${fields.linkedInUserId}` : ""}.

Please follow the authorization instructions [here](${LINKEDIN_AUTHORIZATION_DOCS_URL}).

Once completed, please mark this interrupt as resolved to end this task, and restart with the proper configuration fields or environment variables set.

----

If you have already authorized and set the required configuration fields or environment variables for posting on LinkedIn, please accept this interrupt event to continue the task.`;

    const authInterrupt: HumanInterrupt = {
      action_request: {
        action: "[AUTHORIZATION REQUIRED]: LinkedIn",
        args: {
          authorizationDocs: LINKEDIN_AUTHORIZATION_DOCS_URL,
        },
      },
      config: {
        allow_ignore: true,
        allow_accept: true,
        allow_edit: false,
        allow_respond: false,
      },
      description,
    };

    if (fields?.returnInterrupt) {
      return authInterrupt;
    }

    const res = interrupt<HumanInterrupt[], HumanResponse[]>([
      authInterrupt,
    ])[0];
    if (res.type === "accept") {
      // The user has accepted, indicating the required fields have been set.
      // Trust this and continue with the task.
      return undefined;
    }
    if (res.type === "ignore") {
      // Throw an error to end the graph.
      throw new Error("Authorization denied by user.");
    }
  }

  return undefined;
}

async function getArcadeLinkedInAuthOrInterrupt(
  linkedInUserId: string,
  arcade: Arcade,
  fields?: {
    returnInterrupt?: boolean;
    postToOrg?: boolean;
  },
) {
  const scopes = fields?.postToOrg
    ? ["w_member_social", "w_organization_social"]
    : ["w_member_social"];
  const authResponse = await arcade.auth.start(linkedInUserId, "linkedin", {
    scopes,
  });
  const authUrl = authResponse.url;

  if (authUrl) {
    const description = `# Authorization Required
  
Please visit the following URL to authorize reading & posting to LinkedIn.

${authUrl}

----

If you have already authorized reading/posting on Twitter, please accept this interrupt event.`;

    const authInterrupt: HumanInterrupt = {
      action_request: {
        action: "[AUTHORIZATION REQUIRED]: LinkedIn",
        args: {
          authorizeLinkedInURL: authUrl,
        },
      },
      config: {
        allow_ignore: true,
        allow_accept: true,
        allow_edit: false,
        allow_respond: false,
      },
      description,
    };

    if (fields?.returnInterrupt) {
      return authInterrupt;
    }

    const res = interrupt<HumanInterrupt[], HumanResponse[]>([
      authInterrupt,
    ])[0];
    if (res.type === "accept") {
      // This means that the user has accepted, however the
      // authorization is still needed. Throw an error.
      throw new Error(
        "User accepted authorization, but authorization is still needed.",
      );
    } else if (res.type === "ignore") {
      // Throw an error to end the graph.
      throw new Error("Authorization denied by user.");
    }
  }
  return undefined;
}

export async function getLinkedInAuthOrInterrupt(fields?: {
  linkedInUserId?: string;
  returnInterrupt?: boolean;
  postToOrg?: boolean;
}) {
  const linkedInUserId = fields?.linkedInUserId || process.env.LINKEDIN_USER_ID;
  if (useArcadeAuth()) {
    if (!fields?.linkedInUserId) {
      throw new Error("Must provide LinkedIn User ID when using Arcade auth.");
    }

    return getArcadeLinkedInAuthOrInterrupt(
      fields.linkedInUserId,
      new Arcade({ apiKey: process.env.ARCADE_API_KEY }),
      {
        returnInterrupt: fields?.returnInterrupt,
        postToOrg: fields?.postToOrg,
      },
    );
  }

  return getBasicLinkedInAuthOrInterrupt({
    linkedInUserId,
    returnInterrupt: fields?.returnInterrupt,
  });
}



================================================
FILE: src/agents/shared/auth/twitter.ts
================================================
// @ts-expect-error - The type is used in the JSDoc comment, but not defined in the code.
// eslint-disable-next-line @typescript-eslint/no-unused-vars
import { interrupt, type NodeInterrupt } from "@langchain/langgraph";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import Arcade from "@arcadeai/arcadejs";
import { TwitterClient } from "../../../clients/twitter/client.js";
import { useArcadeAuth } from "../../utils.js";

/**
 * Checks Twitter authorization status and triggers an interrupt if authorization is needed.
 * This function attempts to authorize both tweet lookup and posting capabilities.
 * If either authorization is missing, it will interrupt the flow to request user authorization.
 *
 * @param twitterUserId - The user ID for Twitter authorization
 * @param arcade - The Arcade instance used for tool authorization
 * @throws {NodeInterrupt} When authorization is needed, throws an interrupt to request user action
 * @returns {Promise<HumanInterrupt | undefined>} A promise that resolves to the interrupt if `options.returnInterrupt` is true, or undefined if `options.returnInterrupt` is false
 *
 * @example
 * ```typescript
 * await getArcadeTwitterAuthOrInterrupt("user123", arcadeInstance);
 * ```
 */
export async function getArcadeTwitterAuthOrInterrupt(
  twitterUserId: string,
  arcade: Arcade,
  options?: {
    returnInterrupt?: boolean;
  },
) {
  const authResponse = await TwitterClient.authorizeUser(twitterUserId, arcade);

  const authUrl = authResponse.authorizationUrl;

  if (authUrl) {
    const description = `# Authorization Required
  
Please visit the following URL to authorize reading & posting Tweets.

${authUrl}

----

If you have already authorized reading/posting on Twitter, please accept this interrupt event.`;

    const authInterrupt: HumanInterrupt = {
      action_request: {
        action: "[AUTHORIZATION REQUIRED]: Twitter",
        args: {
          authorizeTwitterURL: authUrl,
        },
      },
      config: {
        allow_ignore: true,
        allow_accept: true,
        allow_edit: false,
        allow_respond: false,
      },
      description,
    };

    if (options?.returnInterrupt) {
      return authInterrupt;
    }

    const res = interrupt<HumanInterrupt[], HumanResponse[]>([
      authInterrupt,
    ])[0];
    if (res.type === "accept") {
      // This means that the user has accepted, however the
      // authorization is still needed. Throw an error.
      throw new Error(
        "User accepted authorization, but authorization is still needed.",
      );
    } else if (res.type === "ignore") {
      // Throw an error to end the graph.
      throw new Error("Authorization denied by user.");
    }
  }

  return undefined;
}

export async function getBasicTwitterAuthOrInterrupt(options?: {
  returnInterrupt?: boolean;
}) {
  const authInterrupt: HumanInterrupt = {
    action_request: {
      action: "[AUTHORIZATION REQUIRED]: Twitter",
      args: {},
    },
    config: {
      allow_ignore: true,
      allow_accept: true,
      allow_edit: false,
      allow_respond: false,
    },
    description:
      "Failed to fetch user authorization status.\n\nPlease ensure the proper Twitter credentials and user secrets are set in the environment.",
  };

  try {
    const client = TwitterClient.fromBasicTwitterAuth();
    const authed = await client.testAuthentication();
    if (authed) {
      // User is successfully authed. Return undefined.
      return undefined;
    }
  } catch (error: any) {
    if (typeof error === "object" && error?.message) {
      // Use error message in interrupt description
      authInterrupt.description = `Failed to fetch user authorization status.\n\n${error.message}`;
    }
  }

  if (options?.returnInterrupt) {
    return authInterrupt;
  }

  const res = interrupt<HumanInterrupt[], HumanResponse[]>([authInterrupt])[0];
  if (res.type === "accept") {
    // This means that the user has accepted, however the
    // authorization is still needed. Throw an error.
    throw new Error(
      "User accepted authorization, but authorization is still needed.",
    );
  } else if (res.type === "ignore") {
    // Throw an error to end the graph.
    throw new Error("Authorization denied by user.");
  }

  return undefined;
}

export async function getTwitterAuthOrInterrupt(fields?: {
  twitterUserId?: string;
  returnInterrupt?: boolean;
}) {
  if (useArcadeAuth()) {
    if (!fields?.twitterUserId) {
      throw new Error("Must provide Twitter User ID when using Arcade auth.");
    }

    return getArcadeTwitterAuthOrInterrupt(
      fields.twitterUserId,
      new Arcade({ apiKey: process.env.ARCADE_API_KEY }),
      {
        returnInterrupt: fields?.returnInterrupt,
      },
    );
  }

  return getBasicTwitterAuthOrInterrupt({
    returnInterrupt: fields?.returnInterrupt,
  });
}



================================================
FILE: src/agents/shared/nodes/route-response.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";

const ROUTE_RESPONSE_PROMPT = `You are an AI assistant tasked with routing a user's response to one of two possible routes based on their intention. The two possible routes are:

1. Rewrite post - The user's response indicates they want to rewrite the generated post.
2. Update scheduled date - The user wants to update the scheduled date for the post. This can either be a new date or a priority level (P1, P2, P3).
3. Rewrite with split URL - Split the call to action URL in the post into a reply.

Here is the generated post:
<post>
{POST}
</post>

Here is the current date/priority level for scheduling the post:
<date-or-priority>
{DATE_OR_PRIORITY}
</date-or-priority>

Carefully analyze the user's response:
<user-response>
{USER_RESPONSE}
</user-response>

Based on the user's response, determine which of the two routes they intend to take. Consider the following:

1. If the user mentions editing, changing, or rewriting the content of the post, choose the "rewrite_post" route.
2. If the user mentions changing the date, time, or priority level of the post, choose the "update_date" route. Ensure you only call this if the user mentions a date, or one of P1, P2 or P3.

If the user's response can not be handled by one of the two routes, choose the "unknown_response" route.

Provide your answer in the following format:
<explanation>
[A brief explanation of why you chose this route based on the user's response]
</explanation>
(call the 'route' tool to choose the route)

Here are some examples of possible user responses and the corresponding routes:

Example 1:
User: "Can we change the wording in the second paragraph?"
Route: rewrite_post
Explanation: The user is requesting changes to the content of the post.

Example 2:
User: "Schedule this for next Tuesday."
Route: update_date
Explanation: The user wants to change the posting date.

Example 3:
User: "Split the URL from the post"
Route: rewrite_with_split_url
Explanation: The user wants to split the post into two unique posts, one without the URL and the second with the URL.

Example 4:
User: "This should be a P1 priority."
Route: update_date
Explanation: The user wants to change the priority level of the post.

Example 5:
User: "This should be a P0 priority."
Route: unknown_response
Explanation: P0 is not a valid priority level.

Example 6:
User: "Hi! How are you?"
Route: unknown_response
Explanation: The user is engaging in general conversation, not a request to change the post.

Example 7:
User: "Can you split the post into two?"
Route: rewrite_with_split_url
Explanation: The user wants to split the post into two unique posts, one without the URL and the second with the URL.

Once again, here is the users response:
<user-response>
{USER_RESPONSE}
</user-response>

Remember to always base your decision on the actual content of the user's response, not on these examples.`;

interface RouteResponseArgs {
  post: string;
  dateOrPriority: string;
  userResponse: string;
}

export async function routeResponse({
  post,
  dateOrPriority,
  userResponse,
}: RouteResponseArgs) {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  });

  const routeSchema = z.object({
    route: z.enum([
      "rewrite_post",
      "update_date",
      "unknown_response",
      "rewrite_with_split_url",
    ]),
  });
  const modelWithSchema = model.withStructuredOutput(routeSchema, {
    name: "route",
  });

  const formattedPrompt = ROUTE_RESPONSE_PROMPT.replace("{POST}", post)
    .replace("{DATE_OR_PRIORITY}", dateOrPriority)
    .replace("{USER_RESPONSE}", userResponse);

  const result = await modelWithSchema.invoke([
    {
      role: "user",
      content: formattedPrompt,
    },
  ]);

  return result;
}



================================================
FILE: src/agents/shared/nodes/update-scheduled-date.ts
================================================
import { z } from "zod";
import { ChatAnthropic } from "@langchain/anthropic";
import { toZonedTime } from "date-fns-tz";
import { DateType } from "../../types.js";
import { timezoneToUtc } from "../../../utils/date.js";

const SCHEDULE_POST_DATE_PROMPT = `You're an intelligent AI assistant tasked with extracting the date to schedule a social media post from the user's message.

The user may respond with either:
1. A priority level (P1, P2, P3)
  - **P1**: Saturday/Sunday between 8:00 AM and 10:00 AM PST.
  - **P2**: Friday/Monday between 8:00 AM and 10:00 AM PST _OR_ Saturday/Sunday between 11:30 AM and 1:00 PM PST.
  - **P3**: Saturday/Sunday between 1:00 PM and 5:00 PM PST.
2. A date

Your task is to extract the date/priority level from the user's message and return it in a structured format the system can handle.

If the user's message is asking for a date, convert it to the following format:
'MM/dd/yyyy hh:mm a z'. Example: '12/25/2024 10:00 AM PST'
Always use PST for the timezone. If they don't specify a time, you can make one up, as long as it's between 8:00 AM and 3:00 PM PST (5 minute intervals).

If the user's message is asking for a priority level, return it in the following format:
'p1', 'p2', or 'p3'

The current date and time (in PST) are: {currentDateAndTime}

You should use this to infer the date if the user's message does not contain an exact date,
Example: 'this saturday'

If the user's message can not be interpreted as a date or priority level, return 'p3'.`;

const scheduleDateSchema = z.object({
  scheduleDate: z
    .string()
    .describe(
      "The date in the format 'MM/dd/yyyy hh:mm a z' or a priority level (p1, p2, p3).",
    ),
});

export async function updateScheduledDate(
  state: Record<string, any>,
): Promise<Record<string, any>> {
  if (!state.userResponse) {
    throw new Error("No user response found");
  }
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0.5,
  }).withStructuredOutput(scheduleDateSchema, {
    name: "scheduleDate",
  });
  const pstDate = toZonedTime(new Date(), "America/Los_Angeles");
  const pstDateString = pstDate.toISOString();

  const prompt = SCHEDULE_POST_DATE_PROMPT.replace(
    "{currentDateAndTime}",
    pstDateString,
  );

  const result = await model.invoke([
    {
      role: "system",
      content: prompt,
    },
    {
      role: "user",
      content: state.userResponse,
    },
  ]);

  if (
    typeof result.scheduleDate === "string" &&
    ["p1", "p2", "p3"].includes(result.scheduleDate)
  ) {
    return {
      scheduleDate: result.scheduleDate as DateType,
    };
  }

  return {
    next: undefined,
    userResponse: undefined,
    scheduleDate: timezoneToUtc(result.scheduleDate),
  };
}



================================================
FILE: src/agents/shared/nodes/verify-content.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { traceable } from "langsmith/traceable";
import { z } from "zod";

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the webpage is or isn't relevant to your company's products.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the webpage is relevant to your company's products.",
      ),
  })
  .describe("The relevancy of the content to your company's products.");

async function verifyContentIsRelevantFunc(
  content: string,
  args: {
    systemPrompt: string;
    schema: z.ZodType<z.infer<typeof RELEVANCY_SCHEMA>>;
  },
): Promise<boolean> {
  const relevancyModel = new ChatAnthropic({
    model: "claude-3-7-sonnet-latest",
    temperature: 0,
  }).withStructuredOutput(args.schema, {
    name: "relevancy",
  });

  const { relevant } = await relevancyModel.invoke([
    {
      role: "system",
      content: args.systemPrompt,
    },
    {
      role: "user",
      content: content,
    },
  ]);
  return relevant;
}

/**
 * Verifies if the content provided is relevant based on the provided system prompt,
 * using the provided relevancy schema.
 *
 * @param {string} content - The content to verify.
 * @param {object} args - The arguments containing the system prompt and relevancy schema.
 * @param {string} args.systemPrompt - The system prompt to use for verification.
 * @param {z.ZodType<z.infer<typeof RELEVANCY_SCHEMA>>} args.schema - The relevancy schema to use for verification.
 * @returns {Promise<boolean>} A promise that resolves to a boolean indicating whether the content is relevant.
 */
export const verifyContentIsRelevant = traceable(verifyContentIsRelevantFunc, {
  name: "verify-content-relevancy",
});



================================================
FILE: src/agents/shared/nodes/verify-general.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { z } from "zod";
import { FireCrawlLoader } from "@langchain/community/document_loaders/web/firecrawl";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyContentAnnotation } from "../shared-state.js";
import { getPageText, skipContentRelevancyCheck } from "../../utils.js";
import { getImagesFromFireCrawlMetadata } from "../../../utils/firecrawl.js";
import { CurateDataState } from "../../curate-data/state.js";
import { shouldExcludeGeneralContent } from "../../should-exclude.js";
import { traceable } from "langsmith/traceable";
import { verifyContentIsRelevant } from "./verify-content.js";

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the webpage is or isn't relevant to your company's products.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the webpage is relevant to your company's products.",
      ),
  })
  .describe("The relevancy of the content to your company's products.");

const VERIFY_COMPANY_RELEVANT_CONTENT_PROMPT = `You are a highly regarded marketing employee.
You're provided with a webpage containing content a third party submitted to you claiming it's relevant to your business context.
Your task is to carefully read over the entire page, and determine whether or not the content is actually relevant to your context.

${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

Given this context, examine the webpage content closely, and determine if the content is relevant to your context.
You should provide reasoning as to why or why not the content is relevant to your context, then a simple true or false for whether or not it is relevant.`;

type UrlContents = {
  content: string;
  imageUrls?: string[];
};

async function getUrlContentsFunc(url: string): Promise<UrlContents> {
  const loader = new FireCrawlLoader({
    url,
    mode: "scrape",
    params: {
      formats: ["markdown", "screenshot"],
    },
  });
  const docs = await loader.load();

  const docsText = docs.map((d) => d.pageContent).join("\n");
  if (docsText.length) {
    return {
      content: docsText,
      imageUrls: docs.flatMap(
        (d) => getImagesFromFireCrawlMetadata(d.metadata) || [],
      ),
    };
  }

  const text = await getPageText(url);
  if (text) {
    return {
      content: text,
    };
  }
  throw new Error(`Failed to fetch content from ${url}.`);
}

export const getUrlContents = traceable(getUrlContentsFunc, {
  name: "get-url-contents",
});

/**
 * Verifies if the general content from a provided URL is relevant to your company's products.
 *
 * @param state - The current state containing the link to verify.
 * @param config - Configuration for the LangGraph runtime.
 * @returns An object containing relevant links and page contents if the content is relevant;
 * otherwise, returns empty arrays.
 */
export async function verifyGeneralContent(
  state: typeof VerifyContentAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<CurateDataState>> {
  const shouldExclude = shouldExcludeGeneralContent(state.link);
  if (shouldExclude) {
    return {};
  }

  const urlContents = await getUrlContents(state.link);

  const returnValue = {
    relevantLinks: [state.link],
    pageContents: [urlContents.content],
    ...(urlContents.imageUrls?.length
      ? { imageOptions: urlContents.imageUrls }
      : {}),
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  if (
    await verifyContentIsRelevant(urlContents.content, {
      systemPrompt: VERIFY_COMPANY_RELEVANT_CONTENT_PROMPT,
      schema: RELEVANCY_SCHEMA,
    })
  ) {
    return returnValue;
  }

  // Not relevant, return empty arrays so this URL is not included.
  return {
    relevantLinks: [],
    pageContents: [],
  };
}



================================================
FILE: src/agents/shared/nodes/verify-github.ts
================================================
import { z } from "zod";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyContentAnnotation } from "../shared-state.js";
import { GeneratePostAnnotation } from "../../generate-post/generate-post-state.js";
import {
  getRepoContents,
  getFileContents,
  getOwnerRepoFromUrl,
} from "../../../utils/github-repo-contents.js";
import { Octokit } from "@octokit/rest";
import { shouldExcludeGitHubContent } from "../../should-exclude.js";
import { skipContentRelevancyCheck } from "../../utils.js";
import { verifyContentIsRelevant } from "./verify-content.js";

function getOctokit() {
  const token = process.env.GITHUB_TOKEN;
  if (!token) {
    throw new Error("GITHUB_TOKEN environment variable is required");
  }

  const octokit = new Octokit({
    auth: token,
  });

  return octokit;
}

type VerifyGitHubContentReturn = {
  relevantLinks: (typeof GeneratePostAnnotation.State)["relevantLinks"];
  pageContents: (typeof GeneratePostAnnotation.State)["pageContents"];
};

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the content from the GitHub repository is or isn't relevant to your company's products.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the content from the GitHub repository is relevant to your company's products.",
      ),
  })
  .describe("The relevancy of the content to your company's products.");

const REPO_DEPENDENCY_PROMPT = `Here are the dependencies of the repository. You should use the dependencies listed to determine if the repository is relevant.
<repository-dependency-files>
{dependencyFiles}
</repository-dependency-files>`;

const VERIFY_LANGCHAIN_RELEVANT_CONTENT_PROMPT = `You are a highly regarded marketing employee at LangChain.
You're given a {file_type} from a GitHub repository and need to verify the repository implements your company's products.
You're doing this to ensure the content is relevant to LangChain, and it can be used as marketing material to promote LangChain.

${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

{repoDependenciesPrompt}

Given this context, examine the  {file_type} closely, and determine if the repository implements your company's products.
You should provide reasoning as to why or why not the repository implements your company's products, then a simple true or false for whether or not it implements some.`;

const getDependencies = async (
  githubUrl: string,
): Promise<Array<{ fileContents: string; fileName: string }> | undefined> => {
  const octokit = getOctokit();

  const { owner, repo } = getOwnerRepoFromUrl(githubUrl);

  const dependenciesCodeFileQuery = `filename:package.json OR filename:requirements.txt OR filename:pyproject.toml`;
  const dependenciesCodeFiles = await octokit.search.code({
    q: `${dependenciesCodeFileQuery} repo:${owner}/${repo}`,
    limit: 5,
  });
  if (dependenciesCodeFiles.data.total_count === 0) {
    return undefined;
  }

  const fileContents = (
    await Promise.all(
      dependenciesCodeFiles.data.items.flatMap(async (item) => {
        const { data } = await octokit.repos.getContent({
          owner,
          repo,
          path: item.path,
        });

        if (!("content" in data)) {
          return undefined;
        }

        return {
          fileName: item.name,
          fileContents: Buffer.from(data.content, "base64").toString("utf-8"),
        };
      }),
    )
  ).filter((file) => file !== undefined) as Array<{
    fileName: string;
    fileContents: string;
  }>;

  return fileContents;
};

export async function getGitHubContentsAndTypeFromUrl(url: string): Promise<
  | {
      contents: string;
      fileType: string;
    }
  | undefined
> {
  const repoContents = await getRepoContents(url);
  const readmePath = repoContents.find(
    (c) =>
      c.name.toLowerCase() === "readme.md" || c.name.toLowerCase() === "readme",
  )?.path;
  if (!readmePath) {
    return undefined;
  }
  const readmeContents = await getFileContents(url, readmePath);
  return {
    contents: readmeContents.content,
    fileType: "README file",
  };
}

interface VerifyGitHubContentParams {
  contents: string;
  fileType: string;
  dependencyFiles:
    | Array<{ fileContents: string; fileName: string }>
    | undefined;
}

async function verifyGitHubContentIsRelevant({
  contents,
  fileType,
  dependencyFiles,
}: VerifyGitHubContentParams): Promise<boolean> {
  let dependenciesPrompt = "";
  if (dependencyFiles) {
    dependencyFiles.forEach((f) => {
      // Format it as a markdown code block with the file name as the header.
      dependenciesPrompt += `\`\`\`${f.fileName}\n${f.fileContents}\n\`\`\`\n`;
    });

    dependenciesPrompt = REPO_DEPENDENCY_PROMPT.replace(
      "{dependencyFiles}",
      dependenciesPrompt,
    );
  }

  const systemPrompt = VERIFY_LANGCHAIN_RELEVANT_CONTENT_PROMPT.replaceAll(
    "{file_type}",
    fileType,
  ).replaceAll("{repoDependenciesPrompt}", dependenciesPrompt);

  return verifyContentIsRelevant(contents, {
    systemPrompt,
    schema: RELEVANCY_SCHEMA,
  });
}

/**
 * Verifies the content provided is relevant to LangChain products.
 */
export async function verifyGitHubContent(
  state: typeof VerifyContentAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<VerifyGitHubContentReturn> {
  const shouldExclude = shouldExcludeGitHubContent(state.link);
  if (shouldExclude) {
    return {
      relevantLinks: [],
      pageContents: [],
    };
  }

  const contentsAndType = await getGitHubContentsAndTypeFromUrl(state.link);
  if (!contentsAndType) {
    console.warn("No contents found for GitHub URL", state.link);
    return {
      relevantLinks: [],
      pageContents: [],
    };
  }

  const returnValue = {
    relevantLinks: [state.link],
    pageContents: [contentsAndType.contents],
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  const dependencyFiles = await getDependencies(state.link);
  if (
    await verifyGitHubContentIsRelevant({
      contents: contentsAndType.contents,
      fileType: contentsAndType.fileType,
      dependencyFiles,
    })
  ) {
    return returnValue;
  }

  // Not relevant, return empty arrays so this URL is not included.
  return {
    relevantLinks: [],
    pageContents: [],
  };
}



================================================
FILE: src/agents/shared/nodes/verify-luma.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { z } from "zod";
import { FireCrawlLoader } from "@langchain/community/document_loaders/web/firecrawl";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyContentAnnotation } from "../shared-state.js";
import { getPageText, skipContentRelevancyCheck } from "../../utils.js";
import { getImagesFromFireCrawlMetadata } from "../../../utils/firecrawl.js";
import { CurateDataState } from "../../curate-data/state.js";
import { shouldExcludeGeneralContent } from "../../should-exclude.js";
import { traceable } from "langsmith/traceable";
import { verifyContentIsRelevant } from "./verify-content.js";

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the event is or isn't relevant to your company.",
      ),
    relevant: z
      .boolean()
      .describe("Whether or not the event is relevant to your company."),
  })
  .describe("The relevancy of the event to your company.");

const VERIFY_LUMA_RELEVANT_CONTENT_PROMPT = `You are a highly regarded marketing employee.
You're provided with the contents of an event promotion page.
Your task is to carefully read over the entire event details, and determine whether or not the event is actually relevant to your company.

${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

Given this context, examine the event details closely, and determine if the event is relevant to your company.
Keep in mind, event descriptions are often sparse with details, so if you're not sure, err on the side of relevance.
You should provide reasoning as to why or why not the event is relevant to your company, then a simple true or false for whether or not it is relevant.`;

type UrlContents = {
  content: string;
  imageUrls?: string[];
};

async function getUrlContentsFunc(url: string): Promise<UrlContents> {
  try {
    const loader = new FireCrawlLoader({
      url,
      mode: "scrape",
      params: {
        formats: ["markdown", "screenshot"],
      },
    });
    const docs = await loader.load();

    const docsText = docs.map((d) => d.pageContent).join("\n");
    if (docsText.length) {
      return {
        content: docsText,
        imageUrls: docs.flatMap(
          (d) => getImagesFromFireCrawlMetadata(d.metadata) || [],
        ),
      };
    }
  } catch (e) {
    console.error(`Failed to fetch content from ${url}\nError:\n`, e);
  }

  const text = await getPageText(url);
  if (text) {
    return {
      content: text,
    };
  }
  throw new Error(`Failed to fetch content from ${url}.`);
}

export const getUrlContents = traceable(getUrlContentsFunc, {
  name: "get-url-contents",
});

/**
 * Verifies if the Luma event is relevant to your company's products.
 *
 * @param state - The current state containing the link to verify.
 * @param _config - Configuration for the LangGraph runtime (unused in this function).
 * @returns An object containing relevant links and page contents if the event is relevant;
 * otherwise, returns empty arrays.
 */
export async function verifyLumaEvent(
  state: typeof VerifyContentAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<CurateDataState>> {
  const shouldExclude = shouldExcludeGeneralContent(state.link);
  if (shouldExclude) {
    return {};
  }

  const urlContents = await getUrlContents(state.link);

  const returnValue = {
    relevantLinks: [state.link],
    pageContents: [urlContents.content],
    ...(urlContents.imageUrls?.length
      ? { imageOptions: urlContents.imageUrls }
      : {}),
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  if (
    await verifyContentIsRelevant(urlContents.content, {
      systemPrompt: VERIFY_LUMA_RELEVANT_CONTENT_PROMPT,
      schema: RELEVANCY_SCHEMA,
    })
  ) {
    return returnValue;
  }

  // Not relevant, return empty arrays so this URL is not included.
  return {
    relevantLinks: [],
    pageContents: [],
  };
}



================================================
FILE: src/agents/shared/nodes/verify-youtube.ts
================================================
import { z } from "zod";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { GeneratePostAnnotation } from "../../generate-post/generate-post-state.js";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyContentAnnotation } from "../shared-state.js";
import { getVideoSummary } from "../youtube/video-summary.js";
import { skipContentRelevancyCheck } from "../../utils.js";
import { verifyContentIsRelevant } from "./verify-content.js";

type VerifyYouTubeContentReturn = {
  relevantLinks: (typeof GeneratePostAnnotation.State)["relevantLinks"];
  pageContents: (typeof GeneratePostAnnotation.State)["pageContents"];
};

const VERIFY_RELEVANT_CONTENT_PROMPT = `You are a highly regarded marketing employee.
You're given a summary/report on some content a third party submitted to you in hopes of having it promoted by you.
You need to verify if the content is relevant to the following context before approving or denying the request.

${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

Given this context, examine the summary/report closely, and determine if the content is relevant to your company's products.
You should provide reasoning as to why or why not the content is relevant to your company's products, then a simple true or false for whether or not it's relevant.
`;

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the content is or isn't relevant to your company's products.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the content is relevant to your company's products.",
      ),
  })
  .describe("The relevancy of the content to your company's products.");

/**
 * Verifies the content provided is relevant to your company's products.
 */
export async function verifyYouTubeContent(
  state: typeof VerifyContentAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<VerifyYouTubeContentReturn> {
  const { summary, thumbnail } = await getVideoSummary(state.link);

  const returnValue = {
    relevantLinks: [state.link],
    pageContents: [summary as string],
    ...(thumbnail ? { imageOptions: [thumbnail] } : {}),
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  if (
    await verifyContentIsRelevant(summary, {
      systemPrompt: VERIFY_RELEVANT_CONTENT_PROMPT,
      schema: RELEVANCY_SCHEMA,
    })
  ) {
    return returnValue;
  }

  // Not relevant, return empty arrays so this URL is not included.
  return {
    relevantLinks: [],
    pageContents: [],
  };
}



================================================
FILE: src/agents/shared/nodes/youtube.utils.ts
================================================
import { youtube, youtube_v3 } from "@googleapis/youtube";
import { GoogleAuth } from "google-auth-library";

/**
 * Extracts the videoId from a YouTube video URL.
 * @param url The URL of the YouTube video.
 * @returns The videoId of the YouTube video.
 */
function getVideoID(url: string): string | undefined {
  try {
    const urlObj = new URL(url);
    const videoId = urlObj.searchParams.get("v");
    if (videoId) {
      return videoId;
    }
  } catch (_) {
    // no-op
  }

  const match = url.match(
    /.*(?:youtu.be\/|v\/|u\/\w\/|embed\/|watch\?v=)([^#&?]*).*/,
  );
  if (match !== null && match[1].length === 11) {
    return match[1];
  } else {
    return undefined;
  }
}

/**
 * Converts ISO 8601 duration to seconds
 * @param duration ISO 8601 duration string (e.g., "PT15M51S")
 * @returns number of seconds
 */
function parseDuration(duration: string): number {
  const match = duration.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
  if (!match) return 0;

  const hours = parseInt(match[1] || "0");
  const minutes = parseInt(match[2] || "0");
  const seconds = parseInt(match[3] || "0");

  return hours * 3600 + minutes * 60 + seconds;
}

function getYouTubeClientFromUrl(): youtube_v3.Youtube {
  if (!process.env.GOOGLE_VERTEX_AI_WEB_CREDENTIALS) {
    throw new Error("GOOGLE_VERTEX_AI_WEB_CREDENTIALS is not set");
  }
  const parsedGoogleCredentials = JSON.parse(
    process.env.GOOGLE_VERTEX_AI_WEB_CREDENTIALS,
  );

  const auth = new GoogleAuth({
    credentials: parsedGoogleCredentials,
    scopes: ["https://www.googleapis.com/auth/youtube.readonly"],
  });

  const youtubeClient = youtube({
    version: "v3",
    auth,
  });

  return youtubeClient;
}

/**
 * Get the duration of a video from a YouTube URL.
 * @param videoUrl The URL of the YouTube video
 * @returns The duration of the video in seconds
 */
export async function getYouTubeVideoDuration(
  videoUrl: string,
): Promise<number | undefined> {
  const youtubeClient = getYouTubeClientFromUrl();
  const videoId = getVideoID(videoUrl);
  if (!videoId) {
    throw new Error(`Invalid YouTube URL: ${videoUrl}`);
  }

  const videoInfo = await youtubeClient.videos.list({
    id: [videoId],
    part: ["contentDetails"], // Add this to get duration info
  });

  if (!videoInfo.data.items?.length || videoInfo.data.items?.length > 1) {
    // TODO: Handle this better
    throw new Error(`Expected 1 item, got ${videoInfo.data.items?.length}`);
  }

  let videoDuration: number | undefined = undefined;
  videoInfo.data.items?.forEach((i) => {
    const duration = i.contentDetails?.duration;
    if (duration) {
      videoDuration = parseDuration(duration);
    }
  });
  return videoDuration;
}

/**
 * Gets the highest quality thumbnail URL for a YouTube video.
 * @param videoUrl The URL of the YouTube video
 * @returns A promise that resolves to the URL of the video's thumbnail, or undefined if there's an error
 * @throws Error if the video URL is invalid or if there's an error fetching the thumbnail
 */
export async function getVideoThumbnailUrl(
  videoUrl: string,
): Promise<string | undefined> {
  const youtubeClient = getYouTubeClientFromUrl();
  const videoId = getVideoID(videoUrl);
  if (!videoId) {
    throw new Error(`Invalid YouTube URL: ${videoUrl}`);
  }

  const response = await youtubeClient.videos.list({
    part: ["snippet"],
    id: [videoId],
  });

  if (!response.data.items || response.data.items.length === 0) {
    throw new Error(`No video found for ID: ${videoId}`);
  }

  const thumbnails = response.data.items[0].snippet?.thumbnails;
  if (!thumbnails) {
    throw new Error(`No thumbnails found for video: ${videoId}`);
  }

  // Return the highest quality thumbnail available
  // Order of preference: maxres -> standard -> high -> medium -> default
  return (
    thumbnails.maxres?.url ||
    thumbnails.standard?.url ||
    thumbnails.high?.url ||
    thumbnails.medium?.url ||
    thumbnails.default?.url ||
    undefined
  );
}

/**
 * Gets information about the channel that posted a YouTube video.
 * @param videoUrl The URL of the YouTube video
 * @returns An object containing the channel's name and ID
 * @throws Error if the video URL is invalid or if there's an error fetching the channel info
 */
export async function getChannelInfo(
  videoUrl: string,
): Promise<{ channelName: string; channelId: string }> {
  const youtubeClient = getYouTubeClientFromUrl();
  const videoId = getVideoID(videoUrl);
  if (!videoId) {
    throw new Error(`Invalid YouTube URL: ${videoUrl}`);
  }

  const response = await youtubeClient.videos.list({
    part: ["snippet"],
    id: [videoId],
  });

  if (!response.data.items || response.data.items.length === 0) {
    throw new Error(`No video found for ID: ${videoId}`);
  }

  const snippet = response.data.items[0].snippet;
  if (!snippet) {
    throw new Error(`No snippet information found for video: ${videoId}`);
  }

  const channelName = snippet.channelTitle;
  const channelId = snippet.channelId;

  if (!channelName || !channelId) {
    throw new Error(`Could not find channel information for video: ${videoId}`);
  }

  return {
    channelName,
    channelId,
  };
}



================================================
FILE: src/agents/shared/nodes/generate-post/human-node.ts
================================================
import { END, LangGraphRunnableConfig, interrupt } from "@langchain/langgraph";
import { BaseGeneratePostState, BaseGeneratePostUpdate } from "./types.js";
import { formatInTimeZone } from "date-fns-tz";
import { isTextOnly, processImageInput } from "../../../utils.js";
import {
  getNextSaturdayDate,
  parseDateResponse,
} from "../../../../utils/date.js";
import { routeResponse } from "../../../shared/nodes/route-response.js";
import { saveUsedUrls } from "../../../shared/stores/post-subject-urls.js";
import { HumanInterrupt, HumanResponse } from "@langchain/langgraph/prebuilt";
import { DateType } from "../../../types.js";

interface ConstructDescriptionArgs {
  unknownResponseDescription: string;
  report: string;
  originalLink: string;
  relevantLinks: string[];
  post: string;
  imageOptions?: string[];
  isTextOnlyMode: boolean;
}

function constructDescription({
  unknownResponseDescription,
  report,
  originalLink,
  relevantLinks,
  post,
  imageOptions,
  isTextOnlyMode,
}: ConstructDescriptionArgs): string {
  const linksText = `### Relevant URLs:\nOriginal URL: ${originalLink}\n\n- ${relevantLinks.join("\n- ")}\n`;
  const imageOptionsText =
    imageOptions?.length && !isTextOnlyMode
      ? `## Image Options\n\nThe following image options are available. Select one by copying and pasting the URL into the 'image' field.\n\n${imageOptions.map((url) => `URL: ${url}\nImage: <details><summary>Click to view image</summary>\n\n![](${url})\n</details>\n`).join("\n")}`
      : "";

  const unknownResponseString = unknownResponseDescription
    ? `${unknownResponseDescription}\n\n`
    : "";

  const imageInstructionsString =
    imageOptions?.length && !isTextOnlyMode
      ? `If you wish to attach an image to the post, please add a public image URL.

You may remove the image by setting the 'image' field to 'remove', or by removing all text from the field
To replace the image, simply add a new public image URL to the field.

MIME types will be automatically extracted from the image.
Supported image types: \`image/jpeg\` | \`image/gif\` | \`image/png\` | \`image/webp\``
      : isTextOnlyMode
        ? "Text only mode enabled. Image support has been disabled.\n"
        : "No image options available.";

  return `${unknownResponseString}# Schedule post
  
Using these URL(s), a post was generated for Twitter/LinkedIn:
${linksText}

### Post:
\`\`\`
${post}
\`\`\`

${imageOptionsText}

## Instructions

There are a few different actions which can be taken:\n
- **Edit**: If the post is edited and submitted, it will be scheduled for Twitter/LinkedIn.
- **Response**: If a response is sent, it will be sent to a router which can be routed to either
  1. A node which will be used to rewrite the post. Please note, the response will be used as the 'user' message in an LLM call to rewrite the post, so ensure your response is properly formatted.
  2. A node which will be used to update the scheduled date for the post.
  If an unknown/invalid response is sent, nothing will happen, and it will be routed back to the human node.
- **Accept**: If 'accept' is selected, the post will be scheduled for Twitter/LinkedIn.
- **Ignore**: If 'ignore' is selected, this post will not be scheduled, and the thread will end.

## Additional Instructions

### Schedule Date

The date the post will be scheduled for may be edited, but it must follow the format 'MM/dd/yyyy hh:mm a z'. Example: '12/25/2024 10:00 AM PST', _OR_ you can use a priority level:
- **P1**: Saturday/Sunday between 8:00 AM and 10:00 AM PST.
- **P2**: Friday/Monday between 8:00 AM and 10:00 AM PST _OR_ Saturday/Sunday between 11:30 AM and 1:00 PM PST.
- **P3**: Saturday/Sunday between 1:00 PM and 5:00 PM PST.

### Image

${imageInstructionsString}

## Report

Here is the report that was generated for the posts:\n${report}
`;
}

const getUnknownResponseDescription = (state: BaseGeneratePostState) => {
  if (state.next === "unknownResponse" && state.userResponse) {
    return `# <div style="color: red;">UNKNOWN/INVALID RESPONSE RECEIVED: '${state.userResponse}'</div>

<div style="color: red;">Please respond with either a request to update/rewrite the post, or a valid priority level or a date to schedule the post.</div>

<div style="color: red;">See the \`Schedule Date\`, or \`Instructions\` sections for more information.</div>

<hr />`;
  }
  return "";
};

export async function humanNode<
  State extends BaseGeneratePostState = BaseGeneratePostState,
  Update extends BaseGeneratePostUpdate = BaseGeneratePostUpdate,
>(state: State, config: LangGraphRunnableConfig): Promise<Update> {
  if (!state.post) {
    throw new Error("No post found");
  }
  const isTextOnlyMode = isTextOnly(config);

  const unknownResponseDescription = getUnknownResponseDescription(state);
  const defaultDate = state.scheduleDate || getNextSaturdayDate();
  let defaultDateString = "";
  if (
    typeof state.scheduleDate === "string" &&
    ["p1", "p2", "p3"].includes(state.scheduleDate)
  ) {
    defaultDateString = state.scheduleDate as string;
  } else {
    defaultDateString = formatInTimeZone(
      defaultDate,
      "America/Los_Angeles",
      "MM/dd/yyyy hh:mm a z",
    );
  }

  const postArgs = state.complexPost
    ? {
        main_post: state.complexPost.main_post,
        reply_post: state.complexPost.reply_post,
      }
    : {
        post: state.post,
      };

  const interruptValue: HumanInterrupt = {
    action_request: {
      action: "Schedule Twitter/LinkedIn post",
      args: {
        ...postArgs,
        date: defaultDateString,
        // Do not provide an image field if the mode is text only
        ...(!isTextOnlyMode && { image: state.image?.imageUrl ?? "" }),
      },
    },
    config: {
      allow_accept: true,
      allow_edit: true,
      allow_ignore: true,
      allow_respond: true,
    },
    description: constructDescription({
      report: state.report,
      originalLink: state.links[0],
      relevantLinks: state.relevantLinks || [],
      post: state.post,
      imageOptions: state.imageOptions,
      unknownResponseDescription,
      isTextOnlyMode,
    }),
  };

  // Save ALL links used to generate this post so that they are not used to generate future posts (duplicates).
  await saveUsedUrls([...(state.relevantLinks ?? []), ...state.links], config);

  const response = interrupt<HumanInterrupt[], HumanResponse[]>([
    interruptValue,
  ])[0];

  if (!["edit", "ignore", "accept", "response"].includes(response.type)) {
    throw new Error(
      `Unexpected response type: ${response.type}. Must be "edit", "ignore", "accept", or "response".`,
    );
  }
  if (response.type === "ignore") {
    return {
      next: END,
    } as Update;
  }
  if (!response.args) {
    throw new Error(
      `Unexpected response args: ${response.args}. Must be defined.`,
    );
  }

  if (response.type === "response") {
    if (typeof response.args !== "string") {
      throw new Error("Response args must be a string.");
    }

    const { route } = await routeResponse({
      post: state.post,
      dateOrPriority: defaultDateString,
      userResponse: response.args,
    });

    if (route === "rewrite_post") {
      return {
        userResponse: response.args,
        next: "rewritePost",
      } as Update;
    } else if (route === "update_date") {
      return {
        userResponse: response.args,
        next: "updateScheduleDate",
      } as Update;
    } else if (route === "rewrite_with_split_url") {
      return {
        userResponse: undefined,
        next: "rewriteWithSplitUrl",
      } as Update;
    }

    return {
      userResponse: response.args,
      next: "unknownResponse",
    } as Update;
  }

  if (typeof response.args !== "object") {
    throw new Error(
      `Unexpected response args type: ${typeof response.args}. Must be an object.`,
    );
  }
  if (!("args" in response.args)) {
    throw new Error(
      `Unexpected response args value: ${response.args}. Must be defined.`,
    );
  }

  const castArgs = response.args.args as unknown as Record<string, string>;

  const post = castArgs.post;
  const complexPost =
    castArgs.main_post && castArgs.reply_post
      ? {
          main_post: castArgs.main_post,
          reply_post: castArgs.reply_post,
        }
      : undefined;
  if (!post && !complexPost) {
    throw new Error(
      `Unexpected response args value: ${post}. Must be defined.\n\nResponse args:\n${JSON.stringify(response.args, null, 2)}`,
    );
  }

  const postDateString = castArgs.date;
  let postDate: DateType | undefined;
  if (postDateString) {
    postDate = parseDateResponse(postDateString);
    if (!postDate) {
      throw new Error(
        "Invalid date provided.\n\n" +
          "Expected format: 'MM/dd/yyyy hh:mm a z' or 'P1'/'P2'/'P3' or leave empty to post now.\n\n" +
          `Received: '${postDateString}'`,
      );
    }
  }

  let imageState: { imageUrl: string; mimeType: string } | undefined =
    undefined;
  if (!isTextOnlyMode) {
    const processedImage = await processImageInput(castArgs.image);
    if (processedImage && processedImage !== "remove") {
      imageState = processedImage;
    } else if (processedImage === "remove") {
      imageState = undefined;
    } else {
      imageState = state.image;
    }
  }

  return {
    next: "schedulePost",
    scheduleDate: postDate,
    ...(post ? { post } : {}),
    ...(complexPost ? { complexPost } : {}),
    // TODO: Update so if the mime type is blacklisted, it re-routes to human node with an error message.
    image: imageState,
    userResponse: undefined,
  } as Update;
}



================================================
FILE: src/agents/shared/nodes/generate-post/rewrite-post.ts
================================================
import { Client } from "@langchain/langgraph-sdk";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { BaseGeneratePostState, BaseGeneratePostUpdate } from "./types.js";
import { ChatAnthropic } from "@langchain/anthropic";
import {
  getReflectionsPrompt,
  REFLECTIONS_PROMPT,
} from "../../../../utils/reflections.js";

const REWRITE_POST_PROMPT = `You're a highly regarded marketing employee, working on crafting thoughtful and engaging content for the LinkedIn and Twitter pages.
You wrote a post for the LinkedIn and Twitter pages, however your boss has asked for some changes to be made before it can be published.

The original post you wrote is as follows:
<original-post>
{originalPost}
</original-post>

{reflectionsPrompt}

Listen to your boss closely, and make the necessary changes to the post. You should respond ONLY with the updated post, with no additional information, or text before or after the post.`;

interface RunReflectionsArgs {
  originalPost: string;
  newPost: string;
  userResponse: string;
}

/**
 * Kick off a new run to generate reflections.
 * @param param0
 */
async function runReflections({
  originalPost,
  newPost,
  userResponse,
}: RunReflectionsArgs) {
  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const thread = await client.threads.create();
  await client.runs.create(thread.thread_id, "reflection", {
    input: {
      originalPost,
      newPost,
      userResponse,
    },
  });
}

export async function rewritePost<
  State extends BaseGeneratePostState = BaseGeneratePostState,
  Update extends BaseGeneratePostUpdate = BaseGeneratePostUpdate,
>(state: State, config: LangGraphRunnableConfig): Promise<Update> {
  if (!state.post) {
    throw new Error("No post found");
  }
  if (!state.userResponse) {
    throw new Error("No user response found");
  }

  const rewritePostModel = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0.5,
  });

  const reflections = await getReflectionsPrompt(config);
  const reflectionsPrompt = REFLECTIONS_PROMPT.replace(
    "{reflections}",
    reflections,
  );

  const systemPrompt = REWRITE_POST_PROMPT.replace(
    "{originalPost}",
    state.post,
  ).replace("{reflectionsPrompt}", reflectionsPrompt);

  const revisePostResponse = await rewritePostModel.invoke([
    {
      role: "system",
      content: systemPrompt,
    },
    {
      role: "user",
      content: state.userResponse,
    },
  ]);

  await runReflections({
    originalPost: state.post,
    newPost: revisePostResponse.content as string,
    userResponse: state.userResponse,
  });

  return {
    post: revisePostResponse.content as string,
    next: undefined,
    userResponse: undefined,
  } as Update;
}



================================================
FILE: src/agents/shared/nodes/generate-post/schedule-post.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import {
  BaseGeneratePostState,
  BaseGeneratePostUpdate,
  ComplexPost,
} from "./types.js";
import { Client } from "@langchain/langgraph-sdk";
import {
  getScheduledDateSeconds,
  getFutureDate,
} from "../../../../utils/schedule-date/index.js";
import { SlackClient } from "../../../../clients/slack/client.js";
import { isTextOnly, shouldPostToLinkedInOrg } from "../../../utils.js";
import {
  POST_TO_LINKEDIN_ORGANIZATION,
  TEXT_ONLY_MODE,
} from "../../../generate-post/constants.js";

interface SendSlackMessageArgs {
  isTextOnlyMode: boolean;
  afterSeconds: number | undefined;
  threadId: string;
  runId: string;
  postContent: string | ComplexPost;
  image?: {
    imageUrl: string;
    mimeType: string;
  };
}

async function sendSlackMessage({
  isTextOnlyMode,
  afterSeconds,
  threadId,
  runId,
  postContent,
  image,
}: SendSlackMessageArgs) {
  if (!process.env.SLACK_CHANNEL_ID) {
    console.warn(
      "No SLACK_CHANNEL_ID found in environment variables. Can not send error message to Slack.",
    );
    return;
  }

  const slackClient = new SlackClient();

  const postStr =
    typeof postContent === "string"
      ? `Post:
\`\`\`
${postContent}
\`\`\``
      : `Main post:
\`\`\`
${postContent.main_post}
\`\`\`
Reply post:
\`\`\`
${postContent.reply_post}
\`\`\``;

  const imageString = image?.imageUrl
    ? `Image:
${image?.imageUrl}`
    : "No image provided";

  const messageString = `*New Post Scheduled*
    
Scheduled post for: *${afterSeconds ? getFutureDate(afterSeconds) : "now"}*
Run ID: *${runId}*
Thread ID: *${threadId}*

${postStr}

${!isTextOnlyMode ? imageString : "Text only mode enabled. Image support has been disabled."}`;

  await slackClient.sendMessage(process.env.SLACK_CHANNEL_ID, messageString);
}

export async function schedulePost<
  State extends BaseGeneratePostState = BaseGeneratePostState,
  Update extends BaseGeneratePostUpdate = BaseGeneratePostUpdate,
>(state: State, config: LangGraphRunnableConfig): Promise<Update> {
  if (!state.post) {
    throw new Error("No post to schedule found");
  }
  const isTextOnlyMode = isTextOnly(config);
  const postToLinkedInOrg = shouldPostToLinkedInOrg(config);

  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  let afterSeconds: number | undefined;
  if (state.scheduleDate) {
    afterSeconds = await getScheduledDateSeconds({
      scheduleDate: state.scheduleDate,
      config,
    });
  }

  const thread = await client.threads.create();
  const run = await client.runs.create(thread.thread_id, "upload_post", {
    input: {
      post: state.post,
      complexPost: state.complexPost,
      image: state.image,
    },
    config: {
      configurable: {
        [POST_TO_LINKEDIN_ORGANIZATION]: postToLinkedInOrg,
        [TEXT_ONLY_MODE]: isTextOnlyMode,
      },
    },
    ...(afterSeconds ? { afterSeconds } : {}),
  });

  try {
    await sendSlackMessage({
      isTextOnlyMode,
      afterSeconds,
      threadId: thread.thread_id,
      runId: run.run_id,
      postContent: state.complexPost || state.post,
      image: state.image,
    });
  } catch (e) {
    console.error("Failed to send schedule post Slack message", e);
  }

  return {} as Update;
}



================================================
FILE: src/agents/shared/nodes/generate-post/types.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { DateType } from "../../../types.js";
import { IngestDataAnnotation } from "../../../ingest-data/ingest-data-state.js";
import { VerifyLinksResultAnnotation } from "../../../verify-links/verify-links-state.js";

export type ComplexPost = {
  /**
   * The main post content.
   */
  main_post: string;
  /**
   * The reply post content.
   */
  reply_post: string;
};

const BaseGeneratePostAnnotation = Annotation.Root({
  /**
   * The generated post for LinkedIn/Twitter.
   */
  post: Annotation<string>,
  /**
   * The complex post, if the user decides to split the URL from the main body.
   *
   * TODO: Refactor the post/complexPost state interfaces to use a single shared interface
   * which includes images too.
   * Tracking issue: https://github.com/langchain-ai/social-media-agent/issues/144
   */
  complexPost: Annotation<ComplexPost | undefined>,
  /**
   * The date to schedule the post for.
   */
  scheduleDate: Annotation<DateType>,
  /**
   * The image to attach to the post, and the MIME type.
   */
  image: Annotation<
    | {
        imageUrl: string;
        mimeType: string;
      }
    | undefined
  >,
  /**
   * The links to use to generate a post.
   */
  links: Annotation<string[]>,
  /**
   * The report generated on the content of the message. Used
   * as context for generating the post.
   */
  report: IngestDataAnnotation.spec.report,
  ...VerifyLinksResultAnnotation.spec,
  /**
   * The node to execute next.
   */
  next: Annotation<string | undefined>,
  /**
   * Response from the user for the post. Typically used to request
   * changes to be made to the post.
   */
  userResponse: Annotation<string | undefined>,
});

export type BaseGeneratePostState = typeof BaseGeneratePostAnnotation.State;
export type BaseGeneratePostUpdate = typeof BaseGeneratePostAnnotation.Update;



================================================
FILE: src/agents/shared/stores/post-subject-urls.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { skipUsedUrlsCheck } from "../../utils.js";

const NAMESPACE = ["saved_data", "used_urls"];
const KEY = "urls";
const OBJECT_KEY = "data";

/**
 * Save a list of URLs of which have been included in posts already.
 *
 * @param urls The list of URLs to save
 * @param config The configuration for the langgraph
 * @param overwrite Whether to overwrite the stored URLs if they already exist.
 *  If true, it will overwrite. If false (default), it will first fetch the current stored URLs and add the new ones.
 * @returns {Promise<void>}
 */
export async function saveUsedUrls(
  urls: string[],
  config: LangGraphRunnableConfig,
  overwrite = false,
): Promise<void> {
  if (await skipUsedUrlsCheck(config.configurable)) {
    return;
  }

  const store = config.store;
  if (!store) {
    throw new Error("No store provided");
  }

  const urlsToSaveSet = new Set(urls);
  if (!overwrite) {
    const existingUrls = await getSavedUrls(config);
    existingUrls.forEach((url) => urlsToSaveSet.add(url));
  }

  await store.put(NAMESPACE, KEY, {
    [OBJECT_KEY]: Array.from(urlsToSaveSet),
  });
}

/**
 * Get the list of URLs of which have been included in posts already.
 *
 * @param config The configuration for the langgraph
 * @returns {Promise<string[]>} The list of URLs which have been included in posts already.
 */
export async function getSavedUrls(
  config: LangGraphRunnableConfig,
): Promise<string[]> {
  if (await skipUsedUrlsCheck(config.configurable)) {
    return [];
  }

  const store = config.store;
  if (!store) {
    throw new Error("No store provided");
  }
  const urls = await store.get(NAMESPACE, KEY);
  if (!urls) {
    return [];
  }
  return urls.value?.[OBJECT_KEY] || [];
}



================================================
FILE: src/agents/shared/youtube/video-summary.ts
================================================
import { ChatVertexAI } from "@langchain/google-vertexai-web";
import { getPrompts } from "../../generate-post/prompts/index.js";
import {
  getChannelInfo,
  getVideoThumbnailUrl,
  getYouTubeVideoDuration,
} from "../nodes/youtube.utils.js";
import { HumanMessage } from "@langchain/core/messages";
import { shouldExcludeYouTubeContent } from "../../should-exclude.js";

const GENERATE_REPORT_PROMPT = `You are a highly regarded marketing employee at a large software company.
You have been assigned the provided YouTube video, and you need to generate a summary report of the content in the video.
Specifically, you should be focusing on the technical details, why people should care about it, and any problems it solves.
You should also focus on the products the video might talk about (although not all videos will have your company content).

${getPrompts().businessContext}

Given this context, examine the YouTube videos contents closely, and generate a report on the video.
For context, this report will be used to generate a Tweet and LinkedIn post promoting the video and the company products it uses, if any.
Ensure to include in your report if this video is relevant to your company's products, and if so, include content in your report on what the video covered in relation to your company's products.`;

async function generateVideoSummary(url: string): Promise<string> {
  const model = new ChatVertexAI({
    model: "gemini-2.0-flash",
    temperature: 0,
  });

  const mediaMessage = new HumanMessage({
    content: [
      {
        type: "text",
        text: "Here is the YouTube video",
      },
      {
        type: "media",
        mimeType: "video/mp4",
        fileUri: url,
      },
    ],
  });

  const summaryResult = await model
    .withConfig({
      runName: "generate-video-summary-model",
    })
    .invoke([
      {
        role: "system",
        content: GENERATE_REPORT_PROMPT,
      },
      mediaMessage,
    ]);
  return summaryResult.content as string;
}

export async function getVideoSummary(
  url: string,
  skipExclusionCheck = false,
): Promise<{
  thumbnail: string | undefined;
  summary: string;
}> {
  const [videoDurationS, videoThumbnail, channelInfo] = await Promise.all([
    getYouTubeVideoDuration(url),
    getVideoThumbnailUrl(url),
    getChannelInfo(url),
  ]);

  if (!skipExclusionCheck) {
    const shouldExclude = shouldExcludeYouTubeContent(channelInfo.channelName);
    if (shouldExclude) {
      return {
        thumbnail: "",
        summary: "",
      };
    }
  }

  if (videoDurationS === undefined) {
    // TODO: Handle this better
    throw new Error("Failed to get video duration");
  }

  // 1800 = 30 minutes
  if (videoDurationS > 1800) {
    // TODO: Replace with interrupt requesting user confirm if they want to continue
    throw new Error(
      "Video is longer than 30 minutes, please confirm you want to continue.",
    );
  }

  const videoSummary = await generateVideoSummary(url);

  return {
    thumbnail: videoThumbnail,
    summary: videoSummary,
  };
}



================================================
FILE: src/agents/supervisor/supervisor-graph.ts
================================================
import { Annotation, END, Send, START, StateGraph } from "@langchain/langgraph";
import {
  SupervisorAnnotation,
  SupervisorConfigurableAnnotation,
  SupervisorState,
} from "./supervisor-state.js";
import { curateDataGraph } from "../curate-data/index.js";
import { convertPostToString } from "../verify-reddit-post/utils.js";
import { generateReportGraph } from "../generate-report/index.js";
import { groupReports } from "./nodes/group-reports.js";
import { determinePostType } from "./nodes/determine-post-type.js";
import { generatePosts } from "./nodes/generate-posts.js";

function startGenerateReportRuns(state: SupervisorState): Send[] {
  const {
    tweetsGroupedByContent,
    githubTrendingData,
    generalContents,
    redditPosts,
  } = state.curatedData;
  const tweetSends =
    tweetsGroupedByContent?.map((tweetGroup) => {
      return new Send("generateReport", {
        tweetGroup,
      });
    }) || [];
  const githubSends =
    githubTrendingData?.map((ghTrendingItem) => {
      return new Send("generateReport", {
        pageContent: [ghTrendingItem.pageContent],
        relevantLinks: [ghTrendingItem.repoURL],
      });
    }) || [];
  const generalSends =
    generalContents?.map((gc) => {
      return new Send("generateReport", {
        pageContent: [gc.pageContent],
        relevantLinks: gc.relevantLinks,
      });
    }) || [];
  const redditSends =
    redditPosts?.map((rp) => {
      return new Send("generateReport", {
        pageContent: [convertPostToString(rp)],
        relevantLinks: [rp.post.url],
      });
    }) || [];

  return [...tweetSends, ...githubSends, ...generalSends, ...redditSends];
}

const supervisorWorkflow = new StateGraph(
  { stateSchema: SupervisorAnnotation, input: Annotation.Root({}) },
  SupervisorConfigurableAnnotation,
)
  // Calls the curate-data agent to fetch data from different sources.
  // This also means grouping the data into related groups, and expanding
  // the external URLs found in the tweets.
  .addNode("ingestData", curateDataGraph)
  .addNode("generateReport", generateReportGraph)
  .addNode("groupReports", groupReports)
  .addNode("determinePostType", determinePostType)
  .addNode("generatePosts", generatePosts)

  .addEdge(START, "ingestData")
  .addConditionalEdges("ingestData", startGenerateReportRuns, [
    "generateReport",
  ])
  .addEdge("generateReport", "groupReports")
  .addEdge("groupReports", "determinePostType")
  .addEdge("determinePostType", "generatePosts")
  .addEdge("generatePosts", END);

export const supervisorGraph = supervisorWorkflow.compile();
supervisorGraph.name = "Supervisor Graph";



================================================
FILE: src/agents/supervisor/supervisor-state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { Source } from "./types.js";
import { CuratedData } from "../curate-data/types.js";

export const SupervisorAnnotation = Annotation.Root({
  /**
   * The final data object from ingesting all sources.
   */
  curatedData: Annotation<CuratedData>,
  /**
   * A list of reports, each containing a report & key details on a given data source/data group.
   * The report is used for generating a post, and key details are used for identifying reports on the same topic.
   */
  reports: Annotation<
    Array<{
      report: string;
      keyDetails: string;
    }>
  >({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  /**
   * The list of reports after they have been grouped.
   */
  groupedReports: Annotation<
    Array<{
      reports: string[];
      keyDetails: string[];
    }>
  >,
  /**
   * The report and type of post to generate.
   */
  reportAndPostType: Annotation<
    Array<{
      reports: string[];
      keyDetails: string[];
      reason: string;
      type: "thread" | "post";
    }>
  >,
  /**
   * Thread and run IDs, along with the type of post to generate.
   */
  idsAndTypes: Annotation<
    Array<{
      type: "thread" | "post";
      thread_id: string;
      run_id: string;
    }>
  >({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
});

export const SupervisorConfigurableAnnotation = Annotation.Root({
  /**
   * The sources to ingest from.
   */
  sources: Annotation<Source[]>,
});

export type SupervisorState = typeof SupervisorAnnotation.State;



================================================
FILE: src/agents/supervisor/types.ts
================================================
export type Source =
  | "github"
  | "twitter"
  | "latent_space"
  | "ai_news"
  | "reddit";



================================================
FILE: src/agents/supervisor/nodes/determine-post-type.ts
================================================
import { ChatAnthropic } from "@langchain/anthropic";
import { SupervisorState } from "../supervisor-state.js";
import { z } from "zod";

const DETERMINE_POST_TYPE_PROMPT = `You're a highly skilled marketer, working to craft new social media content for your Twitter & LinkedIn pages.
You're given a report (or reports) on a technical AI topic. Based on the report(s), you should determine if this report should be used to generate a long form 'thread' like post, or a shorter, more concise and straightforward post.

To do this, you should consider the following points:
- If the report is on a new product, release, academic paper, or similar, it's likely that you should generate a long form 'thread' like post.
- If the report is on a new method of doing something, or a smaller feature, it is likely that you should generate a shorter, more concise and straightforward post.
- Writing long form threads should be reserved for only the most detailed, interesting, and technical reports. All other reports should likely be categorized as a shorter, more concise and straightforward post.

Use your best judgement to determine the type of post to generate based on the report provided by the user.`;

const postTypeSchema = z
  .object({
    reason: z.string().describe("The reasoning behind your decision."),
    type: z
      .enum(["thread", "post"])
      .describe(
        "The type of post to generate. Thread for long form posts, post for shorter, more concise and straightforward posts.",
      ),
  })
  .describe(
    "The type of post to generate and reasoning behind your decision based on the report(s) provided.",
  );

function formatReportUserPrompt(report: {
  reports: string[];
  keyDetails: string[];
}) {
  if (report.reports.length === 1) {
    return `Here are the key details for the report:
<key-details>
${report.keyDetails[0] || "no key details"}
</key-details>

And here is the full report:
<report>
${report.reports[0]}
</report>

Please take your time, and identify the best type of post to generate for this report, and why! Thank you!`;
  }

  return `Here are all of the key details & reports I've written for this post:
<key-details-and-reports>
  ${report.reports
    .map(
      (r, index) => `
  <report index="${index}">
    ${r}
  </report>
  
  <key-details index="${index}">
    ${report.keyDetails[index] || "no key details"}
  </key-details>
  `,
    )
    .join("\n")}
</key-details-and-reports>

Please take your time, and identify the best type of post to generate for these reports, and why! Thank you!`;
}

export async function determinePostType(
  state: SupervisorState,
): Promise<Partial<SupervisorState>> {
  const model = new ChatAnthropic({
    model: "claude-3-5-sonnet-latest",
    temperature: 0,
  }).withStructuredOutput(postTypeSchema, {
    name: "postType",
  });

  const reportAndPostType: {
    reports: string[];
    keyDetails: string[];
    reason: string;
    type: "thread" | "post";
  }[] = [];

  for await (const report of state.groupedReports) {
    const result = await model.invoke([
      ["system", DETERMINE_POST_TYPE_PROMPT],
      ["user", formatReportUserPrompt(report)],
    ]);

    reportAndPostType.push({
      ...result,
      ...report,
    });
  }

  return {
    reportAndPostType,
  };
}



================================================
FILE: src/agents/supervisor/nodes/generate-posts.ts
================================================
import { Client } from "@langchain/langgraph-sdk";
import { SupervisorState } from "../supervisor-state.js";
import { extractUrls } from "../../utils.js";
import { SlackClient } from "../../../clients/slack/client.js";

export async function generatePosts(
  state: SupervisorState,
): Promise<Partial<SupervisorState>> {
  const client = new Client({
    apiUrl: `http://localhost:${process.env.PORT}`,
  });

  const idsAndTypes: Array<{
    type: "thread" | "post";
    thread_id: string;
    run_id: string;
  }> = [];

  for await (const reportAndPostType of state.reportAndPostType) {
    const { thread_id } = await client.threads.create();
    const reportsMapped = reportAndPostType.reports.map((report, index) => {
      if (!reportAndPostType.keyDetails[index]) {
        return report;
      }

      return `# Report Key Details:\n${reportAndPostType.keyDetails[index]}\n\n${report}`;
    });
    if (reportAndPostType.type === "thread") {
      const run = await client.runs.create(thread_id, "generate_thread", {
        input: {
          reports: reportsMapped,
        },
      });

      idsAndTypes.push({
        type: "thread",
        thread_id,
        run_id: run.run_id,
      });
    } else {
      const reportString = reportsMapped.join("\n");
      const linksInReport = extractUrls(reportString);
      const run = await client.runs.create(thread_id, "generate_post", {
        input: {},
        command: {
          goto: "generatePost",
          update: {
            report: reportString,
            links: [linksInReport?.[0] || ""],
            relevantLinks: [linksInReport?.[0] || ""],
          },
        },
      });

      idsAndTypes.push({
        type: "post",
        thread_id,
        run_id: run.run_id,
      });
    }
  }

  if (!process.env.SLACK_CHANNEL_ID || !process.env.SLACK_BOT_OAUTH_TOKEN) {
    return {};
  }

  const slackClient = new SlackClient();

  const messageText = `*Ingested data successfully processed*
  
Number of threads: *${idsAndTypes.filter((x) => x.type === "thread").length}*
Number of individual posts: *${idsAndTypes.filter((x) => x.type === "post").length}*

Thread post IDs:
${idsAndTypes
  .filter((x) => x.type === "thread")
  .map((x) => `- *${x.thread_id}* : *${x.run_id}*`)
  .join("\n")}
  
Single post IDs:
${idsAndTypes
  .filter((x) => x.type === "post")
  .map((x) => `- *${x.thread_id}* : *${x.run_id}*`)
  .join("\n")}`;

  await slackClient.sendMessage(process.env.SLACK_CHANNEL_ID, messageText);

  return {
    idsAndTypes,
  };
}



================================================
FILE: src/agents/supervisor/nodes/group-reports.ts
================================================
import { z } from "zod";
import { SupervisorState } from "../supervisor-state.js";
import { ChatVertexAI } from "@langchain/google-vertexai-web";

const IDENTIFY_SIMILAR_REPORTS_PROMPT = `You are an advanced AI assistant tasked with identifying similar reports based on key details.

You should first read the key details carefully, identify any possible reports which are on the same 'topic'. Here are some rules to use when identifying similar reports:
- The reports are similar if the details are on the same product, product release, or announcement.
- The reports are NOT similar if the details are on the same product, but a different release, announcement, or subject.


Given this, inspect each report carefully, then determine if it's similar enough to any other reports, which would warrant the reports being combined.

Your response should contain a list of objects, where each object contains an array of indices of the reports which are similar.
If a report is not similar to any other reports, you may omit it from the list.

Here are the key details from all of the reports:
<report-key-details>
{reportKeyDetails}
</report-key-details>`;

function formatReportKeyDetails(
  reports: Array<{ report: string; keyDetails: string }>,
) {
  return reports
    .map(
      (report, index) =>
        `<report index="${index}">\n${report.keyDetails}\n</report>`,
    )
    .join("\n");
}

const responseSchema = z
  .object({
    similarReports: z
      .array(
        z.object({
          indices: z
            .array(z.number())
            .describe(
              "A list of report indices which are similar enough to be grouped into one report.",
            ),
        }),
      )
      .describe(
        "A list of objects, each containing an array of indices of similar reports",
      ),
  })
  .describe(
    "A tool schema to call when grouping similar reports. Ensure this tool is ALWAYS CALLED NO MATTER WHAT.",
  );

function processGroupedReports(
  reports: Array<{ report: string; keyDetails: string }>,
  similarReports: { indices: number[] }[],
): Array<{ reports: string[]; keyDetails: string[] }> {
  // Create a Set to keep track of processed indices
  const processedIndices = new Set<number>();
  const result: Array<{ reports: string[]; keyDetails: string[] }> = [];

  // Process grouped reports first
  for (const group of similarReports) {
    if (!group.indices.length) {
      continue;
    }

    const reportGroup = {
      reports: group.indices.map((index) => {
        processedIndices.add(index);
        return reports[index].report;
      }),
      keyDetails: group.indices.map((index) => reports[index].keyDetails),
    };

    result.push(reportGroup);
  }

  // Add ungrouped reports as single-item groups
  for (let i = 0; i < reports.length; i++) {
    if (!processedIndices.has(i)) {
      result.push({
        reports: [reports[i].report],
        keyDetails: [reports[i].keyDetails],
      });
    }
  }

  return result;
}

export async function groupReports(
  state: SupervisorState,
): Promise<Partial<SupervisorState>> {
  const model = new ChatVertexAI({
    model: "gemini-2.0-flash",
    temperature: 0,
  }).withStructuredOutput(responseSchema, {
    name: "groupSimilarReports",
  });

  const formattedPrompt = IDENTIFY_SIMILAR_REPORTS_PROMPT.replace(
    "{reportKeyDetails}",
    formatReportKeyDetails(state.reports),
  );

  const result = await model.invoke([["user", formattedPrompt]]);

  const newReports = processGroupedReports(
    state.reports,
    result.similarReports,
  );

  return {
    groupedReports: newReports,
  };
}



================================================
FILE: src/agents/supervisor/tests/e2e.int.test.ts
================================================
import * as ls from "langsmith/jest";
import { SimpleEvaluator } from "langsmith/jest";
import { Client } from "@langchain/langgraph-sdk";
import { Client as LSClient } from "langsmith";
import { SupervisorState } from "../supervisor-state.js";

const e2eEvaluator: SimpleEvaluator = () => {
  return {
    key: "successful_run",
    score: 1,
  };
};

ls.describe("SMA - Supervisor - E2E", () => {
  ls.test(
    "Can run end-to-end for Twitter posts",
    {
      inputs: {},
      expected: {},
    },
    async () => {
      const lsClient = new LSClient();
      const ingestDataExamples = lsClient.listExamples({
        datasetId: "4ebe89f0-f008-4d97-b8dd-86b70221ab0f",
        exampleIds: ["a112b870-6826-472e-8f67-317068d5a8bb"],
      });
      let inputs: Record<string, any> = {};
      for await (const ex of ingestDataExamples) {
        inputs = {
          ...ex.inputs,
        };
      }
      const client = new Client({
        // apiUrl: `http://localhost:${process.env.PORT || "2024"}`,
        apiUrl: "http://localhost:54367",
      });

      console.log("Before invoking supervisor graph", inputs);
      const { thread_id } = await client.threads.create();
      const result = await client.runs.wait(thread_id, "supervisor", {
        input: inputs,
        config: {
          configurable: {
            sources: ["twitter"],
          },
        },
      });
      console.log("After invoking supervisor graph");

      const { idsAndTypes } = result as SupervisorState;

      console.log("Waiting for all generate post results");
      const allGeneratePostResults = await Promise.allSettled(
        idsAndTypes.map(async ({ thread_id, run_id, type }) => {
          const result = await client.runs.join(thread_id, run_id);
          console.log(`Got generate ${type} result:\n`, result);
          return result;
        }),
      );

      await ls.expect(allGeneratePostResults).evaluatedBy(e2eEvaluator).toBe(1);
      return result;
    },
    480000, // 8 minutes
  );
});



================================================
FILE: src/agents/upload-post/index.ts
================================================
import {
  Annotation,
  END,
  LangGraphRunnableConfig,
  START,
  StateGraph,
} from "@langchain/langgraph";
import { TwitterClient } from "../../clients/twitter/client.js";
import {
  imageUrlToBuffer,
  isTextOnly,
  shouldPostToLinkedInOrg,
  useArcadeAuth,
  useTwitterApiOnly,
} from "../utils.js";
import { CreateMediaRequest } from "../../clients/twitter/types.js";
import { LinkedInClient } from "../../clients/linkedin.js";
import {
  LINKEDIN_ACCESS_TOKEN,
  LINKEDIN_ORGANIZATION_ID,
  LINKEDIN_PERSON_URN,
  POST_TO_LINKEDIN_ORGANIZATION,
  TEXT_ONLY_MODE,
} from "../generate-post/constants.js";
import { SlackClient } from "../../clients/slack/client.js";
import { ComplexPost } from "../shared/nodes/generate-post/types.js";

async function getMediaFromImage(image?: {
  imageUrl: string;
  mimeType: string;
}): Promise<CreateMediaRequest | undefined> {
  if (!image) return undefined;
  const { buffer, contentType } = await imageUrlToBuffer(image.imageUrl);
  return {
    media: buffer,
    mimeType: contentType,
  };
}

const UploadPostAnnotation = Annotation.Root({
  post: Annotation<string>,
  /**
   * The complex post, if the user decides to split the URL from the main body.
   *
   * TODO: Refactor the post/complexPost state interfaces to use a single shared interface
   * which includes images too.
   * Tracking issue: https://github.com/langchain-ai/social-media-agent/issues/144
   */
  complexPost: Annotation<ComplexPost | undefined>,
  image: Annotation<
    | {
        imageUrl: string;
        mimeType: string;
      }
    | undefined
  >,
});

const UploadPostGraphConfiguration = Annotation.Root({
  [POST_TO_LINKEDIN_ORGANIZATION]: Annotation<boolean | undefined>,
  /**
   * Whether or not to use text only mode throughout the graph.
   * If true, it will not try to extract, validate, or upload images.
   * Additionally, it will not be able to handle validating YouTube videos.
   * @default false
   */
  [TEXT_ONLY_MODE]: Annotation<boolean | undefined>({
    reducer: (_state, update) => update,
    default: () => false,
  }),
});

interface PostUploadFailureToSlackArgs {
  uploadDestination: "twitter" | "linkedin";
  error: any;
  threadId: string;
  postContent: string | ComplexPost;
  image?: {
    imageUrl: string;
    mimeType: string;
  };
}

async function postUploadFailureToSlack({
  uploadDestination,
  error,
  threadId,
  postContent,
  image,
}: PostUploadFailureToSlackArgs) {
  if (!process.env.SLACK_CHANNEL_ID) {
    console.warn(
      "No SLACK_CHANNEL_ID found in environment variables. Can not send error message to Slack.",
    );
    return;
  }
  const slackClient = new SlackClient();

  const postStr =
    typeof postContent === "string"
      ? `Post:
\`\`\`
${postContent}
\`\`\``
      : `Main post:
\`\`\`
${postContent.main_post}
\`\`\`
Reply post:
\`\`\`
${postContent.reply_post}
\`\`\``;

  const slackMessageContent = `âŒ FAILED TO UPLOAD POST TO ${uploadDestination.toUpperCase()} âŒ

Error message:
\`\`\`
${error}
\`\`\`

Thread ID: *${threadId}*

${postStr}

${image ? `Image:\nURL: ${image.imageUrl}\nMIME type: ${image.mimeType}` : ""}
`;
  await slackClient.sendMessage(
    process.env.SLACK_CHANNEL_ID,
    slackMessageContent,
  );
}

export async function uploadPost(
  state: typeof UploadPostAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof UploadPostAnnotation.State>> {
  if (!state.post) {
    throw new Error("No post text found");
  }
  const isTextOnlyMode = isTextOnly(config);
  const postToLinkedInOrg = shouldPostToLinkedInOrg(config);

  try {
    let twitterClient: TwitterClient;

    if (useTwitterApiOnly() || !useArcadeAuth()) {
      twitterClient = TwitterClient.fromBasicTwitterAuth();
    } else {
      const twitterUserId = process.env.TWITTER_USER_ID;
      if (!twitterUserId) {
        throw new Error("Twitter user ID not found in configurable fields.");
      }

      const twitterToken = process.env.TWITTER_USER_TOKEN;
      const twitterTokenSecret = process.env.TWITTER_USER_TOKEN_SECRET;

      twitterClient = await TwitterClient.fromArcade(
        twitterUserId,
        {
          twitterToken,
          twitterTokenSecret,
        },
        {
          textOnlyMode: isTextOnlyMode,
        },
      );
    }

    let mediaBuffer: CreateMediaRequest | undefined = undefined;
    if (!isTextOnlyMode) {
      mediaBuffer = await getMediaFromImage(state.image);
    }

    if (state.complexPost) {
      await twitterClient.uploadThread([
        {
          text: state.complexPost.main_post,
          ...(mediaBuffer && { media: mediaBuffer }),
        },
        {
          text: state.complexPost.reply_post,
        },
      ]);
    } else {
      await twitterClient.uploadTweet({
        text: state.post,
        ...(mediaBuffer && { media: mediaBuffer }),
      });
    }

    console.log("âœ… Successfully uploaded Tweet âœ…");
  } catch (e: any) {
    console.error("Failed to upload post:", e);
    let errorString = "";
    if (typeof e === "object" && "message" in e) {
      errorString = e.message;
    } else {
      errorString = e;
    }
    await postUploadFailureToSlack({
      uploadDestination: "twitter",
      error: errorString,
      threadId:
        config.configurable?.thread_id || "no thread id found in configurable",
      postContent: state.complexPost || state.post,
      image: state.image,
    });
  }

  try {
    let linkedInClient: LinkedInClient;

    if (useArcadeAuth()) {
      const linkedInUserId = process.env.LINKEDIN_USER_ID;
      if (!linkedInUserId) {
        throw new Error("LinkedIn user ID not found in configurable fields.");
      }

      linkedInClient = await LinkedInClient.fromArcade(linkedInUserId, {
        postToOrganization: postToLinkedInOrg,
      });
    } else {
      const accessToken =
        process.env.LINKEDIN_ACCESS_TOKEN ||
        config.configurable?.[LINKEDIN_ACCESS_TOKEN];
      if (!accessToken) {
        throw new Error(
          "LinkedIn access token not found in environment or configurable fields. Either set it, or use Arcade Auth.",
        );
      }

      const personUrn =
        process.env.LINKEDIN_PERSON_URN ||
        config.configurable?.[LINKEDIN_PERSON_URN];
      const organizationId =
        process.env.LINKEDIN_ORGANIZATION_ID ||
        config.configurable?.[LINKEDIN_ORGANIZATION_ID];
      linkedInClient = new LinkedInClient({
        accessToken: accessToken,
        personUrn: personUrn,
        organizationId: organizationId,
      });
    }

    if (!isTextOnlyMode && state.image) {
      await linkedInClient.createImagePost(
        {
          text: state.post,
          imageUrl: state.image.imageUrl,
        },
        {
          postToOrganization: postToLinkedInOrg,
        },
      );
    } else {
      await linkedInClient.createTextPost(state.post, {
        postToOrganization: postToLinkedInOrg,
      });
    }

    console.log("âœ… Successfully uploaded post to LinkedIn âœ…");
  } catch (e: any) {
    console.error("Failed to upload post:", e);
    let errorString = "";
    if (typeof e === "object" && "message" in e) {
      errorString = e.message;
    } else {
      errorString = e;
    }
    await postUploadFailureToSlack({
      uploadDestination: "linkedin",
      error: errorString,
      threadId:
        config.configurable?.thread_id || "no thread id found in configurable",
      postContent: state.complexPost || state.post,
      image: state.image,
    });
  }

  return {};
}

const uploadPostWorkflow = new StateGraph(
  UploadPostAnnotation,
  UploadPostGraphConfiguration,
)
  .addNode("uploadPost", uploadPost)
  .addEdge(START, "uploadPost")
  .addEdge("uploadPost", END);

export const uploadPostGraph = uploadPostWorkflow.compile();
uploadPostGraph.name = "Upload Post Graph";



================================================
FILE: src/agents/verify-links/verify-links-graph.ts
================================================
import { END, Send, START, StateGraph } from "@langchain/langgraph";
import { VerifyContentAnnotation } from "../shared/shared-state.js";
import { verifyYouTubeContent } from "../shared/nodes/verify-youtube.js";
import { verifyGeneralContent } from "../shared/nodes/verify-general.js";
import { verifyGitHubContent } from "../shared/nodes/verify-github.js";
import { verifyTweetGraph } from "../verify-tweet/verify-tweet-graph.js";
import {
  VerifyLinksGraphAnnotation,
  VerifyLinksGraphConfigurableAnnotation,
} from "./verify-links-state.js";
import { getUrlType } from "../utils.js";
import { verifyRedditPostGraph } from "../verify-reddit-post/verify-reddit-post-graph.js";
import { VerifyRedditPostAnnotation } from "../verify-reddit-post/verify-reddit-post-state.js";
import { verifyLumaEvent } from "../shared/nodes/verify-luma.js";

function routeLinkTypes(state: typeof VerifyLinksGraphAnnotation.State) {
  return state.links.map((link) => {
    const type = getUrlType(link);
    if (type === "twitter") {
      return new Send("verifyTweetSubGraph", {
        link,
      });
    }
    if (type === "youtube") {
      return new Send("verifyYouTubeContent", {
        link,
      });
    }
    if (type === "github") {
      return new Send("verifyGitHubContent", {
        link,
      });
    }
    if (type === "reddit") {
      return new Send("verifyRedditContent", {
        link,
      });
    }
    if (type === "luma") {
      return new Send("verifyLumaEvent", {
        link,
      });
    }
    return new Send("verifyGeneralContent", {
      link,
    });
  });
}

const verifyLinksWorkflow = new StateGraph(
  VerifyLinksGraphAnnotation,
  VerifyLinksGraphConfigurableAnnotation,
)
  .addNode("verifyYouTubeContent", verifyYouTubeContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGeneralContent", verifyGeneralContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGitHubContent", verifyGitHubContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyTweetSubGraph", verifyTweetGraph, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyRedditContent", verifyRedditPostGraph, {
    input: VerifyRedditPostAnnotation,
  })
  .addNode("verifyLumaEvent", verifyLumaEvent, {
    input: VerifyContentAnnotation,
  })
  // Start node
  .addConditionalEdges(START, routeLinkTypes, [
    "verifyYouTubeContent",
    "verifyGeneralContent",
    "verifyGitHubContent",
    "verifyTweetSubGraph",
    "verifyRedditContent",
    "verifyLumaEvent",
  ])
  .addEdge("verifyRedditContent", END)
  .addEdge("verifyYouTubeContent", END)
  .addEdge("verifyGeneralContent", END)
  .addEdge("verifyGitHubContent", END)
  .addEdge("verifyTweetSubGraph", END)
  .addEdge("verifyLumaEvent", END);

export const verifyLinksGraph = verifyLinksWorkflow.compile();
verifyLinksGraph.name = "Verify Links Subgraph";



================================================
FILE: src/agents/verify-links/verify-links-state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { filterUnwantedImageUrls } from "../utils.js";
import { SKIP_CONTENT_RELEVANCY_CHECK } from "../generate-post/constants.js";

export const VerifyLinksGraphSharedAnnotation = Annotation.Root({
  /**
   * The links to verify.
   */
  links: Annotation<string[]>,
});

const sharedLinksReducer = (
  state: string[] | undefined,
  update: string[] | undefined,
) => {
  if (update === undefined) return undefined;
  // Use a set to ensure no duplicate links are added.
  const stateSet = new Set(state || []);
  update.filter((u): u is string => !!u).forEach((link) => stateSet.add(link));
  return filterUnwantedImageUrls(Array.from(stateSet));
};

export const VerifyLinksResultAnnotation = Annotation.Root({
  /**
   * Page content used in the verification nodes. Will be used in the report
   * generation node.
   */
  pageContents: Annotation<string[] | undefined>({
    reducer: (state, update) => {
      if (update === undefined) return undefined;
      return (state || []).concat(update);
    },
    default: () => [],
  }),
  /**
   * Relevant links found in the message.
   */
  relevantLinks: Annotation<string[] | undefined>({
    reducer: sharedLinksReducer,
    default: () => [],
  }),
  /**
   * Image options to provide to the user.
   */
  imageOptions: Annotation<string[] | undefined>({
    reducer: sharedLinksReducer,
    default: () => [],
  }),
});

export const VerifyLinksGraphAnnotation = Annotation.Root({
  /**
   * The links to verify.
   */
  links: VerifyLinksGraphSharedAnnotation.spec.links,
  ...VerifyLinksResultAnnotation.spec,
});

export const VerifyLinksGraphConfigurableAnnotation = Annotation.Root({
  /**
   * Whether or not to skip the content relevancy check.
   */
  [SKIP_CONTENT_RELEVANCY_CHECK]: Annotation<boolean | undefined>(),
});



================================================
FILE: src/agents/verify-reddit-post/types.ts
================================================
import { SimpleRedditPostWithComments } from "../../clients/reddit/types.js";
import { VerifyRedditPostAnnotation } from "./verify-reddit-post-state.js";

export type RedditPostRoot = RedditPostRoot2[];

export interface RedditPostRoot2 {
  kind: string;
  data: RedditPostData;
}

export interface RedditPostData {
  after: any;
  dist?: number;
  modhash: string;
  geo_filter: string;
  children: RedditPostChildren[];
  before: any;
}

export interface RedditPostChildren {
  kind: string;
  data: RedditPostData2;
}

export interface RedditPostData2 {
  approved_at_utc: any;
  subreddit: string;
  selftext?: string;
  user_reports: any[];
  saved: boolean;
  mod_reason_title: any;
  gilded: number;
  clicked?: boolean;
  title?: string;
  link_flair_richtext?: any[];
  subreddit_name_prefixed: string;
  hidden?: boolean;
  pwls?: number;
  link_flair_css_class?: string;
  downs: number;
  thumbnail_height: any;
  top_awarded_type: any;
  hide_score?: boolean;
  name: string;
  quarantine?: boolean;
  link_flair_text_color?: string;
  upvote_ratio?: number;
  author_flair_background_color?: string;
  subreddit_type: string;
  ups: number;
  total_awards_received: number;
  media_embed?: RedditPostMediaEmbed;
  thumbnail_width: any;
  author_flair_template_id: any;
  is_original_content?: boolean;
  author_fullname?: string;
  secure_media: any;
  is_reddit_media_domain?: boolean;
  is_meta?: boolean;
  category: any;
  secure_media_embed?: RedditPostSecureMediaEmbed;
  link_flair_text?: string;
  can_mod_post: boolean;
  score: number;
  approved_by: any;
  is_created_from_ads_ui?: boolean;
  author_premium?: boolean;
  thumbnail?: string;
  edited: any;
  author_flair_css_class: any;
  author_flair_richtext?: any[];
  gildings: RedditPostGildings;
  content_categories: any;
  is_self?: boolean;
  mod_note: any;
  created: number;
  link_flair_type?: string;
  wls?: number;
  removed_by_category: any;
  banned_by: any;
  author_flair_type?: string;
  domain?: string;
  allow_live_comments?: boolean;
  selftext_html?: string;
  likes: any;
  suggested_sort: any;
  banned_at_utc: any;
  view_count: any;
  archived: boolean;
  no_follow: boolean;
  is_crosspostable?: boolean;
  pinned?: boolean;
  over_18?: boolean;
  all_awardings: any[];
  awarders: any[];
  media_only?: boolean;
  link_flair_template_id?: string;
  can_gild: boolean;
  spoiler?: boolean;
  locked: boolean;
  author_flair_text: any;
  treatment_tags: any[];
  visited?: boolean;
  removed_by: any;
  num_reports: any;
  distinguished: any;
  subreddit_id: string;
  author_is_blocked: boolean;
  mod_reason_by: any;
  removal_reason: any;
  link_flair_background_color?: string;
  id: string;
  is_robot_indexable?: boolean;
  num_duplicates?: number;
  report_reasons: any;
  author: string;
  discussion_type: any;
  num_comments?: number;
  send_replies: boolean;
  media: any;
  contest_mode?: boolean;
  author_patreon_flair?: boolean;
  author_flair_text_color?: string;
  permalink: string;
  stickied: boolean;
  url?: string;
  subreddit_subscribers?: number;
  created_utc: number;
  num_crossposts?: number;
  mod_reports: any[];
  is_video?: boolean;
  comment_type: any;
  link_id?: string;
  replies: any;
  collapsed_reason_code: any;
  parent_id?: string;
  body?: string;
  collapsed?: boolean;
  is_submitter?: boolean;
  body_html?: string;
  collapsed_reason: any;
  associated_award: any;
  unrepliable_reason: any;
  score_hidden?: boolean;
  controversiality?: number;
  depth?: number;
  collapsed_because_crowd_control: any;
}

export interface RedditPostMediaEmbed {}

export interface RedditPostSecureMediaEmbed {}

export interface RedditPostGildings {}

export interface FormattedRedditPost {
  post: string;
  replies: string[];
}

export type RedditPostsWithExternalData = SimpleRedditPostWithComments & {
  externalData: {
    url: string;
    pageContent: string;
  }[];
};

export type VerifyRedditGraphState = typeof VerifyRedditPostAnnotation.State;



================================================
FILE: src/agents/verify-reddit-post/utils.ts
================================================
import {
  SimpleRedditComment,
  SimpleRedditPostWithComments,
} from "../../clients/reddit/types.js";

export function formatComments(comments: SimpleRedditComment[]): string {
  return comments
    .map(
      (c) =>
        `${c.author}: ${c.body}${c.replies ? "\nReply:\n" + formatComments(c.replies) : ""}`,
    )
    .join("\n");
}

export function convertPostToString(
  redditPostWithComments: SimpleRedditPostWithComments,
): string {
  const mainPost = `${redditPostWithComments.post.title}
${redditPostWithComments.post.selftext}
${redditPostWithComments.post.url || ""}`;
  const comments = redditPostWithComments.comments
    ? formatComments(redditPostWithComments.comments)
    : "";
  return `${mainPost}${comments ? "\n\nComments:\n" + comments : ""}`;
}



================================================
FILE: src/agents/verify-reddit-post/verify-reddit-post-graph.ts
================================================
/**
 * Get content
 * Fetch & validate any links in main post
 * Validate main post, including content from links & images
 * return all content as page contents.
 */
import { END, Send, START, StateGraph } from "@langchain/langgraph";
import { verifyYouTubeContent } from "../shared/nodes/verify-youtube.js";
import { verifyGeneralContent } from "../shared/nodes/verify-general.js";
import { verifyGitHubContent } from "../shared/nodes/verify-github.js";
import { VerifyContentAnnotation } from "../shared/shared-state.js";
import {
  VerifyRedditPostAnnotation,
  VerifyRedditPostConfigurableAnnotation,
} from "./verify-reddit-post-state.js";
import { validateRedditPost } from "./nodes/validate-reddit-post.js";
import { getExternalUrls } from "./nodes/get-external-urls.js";
import { getUrlType } from "../utils.js";
import { getPost } from "./nodes/get-post.js";

/**
 * This conditional edge will iterate over all the links in a Reddit post.
 * It creates a `Send` for each link, which will invoke a node specific to that website.
 */
function routePostUrls(state: typeof VerifyRedditPostAnnotation.State) {
  if (!state.externalURLs.length) {
    // No external URLs found in the post, end the graph
    return END;
  }

  return state.externalURLs.map((link) => {
    const urlType = getUrlType(link);
    if (urlType === "youtube") {
      return new Send("verifyYouTubeContent", {
        link,
      });
    } else if (urlType === "github") {
      return new Send("verifyGitHubContent", {
        link,
      });
    } else {
      return new Send("verifyGeneralContent", {
        link,
      });
    }
  });
}

const verifyRedditPostBuilder = new StateGraph(
  VerifyRedditPostAnnotation,
  VerifyRedditPostConfigurableAnnotation,
)
  .addNode("getPost", getPost)
  .addNode("getExternalUrls", getExternalUrls)

  // Validates any GitHub, YouTube, or other URLs found in the Reddit post content.
  .addNode("verifyYouTubeContent", verifyYouTubeContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGeneralContent", verifyGeneralContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGitHubContent", verifyGitHubContent, {
    input: VerifyContentAnnotation,
  })

  // Validates the final Reddit post content, including any content/summaries generated
  // or extracted from URLs inside the Reddit post.
  .addNode("validateRedditPost", validateRedditPost)

  // Start node
  .addEdge(START, "getPost")
  .addEdge("getPost", "getExternalUrls")
  // After getting the content & nested URLs, route to either the nodes, or go
  // straight to validation if no URLs found.
  .addConditionalEdges("getExternalUrls", routePostUrls, [
    "verifyYouTubeContent",
    "verifyGeneralContent",
    "verifyGitHubContent",
    END,
  ])

  // After verifying the different content types, we should validate them in combination with the Reddit post content.
  .addEdge("verifyYouTubeContent", "validateRedditPost")
  .addEdge("verifyGeneralContent", "validateRedditPost")
  .addEdge("verifyGitHubContent", "validateRedditPost")

  // Finally, finish the graph after validating the Reddit post content.
  .addEdge("validateRedditPost", END);

export const verifyRedditPostGraph = verifyRedditPostBuilder.compile();

verifyRedditPostGraph.name = "Verify Reddit Post Subgraph";



================================================
FILE: src/agents/verify-reddit-post/verify-reddit-post-state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { SimpleRedditPostWithComments } from "../../clients/reddit/types.js";
import {
  VerifyLinksGraphConfigurableAnnotation,
  VerifyLinksResultAnnotation,
} from "../verify-links/verify-links-state.js";
import { SKIP_CONTENT_RELEVANCY_CHECK } from "../generate-post/constants.js";

export const VerifyRedditPostAnnotation = Annotation.Root({
  /**
   * The reddit post to verify. Optional, if not provided then a `link`, or `postID` must be provided.
   */
  redditPost: Annotation<SimpleRedditPostWithComments | undefined>,
  /**
   * A link to a Reddit post. Optional, if not provided then a `redditPost` or `postID` must be provided.
   */
  link: Annotation<string | undefined>,
  /**
   * The ID of a Reddit post. Optional, if not provided then a `redditPost` or `link` must be provided.
   */
  postID: Annotation<string | undefined>,
  /**
   * The external URLs found in the body of the Reddit post.
   */
  externalURLs: Annotation<string[]>({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  // REQUIRED DUE TO USING SHARED NODES
  ...VerifyLinksResultAnnotation.spec,
});

export const VerifyRedditPostConfigurableAnnotation = Annotation.Root({
  ...VerifyLinksGraphConfigurableAnnotation.spec,
});

export type VerifyRedditPostConfigurable =
  typeof VerifyRedditPostConfigurableAnnotation.State;

export const BASE_VERIFY_REDDIT_CONFIG: VerifyRedditPostConfigurable = {
  [SKIP_CONTENT_RELEVANCY_CHECK]: undefined,
};



================================================
FILE: src/agents/verify-reddit-post/nodes/get-external-urls.ts
================================================
import { extractUrls, getUrlType } from "../../utils.js";
import { VerifyRedditGraphState } from "../types.js";

export async function getExternalUrls(
  state: VerifyRedditGraphState,
): Promise<Partial<VerifyRedditGraphState>> {
  if (!state.redditPost) {
    throw new Error("No reddit post found");
  }
  const urls = extractUrls(state.redditPost.post.selftext);
  const filteredUrls = urls.filter((url) => getUrlType(url) !== "reddit");

  const postUrl = state.redditPost.post.url;
  if (
    postUrl &&
    getUrlType(postUrl) !== "reddit" &&
    !filteredUrls.includes(postUrl)
  ) {
    filteredUrls.push(postUrl);
  }

  return {
    externalURLs: filteredUrls,
  };
}



================================================
FILE: src/agents/verify-reddit-post/nodes/get-post.ts
================================================
import { RedditClient } from "../../../clients/reddit/client.js";
import { VerifyRedditGraphState } from "../types.js";

export async function getPost(
  state: VerifyRedditGraphState,
): Promise<Partial<VerifyRedditGraphState>> {
  if (state.redditPost) {
    // Post already exists, don't do anything
    return {};
  }

  const client = await RedditClient.fromUserless();
  const redditPost = await client.getSimplePostAndComments(
    state.postID || state.link || "",
  );

  return {
    redditPost,
  };
}



================================================
FILE: src/agents/verify-reddit-post/nodes/validate-reddit-post.ts
================================================
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyRedditGraphState } from "../types.js";
import { z } from "zod";
import { convertPostToString, formatComments } from "../utils.js";
import { skipContentRelevancyCheck } from "../../utils.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { verifyContentIsRelevant } from "../../shared/nodes/verify-content.js";

const VALIDATE_REDDIT_POST_PROMPT = `You are a highly regarded marketing employee.
You're provided with a Reddit post, and some of the comments (not guaranteed, some Reddit posts don't have comments).
Additionally, if the Reddit post contains links to other webpages, you'll be provided with the content of those webpages.

Now, given all of this context, your task is to determine whether or not the post & optionally linked content is relevant to the context outlined below:
${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

You should carefully read over all of the content submitted to you, and determine whether or not the content is actually relevant to you.
You should provide reasoning as to why or why not the post & additional content is relevant to you, then a simple true or false for whether or not it is relevant.`;

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the content from the Reddit post is or isn't relevant to your business context.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the content from the Reddit post is relevant to your business context.",
      ),
  })
  .describe("The relevancy of the content to your business context.");

function formatUserPrompt(state: VerifyRedditGraphState) {
  if (!state.redditPost) {
    throw new Error("No reddit post found");
  }

  const formattedPost = `<reddit_post>
${state.redditPost.post.title}
${state.redditPost.post.selftext}
</reddit_post>`;
  const formattedComments = state.redditPost.comments?.length
    ? `<reddit_post_comments>
${formatComments(state.redditPost.comments)}
</reddit_post_comments>`
    : "";
  const fullFormattedPost = `${formattedPost}\n${formattedComments}`;

  const formattedExternalContent = state.pageContents?.length
    ? state.pageContents
        .map(
          (
            c,
            idx,
          ) => `<external_content${state.relevantLinks?.[idx] ? ` url="${state.relevantLinks[idx]}"` : ""}>
${c}
</external_content>`,
        )
        .join("\n")
    : "";

  const fullPrompt = `${fullFormattedPost}\n\n${formattedExternalContent}`;
  return fullPrompt;
}

export async function validateRedditPost(
  state: VerifyRedditGraphState,
  config: LangGraphRunnableConfig,
): Promise<Partial<VerifyRedditGraphState>> {
  const returnValue = {
    relevantLinks: [...state.externalURLs],
    pageContents: [
      ...(state.redditPost ? [convertPostToString(state.redditPost)] : []),
    ],
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  if (
    await verifyContentIsRelevant(formatUserPrompt(state), {
      systemPrompt: VALIDATE_REDDIT_POST_PROMPT,
      schema: RELEVANCY_SCHEMA,
    })
  ) {
    return returnValue;
  }

  // If the content is not relevant, reset the state so it contains empty values
  return {
    redditPost: undefined,
    externalURLs: undefined,
    relevantLinks: undefined,
    pageContents: undefined,
    imageOptions: undefined,
  };
}



================================================
FILE: src/agents/verify-reddit-post/tests/e2e.int.test.ts
================================================
import { v4 as uuidv4 } from "uuid";
import * as ls from "langsmith/jest";
import { type SimpleEvaluator } from "langsmith/jest";
import { InMemoryStore, MemorySaver } from "@langchain/langgraph";
import { INPUTS } from "./data/inputs-outputs.js";
import { verifyRedditPostGraph } from "../verify-reddit-post-graph.js";
import { VerifyRedditGraphState } from "../types.js";
import { BASE_VERIFY_REDDIT_CONFIG } from "../verify-reddit-post-state.js";

const checkVerifyPostResult: SimpleEvaluator = ({ expected, actual }) => {
  const { pageContents } = actual as VerifyRedditGraphState;
  const { relevant } = expected as { relevant: boolean };

  const hasPageContentsAndLinks = pageContents && pageContents?.length > 0;

  if (relevant) {
    return {
      key: "validation_result_expected",
      score: Number(hasPageContentsAndLinks),
    };
  }

  return {
    key: "validation_result_expected",
    score: Number(!hasPageContentsAndLinks),
  };
};

ls.describe("SMA - Verify Reddit Post - E2E", () => {
  ls.test.each(INPUTS)(
    "Evaluates the verify reddit post agent",
    async ({ inputs }) => {
      verifyRedditPostGraph.checkpointer = new MemorySaver();
      verifyRedditPostGraph.store = new InMemoryStore();

      const threadId = uuidv4();
      const config = {
        configurable: {
          ...BASE_VERIFY_REDDIT_CONFIG,
          thread_id: threadId,
        },
      };

      const results = await verifyRedditPostGraph.invoke(inputs, config);
      console.log("Finished invoking graph with URL", inputs.link);
      await ls
        .expect(results)
        .evaluatedBy(checkVerifyPostResult)
        // Expect this to be 1, if it's 0 that means there's a discrepancy between the expected, and whether or not page contents and links were found
        .toBe(1);
      return results;
    },
  );
});



================================================
FILE: src/agents/verify-reddit-post/tests/data/inputs-outputs.ts
================================================
export const INPUTS = [
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1grcgeg/how_can_i_parallelize_nodes_in_langgraph_without/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1gs0b2p/hierarchical_agent_teams_keyerrornext/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1gujxbv/llmcompile_example_error_received_multiple/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1gz0140/launch_langgraph_unofficial_virtual_meetup_series/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1gzpz2n/overcoming_output_token_limit_with_agent/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1h20jwz/mcp_server_tools_langgraph_integration_example/",
    },
    expected: {
      relevant: true,
    },
  },
  // This is linking to the LangChain documentation. TBD if we want to add prompting to exclude this, since we might want to generate posts from our docs in the future.
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1h7d6it/adding_authentication_to_self_hosted_langgraph/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1i4lzzk/created_langgraphuisdk_package_with_tools/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangGraph/comments/1i5uvt0/universal_assistant_with_langgraph_and_anthropics/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i5xdru/what_setups_do_i_need_to_build_a_llm_use_case_on/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i6bte8/what_are_you_working_on/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i6fxcv/need_help_regarding_crm_integration/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i79ua0/which_version_of_langchain_and_promptflow_to_use/",
    },
    expected: {
      relevant: false,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i75ixh/gurubase_an_opensource_rag_system_built_with/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i5h3o6/new_walkthrough_video_for_langgraph_fastapi/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i5t5ry/notate_opensource_rag_desktop_application/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    // TBD If this works. Depends on if the reddit api will also pull in the content from the post this is "reposting"
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i5ow10/hugging_face_will_teach_you_how_to_use_langchain/",
    },
    expected: {
      relevant: true,
    },
  },
  {
    inputs: {
      link: "https://www.reddit.com/r/LangChain/comments/1i4v60d/sharing_our_open_source_poc_for_openai_realtime/",
    },
    expected: {
      relevant: true,
    },
  },
];



================================================
FILE: src/agents/verify-tweet/verify-tweet-graph.ts
================================================
import { END, Send, START, StateGraph } from "@langchain/langgraph";
import {
  VerifyTweetAnnotation,
  VerifyTweetConfigurableAnnotation,
} from "./verify-tweet-state.js";
import { getTweetContent } from "./nodes/get-tweet-content.js";
import { verifyYouTubeContent } from "../shared/nodes/verify-youtube.js";
import { verifyGeneralContent } from "../shared/nodes/verify-general.js";
import { verifyGitHubContent } from "../shared/nodes/verify-github.js";
import { VerifyContentAnnotation } from "../shared/shared-state.js";
import { validateTweetContent } from "./nodes/validate-tweet.js";

/**
 * This conditional edge will iterate over all the links in a Tweet.
 * It creates a `Send` for each link, which will invoke a node specific to that website.
 */
function routeTweetUrls(state: typeof VerifyTweetAnnotation.State) {
  if (!state.tweetContent?.length) {
    return END;
  }

  if (!state.tweetContentUrls.length) {
    return "validateTweet";
  }

  return state.tweetContentUrls.map((link) => {
    if (link.includes("youtube.com")) {
      return new Send("verifyYouTubeContent", {
        link,
      });
    } else if (link.includes("github.com")) {
      return new Send("verifyGitHubContent", {
        link,
      });
    } else {
      return new Send("verifyGeneralContent", {
        link,
      });
    }
  });
}

const verifyTweetBuilder = new StateGraph(
  VerifyTweetAnnotation,
  VerifyTweetConfigurableAnnotation,
)
  // Calls the Twitter API to get the content, and extracts + validates any
  // URLs found in the Tweet content.
  .addNode("getTweetContent", getTweetContent)

  // Validates any GitHub, YouTube, or other URLs found in the Tweet content.
  .addNode("verifyYouTubeContent", verifyYouTubeContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGeneralContent", verifyGeneralContent, {
    input: VerifyContentAnnotation,
  })
  .addNode("verifyGitHubContent", verifyGitHubContent, {
    input: VerifyContentAnnotation,
  })

  // Validates the final Tweet content, including any content/summaries generated
  // or extracted from URLs inside the Tweet.
  .addNode("validateTweet", validateTweetContent)

  // Start node
  .addEdge(START, "getTweetContent")
  // After getting the content & nested URLs, route to either the nodes, or go
  // straight to validation if no URLs found.
  .addConditionalEdges("getTweetContent", routeTweetUrls, [
    "verifyYouTubeContent",
    "verifyGeneralContent",
    "verifyGitHubContent",
    "validateTweet",
    END,
  ])

  // After verifying the different content types, we should validate them in combination with the Tweet content.
  .addEdge("verifyYouTubeContent", "validateTweet")
  .addEdge("verifyGeneralContent", "validateTweet")
  .addEdge("verifyGitHubContent", "validateTweet")

  // Finally, finish the graph after validating the Tweet content.
  .addEdge("validateTweet", END);

export const verifyTweetGraph = verifyTweetBuilder.compile();

verifyTweetGraph.name = "Verify Tweet Subgraph";



================================================
FILE: src/agents/verify-tweet/verify-tweet-state.ts
================================================
import { Annotation } from "@langchain/langgraph";
import { VerifyContentAnnotation } from "../shared/shared-state.js";
import {
  VerifyLinksGraphConfigurableAnnotation,
  VerifyLinksResultAnnotation,
} from "../verify-links/verify-links-state.js";

export const VerifyTweetAnnotation = Annotation.Root({
  /**
   * The link to the content to verify.
   */
  link: VerifyContentAnnotation.spec.link,
  /**
   * The raw content of the Tweet
   */
  tweetContent: Annotation<string>,
  /**
   * URLs which were found in the Tweet
   */
  tweetContentUrls: Annotation<string[]>({
    reducer: (state, update) => state.concat(update),
    default: () => [],
  }),
  ...VerifyLinksResultAnnotation.spec,
  /**
   * Page content used in the verification nodes. Will be used in the report
   * generation node.
   *
   * pageContents is defined in the VerifyLinksResultAnnotation spec, so
   *  we spread it above this to ensure it uses this custom reducer.
   */
  pageContents: Annotation<string[] | undefined>({
    reducer: (state, update) => {
      if (update === undefined) return undefined;

      if (update[0]?.startsWith("The following is the content of the Tweet:")) {
        // This means the update is from validateTweetContent so we can remove
        // all other state fields.
        return update;
      }

      return (state || []).concat(update);
    },
    default: () => [],
  }),
});

export const VerifyTweetConfigurableAnnotation = Annotation.Root({
  ...VerifyLinksGraphConfigurableAnnotation.spec,
});



================================================
FILE: src/agents/verify-tweet/nodes/get-tweet-content.ts
================================================
import { VerifyTweetAnnotation } from "../verify-tweet-state.js";
import { extractTweetId } from "../../utils.js";
import {
  getFullThreadText,
  getMediaUrls,
  resolveAndReplaceTweetTextLinks,
} from "../../../clients/twitter/utils.js";
import { TweetV2, TweetV2SingleResult } from "twitter-api-v2";
import { shouldExcludeTweetContent } from "../../should-exclude.js";
import { getTwitterClient } from "../../../clients/twitter/client.js";

export async function getTweetContent(
  state: typeof VerifyTweetAnnotation.State,
) {
  const tweetId = extractTweetId(state.link);
  if (!tweetId) {
    console.error("Failed to extract tweet ID from link:", state.link);
    return {};
  }

  const twitterClient = await getTwitterClient();

  let tweetContent: TweetV2SingleResult | undefined;
  try {
    tweetContent = await twitterClient.getTweet(tweetId);
    if (!tweetContent) {
      throw new Error("No tweet content returned from Twitter API.");
    }
  } catch (e: any) {
    console.error("Failed to get tweet content", e);
    return {};
  }

  const threadReplies: TweetV2[] = [];
  try {
    if (tweetContent.data.author_id) {
      threadReplies.push(
        ...(await twitterClient.getThreadReplies(
          tweetId,
          tweetContent.data.author_id,
        )),
      );
    }
  } catch (e) {
    console.error("Failed to get thread replies", e);
  }

  const mediaUrls = await getMediaUrls(tweetContent, threadReplies);
  const tweetContentText = getFullThreadText(tweetContent, threadReplies);

  const { content, externalUrls } =
    await resolveAndReplaceTweetTextLinks(tweetContentText);

  const shouldExclude = shouldExcludeTweetContent(externalUrls);
  if (shouldExclude) {
    return {};
  }

  if (!externalUrls.length) {
    return {
      tweetContent: content,
      imageOptions: mediaUrls,
    };
  }

  return {
    tweetContent: content,
    tweetContentUrls: externalUrls,
    imageOptions: mediaUrls,
  };
}



================================================
FILE: src/agents/verify-tweet/nodes/validate-tweet.ts
================================================
import { z } from "zod";
import { getPrompts } from "../../generate-post/prompts/index.js";
import { VerifyTweetAnnotation } from "../verify-tweet-state.js";
import { skipContentRelevancyCheck } from "../../utils.js";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import { verifyContentIsRelevant } from "../../shared/nodes/verify-content.js";

const RELEVANCY_SCHEMA = z
  .object({
    reasoning: z
      .string()
      .describe(
        "Reasoning for why the webpage is or isn't relevant to your company's products.",
      ),
    relevant: z
      .boolean()
      .describe(
        "Whether or not the webpage is relevant to your company's products.",
      ),
  })
  .describe("The relevancy of the content to your company's products.");

const VERIFY_RELEVANT_CONTENT_PROMPT = `You are a highly regarded marketing employee.
You're provided with a Tweet, and the page content of links in the Tweet. This Tweet was sent to you by a third party claiming it's relevant and implements your company's products.
Your task is to carefully read over the entire page, and determine whether or not the content actually implements and is relevant to your company's products.
You're doing this to ensure the content is relevant to your company, and it can be used as marketing material to promote your company.

${getPrompts().businessContext}

${getPrompts().contentValidationPrompt}

Given this context, examine the entire Tweet plus webpage content closely, and determine if the content implements your company's products.
You should provide reasoning as to why or why not the content implements your company's products, then a simple true or false for whether or not it implements some.`;

function constructContext({
  tweetContent,
  pageContents,
}: {
  tweetContent: string;
  pageContents: string[];
}): string {
  const tweetPrompt = `The following is the content of the Tweet:
<tweet-content>
${tweetContent}
</tweet-content>`;
  const webpageContents =
    pageContents.length > 0
      ? `The following are the contents of the webpage${pageContents.length > 1 ? "s" : ""} linked in the Tweet:
${pageContents.map((content, index) => `<webpage-content key="${index}">\n${content}\n</webpage-content>`).join("\n")}`
      : "No webpage content was found in the Tweet.";

  return `${tweetPrompt}\n\n${webpageContents}`;
}

/**
 * Verifies the Tweet & webpage contents provided is relevant to your company's products.
 */
export async function validateTweetContent(
  state: typeof VerifyTweetAnnotation.State,
  config: LangGraphRunnableConfig,
): Promise<Partial<typeof VerifyTweetAnnotation.State>> {
  if (!state.pageContents?.length && !state.tweetContent) {
    throw new Error(
      "Missing page contents and tweet contents. One of these must be defined to verify the Tweet content.",
    );
  }
  const context = constructContext({
    tweetContent: state.tweetContent,
    pageContents: state.pageContents || [],
  });

  const returnValue = {
    relevantLinks: [state.link, ...state.tweetContentUrls],
    pageContents: [context],
  };

  if (await skipContentRelevancyCheck(config.configurable)) {
    return returnValue;
  }

  if (
    await verifyContentIsRelevant(context, {
      systemPrompt: VERIFY_RELEVANT_CONTENT_PROMPT,
      schema: RELEVANCY_SCHEMA,
    })
  ) {
    return returnValue;
  }

  return {
    relevantLinks: [],
    pageContents: [],
    imageOptions: [],
  };
}



================================================
FILE: src/agents/verify-tweet/nodes/tests/get-tweet-content.int.test.ts
================================================
import "dotenv/config";
import * as ls from "langsmith/jest";
import { type SimpleEvaluator } from "langsmith/jest";
import { getTweetContent } from "../get-tweet-content.js";

const checkCorrectTweetContents: SimpleEvaluator = ({ expected, actual }) => {
  const { tweetContent, tweetContentUrls, imageOptions } = actual as {
    tweetContent: string;
    tweetContentUrls: string[];
    imageOptions: string[];
  };

  const lengthDifference = Math.abs(
    tweetContent.length - expected.tweetContent.length,
  );
  if (lengthDifference > 10) {
    console.log("tweet content length differs by more than 10 characters");
    console.log("tweet content length:\n", tweetContent.length);
    console.log("\n\n---\n\n");
    console.log(
      "expected tweet content length:\n",
      expected.tweetContent.length,
    );
    console.log("difference:\n", lengthDifference);
    return {
      key: "tweet_content",
      score: 0,
    };
  }

  if (
    tweetContentUrls.every((url) => !expected.tweetContentUrls.includes(url))
  ) {
    console.log(
      "tweet content urls does NOT match expected tweet content urls",
    );
    console.log("tweet content urls:\n", tweetContentUrls);
    console.log("\n\n---\n\n");
    console.log("expected tweet content urls:\n", expected.tweetContentUrls);
    return {
      key: "tweet_content",
      score: 0,
    };
  }

  if (imageOptions.every((url) => !expected.imageOptions.includes(url))) {
    console.log("image options does NOT match expected image options");
    console.log("image options:\n", imageOptions);
    console.log("\n\n---\n\n");
    console.log("expected image options:\n", expected.imageOptions);
    return {
      key: "tweet_content",
      score: 0,
    };
  }

  return {
    key: "tweet_content",
    score: 1,
  };
};

const TEST_EACH_INPUTS_OUTPUTS = [
  {
    inputs: {
      link: "https://x.com/EdenEmarco177/status/1874884500062122296",
    },
    expected: {
      tweetContent: `Just casually sipping coffee with 4 Gen AI legends solving multi-agent systems ðŸ¤–â˜•ï¸
    
@assaf_elovic Head of AI @mondaydotcom creator of GPT ResearcherðŸ”Ž 
https://t.co/vEhroGRmWa
@ulidabess @ataiiam co founders of @CopilotKit ðŸª
@NirDiamantAI founder of DiamantAI and creator of 
https://t.co/cKNjHuTG9V
and 
https://t.co/D8wpEN4LJa

Still pinching myself - did this actually happen? ðŸ¤–

Shoutout to the @LangChainAI ðŸ¦œðŸ”—Ecosystem for bringing us together`,
      imageOptions: ["https://pbs.twimg.com/media/GgTtE-oXoAAxe3n.jpg"],
      tweetContentUrls: [
        "https://github.com/NirDiamant/RAG_Techniques",
        "https://github.com/NirDiamant/GenAI_Agents",
        "https://github.com/assafelovic/gpt-researcher",
      ],
    },
  },
];

ls.describe("SMA - Verify Tweet - Get Tweet Content", () => {
  ls.test.each(TEST_EACH_INPUTS_OUTPUTS)(
    "Should get the full content, urls and attachments",
    async ({ inputs }) => {
      // Import and run your app, or some part of it here
      const result = await getTweetContent(inputs as any);
      const evalResult = ls
        .expect(result)
        .evaluatedBy(checkCorrectTweetContents);
      await evalResult.toBe(1);
      return result;
    },
  );
});



================================================
FILE: src/clients/auth-server.ts
================================================
import express, { Request, Response, NextFunction } from "express";
import passport from "passport";
import { Strategy as TwitterStrategy } from "passport-twitter";
import session from "express-session";
import dotenv from "dotenv";

dotenv.config();

// Extend Express Session type
declare module "express-session" {
  interface SessionData {
    linkedinToken?: string;
    linkedinUserInfo?: any;
  }
}

interface TwitterUser {
  id: string;
  username: string;
  displayName: string;
  photos?: { value: string }[];
  _json: {
    id_str: string;
    screen_name: string;
    name: string;
    profile_image_url_https?: string;
  };
}

export class SocialAuthServer {
  private app: express.Application;
  private port: number;

  constructor(port = 3000) {
    this.app = express();
    this.port = port;
    this.configureMiddleware();
    this.configurePassport();
    this.setupRoutes();
  }

  private configureMiddleware(): void {
    this.app.use(
      session({
        secret: process.env.SESSION_SECRET || "your-session-secret",
        resave: true,
        saveUninitialized: true,
        cookie: {
          secure: false,
          maxAge: 60000,
        },
      }),
    );
    this.app.use(passport.initialize());
    this.app.use(passport.session());
  }

  private configurePassport(): void {
    this.configureTwitterStrategy();
    this.configureLinkedInStrategy();

    passport.serializeUser((user, done) => {
      done(null, user);
    });

    passport.deserializeUser<any>((user, done) => {
      done(null, user);
    });
  }

  private configureTwitterStrategy(): void {
    if (!process.env.TWITTER_API_KEY || !process.env.TWITTER_API_KEY_SECRET) {
      throw new Error("Twitter API credentials are not configured");
    }

    passport.use(
      new TwitterStrategy(
        {
          consumerKey: process.env.TWITTER_API_KEY,
          consumerSecret: process.env.TWITTER_API_KEY_SECRET,
          callbackURL: `http://localhost:${this.port}/auth/twitter/callback`,
        },
        (
          token: string,
          tokenSecret: string,
          profile: TwitterUser,
          done: (error: any, user?: any) => void,
        ) => {
          const user = {
            ...profile,
            token,
            tokenSecret,
          };
          console.log("\nâœ… TWITTER USER AUTHENTICATED âœ…\n");
          console.dir(user, { depth: null });
          return done(null, user);
        },
      ),
    );
  }

  private configureLinkedInStrategy(): void {
    if (
      !process.env.LINKEDIN_CLIENT_ID ||
      !process.env.LINKEDIN_CLIENT_SECRET
    ) {
      throw new Error("LinkedIn API credentials are not configured");
    }

    // We'll handle LinkedIn authentication directly through routes
    // instead of using passport strategy since we're implementing
    // the OAuth flow manually
  }

  private setupRoutes(): void {
    this.app.get("/", (_req: Request, res: Response) => {
      res.send(
        '<a href="/auth/twitter">Login with Twitter</a><br><a href="/auth/linkedin">Login with LinkedIn</a>',
      );
    });

    // Twitter routes
    this.app.get("/auth/twitter", passport.authenticate("twitter"));
    this.app.get("/auth/twitter/callback", (req: Request, res: Response) => {
      passport.authenticate("twitter", {
        failureRedirect: "/login",
      })(req, res, (err: any) => {
        if (err) {
          console.error("Authentication error:", err);
          return res.redirect("/");
        }
        res.redirect("/");
      });
    });

    // LinkedIn routes
    this.app.get("/auth/linkedin", (_req: Request, res: Response) => {
      const authUrl = new URL(
        "https://www.linkedin.com/oauth/v2/authorization",
      );
      authUrl.searchParams.append("response_type", "code");
      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
      authUrl.searchParams.append("client_id", process.env.LINKEDIN_CLIENT_ID!);
      authUrl.searchParams.append(
        "redirect_uri",
        "http://localhost:3000/auth/linkedin/callback",
      );
      // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
      authUrl.searchParams.append("state", process.env.SESSION_SECRET!);
      authUrl.searchParams.append(
        "scope",
        "openid profile email w_member_social",
        // Use these scopes if you plan on posting to a LinkedIn company page.
        // Posting to company pages requires the "w_organization_social" scope.
        // "openid profile email w_member_social w_organization_social",
      );

      res.redirect(authUrl.toString());
    });

    this.app.get("/auth/linkedin/callback", (req: Request, res: Response) => {
      const { code, state } = req.query;

      if (state !== process.env.SESSION_SECRET) {
        return res
          .status(401)
          .send("State mismatch. Possible CSRF attack.") as any;
      }

      if (!code) {
        return res.redirect("/");
      }

      // Handle the LinkedIn OAuth token exchange
      (async () => {
        try {
          const tokenResponse = await fetch(
            "https://www.linkedin.com/oauth/v2/accessToken",
            {
              method: "POST",
              headers: {
                "Content-Type": "application/x-www-form-urlencoded",
              },
              body: new URLSearchParams({
                grant_type: "authorization_code",
                code: code as string,
                // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
                client_id: process.env.LINKEDIN_CLIENT_ID!,
                // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
                client_secret: process.env.LINKEDIN_CLIENT_SECRET!,
                redirect_uri: "http://localhost:3000/auth/linkedin/callback",
              }),
            },
          );

          if (!tokenResponse.ok) {
            throw new Error("Failed to get access token");
          }

          const data = await tokenResponse.json();
          console.log("\nâœ… LINKEDIN USER AUTHENTICATED âœ…\n");
          console.dir(data, { depth: null });

          // Fetch user info using OpenID Connect
          const userInfoResponse = await fetch(
            "https://api.linkedin.com/v2/userinfo",
            {
              headers: {
                Authorization: `Bearer ${data.access_token}`,
              },
            },
          );

          if (!userInfoResponse.ok) {
            throw new Error("Failed to get user info");
          }

          const userInfo = await userInfoResponse.json();
          console.log("\nâœ… LINKEDIN USER INFO âœ…\n");
          console.dir(userInfo, { depth: null });

          // Store both the token and user info in session
          if (req.session) {
            req.session.linkedinToken = data.access_token;
            req.session.linkedinUserInfo = userInfo;
          }

          res.redirect("/");
        } catch (error) {
          console.error("LinkedIn authentication error:", error);
          res.redirect("/");
        }
      })().catch(console.error);
    });

    this.app.get(
      "/profile",
      this.ensureAuthenticated,
      (req: Request, res: Response) => {
        res.json(req.user);
      },
    );
  }

  private ensureAuthenticated(
    req: Request,
    res: Response,
    next: NextFunction,
  ): void {
    if (req.isAuthenticated()) {
      return next();
    }
    res.redirect("/");
  }

  public start(): void {
    this.app.listen(this.port, () => {
      console.log(
        `Social authentication server is running on port ${this.port}. Go to http://localhost:${this.port} to login`,
      );
    });
  }
}

async function main() {
  const server = new SocialAuthServer();
  server.start();
}

if (import.meta.url === `file://${process.argv[1]}`) {
  main().catch(console.error);
}

export default SocialAuthServer;



================================================
FILE: src/clients/linkedin.ts
================================================
import Arcade from "@arcadeai/arcadejs";
import { AuthorizeUserResponse } from "./types.js";

interface LinkedInPost {
  author: string;
  lifecycleState: string;
  specificContent: {
    "com.linkedin.ugc.ShareContent": {
      shareCommentary: {
        text: string;
      };
      shareMediaCategory: string;
    };
  };
  visibility: {
    "com.linkedin.ugc.MemberNetworkVisibility": string;
  };
}

interface CreateLinkedInImagePostRequest {
  text: string;
  imageUrl: string;
  imageDescription?: string;
  imageTitle?: string;
}

interface MediaUploadResponse {
  value: {
    uploadMechanism: {
      "com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest": {
        headers: Record<string, string>;
        uploadUrl: string;
      };
    };
    mediaArtifact: string;
    asset: string;
  };
}

interface RegisterUploadRequest {
  registerUploadRequest: {
    recipes: string[];
    owner: string;
    serviceRelationships: Array<{
      relationshipType: string;
      identifier: string;
    }>;
  };
}

export class LinkedInClient {
  private baseURL = "https://api.linkedin.com/v2";
  private accessToken: string;
  private personUrn: string | undefined;
  private organizationId: string | undefined;

  constructor(input?: {
    accessToken: string | undefined;
    personUrn: string | undefined;
    organizationId: string | undefined;
  }) {
    const { accessToken, personUrn, organizationId } = {
      accessToken: process.env.LINKEDIN_ACCESS_TOKEN || input?.accessToken,
      organizationId:
        process.env.LINKEDIN_ORGANIZATION_ID || input?.organizationId,
      personUrn: process.env.LINKEDIN_PERSON_URN || input?.personUrn,
    };
    if (!accessToken) {
      throw new Error(
        "Missing LinkedIn access token. Please pass it via the constructor, or set the LINKEDIN_ACCESS_TOKEN environment variable.",
      );
    }
    if (!personUrn && !organizationId) {
      throw new Error(
        "Must provide at least one of personUrn or organizationId.",
      );
    }

    this.accessToken = accessToken;
    this.personUrn = personUrn;
    this.organizationId = organizationId;
  }

  /**
   * Returns the author string for making a post with the LinkedIn API.
   * @param options
   * @throws {Error} If neither personUrn nor organizationId is provided
   */
  private getAuthorString(options?: { postToOrganization?: boolean }): string {
    // First, attempt to use the organization ID if either the postToOrganization option is set, or the personUrn is not set
    if (options?.postToOrganization || !this.personUrn) {
      if (!this.organizationId) {
        throw new Error(
          "Missing organization ID. Please pass it via the constructor, or set the LINKEDIN_ORGANIZATION_ID environment variable.",
        );
      }
      return `urn:li:organization:${this.organizationId}`;
    }

    if (!this.personUrn) {
      throw new Error(
        "Missing person URN. Please pass it via the constructor, or set the LINKEDIN_PERSON_URN environment variable.",
      );
    }
    return `urn:li:person:${this.personUrn}`;
  }

  private async makeRequest<T>(url: string, options: RequestInit): Promise<T> {
    const response = await fetch(url, {
      ...options,
      headers: {
        Authorization: `Bearer ${this.accessToken}`,
        "Content-Type": "application/json",
        "X-Restli-Protocol-Version": "2.0.0",
        ...options.headers,
      },
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    return response.json();
  }

  // Create a text-only post
  async createTextPost(
    text: string,
    options?: {
      postToOrganization?: boolean;
    },
  ): Promise<Response> {
    const endpoint = `${this.baseURL}/ugcPosts`;
    const author = this.getAuthorString(options);

    const postData: LinkedInPost = {
      author,
      lifecycleState: "PUBLISHED",
      specificContent: {
        "com.linkedin.ugc.ShareContent": {
          shareCommentary: {
            text: text,
          },
          shareMediaCategory: "NONE",
        },
      },
      visibility: {
        "com.linkedin.ugc.MemberNetworkVisibility": "PUBLIC",
      },
    };

    return this.makeRequest(endpoint, {
      method: "POST",
      body: JSON.stringify(postData),
    });
  }

  private async registerAndUploadMedia(
    imageUrl: string,
    options: {
      author: string;
    },
  ): Promise<string> {
    // Step 1: Register the upload
    const registerEndpoint = `${this.baseURL}/assets?action=registerUpload`;

    const registerData: RegisterUploadRequest = {
      registerUploadRequest: {
        recipes: ["urn:li:digitalmediaRecipe:feedshare-image"],
        owner: options.author,
        serviceRelationships: [
          {
            relationshipType: "OWNER",
            identifier: "urn:li:userGeneratedContent",
          },
        ],
      },
    };

    const registerResponse = await this.makeRequest<MediaUploadResponse>(
      registerEndpoint,
      {
        method: "POST",
        body: JSON.stringify(registerData),
      },
    );

    // Step 2: Get the image data from the URL
    const imageResponse = await fetch(imageUrl);
    const imageBuffer = await imageResponse.arrayBuffer();

    // Step 3: Upload the image to LinkedIn
    const uploadUrl =
      registerResponse.value.uploadMechanism[
        "com.linkedin.digitalmedia.uploading.MediaUploadHttpRequest"
      ].uploadUrl;

    const uploadResponse = await fetch(uploadUrl, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${this.accessToken}`,
        "Content-Type": "application/octet-stream",
      },
      body: imageBuffer,
    });

    if (!uploadResponse.ok) {
      throw new Error(`Failed to upload image: ${uploadResponse.statusText}`);
    }

    return registerResponse.value.asset;
  }

  // Create a post with an image
  async createImagePost(
    {
      text,
      imageUrl,
      imageDescription,
      imageTitle,
    }: CreateLinkedInImagePostRequest,
    options?: {
      postToOrganization?: boolean;
    },
  ): Promise<Response> {
    // First register and upload the media
    const author = this.getAuthorString(options);
    const mediaAsset = await this.registerAndUploadMedia(imageUrl, { author });

    const endpoint = `${this.baseURL}/ugcPosts`;

    const postData = {
      author,
      lifecycleState: "PUBLISHED",
      specificContent: {
        "com.linkedin.ugc.ShareContent": {
          shareCommentary: {
            text,
          },
          shareMediaCategory: "IMAGE",
          media: [
            {
              status: "READY",
              description: {
                text: imageDescription ?? "Image description",
              },
              media: mediaAsset,
              title: {
                text: imageTitle ?? "Image title",
              },
            },
          ],
        },
      },
      visibility: {
        "com.linkedin.ugc.MemberNetworkVisibility": "PUBLIC",
      },
    };

    return this.makeRequest(endpoint, {
      method: "POST",
      body: JSON.stringify(postData),
    });
  }

  static getScopes(postToOrg?: boolean): string[] {
    return postToOrg
      ? ["w_member_social", "w_organization_social"]
      : ["w_member_social"];
  }

  /**
   * Authorizes a user through Arcade's OAuth flow for LinkedIn access.
   * This method is used exclusively in Arcade authentication mode.
   *
   * @param {string} id - The user's unique identifier in your system
   * @param {Arcade} client - An initialized Arcade client instance
   * @returns {Promise<AuthorizeUserResponse>} Object containing either an authorization URL or token
   * @throws {Error} If authorization fails or required tokens are missing
   */
  static async authorizeUser(
    id: string,
    client: Arcade,
    fields?: {
      postToOrganization?: boolean;
    },
  ): Promise<AuthorizeUserResponse> {
    const scopes = LinkedInClient.getScopes(fields?.postToOrganization);
    const authRes = await client.auth.start(id, "linkedin", {
      scopes,
    });

    if (authRes.status === "completed") {
      if (!authRes.context?.token) {
        throw new Error(
          "Authorization status is completed, but token not found",
        );
      }
      return { token: authRes.context.token };
    }

    if (authRes.url) {
      return { authorizationUrl: authRes.url };
    }

    throw new Error(
      `Authorization failed for user ID: ${id}\nStatus: '${authRes.status}'`,
    );
  }

  static async fromArcade(
    linkedInUserId: string,
    fields?: {
      postToOrganization?: boolean;
    },
  ): Promise<LinkedInClient> {
    const arcade = new Arcade({
      apiKey: process.env.ARCADE_API_KEY,
    });
    const scopes = LinkedInClient.getScopes(fields?.postToOrganization);
    const authRes = await arcade.auth.start(linkedInUserId, "linkedin", {
      scopes,
    });

    if (!authRes.context?.token || !authRes.context?.user_info?.sub) {
      throw new Error(
        "Authorization not completed for user ID: " + linkedInUserId,
      );
    }

    return new LinkedInClient({
      accessToken: authRes.context.token,
      personUrn: authRes.context.user_info.sub as string,
      organizationId: undefined,
    });
  }
}



================================================
FILE: src/clients/types.ts
================================================
/**
 * Interface for authorizing a user to use the Twitter API
 */
export type AuthorizeUserResponse =
  | {
      /**
       * The Bearer token used to authenticate requests with the Twitter API
       */
      token: string;
      /**
       * The URL to visit to authorize the user
       */
      authorizationUrl?: never;
    }
  | {
      /**
       * The Bearer token used to authenticate requests with the Twitter API
       */
      token?: never;
      /**
       * The URL to visit to authorize the user
       */
      authorizationUrl: string;
    };



================================================
FILE: src/clients/reddit/client.ts
================================================
import Snoowrap, { Submission } from "snoowrap";
import fs from "fs/promises";
import path from "path";
import { getRedditUserlessToken } from "./get-user-less-token.js";
import { createDirIfNotExists } from "../../utils/create-dir.js";
import {
  SimpleRedditPost,
  SimpleRedditComment,
  SimpleRedditPostWithComments,
} from "./types.js";

const REDDIT_POST_URL_REGEX = /^\/r\/[\w-]+\/comments\/[\w-]+\/.+/;

function isRedditPostUrl(url: string): boolean {
  return REDDIT_POST_URL_REGEX.test(url);
}

export class RedditClient {
  snoowrapClient: Snoowrap;

  /**
   * Creates a new instance of RedditClient
   * @param accessToken - Reddit API access token
   */
  constructor(accessToken: string) {
    this.snoowrapClient = new Snoowrap({
      clientId: process.env.REDDIT_CLIENT_ID,
      clientSecret: process.env.REDDIT_CLIENT_SECRET,
      accessToken,
      userAgent: "SocialMediaAgent/1.0.0",
    });
  }

  /**
   * Loads an existing access token or obtains a new one if expired
   * @returns Promise containing the access token string
   */
  static async loadOrRefreshAccessToken(): Promise<string> {
    const accessTokenSecretsDir = path.resolve("src/clients/reddit/.secrets");
    const accessTokenSecretPath = path.join(
      accessTokenSecretsDir,
      "access-token.json",
    );

    createDirIfNotExists(accessTokenSecretsDir);

    try {
      const accessToken: { accessToken: string; expiry: string } | undefined =
        JSON.parse(await fs.readFile(accessTokenSecretPath, "utf-8"));

      if (accessToken && new Date(accessToken.expiry) > new Date()) {
        return accessToken.accessToken;
      }
    } catch (_) {
      // no-op
    }

    const tokenRes = await getRedditUserlessToken();
    await fs.writeFile(
      accessTokenSecretPath,
      JSON.stringify({
        accessToken: tokenRes.access_token,
        expiry: new Date(Date.now() + tokenRes.expires_in * 1000).toISOString(),
      }),
      "utf-8",
    );
    return tokenRes.access_token;
  }

  /**
   * Creates a new RedditClient instance using userless authentication
   * @returns Promise containing a new RedditClient instance
   */
  static async fromUserless() {
    const token = await RedditClient.loadOrRefreshAccessToken();
    return new RedditClient(token);
  }

  /**
   * Retrieves top posts from a specified subreddit
   * @param subreddit - Name of the subreddit to fetch posts from
   * @param options - Optional configuration object
   * @param options.limit - Maximum number of posts to retrieve (default: 10)
   * @returns Promise containing an array of Snoowrap Submissions
   */
  async getTopPosts(
    subreddit: string,
    options?: {
      limit?: number;
    },
  ): Promise<Snoowrap.Submission[]> {
    const limitWithDefaults = options?.limit != null ? options.limit : 10;

    const posts = (await (this.snoowrapClient.getSubreddit(subreddit).getTop({
      time: "day",
      limit: limitWithDefaults,
    }) as any)) as Snoowrap.Submission[];

    return posts;
  }

  /**
   * Retrieves new posts from a specified subreddit
   * @param subreddit - Name of the subreddit to fetch posts from
   * @param options - Optional configuration object
   * @param options.limit - Maximum number of posts to retrieve (default: 10)
   * @returns Promise containing an array of Snoowrap Submissions
   */
  async getNewPosts(
    subreddit: string,
    options?: {
      limit?: number;
    },
  ): Promise<Snoowrap.Submission[]> {
    const limitWithDefaults = options?.limit != null ? options.limit : 10;

    const posts = (await (this.snoowrapClient.getSubreddit(subreddit).getNew({
      limit: limitWithDefaults,
    }) as any)) as Snoowrap.Submission[];

    return posts;
  }

  /**
   * Converts a Snoowrap Submission into a simplified post format
   * @param post - Snoowrap Submission object to simplify
   * @returns Simplified Reddit post object
   */
  simplifyPost(post: Snoowrap.Submission): SimpleRedditPost {
    return {
      id: post.id,
      title: post.title,
      url: post.url,
      created_utc: post.created_utc,
      selftext: post.selftext,
    };
  }

  /**
   * Retrieves comments from a specific Reddit post
   * @param postId - ID of the Reddit post
   * @param options - Optional configuration object
   * @param options.limit - Maximum number of comments to retrieve (default: 10)
   * @param options.depth - Maximum depth of nested comments to retrieve (default: 3)
   * @returns Promise containing an array of Snoowrap Comments
   */
  async getPostComments(
    postId: string,
    options?: {
      limit?: number;
      depth?: number;
    },
  ): Promise<Snoowrap.Comment[]> {
    const limitWithDefaults = options?.limit ?? 10;
    const depthWithDefaults = options?.depth ?? 3;

    const submission = this.snoowrapClient.getSubmission(postId);
    const comments = (await (submission.comments.fetchAll({
      // @ts-expect-error - Weird snoowrap types. Can ignore.
      limit: limitWithDefaults,
      depth: depthWithDefaults,
    }) as any)) as Snoowrap.Comment[];

    return comments;
  }

  /**
   * Converts a Snoowrap Comment into a simplified comment format.
   * Implemented as an arrow function class property to ensure correct `this` binding
   * when used as a callback in array methods like map().
   * @param comment - Snoowrap Comment object to simplify
   * @returns Simplified Reddit comment object with optional nested replies
   */
  simplifyComment = (comment: Snoowrap.Comment): SimpleRedditComment => {
    return {
      id: comment.id,
      author: comment.author.name,
      body: comment.body,
      created_utc: comment.created_utc,
      replies: comment.replies
        ? comment.replies?.map((reply) => this.simplifyComment(reply))
        : undefined,
    };
  };

  async getPostById(postId: string): Promise<any> {
    return await this.snoowrapClient.getSubmission(postId).fetch();
  }

  /**
   * Retrieves post data from a Reddit URL
   * @param url - Full Reddit post URL (e.g., https://www.reddit.com/r/subreddit/comments/postid/title/)
   * @returns Promise containing the post data
   * @throws Error if the URL is not a valid Reddit post URL
   */
  async getPostByURL(url: string): Promise<Submission> {
    // Extract post ID from URL - matches both old and new Reddit URL formats
    const urlPattern = /reddit\.com\/r\/[^/]+\/comments\/([a-zA-Z0-9]+)/;
    const match = url.match(urlPattern);

    if (!match) {
      throw new Error("Invalid Reddit post URL");
    }

    const postId = match[1];
    const submission = (await this.getPostById(postId)) as any;
    return submission as Submission;
  }

  async getSimplePostAndComments(
    idOrUrl: string,
  ): Promise<SimpleRedditPostWithComments> {
    let post: Submission;
    try {
      const url = new URL(idOrUrl);
      if (url) {
        post = await this.getPostByURL(idOrUrl);
      } else {
        throw new Error("Invalid URL");
      }
    } catch (_) {
      // Is an ID.
      post = await this.getPostById(idOrUrl);
    }

    // Check if this post is linking to another Reddit post
    if (post.url && isRedditPostUrl(post.url)) {
      const linkedPost = await this.getPostByURL(
        `https://reddit.com${post.url}`,
      );
      if (linkedPost) {
        const [comments, linkedComments] = await Promise.all([
          this.getPostComments(post.id, {
            limit: 10,
            depth: 3,
          }),
          this.getPostComments(linkedPost.id, {
            limit: 10,
            depth: 3,
          }),
        ]);

        const simplePost = this.simplifyPost(post);
        simplePost.selftext += `\n\nLinked post:\n${linkedPost.title}\n${linkedPost.selftext}`;
        if (linkedPost.url) {
          // The original post URL was linking to this post, so replace it with the linked post URL, if exists
          simplePost.url = linkedPost.url;
        }

        return {
          post: simplePost,
          // Simply concatenate comments from both posts.
          comments: comments
            .map(this.simplifyComment)
            .concat(linkedComments.map(this.simplifyComment)),
        };
      }
    }

    const comments = await this.getPostComments(post.id, {
      limit: 10, // default
      depth: 3, // default
    });

    return {
      post: this.simplifyPost(post),
      comments: comments.map(this.simplifyComment),
    };
  }
}



================================================
FILE: src/clients/reddit/get-user-less-token.ts
================================================
interface RedditTokenResponse {
  access_token: string;
  token_type: string;
  expires_in: number;
  scope: string;
}

export async function getRedditUserlessToken(): Promise<RedditTokenResponse> {
  const redditClientId = process.env.REDDIT_CLIENT_ID;
  const redditClientSecret = process.env.REDDIT_CLIENT_SECRET;
  if (!redditClientId || !redditClientSecret) {
    throw new Error("Missing Reddit client ID or secret");
  }

  const tokenUrl = "https://www.reddit.com/api/v1/access_token";

  // Create Basic Auth header
  const authHeader = Buffer.from(
    `${redditClientId}:${redditClientSecret}`,
  ).toString("base64");

  // Prepare the form data based on grant type
  const formData = new URLSearchParams({ grant_type: "client_credentials" });

  try {
    const response = await fetch(tokenUrl, {
      method: "POST",
      headers: {
        Authorization: `Basic ${authHeader}`,
        "Content-Type": "application/x-www-form-urlencoded",
      },
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data: RedditTokenResponse = await response.json();
    return data;
  } catch (error) {
    throw new Error(`Failed to get Reddit token: ${error}`);
  }
}



================================================
FILE: src/clients/reddit/snoowrap.ts
================================================
import * as snoowrap from "snoowrap";

declare module "snoowrap" {
  // @ts-expect-error Yeah yeah yeah I know
  interface RedditContent {
    then: undefined;
    catch: undefined;
    finally: undefined;
  }

  interface MediaEmbed {
    /** HTML string of the media, usually an iframe */
    content?: string;
    height?: number;
    scrolling?: boolean;
    width?: number;
  }

  interface SecureMediaEmbed extends MediaEmbed {
    media_domain_url?: string;
  }

  interface Media {
    oembed?: {
      /** The username of the uploader of the source media */
      author_name?: string;
      /** URL to the author's profile on the source website */
      author_url?: string;
      description?: string;
      height: number;
      html: string;
      /** Name of the source website, e.g. "gfycat", "YouTube" */
      provider_name: string;
      /** URL of the source website, e.g. "https://www.youtube.com" */
      provider_url: string;
      thumbnail_height: number;
      thumbnail_url: string;
      thumbnail_width: number;
      /** Name of the media on the content site, e.g. YouTube video title */
      title: string;
      type: "video" | "rich";
      version: string;
      width: number;
    };
    reddit_video?: {
      dash_url: string;
      duration: number;
      fallback_url: string;
      height: number;
      hls_url: string;
      is_gif: boolean;
      scrubber_media_url: string;
      transcoding_status: string;
    };
    type?: string;
  }

  interface RichTextFlair {
    /** The string representation of the emoji */
    a?: string;
    /** The type of the flair entry */
    e: "text" | "emoji";
    /** URL of the emoji image */
    u?: string;
    /** The text content of a text flair */
    t?: string;
  }

  interface ImagePreviewSource {
    url: string;
    width: number;
    height: number;
  }

  interface ImagePreview {
    source: ImagePreviewSource;
    resolutions: ImagePreviewSource[];
    variants: any; // ?
    id: string;
  }

  interface ListingOptions {
    limit?: number;
    after?: string;
    before?: string;
    show?: string;
    count?: number;
  }

  interface FlairTemplate {
    flair_css_class: string;
    flair_template_id: string;
    flair_text_editable: string;
    flair_position: string;
    flair_text: string;
  }

  // @ts-expect-error Yeah yeah yeah I know
  class Submission extends snoowrap.VoteableContent<Submission> {
    clicked: boolean;
    comments: snoowrap.Listing<Comment>;
    /** Categories for original content, e.g. "comics", "drawing_and_painting" */
    content_categories: string[] | null;
    contest_mode: boolean;
    domain: string;
    hidden: boolean;
    hide_score: boolean;
    is_crosspostable: boolean;
    is_meta: boolean;
    is_original_content: boolean;
    is_reddit_media_domain: boolean;
    is_robot_indexable: boolean;
    is_self: boolean;
    is_video: boolean;
    link_flair_background_color: string;
    link_flair_css_class: string | null;
    link_flair_richtext: RichTextFlair[];
    link_flair_template_id: string | null;
    link_flair_text: string | null;
    link_flair_text_color: "dark" | "light";
    link_flair_type: "text" | "richtext";
    locked: boolean;
    media: Media | null;
    media_embed: MediaEmbed;
    media_only: boolean;
    num_comments: number;
    num_crossposts: number;
    over_18: boolean;
    parent_whitelist_status: string;
    pinned: boolean;
    previous_visits: number[];
    pwls: number;
    post_hint: string;
    preview: { enabled: boolean; images: ImagePreview[] };
    quarantine: boolean;
    removal_reason: string | null;
    removed_by_category: string | null;
    /** Same content as media, except HTTPS */
    secure_media: Media | null;
    secure_media_embed: SecureMediaEmbed;
    selftext: string;
    selftext_html: string | null;
    spam?: boolean;
    spoiler: boolean;
    subreddit_subscribers: number;
    suggested_sort: snoowrap.Sort | null;
    thumbnail: string;
    thumbnail_height?: number | null;
    thumbnail_width?: number | null;
    title: string;
    upvote_ratio: number;
    url: string;
    view_count: number | null;
    visited: boolean;
    whitelist_status: string;
    wls: number;

    assignFlair(options: { text: string; cssClass: string }): Promise<this>;
    disableContestMode(): Promise<this>;
    enableContestMode(): Promise<this>;
    getDuplicates(
      options?: ListingOptions,
    ): Promise<snoowrap.Listing<Submission>>;
    getLinkFlairTemplates(): Promise<FlairTemplate[]>;
    /* @deprecated */ getRelated(options?: ListingOptions): Submission;
    hide(): Promise<this>;
    lock(): Promise<this>;
    markAsRead(): Promise<this>;
    markNsfw(): Promise<this>;
    markSpoiler(): Promise<this>;
    selectFlair(options: {
      flair_template_id: string;
      text?: string;
    }): Promise<this>;
    setSuggestedSort(sort: snoowrap.Sort): Promise<this>;
    sticky(options?: { num?: number }): Promise<this>;
    unhide(): Promise<this>;
    unlock(): Promise<this>;
    unmarkNsfw(): Promise<this>;
    unmarkSpoiler(): Promise<this>;
    unsticky(): Promise<this>;
  }
}



================================================
FILE: src/clients/reddit/types.ts
================================================
export interface Root {
  kind: string;
  data: Data;
}

export interface Data {
  approved_at_utc: any;
  subreddit: string;
  selftext: string;
  author_fullname: string;
  saved: boolean;
  mod_reason_title: any;
  gilded: number;
  clicked: boolean;
  title: string;
  link_flair_richtext: any[];
  subreddit_name_prefixed: string;
  hidden: boolean;
  pwls: number;
  link_flair_css_class: string;
  downs: number;
  thumbnail_height: number;
  top_awarded_type: any;
  hide_score: boolean;
  name: string;
  quarantine: boolean;
  link_flair_text_color: string;
  upvote_ratio: number;
  author_flair_background_color: any;
  ups: number;
  total_awards_received: number;
  media_embed: MediaEmbed;
  thumbnail_width: number;
  author_flair_template_id: any;
  is_original_content: boolean;
  user_reports: any[];
  secure_media: any;
  is_reddit_media_domain: boolean;
  is_meta: boolean;
  category: any;
  secure_media_embed: SecureMediaEmbed;
  link_flair_text: string;
  can_mod_post: boolean;
  score: number;
  approved_by: any;
  is_created_from_ads_ui: boolean;
  author_premium: boolean;
  thumbnail: string;
  edited: boolean;
  author_flair_css_class: any;
  author_flair_richtext: any[];
  gildings: Gildings;
  post_hint: string;
  content_categories: any;
  is_self: boolean;
  subreddit_type: string;
  created: number;
  link_flair_type: string;
  wls: number;
  removed_by_category: any;
  banned_by: any;
  author_flair_type: string;
  domain: string;
  allow_live_comments: boolean;
  selftext_html: any;
  likes: any;
  suggested_sort: any;
  banned_at_utc: any;
  url_overridden_by_dest: string;
  view_count: any;
  archived: boolean;
  no_follow: boolean;
  is_crosspostable: boolean;
  pinned: boolean;
  over_18: boolean;
  preview: Preview;
  all_awardings: any[];
  awarders: any[];
  media_only: boolean;
  link_flair_template_id: string;
  can_gild: boolean;
  spoiler: boolean;
  locked: boolean;
  author_flair_text: any;
  treatment_tags: any[];
  visited: boolean;
  removed_by: any;
  mod_note: any;
  distinguished: any;
  subreddit_id: string;
  author_is_blocked: boolean;
  mod_reason_by: any;
  num_reports: any;
  removal_reason: any;
  link_flair_background_color: string;
  id: string;
  is_robot_indexable: boolean;
  report_reasons: any;
  author: string;
  discussion_type: any;
  num_comments: number;
  send_replies: boolean;
  contest_mode: boolean;
  mod_reports: any[];
  author_patreon_flair: boolean;
  author_flair_text_color: any;
  permalink: string;
  stickied: boolean;
  url: string;
  subreddit_subscribers: number;
  created_utc: number;
  num_crossposts: number;
  media: any;
  is_video: boolean;
}

export interface MediaEmbed {}

export interface SecureMediaEmbed {}

export interface Gildings {}

export interface Preview {
  images: Image[];
  enabled: boolean;
}

export interface Image {
  source: Source;
  resolutions: Resolution[];
  variants: Variants;
  id: string;
}

export interface Source {
  url: string;
  width: number;
  height: number;
}

export interface Resolution {
  url: string;
  width: number;
  height: number;
}

export interface Variants {}

export interface SimpleRedditPost {
  title: string;
  url: string;
  created_utc: number;
  selftext: string;
  id: string;
}

export interface RedditCommentData {
  id: string;
  author: string;
  body: string;
  created_utc: number;
  score: number;
  replies?: {
    data: {
      children: Root[];
    };
  };
}

export interface SimpleRedditComment {
  id: string;
  author: string;
  body: string;
  created_utc: number;
  replies?: SimpleRedditComment[];
}

export type SimpleRedditPostWithComments = {
  post: SimpleRedditPost;
  comments: SimpleRedditComment[];
};



================================================
FILE: src/clients/reddit/tests/reddit.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { RedditClient } from "../client.js";

test("Reddit client can fetch posts from subreddit", async () => {
  const subredditName = "LocalLLaMA";

  const client = await RedditClient.fromUserless();
  const posts = await client.getTopPosts(subredditName, { limit: 10 });
  console.log("Posts:\n");
  console.dir(posts.map(client.simplifyPost), { depth: null });
  expect(posts.length).toBe(10);
});

test("Reddit client can fetch comments from post", async () => {
  const subredditName = "LocalLLaMA";
  const client = await RedditClient.fromUserless();
  const posts = await client.getTopPosts(subredditName, { limit: 1 });

  const postId = posts[0].id;
  const comments = await client.getPostComments(postId);
  console.log("Comments:\n");
  console.dir(comments.map(client.simplifyComment), { depth: null });
  expect(comments.length).toBeGreaterThan(0);
});

test("Reddit client can fetch post by URL", async () => {
  const client = await RedditClient.fromUserless();
  const url =
    "https://www.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/";
  const post = await client.getPostByURL(url);
  console.log("Post:\n");
  console.dir(post, { depth: null });
  expect(post).toBeDefined();
});

test("Can get posts and comments from URL", async () => {
  const client = await RedditClient.fromUserless();
  const url =
    "https://www.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/";
  const post = await client.getPostByURL(url);
  expect(post).toBeDefined();
  const comments = await client.getPostComments(post.id);
  expect(comments.length).toBeGreaterThan(0);
});

test("Can get posts and comments from URL, then simplify", async () => {
  const client = await RedditClient.fromUserless();
  const url =
    "https://www.reddit.com/r/LocalLLaMA/comments/1i31ji5/what_is_elevenlabs_doing_how_is_it_so_good/";
  const post = await client.getPostByURL(url);
  expect(post).toBeDefined();
  const comments = await client.getPostComments(post.id);
  expect(comments.length).toBeGreaterThan(0);

  const simplePost = client.simplifyPost(post);
  const simpleComments = comments.map(client.simplifyComment);

  expect(simplePost).toBeDefined();
  expect(simpleComments.length).toBe(comments.length);
});



================================================
FILE: src/clients/slack/client.ts
================================================
import { traceable } from "langsmith/traceable";
import {
  WebClient,
  ConversationsHistoryResponse,
  ChatPostMessageResponse,
  ChatDeleteResponse,
  FilesSharedPublicURLResponse,
  FilesInfoResponse,
} from "@slack/web-api";
import moment from "moment";

// Slack does not export their message type so we must extract it from another type they expose.
export type SlackMessage = NonNullable<
  NonNullable<ConversationsHistoryResponse["messages"]>[number]
>;
export type SlackMessageAttachment = NonNullable<
  NonNullable<SlackMessage["attachments"]>[number]
>;
export type SlackMessageFile = NonNullable<
  NonNullable<SlackMessage["files"]>[number]
>;

export interface SimpleSlackMessage {
  id: string;
  threadId?: string;
  username?: string;
  userId?: string;
  text: string;
  type?: string;
  attachmentIds?: string[];
  fileIds?: string[];
}

export interface SlackClientArgs {
  /**
   * An OAuth token for the Slack API.
   * If not provided, the token will be read from the SLACK_BOT_OAUTH_TOKEN environment variable.
   */
  token?: string;
}

export type GetChannelMessagesArgs = {
  /**
   * The maximum number of messages to fetch.
   * @default 100
   */
  maxMessages?: number;
  /**
   * The maximum number of hours of history to fetch.
   * @default 24
   */
  maxHoursHistory?: number;
  /**
   * Whether or not to filter out messages which were sent as "thread_broadcast" messages
   * These are messages which are sent as replies to a thread, but also sent as a top level
   * message to the channel.
   * @default false
   */
  filterMessageBroadcasts?: boolean;
};

export class SlackClient {
  private client: WebClient;

  constructor(args?: SlackClientArgs) {
    const slackToken = process.env.SLACK_BOT_OAUTH_TOKEN || args?.token;
    if (!slackToken) {
      throw new Error(
        "Missing slack OAuth token. Please provide via the 'token' arg, or process.env.SLACK_BOT_OAUTH_TOKEN.",
      );
    }
    this.client = new WebClient(slackToken);

    // Wrap methods with traceable
    this.getChannelMessages = this._wrapWithTraceable(
      this.getChannelMessages,
      "get_channel_messages",
    );
    this.getReplies = this._wrapWithTraceable(this.getReplies, "get_replies");
    this.sendMessage = this._wrapWithTraceable(
      this.sendMessage,
      "send_message",
    );
    this.deleteMessage = this._wrapWithTraceable(
      this.deleteMessage,
      "delete_message",
    );
    this.getPublicFile = this._wrapWithTraceable(
      this.getPublicFile,
      "get_public_file",
    );
    this.makeFilePublic = this._wrapWithTraceable(
      this.makeFilePublic,
      "make_file_public",
    );
  }

  // Helper method to wrap methods with traceable
  private _wrapWithTraceable<T extends (...args: any[]) => Promise<any>>(
    method: T,
    name: string,
  ): T {
    const boundMethod = method.bind(this);
    return traceable(boundMethod, { name }) as T;
  }

  private _convertToSimpleMessages(
    messages: SlackMessage[],
  ): SimpleSlackMessage[] {
    const messagesWithContent = messages.filter((m) => {
      if (
        m.type !== "message" ||
        (!m.username && !m.text) ||
        !m.text ||
        !m.ts
      ) {
        return false;
      }
      return true;
    });

    const simpleMsgs: SimpleSlackMessage[] = messagesWithContent.map((m) => {
      if (!m.ts) {
        throw new Error(
          `Failed to convert Slack message. Missing 'ts':\n${JSON.stringify(m, null, 2)}`,
        );
      }
      return {
        id: m.ts,
        threadId: m.thread_ts,
        username: m.username,
        userId: m.user,
        text: m.text ?? "",
        type: m.type,
        attachmentIds:
          m.attachments?.flatMap((a) => a.id?.toString() ?? []) ?? [],
        fileIds: m.files?.flatMap((f) => f.id?.toString() ?? []) ?? [],
      };
    });

    return simpleMsgs;
  }

  async getChannelMessages(
    channelId: string,
    args?: GetChannelMessagesArgs,
  ): Promise<SimpleSlackMessage[]> {
    try {
      const { maxMessages, maxHoursHistory } = {
        maxMessages: 100,
        maxHoursHistory: 24,
        ...args,
      };
      const oldest = moment()
        .subtract(maxHoursHistory, "hours")
        .unix()
        .toString();
      let messages: SlackMessage[] = [];
      let cursor: string | undefined;

      do {
        // Adjust limit based on remaining messages needed
        const limit = Math.min(100, (maxMessages ?? 0) - messages.length);

        const result = await this.client.conversations.history({
          channel: channelId,
          oldest: oldest,
          limit: limit,
          cursor: cursor,
        });

        if (result.messages && Array.isArray(result.messages)) {
          messages.push(...result.messages);
        }

        cursor = (result.response_metadata?.next_cursor as string) || undefined;

        // Break the loop if we've reached maxMessages
        if (messages.length >= maxMessages) {
          break;
        }
      } while (cursor);

      // If the type if `thread_broadcast`, it's a thread reply which was also sent
      // as a top level message to the channel.
      if (args?.filterMessageBroadcasts) {
        messages = messages.filter((m) => m.subtype !== "thread_broadcast");
      }

      // Trim any excess messages if we went over maxMessages
      return this._convertToSimpleMessages(messages.slice(0, maxMessages));
    } catch (error) {
      console.error("Error fetching Slack messages:", error);
      throw error;
    }
  }

  async getReplies(
    channelId: string,
    parentMessageId: string,
  ): Promise<SimpleSlackMessage[]> {
    const results = await this.client.conversations.replies({
      channel: channelId,
      ts: parentMessageId,
    });
    return this._convertToSimpleMessages(results.messages || []);
  }

  async sendMessage(
    channelId: string,
    message: string,
  ): Promise<ChatPostMessageResponse> {
    const res = await this.client.chat.postMessage({
      channel: channelId,
      text: message,
    });
    return res;
  }

  async deleteMessage(
    channelId: string,
    messageId: string,
  ): Promise<ChatDeleteResponse> {
    const res = await this.client.chat.delete({
      channel: channelId,
      ts: messageId,
    });
    return res;
  }

  async getPublicFile(fileId: string): Promise<FilesInfoResponse> {
    const file = await this.client.files.info({
      file: fileId,
    });
    return file;
  }

  async makeFilePublic(fileId: string): Promise<FilesSharedPublicURLResponse> {
    const res = await this.client.files.sharedPublicURL({
      file: fileId,
      token: process.env.SLACK_BOT_USER_OAUTH_TOKEN,
    });
    return res;
  }
}



================================================
FILE: src/clients/slack/types.ts
================================================
export interface File {
  access?: string;
  alt_txt?: string;
  app_id?: string;
  app_name?: string;
  bot_id?: string;
  can_toggle_canvas_lock?: boolean;
  canvas_template_mode?: string;
  cc?: any[];
  channel_actions_count?: number;
  channel_actions_ts?: string;
  channels?: string[];
  comments_count?: number;
  converted_pdf?: string;
  created?: number;
  deanimate?: string;
  deanimate_gif?: string;
  display_as_bot?: boolean;
  dm_mpdm_users_with_file_access?: any[];
  duration_ms?: number;
  edit_link?: string;
  edit_timestamp?: number;
  editable?: boolean;
  editor?: string;
  editors?: string[];
  external_id?: string;
  external_type?: string;
  external_url?: string;
  favorites?: any[];
  file_access?: string;
  filetype?: string;
  from?: any[];
  groups?: string[];
  has_more?: boolean;
  has_more_shares?: boolean;
  has_rich_preview?: boolean;
  headers?: Headers;
  hls?: string;
  hls_embed?: string;
  id?: string;
  image_exif_rotation?: number;
  ims?: string[];
  initial_comment?: Comment;
  is_channel_space?: boolean;
  is_external?: boolean;
  is_public?: boolean;
  is_restricted_sharing_enabled?: boolean;
  is_starred?: boolean;
  last_editor?: string;
  last_read?: number;
  lines?: number;
  lines_more?: number;
  linked_channel_id?: string;
  list_limits?: any;
  list_metadata?: any;
  media_display_type?: string;
  media_progress?: any;
  mimetype?: string;
  mode?: string;
  mp4?: string;
  mp4_low?: string;
  name?: string;
  non_owner_editable?: boolean;
  num_stars?: number;
  org_or_workspace_access?: string;
  original_attachment_count?: number;
  original_h?: string;
  original_w?: string;
  permalink?: string;
  permalink_public?: string;
  pinned_to?: string[];
  pjpeg?: string;
  plain_text?: string;
  pretty_type?: string;
  preview?: string;
  preview_highlight?: string;
  preview_is_truncated?: boolean;
  preview_plain_text?: string;
  private_channels_with_file_access_count?: number;
  private_file_with_access_count?: number;
  public_url_shared?: boolean;
  quip_thread_id?: string;
  reactions?: any[];
  saved?: any;
  sent_to_self?: boolean;
  shares?: any;
  show_badge?: boolean;
  simplified_html?: string;
  size?: number;
  source_team?: string;
  subject?: string;
  subtype?: string;
  team_pref_version_history_enabled?: boolean;
  teams_shared_with?: any[];
  template_conversion_ts?: number;
  template_description?: string;
  template_icon?: string;
  template_name?: string;
  template_title?: string;
  thumb_1024?: string;
  thumb_1024_gif?: string;
  thumb_1024_h?: string;
  thumb_1024_w?: string;
  thumb_160?: string;
  thumb_160_gif?: string;
  thumb_160_h?: string;
  thumb_160_w?: string;
  thumb_360?: string;
  thumb_360_gif?: string;
  thumb_360_h?: string;
  thumb_360_w?: string;
  thumb_480?: string;
  thumb_480_gif?: string;
  thumb_480_h?: string;
  thumb_480_w?: string;
  thumb_64?: string;
  thumb_64_gif?: string;
  thumb_64_h?: string;
  thumb_64_w?: string;
  thumb_720?: string;
  thumb_720_gif?: string;
  thumb_720_h?: string;
  thumb_720_w?: string;
  thumb_80?: string;
  thumb_800?: string;
  thumb_800_gif?: string;
  thumb_800_h?: string;
  thumb_800_w?: string;
  thumb_80_gif?: string;
  thumb_80_h?: string;
  thumb_80_w?: string;
  thumb_960?: string;
  thumb_960_gif?: string;
  thumb_960_h?: string;
  thumb_960_w?: string;
  thumb_gif?: string;
  thumb_pdf?: string;
  thumb_pdf_h?: string;
  thumb_pdf_w?: string;
  thumb_tiny?: string;
  thumb_video?: string;
  thumb_video_h?: number;
  thumb_video_w?: number;
  timestamp?: number;
  title?: string;
  title_blocks?: any[];
  to?: any[];
  transcription?: any;
  update_notification?: number;
  updated?: number;
  url_private?: string;
  url_private_download?: string;
  url_static_preview?: string;
  user?: string;
  user_team?: string;
  username?: string;
  vtt?: string;
}



================================================
FILE: src/clients/slack/utils.ts
================================================
import { traceable } from "langsmith/traceable";
import { SlackClient } from "./client.js";
import { File } from "./types.js";

export function getUrlForPublicFile(file: File) {
  if (!file.permalink_public || !file.url_private) {
    return undefined;
  }

  const pubSecret = file.permalink_public.split("-").pop();
  return `${file.url_private}?pub_secret=${pubSecret}`;
}

async function getPublicFileUrlsFunc(
  fileIds: string[] | undefined,
): Promise<string[] | undefined> {
  if (!fileIds) return undefined;

  const slackClient = new SlackClient();

  try {
    const publicUrlPromises = fileIds.map(async (fileId) => {
      try {
        const publicFile = await slackClient.makeFilePublic(fileId);
        if (!publicFile.file) {
          return undefined;
        }

        return getUrlForPublicFile(publicFile.file as File);
      } catch (e: any) {
        const isAlreadyPublic = e?.message?.includes("already_public");
        if (!isAlreadyPublic) {
          console.error("Failed to make public URL for file ID:", fileId, e);
          return undefined;
        }

        // File has already been made public.
        // Attempt to get the URL from the file info endpoint
        try {
          const fileInfo = await slackClient.getPublicFile(fileId);
          if (!fileInfo.file) {
            return undefined;
          }

          return getUrlForPublicFile(fileInfo.file as File);
        } catch (e) {
          console.error("Failed to get public file:", fileId, e);
          return undefined;
        }
      }
    });

    return (await Promise.all(publicUrlPromises))
      .filter((u) => u !== undefined)
      .flat();
  } catch (e) {
    console.error(`Failed to make public URLs for file IDs:`, fileIds, e);
    return undefined;
  }
}

export const getPublicFileUrls = traceable(getPublicFileUrlsFunc, {
  name: "get_public_file_urls",
});



================================================
FILE: src/clients/twitter/client.ts
================================================
import Arcade from "@arcadeai/arcadejs";
import { CreateTweetRequest, TwitterClientArgs } from "./types.js";
import {
  SendTweetV2Params,
  TweetV2,
  Tweetv2FieldsParams,
  TweetV2ListTweetsPaginator,
  TweetV2PaginableListParams,
  TweetV2SingleResult,
  TwitterApi,
  TwitterApiReadWrite,
} from "twitter-api-v2";
import { AuthorizeUserResponse } from "../types.js";
import {
  isTextOnly,
  useArcadeAuth,
  useTwitterApiOnly,
} from "../../agents/utils.js";

const BASE_FETCH_TWEET_OPTIONS: Partial<TweetV2PaginableListParams> = {
  "tweet.fields": [
    "note_tweet",
    "created_at",
    "id",
    "author_id",
    "in_reply_to_user_id",
    "referenced_tweets",
  ],
  "user.fields": ["username"],
  expansions: [
    "referenced_tweets.id",
    "referenced_tweets.id.author_id",
    "attachments.media_keys",
  ],
  "media.fields": ["type", "url"],
};

type MediaIdStringArray =
  | [string]
  | [string, string]
  | [string, string, string]
  | [string, string, string, string];

/**
 * TwitterClient class that provides methods for interacting with the Twitter API.
 * This client supports two authentication modes:
 * 1. Basic Twitter Auth - Uses direct Twitter API credentials from environment variables
 * 2. Arcade Auth - Uses Arcade's OAuth flow for enhanced security and user management
 *
 * Basic Auth requires these environment variables:
 * - TWITTER_USER_TOKEN
 * - TWITTER_USER_TOKEN_SECRET
 * - TWITTER_API_KEY
 * - TWITTER_API_KEY_SECRET
 *
 * Arcade Auth requires:
 * - ARCADE_API_KEY environment variable
 * - User tokens obtained through OAuth flow
 */
export class TwitterClient {
  private twitterClient: TwitterApi;

  private twitterClientOauth2?: TwitterApi;

  private twitterToken: string | undefined;

  private twitterTokenSecret: string | undefined;

  /**
   * Initializes a new TwitterClient instance.
   *
   * @param {TwitterClientArgs} args - Configuration options for the Twitter client
   * @param {TwitterApi} args.twitterClient - An initialized Twitter API client instance
   * @param {boolean} [args.useArcade] - Whether to use Arcade authentication mode
   * @param {string} [args.twitterToken] - Twitter access token (required if useArcade is true)
   * @param {string} [args.twitterTokenSecret] - Twitter access token secret (required if useArcade is true)
   * @throws {Error} If required tokens are missing when using Arcade mode
   */
  constructor(args: TwitterClientArgs) {
    this.twitterClient = args.twitterClient;
    this.twitterClientOauth2 = args.twitterClientOauth2;
    const textOnlyMode =
      args.textOnlyMode != null ? args.textOnlyMode : isTextOnly();

    // If we want to use Arcade, we need to set the token and token secret for uploading media.
    // However, this should only be done if text only mode is false.
    if (args.useArcade && !textOnlyMode) {
      const { twitterToken, twitterTokenSecret } = {
        twitterToken: args.twitterToken || process.env.TWITTER_USER_TOKEN,
        twitterTokenSecret:
          args.twitterTokenSecret || process.env.TWITTER_USER_TOKEN_SECRET,
      };
      if (!twitterToken || !twitterTokenSecret) {
        throw new Error(
          "Missing Twitter user credentials in Arcade mode.\n" +
            `TWITTER_USER_TOKEN: ${!!twitterToken}\n` +
            `TWITTER_USER_TOKEN_SECRET: ${!!twitterTokenSecret}\n`,
        );
      }

      this.twitterToken = twitterToken;
      this.twitterTokenSecret = twitterTokenSecret;
    }
  }

  /**
   * Authorizes a user through Arcade's OAuth flow for Twitter access.
   * This method is used exclusively in Arcade authentication mode.
   *
   * @param {string} id - The user's unique identifier in your system
   * @param {Arcade} client - An initialized Arcade client instance
   * @returns {Promise<AuthorizeUserResponse>} Object containing either an authorization URL or token
   * @throws {Error} If authorization fails or required tokens are missing
   */
  static async authorizeUser(
    id: string,
    client: Arcade,
  ): Promise<AuthorizeUserResponse> {
    const authRes = await client.auth.start(id, "x", {
      scopes: ["tweet.write", "users.read", "tweet.read", "offline.access"],
    });

    if (authRes.status === "completed") {
      if (!authRes.context?.token) {
        throw new Error(
          "Authorization status is completed, but token not found",
        );
      }
      return { token: authRes.context.token };
    }

    if (authRes.url) {
      return { authorizationUrl: authRes.url };
    }

    throw new Error(
      `Authorization failed for user ID: ${id}\nStatus: '${authRes.status}'`,
    );
  }

  /**
   * Creates a TwitterClient instance using basic Twitter authentication.
   * This method requires the following environment variables to be set:
   * - TWITTER_USER_TOKEN
   * - TWITTER_USER_TOKEN_SECRET
   * - TWITTER_API_KEY
   * - TWITTER_API_KEY_SECRET
   *
   * @returns {TwitterClient} A new TwitterClient instance
   * @throws {Error} If any required Twitter credentials are missing
   */
  static fromBasicTwitterAuth(): TwitterClient {
    if (
      !process.env.TWITTER_USER_TOKEN ||
      !process.env.TWITTER_USER_TOKEN_SECRET
    ) {
      throw new Error(
        "Missing Twitter user credentials.\n" +
          `TWITTER_USER_TOKEN: ${!!process.env.TWITTER_USER_TOKEN}\n` +
          `TWITTER_USER_TOKEN_SECRET: ${!!process.env.TWITTER_USER_TOKEN_SECRET}\n`,
      );
    }
    if (!process.env.TWITTER_API_KEY || !process.env.TWITTER_API_KEY_SECRET) {
      throw new Error(
        "Missing Twitter app credentials.\n" +
          `TWITTER_API_KEY: ${!!process.env.TWITTER_API_KEY}\n` +
          `TWITTER_API_KEY_SECRET: ${!!process.env.TWITTER_API_KEY_SECRET}\n`,
      );
    }

    const twitterClient = new TwitterApi({
      appKey: process.env.TWITTER_API_KEY,
      appSecret: process.env.TWITTER_API_KEY_SECRET,
      accessToken: process.env.TWITTER_USER_TOKEN,
      accessSecret: process.env.TWITTER_USER_TOKEN_SECRET,
    });

    const twitterClientOauth2 = process.env.TWITTER_BEARER_TOKEN
      ? new TwitterApi(process.env.TWITTER_BEARER_TOKEN)
      : undefined;

    return new TwitterClient({
      twitterClient,
      twitterClientOauth2,
    });
  }

  /**
   * Creates a TwitterClient instance using Arcade authentication.
   * This method handles the OAuth flow through Arcade's service.
   *
   * @param {string} twitterUserId - The user's Twitter ID
   * @param {{ twitterToken: string; twitterTokenSecret: string }} tokens - Object containing Twitter tokens
   * @returns {Promise<TwitterClient>} A new TwitterClient instance
   * @throws {Error} If user is not authorized or if authorization fails
   */
  static async fromArcade(
    twitterUserId: string,
    tokens: {
      twitterToken: string | undefined;
      twitterTokenSecret: string | undefined;
    },
    options?: {
      textOnlyMode?: boolean;
    },
  ): Promise<TwitterClient> {
    const arcadeClient = new Arcade({ apiKey: process.env.ARCADE_API_KEY });
    const authResponse = await TwitterClient.authorizeUser(
      twitterUserId,
      arcadeClient,
    );
    if (authResponse.authorizationUrl) {
      throw new Error(
        `User not authorized. Please visit ${authResponse.authorizationUrl} to authorize the user.`,
      );
    }
    if (!authResponse.token) {
      throw new Error("Authorization token not found");
    }
    const tokenContext = authResponse.token;
    const twitterClient = new TwitterApi(tokenContext);
    return new TwitterClient({
      twitterClient,
      useArcade: true,
      ...tokens,
      ...options,
    });
  }

  /**
   * Posts a tweet with optional media attachment.
   * Works in both basic auth and Arcade auth modes.
   *
   * @param {CreateTweetRequest} params - The tweet parameters
   * @param {string} params.text - The text content of the tweet
   * @param {{ media: Buffer; mimeType: string }} [params.media] - Optional media attachment
   * @returns {Promise<any>} The Twitter API response
   * @throws {Error} If the tweet upload fails
   */
  async uploadTweet({ text, media }: CreateTweetRequest) {
    let mediaIds: MediaIdStringArray | undefined = undefined;
    if (media?.media) {
      const mediaId = await this.uploadMedia(media.media, media.mimeType);
      mediaIds = [mediaId];
    }
    const mediaInput = mediaIds
      ? {
          media: {
            media_ids: mediaIds,
          },
        }
      : {};

    const response = await this.twitterClient.v2.tweet({
      text,
      ...mediaInput,
    });

    if (response.errors) {
      throw new Error(
        `Error uploading tweet: ${JSON.stringify(response.errors, null)}`,
      );
    }
    return response;
  }

  /**
   * Posts a sequence of tweets as a thread, with optional media for each post.
   * Works in both basic auth and Arcade auth modes.
   *
   * @param {CreateTweetRequest[]} posts - An array of tweet objects, each containing text and optional media.
   * @returns {Promise<any[]>} A promise resolving to an array of Twitter API responses for each tweet in the thread.
   * @throws {Error} If any tweet in the thread upload fails.
   */
  async uploadThread(posts: CreateTweetRequest[]) {
    const postsInputPromise: Promise<SendTweetV2Params>[] = posts.map(
      async (p) => {
        let mediaIds: MediaIdStringArray | undefined = undefined;
        if (p.media?.media) {
          const mediaId = await this.uploadMedia(
            p.media.media,
            p.media.mimeType,
          );
          mediaIds = [mediaId];
        }
        const mediaInput = mediaIds
          ? {
              media: {
                media_ids: mediaIds,
              },
            }
          : {};

        return {
          text: p.text,
          ...mediaInput,
        };
      },
    );
    const threadPostsInput = await Promise.all(postsInputPromise);
    const response = await this.twitterClient.v2.tweetThread(threadPostsInput);
    if (response.some((r) => r.errors)) {
      throw new Error(
        `Error uploading thread: ${JSON.stringify(
          response.map((r) => r.errors),
          null,
        )}`,
      );
    }
    return response;
  }

  /**
   * Tests if the current Twitter credentials are valid.
   * Works in both basic auth and Arcade auth modes.
   *
   * @returns {Promise<boolean>} True if authentication is successful, false otherwise
   */
  async testAuthentication() {
    try {
      const authorized = await this.twitterClient.v2.me();
      return !!authorized;
    } catch (error) {
      console.warn("Error checking user authorization:", error);
      return false;
    }
  }

  /**
   * Uploads media to Twitter for use in tweets.
   * Handles authentication differently based on whether using basic auth or Arcade auth.
   *
   * @param {Buffer} media - The media buffer to upload
   * @param {string} mimeType - The MIME type of the media
   * @returns {Promise<string>} The media ID string from Twitter
   * @throws {Error} If media upload fails or if required credentials are missing
   */
  async uploadMedia(media: Buffer, mimeType: string): Promise<string> {
    let client: TwitterApiReadWrite;

    // If the token & token secret are not set, this indicates they've already been set on the client.
    if (!this.twitterToken || !this.twitterTokenSecret) {
      client = this.twitterClient.readWrite;
    } else {
      if (!process.env.TWITTER_API_KEY_SECRET || !process.env.TWITTER_API_KEY) {
        throw new Error(
          "Missing twitter credentials.\n" +
            `TWITTER_API_KEY_SECRET: ${!!process.env.TWITTER_API_KEY_SECRET}\n` +
            `TWITTER_API_KEY: ${!!process.env.TWITTER_API_KEY}\n`,
        );
      }

      client = new TwitterApi({
        appKey: process.env.TWITTER_API_KEY,
        appSecret: process.env.TWITTER_API_KEY_SECRET,
        accessToken: this.twitterToken,
        accessSecret: this.twitterTokenSecret,
      }).readWrite;
    }

    try {
      // Ensure media is a Buffer
      if (!Buffer.isBuffer(media)) {
        throw new Error("Media must be a Buffer");
      }

      // Upload the media directly using the buffer
      const mediaResponse = await client.v1.uploadMedia(media, {
        mimeType,
      });

      return mediaResponse;
    } catch (error: any) {
      throw new Error(`Failed to upload media: ${error.message}`);
    }
  }

  /**
   * Get a tweet by ID using the basic Twitter API. Will return undefined if an error occurs.
   * @param id The tweet ID
   * @returns {Promise<TweetV2SingleResult>} The tweet
   */
  private async getTweetBasicAuth(
    id: string,
    tweetOptions?: Partial<Tweetv2FieldsParams>,
  ): Promise<TweetV2SingleResult | undefined> {
    const fetchTweetOptions: Partial<Tweetv2FieldsParams> = {
      ...BASE_FETCH_TWEET_OPTIONS,
      ...(tweetOptions || {}),
    };
    const tweetContent = await this.twitterClient.v2.singleTweet(
      id,
      fetchTweetOptions,
    );
    return tweetContent;
  }

  /**
   * Get a tweet by ID using the Arcade Twitter API tool.
   * @param id The tweet ID
   * @param twitterUserId The user ID making the request
   * @returns {Promise<TweetV2SingleResult>}
   */
  private async getTweetArcade(
    id: string,
    twitterUserId: string,
  ): Promise<TweetV2SingleResult> {
    const arcade = new Arcade({
      apiKey: process.env.ARCADE_API_KEY,
    });

    const result = await arcade.tools.execute({
      tool_name: "X.LookupTweetById",
      input: { tweet_id: id },
      user_id: twitterUserId,
    });

    return result.output?.value as TweetV2SingleResult;
  }

  /**
   * Retrieves a tweet by its ID. The method supports both basic Twitter authentication and Arcade authentication modes.
   *
   * When using basic Twitter auth (USE_ARCADE_AUTH="false"):
   * - Uses the Twitter API v2 endpoint directly
   * - Requires standard Twitter API credentials to be set in environment variables
   *
   * When using Arcade auth (USE_ARCADE_AUTH="true"):
   * - Uses Arcade's X.LookupTweetById tool
   * - Requires ARCADE_API_KEY environment variable
   * - Requires twitterUserId parameter
   *
   * @param {string} id - The ID of the tweet to retrieve
   * @param {Object} [fields] - Optional parameters for the tweet lookup
   * @param {string} [fields.twitterUserId] - Required when using Arcade auth. The Twitter user ID making the request
   * @param {boolean} [fields.includeMedia=true] - Whether to include media attachments in the response
   * @returns {Promise<TweetV2SingleResult>} The tweet data including text, media (if requested), and other metadata
   * @throws {Error} When using Arcade auth without providing twitterUserId
   *
   * @example
   * ```typescript
   * // Using basic Twitter auth
   * const tweet = await client.getTweet("1234567890");
   *
   * // Using Arcade auth
   * const tweet = await client.getTweet("1234567890", { twitterUserId: "user123" });
   *
   * // Without media attachments
   * const tweet = await client.getTweet("1234567890");
   * ```
   */
  async getTweet(
    id: string,
    fields?: {
      twitterUserId?: string;
    },
  ): Promise<TweetV2SingleResult> {
    if (useTwitterApiOnly() || !useArcadeAuth()) {
      // Use the developer API account for reading tweets, not Arcade.
      const fetchTweetOptions: Partial<Tweetv2FieldsParams> =
        BASE_FETCH_TWEET_OPTIONS;

      const tweetContent = await this.getTweetBasicAuth(id, fetchTweetOptions);

      // If tweetContent is defined, return it. Otherwise fallback to Arcade.
      if (tweetContent) {
        return tweetContent;
      }

      throw new Error("Tweet not found");
    }

    if (!fields?.twitterUserId) {
      throw new Error("Must provide Twitter User ID when using Arcade auth.");
    }

    return this.getTweetArcade(id, fields.twitterUserId);
  }

  /**
   * Get tweets from a Twitter list.
   * @param listId The ID of the Twitter list
   * @param fields
   * @param fields.maxResults The maximum number of tweets to fetch
   * @returns {Promise<TweetV2ListTweetsPaginator>} A paginator for the list of tweets
   */
  async getListTweets(
    listId: string,
    fields?: {
      maxResults?: number;
      paginationToken?: string;
    },
  ): Promise<TweetV2ListTweetsPaginator> {
    const fieldsWithDefaults = {
      maxResults: 100,
      ...fields,
    };
    const fetchTweetOptions: Partial<TweetV2PaginableListParams> =
      BASE_FETCH_TWEET_OPTIONS;

    if (fieldsWithDefaults.paginationToken) {
      fetchTweetOptions.pagination_token = fieldsWithDefaults.paginationToken;
    }

    const listTweets = await this.twitterClient.v2.listTweets(listId, {
      max_results: fieldsWithDefaults.maxResults,
      ...fetchTweetOptions,
    });

    return listTweets;
  }

  /**
   * Searches for tweets using the Twitter API v2 endpoint
   * @param query - The search query string to find relevant tweets
   * @param options - Optional configuration for the search
   * @param options.maxResults - Maximum number of tweets to return (default: 15)
   * @param options.sinceId - Only return tweets newer than this ID
   * @returns {Promise<TweetSearchRecentV2Paginator>} A promise that resolves to the search results
   */
  async searchTweets(
    query: string,
    options?: {
      maxResults?: number;
      sinceId?: string;
    },
  ) {
    const fetchTweetOptions: Partial<Tweetv2FieldsParams> =
      BASE_FETCH_TWEET_OPTIONS;

    const tweets = await this.twitterClient.v2.search(query, {
      ...fetchTweetOptions,
      max_results: options?.maxResults || 15,
      since_id: options?.sinceId,
    });
    return tweets;
  }

  /**
   * Retrieves all tweets in a thread starting from a given tweet ID.
   * A thread is defined as a series of tweets where the author replies to their own tweets.
   *
   * @param {string} id - The ID of the initial tweet in the thread
   * @param {string} username - The username of the author who created the thread
   * @returns {Promise<TweetV2[]>} An array of tweets in chronological order. This does NOT include the initial tweet, only the replies.
   */
  async getThreadReplies(id: string, username: string): Promise<TweetV2[]> {
    if (!this.twitterClientOauth2) {
      console.error("Twitter client OAuth2 is not initialized.");
      return [];
    }
    const fetchTweetOptions: Partial<Tweetv2FieldsParams> =
      BASE_FETCH_TWEET_OPTIONS;

    const thread: TweetV2[] = [];

    // Search for replies by the same author, to the same author. This must result in a thread, or the author is replying to themselves.
    const query = `conversation_id:${id} from:${username} to:${username}`;

    const replies = await this.twitterClientOauth2.v2.searchAll(query, {
      ...fetchTweetOptions,
      sort_order: "recency",
      max_results: 15, // Limit to 15 replies as most threads will not be longer than this.
    });

    if (!replies.data || !replies.data.data) {
      return thread;
    }

    return replies.data.data;
  }
}

/**
 * Gets a TwitterClient instance. Will use Arcade auth if USE_ARCADE_AUTH is true, otherwise will use basic auth.
 * @returns A TwitterClient instance.
 */
export async function getTwitterClient(): Promise<TwitterClient> {
  if (useTwitterApiOnly() || !useArcadeAuth()) {
    return TwitterClient.fromBasicTwitterAuth();
  } else {
    const twitterUserId = process.env.TWITTER_USER_ID;
    if (!twitterUserId) {
      throw new Error("Twitter user ID not found in configurable fields.");
    }

    const twitterToken = process.env.TWITTER_USER_TOKEN;
    const twitterTokenSecret = process.env.TWITTER_USER_TOKEN_SECRET;

    return TwitterClient.fromArcade(twitterUserId, {
      twitterToken,
      twitterTokenSecret,
    });
  }
}



================================================
FILE: src/clients/twitter/SETUP.md
================================================
# Setup Twitter API Instructions

- Create a Twitter developer account
- Create a new app and give it a name.
- Copy the `API Key` and `API Key Secret` and `Bearer Token` and set them as `TWITTER_API_KEY`, `TWITTER_API_KEY_SECRET`, and `TWITTER_BEARER_TOKEN` in your `.env` file.
- After saving, visit the App Dashboard. Find the `User authentication settings` section, and click the `Set up` button. This is how you will authorize users to use the Twitter API on their behalf.
- Set the following fields:
  - `App permissions`: `Read and write`
  - `Type of App`: `Web App, Automated App or Bot`
  - `App info`:
    - `Callback URI/Redirect URL`: `http://localhost:3000/auth/twitter/callback`
    - `Website URL`: Your website URL
- Save. You'll then be given a `Client ID` and `Client Secret`. Set these as `TWITTER_CLIENT_ID` and `TWITTER_CLIENT_SECRET` in your `.env` file.

Once done, run the `yarn start:auth` command to run the Twitter OAuth server. Open [http://localhost:3000](http://localhost:3000) in your browser, and click `Login with Twitter`.

After authorizing your account with the app, navigate to your terminal where you'll see a JSON object logged. Copy the `token` and `tokenSecret` values and set them as `TWITTER_USER_TOKEN` and `TWITTER_USER_TOKEN_SECRET` in your `.env` file.



================================================
FILE: src/clients/twitter/types.ts
================================================
import { EUploadMimeType, TwitterApi } from "twitter-api-v2";

/**
 * Interface for creating a new Tweet request
 */
export interface CreateTweetRequest {
  /**
   * Text of the Tweet being created.
   */
  text: string;

  /**
   * Media to be attached to the Tweet.
   */
  media?: CreateMediaRequest;

  /**
   * Link to the Tweet being quoted
   */
  quoteTweetId?: string;
}

/**
 * Interface for creating a new media request
 */
export interface CreateMediaRequest {
  /**
   * The base64-encoded file content being uploaded.
   */
  media: Buffer;
  /**
   * The type of media being uploaded.
   */
  mimeType: EUploadMimeType | string;
  /**
   * A list of user IDs to set as additional owners allowed to use the returned mediaId in Tweets or Cards.
   * Maximum of 100 additional owners may be specified
   */
  additionalOwners?: string[];
}

export interface TwitterClientArgs {
  twitterClient: TwitterApi;
  twitterClientOauth2?: TwitterApi;
  twitterToken?: string;
  twitterTokenSecret?: string;
  useArcade?: boolean;
  textOnlyMode?: boolean;
}



================================================
FILE: src/clients/twitter/utils.ts
================================================
import { TweetV2, TweetV2SingleResult } from "twitter-api-v2";
import { extractUrls, imageUrlToBuffer } from "../../agents/utils.js";
import { TweetV2WithURLs } from "../../agents/curate-data/types.js";

/**
 * Generates a link to a tweet based on its author ID and tweet ID.
 * @param authorId The ID of the author of the tweet
 * @param tweetId The ID of the tweet
 * @returns The link to the tweet
 */
export function getTweetLink(authorId: string, tweetId: string): string {
  return `https://twitter.com/${authorId}/status/${tweetId}`;
}

/**
 * Resolves a shortened Twitter URL to the original URL.
 * This is because Twitter shortens URLs in tweets and makes
 * you follow a redirect to get the original URL.
 * @param shortUrl The shortened Twitter URL
 * @returns The resolved Twitter URL
 */
export async function resolveTwitterUrl(
  shortUrl: string,
): Promise<string | undefined> {
  try {
    const response = await fetch(shortUrl, {
      method: "HEAD",
      redirect: "follow",
    });
    return response.url;
  } catch (error) {
    console.warn(`Failed to resolve Twitter URL ${shortUrl}:`, error);
    return undefined;
  }
}

/**
 * Resolves and replaces shortened Twitter URLs in a tweet's text content.
 * @param content The text content of the tweet
 * @returns The text content with shortened Twitter URLs replaced
 */
export async function resolveAndReplaceTweetTextLinks(
  content: string,
): Promise<{
  content: string;
  externalUrls: string[];
}> {
  const urlsInTweet = extractUrls(content);
  if (!urlsInTweet) {
    console.warn("No URLs found in tweet content:", content);
    return { content, externalUrls: [] };
  }

  const cleanedUrls = (
    await Promise.all(
      urlsInTweet.map(async (url) => {
        if (
          !url.includes("https://t.co") &&
          !url.includes("https://x.com") &&
          !url.includes("https://twitter.com")
        ) {
          return {
            original: url,
            resolved: undefined,
          };
        }
        const resolvedUrl = await resolveTwitterUrl(url);
        if (
          !resolvedUrl ||
          resolvedUrl.includes("https://t.co") ||
          resolvedUrl.includes("https://twitter.com") ||
          resolvedUrl.includes("https://x.com")
        ) {
          // Do not return twitter URLs.
          return {
            original: url,
            resolved: undefined,
          };
        }
        return {
          original: url,
          resolved: resolvedUrl,
        };
      }),
    )
  ).flat();

  let updatedContent = content;
  for (const urlPair of cleanedUrls) {
    if (urlPair.resolved) {
      updatedContent = updatedContent.replaceAll(
        urlPair.original,
        urlPair.resolved,
      );
    }
  }

  const externalUrlsSet = new Set(
    cleanedUrls
      .filter(
        (url): url is { resolved: string; original: string } => !!url.resolved,
      )
      .map((url) => url.resolved),
  );

  return {
    content: updatedContent,
    externalUrls: Array.from(externalUrlsSet),
  };
}

/**
 * Processes an array of tweets and resolves any shortened URLs in their text content.
 * For each tweet, it extracts URLs from both regular text and note_tweet text (if present),
 * resolves them to their full form, and adds them to the tweet object.
 *
 * @param {TweetV2[]} tweets - An array of Twitter V2 API tweet objects to process
 * @returns {Promise<TweetV2WithURLs[]>} A promise that resolves to an array of processed tweets.
 *                                       Each tweet will have an additional `external_urls` field
 *                                       containing an array of resolved URLs found in the tweet's text.
 *
 * @example
 * const tweets = await client.getTweets();
 * const processedTweets = await resolveTweetsWithUrls(tweets);
 * // Each tweet in processedTweets will have resolved URLs in external_urls field
 */
export async function resolveTweetsWithUrls(
  tweets: TweetV2[],
): Promise<TweetV2WithURLs[]> {
  const resolvedTweets: TweetV2WithURLs[] = [];

  for (const tweet of tweets) {
    const tweetText = tweet.note_tweet?.text || tweet.text || "";
    if (!tweetText) {
      continue;
    }

    const contentAndUrls = await resolveAndReplaceTweetTextLinks(tweetText);

    if (tweet.note_tweet?.text) {
      resolvedTweets.push({
        ...tweet,
        note_tweet: {
          ...tweet.note_tweet,
          text: contentAndUrls.content,
        },
        external_urls: contentAndUrls.externalUrls,
      });
    } else {
      if (tweet.note_tweet?.text) {
        resolvedTweets.push({
          ...tweet,
          text: contentAndUrls.content,
          external_urls: contentAndUrls.externalUrls,
        });
      }
    }
  }

  return resolvedTweets;
}

/**
 * Combines the text content of a parent tweet and its thread replies into a single string.
 * For both parent tweet and replies, it checks for and uses note_tweet text if available,
 * otherwise falls back to regular text content.
 *
 * @param parentTweet - The parent tweet object containing the initial tweet's data
 * @param threadReplies - An array of reply tweets that form the thread
 * @returns A string containing the combined text of the parent tweet and all replies,
 *          separated by newlines
 */
export function getFullThreadText(
  parentTweet: TweetV2SingleResult,
  threadReplies: TweetV2[],
): string {
  let tweetContentText = "";

  if (parentTweet.data.note_tweet?.text) {
    tweetContentText = parentTweet.data.note_tweet.text;
  } else {
    tweetContentText = parentTweet.data.text;
  }

  threadReplies.forEach((r) => {
    if (r.note_tweet?.text?.length) {
      tweetContentText += `\n${r.note_tweet.text}`;
    } else if (r.text?.length) {
      tweetContentText += `\n${r.text}`;
    }
  });

  return tweetContentText;
}

export async function getMediaUrls(
  parentTweet: TweetV2SingleResult,
  threadReplies: TweetV2[],
): Promise<string[]> {
  const mediaUrls: string[] = [];

  if (parentTweet.includes?.media?.length) {
    const parentMediaUrls = parentTweet.includes?.media
      .filter((m) => (m.url && m.type === "photo") || m.type.includes("gif"))
      .flatMap((m) => (m.url ? [m.url] : []));
    mediaUrls.push(...parentMediaUrls);
  }

  const threadMediaKeys = threadReplies
    .flatMap((r) => r.attachments?.media_keys)
    .filter((m): m is string => !!m);
  const threadMediaUrlPromises = threadMediaKeys.map(async (k) => {
    const imgUrl = `https://pbs.twimg.com/media/${k}?format=jpg`;
    try {
      const { contentType } = await imageUrlToBuffer(imgUrl);
      if (contentType.startsWith("image/")) {
        return imgUrl;
      }
    } catch (e) {
      console.error(
        `Failed to get content type for Twitter media URL: ${imgUrl}\n`,
        e,
      );
    }

    return undefined;
  });

  const threadMediaUrls = (await Promise.all(threadMediaUrlPromises)).filter(
    (m): m is string => !!m,
  );
  mediaUrls.push(...threadMediaUrls);

  return mediaUrls;
}



================================================
FILE: src/clients/twitter/tests/arcade.int.test.ts
================================================
import * as fs from "fs/promises";
import { describe, it, expect } from "@jest/globals";
import Arcade from "@arcadeai/arcadejs";
import { TwitterClient } from "../client.js";
import { extractMimeTypeFromBase64 } from "../../../agents/utils.js";

const tweetId = "1864386797788385455";
const userId = "braceasproul@gmail.com";
const username = "bracesproul";

describe.skip("Arcade", () => {
  describe.skip("Can use Arcade to call the Twitter API", () => {
    const arcade = new Arcade({
      apiKey: process.env.ARCADE_API_KEY,
    });

    it("Can load tweets by a user ID", async () => {
      const result = await arcade.tools.execute({
        tool_name: "X.SearchRecentTweetsByUsername",
        input: {
          username,
          max_results: 1,
        },
        user_id: userId,
      });

      console.log("Result\n");
      console.dir(result, { depth: null });
      expect(result).toBeDefined();
    });

    it("Can load a single tweet by ID", async () => {
      const result = await arcade.tools.execute({
        tool_name: "X.LookupTweetById",
        input: { tweet_id: tweetId },
        user_id: userId,
      });

      console.log("Result\n");
      console.dir(result, { depth: null });
      expect(result).toBeDefined();
    });
  });

  describe.skip("Can use the Twitter Client with Arcade", () => {
    const twitterTokens = {
      twitterToken: process.env.TWITTER_USER_TOKEN || "",
      twitterTokenSecret: process.env.TWITTER_USER_TOKEN_SECRET || "",
    };

    it("Can upload a tweet", async () => {
      const client = await TwitterClient.fromArcade(userId, twitterTokens);
      const tweetText = "test 123 hello world!";
      const result = await client.uploadTweet({
        text: tweetText,
      });

      expect(result.errors).not.toBeDefined();
      expect(result.data).toBeDefined();
      expect(result.data.id).toBeDefined();
      expect(result.data.text).toBe(tweetText);
    });

    it("Can upload media", async () => {
      const client = await TwitterClient.fromArcade(userId, twitterTokens);
      const imageBuffer = await fs.readFile(
        "src/tests/data/langchain_logo.png",
      );

      const result = await client.uploadMedia(imageBuffer, "image/png");
      console.log("Media ID", result);
      expect(result).toBeDefined();
    });

    it("Can upload a tweet with media", async () => {
      const client = await TwitterClient.fromArcade(userId, twitterTokens);
      const imageBuffer = await fs.readFile(
        "src/tests/data/langchain_logo.png",
      );
      const tweetText = "test 123 hello world! (with image)";

      const result = await client.uploadTweet({
        text: tweetText,
        media: {
          media: imageBuffer,
          mimeType: "image/png",
        },
      });

      expect(result).toBeDefined();
    });

    it("Can upload this image", async () => {
      const client = await TwitterClient.fromArcade(userId, twitterTokens);
      const imageBuffer = await fs.readFile(
        "src/tests/data/langchain_logo_2.png",
      );
      const imageBase64 = imageBuffer.toString("base64");

      const imageMimeType = extractMimeTypeFromBase64(imageBase64);
      if (!imageMimeType) {
        throw new Error("Could not determine image mime type");
      }
      const result = await client.uploadMedia(imageBuffer, imageMimeType);
      console.log("result", result);
    });

    it("Can upload image from URL", async () => {
      const client = await TwitterClient.fromArcade(userId, twitterTokens);
      const imageUrl =
        "https://miro.medium.com/v2/resize:fit:1200/1*-PlFCd_VBcALKReO3ZaOEg.png";

      const response = await fetch(imageUrl);
      if (!response.ok) {
        throw new Error(`Failed to fetch image: ${response.statusText}`);
      }

      const imageBuffer = Buffer.from(await response.arrayBuffer());
      const contentType = response.headers.get("content-type") || "image/jpeg";
      console.log("contentType", contentType);
      const result = await client.uploadMedia(imageBuffer, contentType);
      console.log("result", result);
    });
  });
});



================================================
FILE: src/clients/twitter/tests/twitter.int.test.ts
================================================
import * as fs from "fs/promises";
import { describe, it, expect } from "@jest/globals";
import { TwitterClient } from "../client.js";
import { imageUrlToBuffer } from "../../../agents/utils.js";

const tweetId = "1864386797788385455";
// const tweetWithMediaId = "1846215982765035677";
const tweetWithMediaId = "1874884500062122296";

describe("Basic Twitter Auth", () => {
  const client = TwitterClient.fromBasicTwitterAuth();

  it("Can confirm the user is authed", async () => {
    const isAuthed = await client.testAuthentication();
    expect(isAuthed).toBe(true);
  });

  it("Can read a tweet from ID", async () => {
    const tweet = await client.getTweet(tweetId);
    expect(tweet).toBeDefined();
    console.log("Tweet\n");
    console.dir(tweet.data, { depth: null });
  });

  it("Can read a tweet from ID and get media", async () => {
    const tweet = await client.getTweet(tweetWithMediaId);
    console.dir(tweet, { depth: null });
    expect(tweet).toBeDefined();
    // Check the full length Tweet is returned
    expect(tweet.data.note_tweet?.text).toBeDefined();
    expect(tweet.data.note_tweet?.text?.length).toBeGreaterThan(280);
    // Check the media is returned
    expect(tweet.includes?.media?.[0]).toBeDefined();
    expect(tweet.includes?.media?.[0].url).toBeDefined();
    expect(tweet.includes?.media?.[0].type).toBe("photo");
    const mediaUrl = tweet.includes?.media?.[0].url || "";
    const mediaData = await imageUrlToBuffer(mediaUrl);
    expect(mediaData.buffer).toBeDefined();
    expect(mediaData.contentType).toBeDefined();
  });

  it("Can post a text only tweet", async () => {
    const result = await client.uploadTweet({
      text: "test 123 hello world!",
    });

    expect(result.errors).not.toBeDefined();
    expect(result.data).toBeDefined();
    expect(result.data.id).toBeDefined();
    expect(result.data.text).toBe("test 123 hello world!");
  });

  it("Can post a text and media tweet", async () => {
    const imageBuffer = await fs.readFile("src/tests/data/langchain_logo.png");
    const tweetText = "test 123 hello world! (with image)";

    const result = await client.uploadTweet({
      text: tweetText,
      media: {
        media: imageBuffer,
        mimeType: "image/png",
      },
    });

    expect(result).toBeDefined();
  });

  it("Can fetch a thread using the original tweet", async () => {
    const thread = await client.getThreadReplies(
      "1857117443065540707",
      "LangChainAI",
    );
    console.log("thread", thread?.length);
    console.dir(thread, { depth: null });
    expect(thread).toBeDefined();
    expect(thread?.length).toBe(8); // The thread should have 8 replies.
  });

  it("Can search tweets", async () => {
    const query = `@LangChainAI -is:reply -is:retweet -is:quote has:links`;
    const langchainTweets = await client.searchTweets(query, {
      maxResults: 10, // Twitter API v2 limits to 60 req/15 min
    });
    expect(langchainTweets.data).toBeDefined();
    expect(langchainTweets.data.data).toBeDefined();
    expect(langchainTweets.data.data.length).toBe(10);
  });
});



================================================
FILE: src/evals/e2e/e2e.int.test.ts
================================================
import { v4 as uuidv4 } from "uuid";
import * as ls from "langsmith/jest";
import { type SimpleEvaluator } from "langsmith/jest";
import { INPUTS } from "./inputs.js";
import { generatePostGraph } from "../../agents/generate-post/generate-post-graph.js";
import { InMemoryStore, MemorySaver } from "@langchain/langgraph";
import { HumanInterrupt } from "@langchain/langgraph/prebuilt";
import { removeUrls } from "../../agents/utils.js";
import {
  BASE_GENERATE_POST_CONFIG,
  GeneratePostAnnotation,
} from "../../agents/generate-post/generate-post-state.js";

const checkGeneratePostResult: SimpleEvaluator = ({ expected, actual }) => {
  // Check the following:
  // 1(a). A post was generated
  // 1(b). Check post length is less than or equal to 280 after removing URL.
  // 2. Check page contents were extracted
  // 3. A report was generated with the proper fields (check markdown headers)
  // 4. Check images were generated
  // 5. Check the interrupt value is as expected

  // If any are false, return 0
  // If all are true, return 1.
  let postScore = 0;
  let pageContentsScore = 0;
  let reportScore = 0;
  let imagesScore = 0;
  let interruptScore = 0;

  const { state, interrupt } = actual as {
    state: typeof GeneratePostAnnotation.State;
    interrupt: HumanInterrupt | undefined;
  };

  if (
    !state.pageContents?.length &&
    !state.relevantLinks?.length &&
    !state.imageOptions?.length
  ) {
    // Likely did not pass the validation step. Fail.
    return {
      key: "correct_post_generation",
      score: 0,
      evaluatorInfo: {
        postScore: 0,
        pageContentsScore: 0,
        reportScore: 0,
        imagesScore: 0,
        interruptScore: 0,
      },
    };
  }

  if (state.post) {
    const cleanedPost = removeUrls(state.post || "");
    if (cleanedPost.length <= 280 && cleanedPost.length > 0) {
      postScore = 1;
    }
  }

  if (state.pageContents?.length) {
    pageContentsScore = 1;
  }

  if (state.report.length > 0) {
    // TODO: Extract headers and validate they are correct
    reportScore = 1;
  }

  if (state.imageOptions?.length) {
    if (state.imageOptions.length === expected.imageOptions.length) {
      imagesScore = 1;
    }
  }

  if (interrupt) {
    interruptScore = 1;
  }

  const totalScore =
    postScore + pageContentsScore + reportScore + imagesScore + interruptScore;
  let score = 0;
  if (totalScore === 5) {
    score = 1;
  }

  return {
    key: "correct_post_generation",
    score,
    evaluatorInfo: {
      postScore,
      pageContentsScore,
      reportScore,
      imagesScore,
      interruptScore,
    },
  };
};

ls.describe("SMA - E2E", () => {
  ls.test.each(INPUTS)(
    "Should validate the end to end flow of the generate post agent",
    async ({ inputs }) => {
      const graph = generatePostGraph;
      graph.checkpointer = new MemorySaver();
      graph.store = new InMemoryStore();

      const threadId = uuidv4();
      const config = {
        configurable: {
          thread_id: threadId,
          ...BASE_GENERATE_POST_CONFIG,
        },
      };

      await generatePostGraph.invoke(inputs, config);
      const graphState = await generatePostGraph.getState(config);
      const state = graphState.values;
      const interruptValue = graphState.tasks[0]?.interrupts?.[0]?.value;
      // console.log("\nState\n", state);
      // console.log("\nInterrupt Value\n", interruptValue);
      console.log("Finished invoking graph with URL", inputs.links[0]);
      await ls
        .expect({
          state,
          interrupt: interruptValue,
        })
        .evaluatedBy(checkGeneratePostResult)
        .toBe(1);
      return graphState;
    },
  );
});



================================================
FILE: src/evals/e2e/inputs.ts
================================================
export const INPUTS = [
  {
    inputs: {
      links: ["https://github.com/langchain-ai/open-canvas"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293062688.jpeg",
        "https://raw.githubusercontent.com/langchain-ai/open-canvas/main/public/screenshot.png",
      ],
    },
  },
  {
    inputs: {
      links: ["https://github.com/muratcankoylan/AI-Investigator"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293234680.jpeg",
        "https://github.com/user-attachments/assets/7b935a1b-b79e-4fb3-85c7-cb18d48601bb",
        "https://github.com/user-attachments/assets/1669df59-a81a-4aab-b62b-149e1480a82a",
        "https://github.com/user-attachments/assets/370e2f63-fd1c-4af4-ae78-130b99fe4b0b",
        "https://github.com/user-attachments/assets/95be2e76-12bd-4dea-bd91-1b7d309f0f6d",
      ],
    },
  },
  {
    inputs: {
      links: ["https://github.com/souzatharsis/podcastfy"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293405216.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: [
        "https://github.com/elizabethsiegle/wnba-analytics-dash-ai-insights/tree/main",
      ],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293458434.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: ["https://github.com/ahmad2b/postbot3000"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293492964.jpeg",
        "https://raw.githubusercontent.com/ahmad2b/postbot3000/main/agent-service/app/graph_image.png",
        "https://raw.githubusercontent.com/ahmad2b/postbot3000/main/agentui-action.png",
      ],
    },
  },
  {
    inputs: {
      links: ["https://github.com/kaarthik108/snowChat"],
    },
    expected: {
      imageOptions: [
        "https://private-user-images.githubusercontent.com/53030784/271793562-7538d25b-a2d4-4a2c-9601-fb4c7db3c0b6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzYyOTU1MzUsIm5iZiI6MTczNjI5NTIzNSwicGF0aCI6Ii81MzAzMDc4NC8yNzE3OTM1NjItNzUzOGQyNWItYTJkNC00YTJjLTk2MDEtZmI0YzdkYjNjMGI2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTA4VDAwMTM1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk5ODA5OTNiNTIxYTMyYjJlMzY2ZDAwNWFlN2VjMjY5NTA0ODYzODUzYmQ2NGNlOWQ3YzMwMGVlYmQxOWNjZmMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.eGsdqf4MUPMnljz9_53yTh-FuUU3R0q2NW5s6lhWyBE",
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293541827.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: [
        "https://github.com/samwit/agent_tutorials/tree/main/agent_write",
      ],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293578042.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: ["https://github.com/starpig1129/AI-Data-Analysis-MultiAgent"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736293622773.jpeg",
        "https://raw.githubusercontent.com/starpig1129/AI-Data-Analysis-MultiAgent/main/Architecture.png",
      ],
    },
  },
  {
    inputs: {
      links: ["https://www.youtube.com/watch?v=gwE3Wv4MNLw"],
    },
    expected: {
      imageOptions: ["https://i.ytimg.com/vi/gwE3Wv4MNLw/sddefault.jpg"],
    },
  },
  {
    inputs: {
      links: ["https://www.youtube.com/watch?v=VyyJFrPlHfk"],
    },
    expected: {
      imageOptions: ["https://i.ytimg.com/vi/VyyJFrPlHfk/maxresdefault.jpg"],
    },
  },
  {
    inputs: {
      links: ["https://www.youtube.com/watch?v=BGvqeRB4Jpk"],
    },
    expected: {
      imageOptions: ["https://i.ytimg.com/vi/BGvqeRB4Jpk/sddefault.jpg"],
    },
  },
  {
    inputs: {
      links: ["https://www.youtube.com/watch?v=u_Xm3vgBQ9Y"],
    },
    expected: {
      imageOptions: ["https://i.ytimg.com/vi/u_Xm3vgBQ9Y/maxresdefault.jpg"],
    },
  },
  {
    inputs: {
      links: ["https://www.youtube.com/watch?v=02IDU8eCX8o"],
    },
    expected: {
      imageOptions: ["https://i.ytimg.com/vi/02IDU8eCX8o/maxresdefault.jpg"],
    },
  },
  {
    inputs: {
      links: [
        "https://levelup.gitconnected.com/learn-how-to-build-ai-agents-chatbots-with-langgraph-1fe09c4558c6",
      ],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-levelup.gitconnected.com-1736293998424.jpeg",
        "https://miro.medium.com/v2/resize:fill:48:48/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
        "https://miro.medium.com/v2/resize:fit:700/1*pvsQBbW2fTYhvYcQIMCt3w.png",
        "https://miro.medium.com/v2/resize:fit:700/1*xwU1Nf9qpggbIB73CGaNRg.png",
        "https://miro.medium.com/v2/resize:fit:700/1*ldsEjgVFdxhpcKtgc9ddvQ.png",
        "https://miro.medium.com/v2/resize:fit:328/1*DiUoE5bztatRHtzwr1Tc5Q.png",
        "https://miro.medium.com/v2/resize:fill:96:96/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
        "https://miro.medium.com/v2/resize:fill:128:128/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
        "https://miro.medium.com/v2/resize:fill:48:48/1*AiTJDz5wwQFiUCf_SrBOQA.jpeg",
        "https://miro.medium.com/v2/resize:fill:48:48/1*rex1OZ5_KcxK2QrsZr3Cgw.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: [
        "https://diamantai.substack.com/p/stop-reading-start-understanding",
      ],
    },
    expected: {
      imageOptions: [
        "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19c2d967-34c9-472c-92a5-508a5fa46855_360x730.png",
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-diamantai.substack.com-1736294093371.jpeg",
      ],
    },
  },
  {
    inputs: {
      links: ["https://www.js-craft.io/blog/fallbacks-langchain-javascript/"],
    },
    expected: {
      imageOptions: [
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-www.js-craft.io-1736294190656.jpeg",
        "https://secure.gravatar.com/avatar/02e06dc0b62ea506a4b0adaa9310adc6?s=44&d=blank&r=g", // TODO: this should not be included, but Vertex isn't able to read it.
      ],
    },
  },
  // {
  //   inputs: {
  //     links: [
  //       "https://www.elastic.co/search-labs/blog/local-rag-agent-elasticsearch-langgraph-llama3", // Has images which the find-image subgraph fails to extract
  //     ],
  //   },
  //   expected: {
  //     imageOptions: [],
  //   },
  // },
  // {
  //   inputs: {
  //     links: [
  //       "https://myscale.com/blog/agentic-rag-with-voyageai-gemini-and-langgraph/",
  //     ],
  //   },
  //   expected: {
  //     imageOptions: [],
  //   },
  // },
];



================================================
FILE: src/evals/general/index.ts
================================================
import { type Example, Run } from "langsmith";
import { evaluate, EvaluationResult } from "langsmith/evaluation";
// eslint-disable-next-line import/no-extraneous-dependencies
import "dotenv/config";
import { generatePostGraph } from "../../agents/generate-post/generate-post-graph.js";

const runGraph = async (
  input: Record<string, any>,
): Promise<Record<string, any>> => {
  return await generatePostGraph.invoke(input);
};

const evaluatePost = (run: Run, example?: Example): EvaluationResult => {
  if (!example) {
    throw new Error("No example provided");
  }
  if (!example.outputs) {
    throw new Error("No example outputs provided");
  }
  if (!run.outputs) {
    throw new Error("No run outputs provided");
  }

  // TODO: Implement evaluation logic
  throw new Error("Evaluation logic not implemented");
};

async function runEval() {
  const datasetName = "sma:generate-post:general";
  await evaluate(runGraph, {
    data: datasetName,
    evaluators: [evaluatePost],
    experimentPrefix: "Post Generation-General",
  });
}

runEval().catch(console.error);

// https://x.com/LangChainAI/status/1858311912091476455
// https://x.com/LangChainAI/status/1857811436984217835
// https://x.com/LangChainAI/status/1856026604180242636
// https://x.com/LangChainAI/status/1855437724536504482



================================================
FILE: src/evals/github/index.ts
================================================
import { type Example, Run } from "langsmith";
import { evaluate, EvaluationResult } from "langsmith/evaluation";
// eslint-disable-next-line import/no-extraneous-dependencies
import "dotenv/config";
import { generatePostGraph } from "../../agents/generate-post/generate-post-graph.js";

const runGraph = async (
  input: Record<string, any>,
): Promise<Record<string, any>> => {
  return await generatePostGraph.invoke(input);
};

const evaluatePost = (run: Run, example?: Example): EvaluationResult => {
  if (!example) {
    throw new Error("No example provided");
  }
  if (!example.outputs) {
    throw new Error("No example outputs provided");
  }
  if (!run.outputs) {
    throw new Error("No run outputs provided");
  }
  console.log("\n\nGENERATED POST:\n", run.outputs.post.join("\n---\n"));
  console.log("\nEXAMPLE POST:\n", example.outputs.post);

  return {
    key: "correct_generation",
    score: true,
  };
};

async function runEval() {
  const datasetName = "sma:generate-post:github";
  await evaluate(runGraph, {
    data: datasetName,
    evaluators: [evaluatePost],
    experimentPrefix: "Post Generation-Github",
  });
}

runEval().catch(console.error);

// Should be approved and posts generated
// https://x.com/LangChainAI/status/1861108590792036799
// https://x.com/LangChainAI/status/1860760295188185246
// https://x.com/LangChainAI/status/1860745200668201148
// https://x.com/LangChainAI/status/1860714493661106562
// https://x.com/LangChainAI/status/1860485484683911584
// https://x.com/LangChainAI/status/1860397908451033240

// Would need review:
// https://x.com/LangChainAI/status/1858175010612916272



================================================
FILE: src/evals/twitter/index.ts
================================================
import { type Example, Run } from "langsmith";
import { evaluate, EvaluationResult } from "langsmith/evaluation";
// eslint-disable-next-line import/no-extraneous-dependencies
import "dotenv/config";
import { generatePostGraph } from "../../agents/generate-post/generate-post-graph.js";

const runGraph = async (
  input: Record<string, any>,
): Promise<Record<string, any>> => {
  return await generatePostGraph.invoke(input);
};

const evaluatePost = (run: Run, example?: Example): EvaluationResult => {
  if (!example) {
    throw new Error("No example provided");
  }
  if (!example.outputs) {
    throw new Error("No example outputs provided");
  }
  if (!run.outputs) {
    throw new Error("No run outputs provided");
  }

  // TODO: Implement evaluation logic
  throw new Error("Evaluation logic not implemented");
};

async function runEval() {
  const datasetName = "sma:generate-post:twitter";
  await evaluate(runGraph, {
    data: datasetName,
    evaluators: [evaluatePost],
    experimentPrefix: "Post Generation-Twitter",
  });
}

runEval().catch(console.error);



================================================
FILE: src/evals/validate-images/inputs.ts
================================================
const INPUTS = [
  {
    report: `News TL;DR: An Intelligent News Processing Agent

The News TL;DR agent is an innovative open-source solution developed by Jason Sheinkopf during a LangChain hackathon. This sophisticated system transforms how users consume news by intelligently processing, analyzing, and summarizing articles across multiple sources. Instead of overwhelming users with content, it delivers concise, meaningful insights by understanding the context and relevance of news articles.


LangChain Implementation

The system is built on LangGraph, which serves as the foundation for orchestrating complex workflows and decision-making processes. LangGraph enables the agent to:



Dynamically adapt its search and analysis strategies

Process multiple articles in parallel

Make intelligent decisions about content relevance

Orchestrate the flow between different components (query processing, news collection, analysis, and summarization)


Technical Deep Dive

The agent implements several sophisticated components:



Query processor for understanding user intent

News collection engine using NewsAPI integration

Content analysis pipeline with web scraping capabilities

Intelligent article selection system

Cross-article summary generation


The entire implementation is available as an open-source tutorial at: https://github.com/NirDiamant/GenAI_Agents/blob/main/all_agents_tutorials/news_tldr_langgraph.ipynb


This project demonstrates how LangGraph can be used to build practical, production-ready AI applications that solve real-world information processing challenges. The system's ability to adapt its approach dynamically and process multiple articles in parallel showcases the power of LangGraph's workflow orchestration capabilities.`,
    post: `ðŸ“° News TL;DR Agent

Meet your intelligent news assistant that analyzes multiple articles simultaneously to deliver concise insights. Built with LangGraph, this open-source agent processes news in parallel, scores content relevance, and generates cross-article summaries.

Learn how to build your own news processing agent ðŸ”
https://diamantai.substack.com/p/stop-reading-start-understanding`,
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-diamantai.substack.com-1735869048739.jpeg", // 0 - approved
      "https://substackcdn.com/image/fetch/w_96,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84bf24d0-f0ec-49fc-8e8f-800eec27706d_1280x1280.png", // 1 - not
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cacafa3-9e4a-49d6-afde-2239493e73a3_6960x4640.jpeg", // 2 - not
      "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19c2d967-34c9-472c-92a5-508a5fa46855_360x730.png", // 3 - approved
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fea2024-a403-469d-9c4e-e7bc06882275_640x550.png", // 4 - not
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce9d2e8d-2701-4ad1-a761-02d6167a02e7_96x96.png", // 5 - not
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d78d477-471b-43f2-9770-38418f9e64bd_500x500.png", // 6 - not
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F637a9f66-65c3-40eb-916e-5952936e24d5_490x464.png", // 7 - not
      "https://substackcdn.com/image/fetch/w_80,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa59ae4c4-030a-45f4-8d0f-42e75c0e12c5_96x96.jpeg", // 8 - not
      "https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png", // 9 - not
      "https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f00ef95-0cc4-439b-9c92-f56a34ee59eb_256x256.png", // 10 - not
      "https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d9cbdb7-eb66-40c2-8c60-2ba9b461f69c_1170x1170.png", // 11 - not
      "https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7731cfa7-fc73-473b-bb49-fabac7432086_1792x1024.png", // 12 - maybe
      "https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F916fbcb0-447f-42b6-8635-71e71e6280b5_1024x1024.png", // 13 - maybe
      "https://substackcdn.com/image/fetch/w_320,h_213,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feefc7325-a910-410d-9fb2-04bf0d38fe2e_1024x1024.png", // 4 - maybe
    ],
  },
  {
    report: `# Marketing Report: Learn How to Build AI Agents & Chatbots with LangGraph

## Introduction & Summary
LangGraph, an open-source framework from LangChain, enables developers to build sophisticated AI agents and multi-agent systems. This comprehensive tutorial demonstrates how to create AI applications, including chatbots, using LangGraph's powerful state management and agent interaction capabilities. The content provides a practical, hands-on approach with working code examples and implementation details.

## LangChain Implementation
The tutorial leverages two key LangChain products:
- **LangGraph**: The core framework used throughout the tutorial for building the AI agents and chatbot application. The content demonstrates LangGraph's key components - nodes, states, and edges - showing how they work together to create agentic applications.
- **LangSmith**: Used for tracing and monitoring the application, with the tutorial showing how to set up LangSmith API keys and enable tracing for development.

## Technical Details
The tutorial provides an in-depth technical implementation including:
- Detailed explanation of LangGraph's architecture using nodes, states, and edges
- Step-by-step code examples showing how to create a basic chatbot
- Integration with LLM services and state management
- Complete working example with a GitHub repository (https://github.com/pavanbelagatti/LangGraph-Chatbot-Tutorial)
- Environment setup instructions using SingleStore notebooks
- Code samples for creating StateGraphs and implementing chat functionality

The content serves as both an introduction to LangGraph and a practical implementation guide, making it valuable for developers looking to build AI applications using LangChain's tools.`,
    post: `ðŸ“š Build AI Agents with LangGraph

A hands-on tutorial that walks you through building AI agents and chatbots using LangGraph, complete with working code examples and a full GitHub repository. Perfect for developers ready to create their first agentic application.

Ready to start building? Check out the tutorial ðŸ‘‰ https://levelup.gitconnected.com/learn-how-to-build-ai-agents-chatbots-with-langgraph-1fe09c4558c6`,
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-levelup.gitconnected.com-1735950183280.jpeg", // yes
      "https://miro.medium.com/v2/resize:fill:88:88/1*BCvL6b99GdWbNEo79BF1sQ.jpeg",
      "https://miro.medium.com/v2/resize:fill:48:48/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
      "https://miro.medium.com/v2/resize:fit:700/1*pvsQBbW2fTYhvYcQIMCt3w.png", // yes
      "https://miro.medium.com/v2/resize:fit:700/1*xwU1Nf9qpggbIB73CGaNRg.png", // yes
      "https://miro.medium.com/v2/resize:fit:700/1*ldsEjgVFdxhpcKtgc9ddvQ.png", // yes
      "https://miro.medium.com/v2/resize:fit:328/1*DiUoE5bztatRHtzwr1Tc5Q.png",
      "https://miro.medium.com/v2/resize:fill:96:96/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
      "https://miro.medium.com/v2/resize:fill:128:128/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg",
      "https://miro.medium.com/v2/resize:fill:48:48/1*AiTJDz5wwQFiUCf_SrBOQA.jpeg",
      "https://miro.medium.com/v2/resize:fill:48:48/1*zjPggFS8yoRtFbAP9R_3lw.jpeg",
      "https://miro.medium.com/v2/resize:fill:48:48/1*PNVLDmurJ5LoCjB9Ovdnpw.png",
      "https://miro.medium.com/v2/resize:fill:48:48/1*rex1OZ5_KcxK2QrsZr3Cgw.jpeg",
      "https://miro.medium.com/v2/resize:fill:20:20/1*R8zEd59FDf0l8Re94ImV0Q.png",
    ],
  },
  {
    report: `## Content Transformation Tool with Model Context Protocol\n\n### Overview\nA sophisticated web content transformation application that leverages the Model Context Protocol (MCP) to convert any website into query-relevant content. The tool specializes in creating LLM-optimized text files, documentation indexing, and research automation, while offering seamless integration with platforms like X and Slack through Arcade.\n\n### LangChain Implementation\nThe application demonstrates robust integration of LangChain's ecosystem:\n- **LangGraph**: Serves as the primary MCP client, handling the core content transformation logic\n- **LangSmith**: Powers the tracing functionality, enabling comprehensive monitoring and debugging of the transformation pipeline\n\n### Technical Details\nThe solution employs a sophisticated tech stack:\n- Firecrawll technology for intelligent web research, including site mapping and selective scraping\n- Integration with OpenAI's structured outputs for reliable content processing\n- Robust error handling through exponential backoff mechanisms\n- Data validation using Pydantic models\n- Asynchronous processing for improved performance\n- Arcade integration for multi-platform support (X, Slack)\n\nThe architecture prioritizes reliability and efficiency, making it an ideal solution for developers looking to transform web content into LLM-ready formats while maintaining high accuracy and performance standards.`,
    post: `ðŸ” Web Content Transformer\n\nTransform websites into LLM-optimized content with this powerful research automation tool. Leveraging LangGraph for content transformation and LangSmith for monitoring, it seamlessly integrates with X and Slack through Arcade.\n\nKey features:\nâ€¢ Intelligent web research with Firecrawll\nâ€¢ Real-time monitoring\nâ€¢ Multi-platform support\n\nðŸš€ Check it out: https://github.com/lgesuellip/researcher_agent/tree/main/servers`,
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736197683143.jpeg",
      "https://raw.githubusercontent.com/lgesuellip/researcher_agent/main/app_architecture.png",
    ],
  },
  {
    report: `<thinking>\nKey observations from the content:\n1. This is an enterprise AI case study analysis tool that uses Claude 3.5 Sonnet and Firecrawl APIs\n2. Two main operation modes: CSV analysis and website discovery\n3. Uses LangChain for orchestrating the AI workflows and processing\n4. Comprehensive reporting system with 3 types of reports\n5. Well-structured technical architecture with clear integration points\n6. Interesting features include automated discovery, intelligent identification, and detailed analysis\n7. Open source project with MIT license\n8. Uses modern AI/ML tech stack\n9. Focuses on enterprise AI implementation analysis\n10. Has visualization capabilities for executive dashboards\n</thinking>\n\n<report>\n## Part 1: Introduction and Summary\nThe AI Enterprise Case Study Analyzer is an intelligent system designed to automate the discovery, analysis, and reporting of enterprise AI case studies. This open-source tool leverages the Claude 3.5 Sonnet API and Firecrawl API to provide comprehensive analysis through two main modes: CSV-based URL analysis and automated website discovery. The system stands out for its ability to automatically identify relevant case studies, extract meaningful content, and generate detailed reports including individual analyses, cross-case comparisons, and executive dashboards.\n\n## Part 2: LangChain Implementation\nThe system utilizes LangChain's orchestration capabilities to create sophisticated AI workflows. LangChain is implemented to:\n- Coordinate interactions between Claude 3.5 Sonnet and Firecrawl APIs\n- Manage the content processing pipeline\n- Handle the multi-step analysis workflow from discovery to report generation\n- Structure and process the extracted data for various report formats\n\n## Part 3: Technical Details\nThe analyzer features a robust technical architecture with several notable components:\n- **Content Processing Pipeline**: Implements Firecrawl's map and scrape endpoints for content discovery and extraction\n- **Intelligent Analysis**: Uses Claude 3.5 Sonnet for case study identification and detailed analysis\n- **Comprehensive Reporting**: Generates three types of reports (individual case studies, cross-case analysis, executive dashboard)\n- **Structured Output**: Produces well-formatted markdown and JSON reports with detailed metrics and insights\n- **API Integration**: Features detailed integration with both Firecrawl and Claude APIs with configurable parameters\n\nThe project is open source (MIT licensed) and available for contributions, with a clear installation process and documentation. View the project structure and setup instructions at [GitHub](https://github.com/yourusername/ai-case-study-analyzer.git).`,
    post: `ðŸ” AI Investigator\n\nAn open-source tool that automates enterprise AI case study discovery and analysis. Powered by Claude 3.5 Sonnet and LangChain's orchestration, it transforms raw data into comprehensive reports and executive dashboards.\n\nExplore this powerful research tool! ðŸš€\nhttps://github.com/muratcankoylan/AI-Investigator`,
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736197436305.jpeg",
      "https://github.com/user-attachments/assets/95be2e76-12bd-4dea-bd91-1b7d309f0f6d",
      "https://github.com/user-attachments/assets/7b935a1b-b79e-4fb3-85c7-cb18d48601bb",
      "https://github.com/user-attachments/assets/1669df59-a81a-4aab-b62b-149e1480a82a",
      "https://github.com/user-attachments/assets/370e2f63-fd1c-4af4-ae78-130b99fe4b0b",
      "https://api.star-history.com/svg?repos=muratcankoylan/AI-Investigator&type=Date",
    ],
  },
];

const OUTPUTS = [
  {
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-diamantai.substack.com-1735869048739.jpeg", // 0 - approved
      "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19c2d967-34c9-472c-92a5-508a5fa46855_360x730.png", // 3 - approved
    ],
  },
  {
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-levelup.gitconnected.com-1735950183280.jpeg", // yes
      "https://miro.medium.com/v2/resize:fit:700/1*pvsQBbW2fTYhvYcQIMCt3w.png", // yes
      "https://miro.medium.com/v2/resize:fit:700/1*xwU1Nf9qpggbIB73CGaNRg.png", // yes
      "https://miro.medium.com/v2/resize:fit:700/1*ldsEjgVFdxhpcKtgc9ddvQ.png", // yes
    ],
  },
  {
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736197683143.jpeg",
      "https://raw.githubusercontent.com/lgesuellip/researcher_agent/main/app_architecture.png",
    ],
  },
  {
    imageOptions: [
      "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1736197436305.jpeg",
      "https://github.com/user-attachments/assets/95be2e76-12bd-4dea-bd91-1b7d309f0f6d",
      "https://github.com/user-attachments/assets/7b935a1b-b79e-4fb3-85c7-cb18d48601bb",
      "https://github.com/user-attachments/assets/1669df59-a81a-4aab-b62b-149e1480a82a",
      "https://github.com/user-attachments/assets/370e2f63-fd1c-4af4-ae78-130b99fe4b0b",
    ],
  },
];

export const TEST_EACH_INPUTS_OUTPUTS = [
  {
    inputs: INPUTS[0],
    expected: OUTPUTS[0],
  },
  {
    inputs: INPUTS[1],
    expected: OUTPUTS[1],
  },
  {
    inputs: INPUTS[2],
    expected: OUTPUTS[2],
  },
  {
    inputs: INPUTS[3],
    expected: OUTPUTS[3],
  },
];



================================================
FILE: src/evals/validate-images/validate-images.int.test.ts
================================================
import * as ls from "langsmith/jest";
import { type SimpleEvaluator } from "langsmith/jest";
import { GeneratePostAnnotation } from "../../agents/generate-post/generate-post-state.js";
import { TEST_EACH_INPUTS_OUTPUTS } from "./inputs.js";
import { validateImages } from "../../agents/find-images/nodes/validate-images.js";

const checkCorrectImages: SimpleEvaluator = ({ expected, actual }) => {
  const expectedImageOptions = expected.imageOptions as string[];
  const actualImageOptions = actual.imageOptions as string[];
  let numCorrect = 0;
  for (const expectedUrl of expectedImageOptions) {
    if (actualImageOptions.find((actualUrl) => actualUrl === expectedUrl)) {
      numCorrect += 1;
    }
  }
  const score = numCorrect / expectedImageOptions.length;

  return {
    key: "correct_images",
    score,
  };
};

ls.describe("SMA - Validate Images", () => {
  ls.test.each(TEST_EACH_INPUTS_OUTPUTS)(
    "Should validate images",
    async ({ inputs }) => {
      // Import and run your app, or some part of it here
      const result = await validateImages(
        inputs as typeof GeneratePostAnnotation.State,
      );
      console.log("result!", result);
      const evalResult = ls.expect(result).evaluatedBy(checkCorrectImages);
      // Ensure the result is greater than 0.8 and less than or equal to 1
      // CHECK IF THIS RUNS THE EVALUATOR TWICE
      await evalResult.toBeGreaterThanOrEqual(0.8);
      await evalResult.toBeLessThanOrEqual(1);
      return result;
    },
  );
});



================================================
FILE: src/evals/youtube/index.ts
================================================
import { type Example, Run } from "langsmith";
import { evaluate, EvaluationResult } from "langsmith/evaluation";
// eslint-disable-next-line import/no-extraneous-dependencies
import "dotenv/config";
import { generatePostGraph } from "../../agents/generate-post/generate-post-graph.js";

const runGraph = async (
  input: Record<string, any>,
): Promise<Record<string, any>> => {
  return await generatePostGraph.invoke(input);
};

const evaluatePost = (run: Run, example?: Example): EvaluationResult => {
  if (!example) {
    throw new Error("No example provided");
  }
  if (!example.outputs) {
    throw new Error("No example outputs provided");
  }
  if (!run.outputs) {
    throw new Error("No run outputs provided");
  }

  // TODO: Implement evaluation logic
  throw new Error("Evaluation logic not implemented");
};

async function runEval() {
  const datasetName = "sma:generate-post:youtube";
  await evaluate(runGraph, {
    data: datasetName,
    evaluators: [evaluatePost],
    experimentPrefix: "Post Generation-YouTube",
  });
}

runEval().catch(console.error);

// https://x.com/LangChainAI/status/1860438927892709871
// https://x.com/LangChainAI/status/1860352611834069056
// https://x.com/LangChainAI/status/1855629502690349326
// https://x.com/LangChainAI/status/1855362227420967092
// https://x.com/LangChainAI/status/1854925528031195148



================================================
FILE: src/tests/agent.test.ts
================================================
import * as fs from "fs/promises";
import { describe, it, expect, jest } from "@jest/globals";
import {
  extractMimeTypeFromBase64,
  extractTweetId,
  extractUrls,
  extractUrlsFromSlackText,
} from "../agents/utils.js";
import { timezoneToUtc } from "../utils/date.js";

describe("extractUrlsFromSlackText", () => {
  it("Can extract URL from Slack-style message text", () => {
    const singleUrlText = `<https://github.com/karimulla0908/capstone_repo|https://github.com/karimulla0908/capstone_repo>`;
    const urls = extractUrlsFromSlackText(singleUrlText);
    expect(urls).toHaveLength(1);
    expect(urls[0]).toBe("https://github.com/karimulla0908/capstone_repo");
  });

  it("Can extract multiple URLs from Slack-style message text", () => {
    const multipleUrlsText = `<https://github.com/karimulla0908/capstone_repo|https://github.com/karimulla0908/capstone_repo> And another one is <this youtube video|https://www.youtube.com/watch?v=OyDfr0xIhss>`;
    const urls = extractUrlsFromSlackText(multipleUrlsText);
    expect(urls).toHaveLength(2);
    expect(urls[0]).toBe("https://github.com/karimulla0908/capstone_repo");
    expect(urls[1]).toBe("https://www.youtube.com/watch?v=OyDfr0xIhss");
  });

  it("Can extract URLs when they do not have a label", () => {
    const urlWithoutLabelText = `<https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py>`;
    const urls = extractUrlsFromSlackText(urlWithoutLabelText);
    expect(urls).toHaveLength(1);
    expect(urls[0]).toBe(
      "https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py",
    );
  });
});

describe("extractTweetId", () => {
  it("Can extract tweet IDs", () => {
    const id = "1422656689476354560";
    const tweetUrl = `https://twitter.com/elonmusk/status/${id}`;
    const tweetId = extractTweetId(tweetUrl);
    expect(tweetId).toBe(id);
  });

  it("Can extract tweet IDs when URL has query params", () => {
    const id = "1422656689476354560";
    const tweetUrl = `https://twitter.com/elonmusk/status/${id}?param=1`;
    const tweetId = extractTweetId(tweetUrl);
    expect(tweetId).toBe(id);
  });

  it("Can extract tweet IDs when URL has extra path fields", () => {
    const id = "1422656689476354560";
    const tweetUrl = `https://twitter.com/elonmusk/status/${id}/extra/path`;
    const tweetId = extractTweetId(tweetUrl);
    expect(tweetId).toBe(id);
  });
});

describe("extractUrls", () => {
  it("can extract a single URL from a string", () => {
    const stringWithUrl = "This is a string with a URL: https://example.com";
    const urls = extractUrls(stringWithUrl);
    expect(urls).toHaveLength(1);
    expect(urls[0]).toBe("https://example.com");
  });

  it("can extract multiple URLs from a string", () => {
    const multiLineMultiUrl = `This is a string with multiple URLs:
  2. But: too much competition, keeps prices down! https://t.co/GI4uWOGPO5
finally, we have a URL on the link below https xyz
https://example.com`;
    const urls = extractUrls(multiLineMultiUrl);
    expect(urls).toHaveLength(2);
    expect(urls[0]).toBe("https://t.co/GI4uWOGPO5");
    expect(urls[1]).toBe("https://example.com");
  });

  it("Can extract URLs from a complex multi line string", () => {
    const complexString = `ðŸ¤–AI-Driven Research Assistant\n\nThis is an advanced AI-powered research assistant system that utilizes multiple specialized agents to assist in tasks such as data analysis, visualization, and report generation\n\nhttps://t.co/s5ChhuMOtK https://t.co/5H2VRjd9hN`;
    const urls = extractUrls(complexString);
    expect(urls).toHaveLength(2);
    expect(urls[0]).toBe("https://t.co/s5ChhuMOtK");
    expect(urls[1]).toBe("https://t.co/5H2VRjd9hN");
  });
});

describe("timezoneToUtc", () => {
  // Mock the current time to ensure consistent test results
  const MOCK_NOW = new Date("2024-12-13T16:26:57-08:00");

  beforeEach(() => {
    jest.useFakeTimers();
    jest.setSystemTime(MOCK_NOW);
  });

  afterEach(() => {
    jest.useRealTimers();
  });

  it("should correctly parse PST time", () => {
    const result = timezoneToUtc("12/13/2024 02:30 PM PST");
    expect(result?.toISOString()).toBe("2024-12-13T22:30:00.000Z");
  });

  it("should correctly parse EST time", () => {
    const result = timezoneToUtc("12/13/2024 02:30 PM EST");
    expect(result?.toISOString()).toBe("2024-12-13T19:30:00.000Z");
  });

  it("should handle invalid date strings", () => {
    const result = timezoneToUtc("invalid date PST");
    expect(result).toBeUndefined();
  });

  it("should handle missing timezone", () => {
    const result = timezoneToUtc("12/13/2024 02:30 PM");
    expect(result).toBeUndefined();
  });

  it("should handle different times of day correctly", () => {
    // Test midnight PST (8am UTC)
    const midnightResult = timezoneToUtc("12/13/2024 12:00 AM PST");
    expect(midnightResult?.toISOString()).toBe("2024-12-13T08:00:00.000Z");

    // Test noon PST (8pm UTC)
    const noonResult = timezoneToUtc("12/13/2024 12:00 PM PST");
    expect(noonResult?.toISOString()).toBe("2024-12-13T20:00:00.000Z");
  });
});

describe("extract mime types", () => {
  it("Can extract mime types from base64 images", async () => {
    const base64Image = await fs.readFile(
      "src/tests/data/langchain_logo.png",
      "base64",
    );
    const mimeType = extractMimeTypeFromBase64(base64Image);
    expect(mimeType).toBe("image/png");
  });
});



================================================
FILE: src/tests/expected.ts
================================================
export const EXPECTED_README = `# LangGraph.js Examples

This repository contains a series of example TypeScript projects which implement LangGraph.js agents.
Each directory focuses on a different problem which LangGraph.js aims to solve/enable solutions for.

## Prerequisites

The following projects all use [LangSmith](https://smith.langchain.com/), LangGraph [Studio](https://github.com/langchain-ai/langgraph-studio) and [Cloud](https://langchain-ai.github.io/langgraph/cloud/), as well as the [LangGraph.js](https://langchain-ai.github.io/langgraphjs/) and [LangChain.js](https://js.langchain.com/v0.2/docs/introduction/) libraries.

Before jumping into any of the projects, you should create a LangSmith account [here](https://smith.langchain.com/), and download the latest LangGraph Studio version [here](https://github.com/langchain-ai/langgraph-studio/releases/latest).

Running LangGraph Studio locally requires [Docker](https://www.docker.com/), so ensure you have it installed _and_ running before starting the Studio (I personally use [OrbStack](https://orbstack.dev/) to manage my Docker containers, which is free to use for personal use).

## Projects

- [Intro](./intro/README.md) - Introduction to LangGraph.js, Studio, and Cloud.
- [Human in the Loop](./human_in_the_loop/README.md) - Introduction to Human in the Loop (HITL) concepts.
- [Stockbroker](./stockbroker/README.md) - A full stack stockbroker & financial analyst app, with HITL for purchasing stocks.
- Streaming Messages ([Examples](./streaming_messages/README.md), [Frontend](./streaming_messages_frontend/README.md)) - Next.js web app connected to a LangGraph Cloud deployment to show off different message streaming types.
`;



================================================
FILE: src/tests/github.int.test.ts
================================================
import { describe, it, expect } from "@jest/globals";
import { getFileContents } from "../utils/github-repo-contents.js";

describe("GitHub get file contents", () => {
  it("Can get the download_url of a Gif from a public GitHub repo", async () => {
    const repoUrl = "https://github.com/Integuru-AI/Integuru";
    const gifFileName = "integuru_demo.gif";
    const contents = await getFileContents(repoUrl, gifFileName);
    expect(contents.download_url).toBeDefined();
    expect(contents.type).toBe("file");
  });

  it("Can get the download_url of an image from a public GitHub repo", async () => {
    const repoUrl = "https://github.com/glance-io/steer-backend";
    const imgFileName = "logo_banner.png";
    const contents = await getFileContents(repoUrl, imgFileName);
    expect(contents.download_url).toBeDefined();
    expect(contents.type).toBe("file");
  });
});



================================================
FILE: src/tests/graph.int.test.ts
================================================
import { describe, it } from "@jest/globals";
import {
  GITHUB_MESSAGE,
  GITHUB_URL_STATE,
  TWITTER_NESTED_GITHUB_MESSAGE,
} from "./states.js";
import { TwitterApi } from "twitter-api-v2";
import { EXPECTED_README } from "./expected.js";
import { getPageText } from "../agents/utils.js";
import { generatePostGraph } from "../agents/generate-post/generate-post-graph.js";
import { getYouTubeVideoDuration } from "../agents/shared/nodes/youtube.utils.js";
import { getGitHubContentsAndTypeFromUrl } from "../agents/shared/nodes/verify-github.js";
import { verifyYouTubeContent } from "../agents/shared/nodes/verify-youtube.js";
import { Command, MemorySaver } from "@langchain/langgraph";
import { verifyTweetGraph } from "../agents/verify-tweet/verify-tweet-graph.js";
import { resolveTwitterUrl } from "../clients/twitter/utils.js";
import { BASE_GENERATE_POST_CONFIG } from "../agents/generate-post/generate-post-state.js";

describe("GeneratePostGraph", () => {
  it("Should be able to generate posts from a GitHub URL slack message", async () => {
    console.log("Starting graph test");
    const result = await generatePostGraph.stream(
      { links: GITHUB_URL_STATE.slackMessage.links },
      {
        streamMode: "values",
      },
    );

    let post = "";
    for await (const value of result) {
      console.log(
        "Event occurred",
        Object.entries(value).map(([k, v]) => ({
          [k]: !!v,
        })),
      );

      if (value.post) {
        post = value.post;
      }
    }

    if (post) {
      console.log("\nPOST:\n");
      console.log(post);
    }
  }, 60000);

  // Skip by default to prevent using up API quota
  it("Can read tweets via Twitter API", async () => {
    if (!process.env.TWITTER_BEARER_TOKEN) {
      throw new Error("TWITTER_BEARER_TOKEN is not set");
    }
    const client = new TwitterApi(process.env.TWITTER_BEARER_TOKEN);
    const singleTweet = await client.v2.singleTweet("1861528104901984330");
    expect(singleTweet.data.text).toBeDefined();
  });

  it("can resolve twitter URLs", async () => {
    const resolvedUrl = await resolveTwitterUrl("https://t.co/GI4uWOGPO5");
    expect(resolvedUrl).toBe(
      "https://twitter.com/GergelyOrosz/status/1861528104901984330/photo/1",
    );
  });
});

test("Can get the proper markdown from a github URL", async () => {
  const url = "https://github.com/bracesproul/langgraphjs-examples";
  const contents = await getGitHubContentsAndTypeFromUrl(url);
  if (!contents) {
    throw new Error("No contents found");
  }
  expect(contents.contents).toBe(EXPECTED_README);
});

describe("generate via twitter posts", () => {
  it("Can generate a post from a tweet with a github link", async () => {
    console.log("Starting graph test");
    const result = await generatePostGraph.stream(
      { links: TWITTER_NESTED_GITHUB_MESSAGE.slackMessage.links },
      {
        streamMode: "values",
      },
    );

    let post = "";
    for await (const value of result) {
      console.log(
        "Event occurred",
        Object.entries(value).map(([k, v]) => ({
          [k]: !!v,
        })),
      );

      if (value.post) {
        post = value.post;
      }
    }

    if (post) {
      console.log("\nPOST:\n");
      console.log(post);
    }
  }, 60000);
});

describe("generate via github repos", () => {
  it("Can generate a post from a github repo", async () => {
    console.log("Starting graph test");
    const result = await generatePostGraph.stream(
      { links: GITHUB_MESSAGE.slackMessage.links },
      {
        streamMode: "values",
      },
    );

    let post = "";
    for await (const value of result) {
      console.log(
        "Event occurred",
        Object.entries(value).map(([k, v]) => ({
          [k]: !!v,
        })),
      );

      if (value.post) {
        post = value.post;
      }
    }

    if (post) {
      console.log("\nPOST:\n");
      console.log(post);
    }
  }, 60000);
});

test("Can get video duration", async () => {
  const duration = await getYouTubeVideoDuration(
    "https://www.youtube.com/watch?v=BGvqeRB4Jpk",
  );
  expect(duration).toBe(91);
});

test("Can get page text", async () => {
  const text = await getPageText("https://buff.ly/4g0ZRXI");
  expect(text).toBeDefined();
  expect(text?.length).toBeGreaterThan(100);
});

test("can generate post", async () => {
  const result = await generatePostGraph.invoke(
    {
      links: ["https://x.com/eitanblumin/status/1861001933294653890"],
    },
    {
      configurable: BASE_GENERATE_POST_CONFIG,
    },
  );
  console.log(result);
});

test("can generate summaries of youtube videos", async () => {
  const result = await verifyYouTubeContent(
    {
      link: "https://www.youtube.com/watch?v=BGvqeRB4Jpk",
    },
    {},
  );
  expect(result.pageContents).toBeDefined();
  expect(result.pageContents?.[0].length).toBeGreaterThan(50); // Check character count
});

test("can interrupt and resume", async () => {
  generatePostGraph.checkpointer = new MemorySaver();
  const config = {
    configurable: {
      ...BASE_GENERATE_POST_CONFIG,
      thread_id: "123",
    },
  };
  await generatePostGraph.invoke(
    {
      links: ["https://github.com/langchain-ai/open-canvas"],
    },
    config,
  );
  console.log("interrupted first time");

  await generatePostGraph.invoke(
    new Command({
      resume: [
        {
          type: "response",
          args: "Add more emojis please",
        },
      ],
    }),
    config,
  );
});

test("Verify tweets returns valid media URLs when tweet has media", async () => {
  const result = await verifyTweetGraph.invoke({
    link: "https://x.com/LangChainAI/status/1869125903139402215",
  });

  console.log("result");
  console.dir(result, { depth: null });
});



================================================
FILE: src/tests/linkedin.int.test.ts
================================================
import { describe, it, expect } from "@jest/globals";
import { LinkedInClient } from "../clients/linkedin.js";

describe("LinkedIN API wrapper", () => {
  it("Can make a text post", async () => {
    const linkedInClient = new LinkedInClient();
    const textPostResponse = await linkedInClient.createTextPost(
      "Hello, this is a test post from LinkedIn API!",
    );
    console.log("Text post created:", textPostResponse);
    expect(textPostResponse).toBeDefined();
  });

  it("Can make an image post", async () => {
    const linkedInClient = new LinkedInClient();
    const textPostResponse = await linkedInClient.createImagePost({
      text: "Hello, this is a test post from LinkedIn API!",
      imageUrl:
        "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1734639569875.jpeg",
      imageDescription: "A screenshot of the Open Canvas Readme",
      imageTitle: "Open Canvas",
    });
    console.log("Image post created:", textPostResponse);
    expect(textPostResponse).toBeDefined();
  });

  it("Can make a text post to an organization", async () => {
    const linkedInClient = new LinkedInClient();
    const textPostResponse = await linkedInClient.createTextPost(
      "Hello, this is a test post from LinkedIn API!",
      {
        postToOrganization: true,
      },
    );
    console.log("Text post created:", textPostResponse);
    expect(textPostResponse).toBeDefined();
  });

  it.only("Can make an image post to an organization", async () => {
    const linkedInClient = new LinkedInClient();
    const textPostResponse = await linkedInClient.createImagePost(
      {
        text: "Hello, this is a test post from LinkedIn API!",
        imageUrl:
          "https://verdyqfuvvtxtygqekei.supabase.co/storage/v1/object/public/images/screenshot-github.com-1734639569875.jpeg",
        imageDescription: "A screenshot of the Open Canvas Readme",
        imageTitle: "Open Canvas",
      },
      {
        postToOrganization: true,
      },
    );
    console.log("Image post created:", textPostResponse);
    expect(textPostResponse).toBeDefined();
  });
});



================================================
FILE: src/tests/scrape-general-content.int.test.ts
================================================
import { describe, expect, it } from "@jest/globals";
import {
  extractAllImageUrlsFromMarkdown,
  getPageText,
} from "../agents/utils.js";
import { getUrlContents } from "../agents/shared/nodes/verify-general.js";

describe("Get page contents", () => {
  it("Can return markdown from a blog URL", async () => {
    const url =
      "https://diamantai.substack.com/p/atlas-when-artificial-intelligence?r=336pe4&amp%3Butm_campaign=post&amp%3Butm_medium=web&amp%3BshowWelcomeOnShare=false&triedRedirect=true";
    const contents = await getPageText(url);
    expect(contents).toBeDefined();

    // Verify it can extract images from the text
    const allImageUrls = extractAllImageUrlsFromMarkdown(contents || "");
    expect(allImageUrls).toBeDefined();
    expect(allImageUrls.length).toBeGreaterThan(0);
  });

  it("Can use firecrawl to extract markdown and images from a page", async () => {
    const url = "https://qdrant.tech/documentation/data-ingestion-beginners/#";
    const contents = await getUrlContents(url);
    expect(contents).toBeDefined();
    expect(contents.content).toBeGreaterThan(10);
    expect(contents.imageUrls).toBeDefined();
    expect(contents.imageUrls?.length).toBeGreaterThan(0);
  });
});



================================================
FILE: src/tests/slack.int.test.ts
================================================
import { test, expect } from "@jest/globals";
import { SlackClient } from "../clients/slack/client.js";

const TEST_CHANNEL_ID = "C06BU7XF5S7";

test("Slack client can fetch messages from channel name", async () => {
  const client = new SlackClient();

  const messages = await client.getChannelMessages(TEST_CHANNEL_ID, {
    maxMessages: 5,
  });
  console.log(messages);
  expect(messages).toBeDefined();
  expect(messages.length).toBeGreaterThan(0);
});



================================================
FILE: src/tests/states.ts
================================================
export const GITHUB_URL_STATE = {
  slackMessage: {
    id: "e4fd6d66-7a47-4457-b532-14facfe93bb0",
    timestamp: "1729953413.037949",
    user: "U04N0HGF869",
    text: "<https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py>",
    type: "message",
    attachments: [
      {
        id: 1,
        footer_icon: "https://slack.github.com/static/img/favicon-neutral.png",
        color: "24292f",
        bot_id: "B04FT0ZDXD2",
        app_unfurl_url:
          "https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py",
        is_app_unfurl: true,
        app_id: "A01BP7R4KNY",
        fallback:
          "<https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py | code_assistant_lg.py>",
        text: '```\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom pydantic import BaseModel, Field\nfrom langchain_openai import AzureChatOpenAI\nimport os\nimport subprocess\nimport pkg_resources\n\nos.environ["AZURE_OPENAI_ENDPOINT"] = "azure deployment end point"\nos.environ["AZURE_OPENAI_API_KEY"] = "provide-your-key"\n\nllm = AzureChatOpenAI(\n    azure_deployment="gpt-4",  # or your deployment\n    api_version="2023-06-01-preview",  # or your api version\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    # other params...\n)\n\ncode_gen_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            "system",\n            """You are a coding assistant with expertise in python language. \\n \n             Structure your answer with list of imports \\n\n             and the functioning code block. Here is the user question:""",\n        ),\n        ("placeholder", "{messages}"),\n    ]\n)\n\ncode_gen_prompt_package_install = ChatPromptTemplate.from_messages(\n    [\n        (\n            "system",\n            """You are a coding assistant with expertise in python language. \\n \n               you need to provide name of package to be installed \n               output expected:\n               display only name of the packagename, not pip install packagename""",\n        ),\n        ("placeholder", "{messages}"),\n    ]\n)\n\n\nclass Code(BaseModel):\n    """Schema for code solutions"""\n\n    imports: str = Field(description="Code block import statements")\n    code: str = Field(description="Code block not including import statements")\n\n\ndef parse_output(solution):\n    """When we add \'include_raw=True\' to structured output,\n    it will return a dict w \'raw\', \'parsed\', \'parsing_error\'."""\n\n    return solution["parsed"]\n\n\nquestion = "selenium webdriver python code to launch a webdriver and close it"\ncode_gen_chain = code_gen_prompt | llm.with_structured_output(Code, include_raw=True) | parse_output\ncode_gen_chain_package_install = code_gen_prompt_package_install | llm\n\n# solution = code_gen_chain.invoke(\n#     {"messages": [("user", question)]}\n# )\n# print(solution)\n\n\nfrom typing import List\nfrom typing_extensions import TypedDict\n\n\nclass GraphState(TypedDict):\n    """\n    Represents the state of our graph.\n\n    Attributes:\n        error : Binary flag for control flow to indicate whether test error was tripped\n        messages : With user question, error messages, reasoning\n        generation : Code solution\n        iterations : Number of tries\n    """\n\n    error: str\n    messages: List\n    generation: str\n    iterations: int\n    package_failed: str\n\n\n### Parameter\n\n# Max tries\nmax_iterations = 3\n\n\ndef generate(state: GraphState):\n    """\n    Generate a code solution\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    """\n\n    print("---GENERATING CODE SOLUTION---")\n\n    # State\n    messages = state["messages"]\n    iterations = state["iterations"]\n    error = state["error"]\n\n    # We have been routed back to generation with an error\n    if error == "yes":\n        messages += [\n            (\n                "user",\n                "Now, try again. Invoke the code tool to structure the output with a imports, and code block:",\n            )\n        ]\n\n    # Solution\n    code_solution = code_gen_chain.invoke(\n        {"messages": messages}\n    )\n    messages += [\n        (\n            "assistant",\n            f"Imports: {code_solution.imports} \\n Code: {code_solution.code}",\n        )\n    ]\n\n    # Increment\n    iterations = iterations + 1\n    return {"generation": code_solution, "messages": messages, "iterations": iterations}\n\n\ndef code_check(state: GraphState):\n    """\n    Check code\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, error\n    """\n\n    print("---CHECKING CODE---")\n\n    # State\n    messages = state["messages"]\n    code_solution = state["generation"]\n    iterations = state["iterations"]\n\n    # Get solution components\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print("---CODE IMPORT CHECK: FAILED---")\n        error_message = [("user", f"Your solution failed the import test: {e}")]\n        messages += error_message\n        return {\n            "generation": code_solution,\n            "messages": messages,\n            "iterations": iterations,\n            "error": "yes",\n            "package_failed": "yes"\n        }\n\n    # Check execution\n    try:\n        exec(imports + "\\n" + code)\n    except Exception as e:\n        print("---CODE BLOCK CHECK: FAILED---")\n        error_message = [("user", f"Your solution failed the code execution test: {e}")]\n        messages += error_message\n        return {\n            "generation": code_solution,\n            "messages": messages,\n            "iterations": iterations,\n            "error": "yes",\n            "package_failed": "no"\n        }\n\n    # No errors\n    print("---NO CODE TEST FAILURES---")\n    return {\n        "generation": code_solution,\n        "messages": messages,\n        "iterations": iterations,\n        "error": "no",\n        "package_failed": "no"\n    }\n\n\ndef check_package(state: GraphState):\n    messages = state["messages"]\n    code_solution = state["generation"]\n    iterations = state["iterations"]\n    package_failed = state["package_failed"]\n\n    if package_failed:\n        package_name = code_gen_chain_package_install.invoke({"messages": state["messages"]})\n        print("---Checking package failure---")\n\n        try:\n            # Check if the package is already installed\n            pkg_resources.get_distribution(package_name.content)\n            print(f"{package_name} is already installed.")\n        except Exception as e:\n            # If not installed, install the package\n            print(f"Installing {package_name.content}...")\n            subprocess.run([\'pip3\', \'install\', package_name.content], check=True)\n            print(f"{package_name.content} installed successfully.")\n            messages += [\n                (\n                    "assistant",\n                    f"{package_name.content} installed successfully.",\n                )\n            ]\n\n    return {\n        "generation": code_solution,\n        "messages": messages,\n        "iterations": iterations,\n        "error": "no",\n        "package_failed": "no"\n    }\n\n\n### Edges\n\n\ndef decide_to_finish(state: GraphState):\n    """\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    """\n    error = state["error"]\n    iterations = state["iterations"]\n\n    if error == "no" or iterations == max_iterations:\n        print("---DECISION: FINISH---")\n        return "end"\n    else:\n        print("---DECISION: RE-TRY SOLUTION---")\n        return "generate"\n\n\nfrom langgraph.graph import END, StateGraph, START\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node("generate", generate)  # generation solution\nworkflow.add_node("check_code", code_check)  # check code\nworkflow.add_node("check_package", check_package)\n\n# Build graph\nworkflow.add_edge(START, "generate")\nworkflow.add_edge("generate", "check_code")\nworkflow.add_edge("check_code", "check_package")\nworkflow.add_conditional_edges(\n    "check_package",\n    decide_to_finish,\n    {\n        "end": END,\n        "generate": "generate",\n    },\n)\n\napp = workflow.compile()\n\nsolution = app.invoke({"messages": [("user", question)], "iterations": 0, "error": "", "package_failed": ""})\nfrom pprint import pprint\n\npprint(solution)\n\n```',
        title:
          "<https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py | code_assistant_lg.py>",
        footer:
          "<https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples|ReddyNitheeesh/AI-Lc-Lg-examples>",
        mrkdwn_in: ["text"],
      },
    ],
    links: [
      "https://github.com/ReddyNitheeesh/AI-Lc-Lg-examples/blob/main/code_assistant_lg.py",
    ],
  },
};

// Tweet with nested youtube video
// https://x.com/LangChainAI/status/1851676266232950985

// Tweet with nested github link
// https://x.com/LangChainAI/status/1861108590792036799

// Tweet with nested general link
// https://x.com/KaranVaidya6/status/1861037496295137314

export const TWITTER_NESTED_YOUTUBE_MESSAGE = {
  slackMessage: {
    id: "e4fd6d66-7a47-4457-b532-14facfe93bb0",
    timestamp: "1729953413.037949",
    user: "U04N0HGF869",
    text: "<https://x.com/LangChainAI/status/1851676266232950985>",
    type: "message",
    links: ["https://x.com/LangChainAI/status/1851676266232950985"],
  },
};

export const TWITTER_NESTED_GITHUB_MESSAGE = {
  slackMessage: {
    id: "e4fd6d66-7a47-4457-b532-14facfe93bb0",
    timestamp: "1729953413.037949",
    user: "U04N0HGF869",
    text: "<https://x.com/LangChainAI/status/1861108590792036799>",
    type: "message",
    links: ["https://x.com/LangChainAI/status/1861108590792036799"],
  },
};

export const TWITTER_NESTED_GENERAL_MESSAGE = {
  slackMessage: {
    id: "e4fd6d66-7a47-4457-b532-14facfe93bb0",
    timestamp: "1729953413.037949",
    user: "U04N0HGF869",
    text: "<https://x.com/KaranVaidya6/status/1861037496295137314>",
    type: "message",
    links: ["https://x.com/KaranVaidya6/status/1861037496295137314"],
  },
};

export const GITHUB_MESSAGE = {
  slackMessage: {
    id: "e4fd6d66-7a47-4457-b532-14facfe93bb0",
    timestamp: "1729953413.037949",
    user: "U04N0HGF869",
    text: "<https://github.com/starpig1129/AI-Data-Analysis-MultiAgent>",
    type: "message",
    links: ["https://github.com/starpig1129/AI-Data-Analysis-MultiAgent"],
  },
};



================================================
FILE: src/tests/utils.int.test.ts
================================================
import * as path from "path";
import * as fs from "fs/promises";
import { describe, it, expect } from "@jest/globals";
import {
  getFileContents,
  getRepoContents,
} from "../utils/github-repo-contents.js";
import { takeScreenshot } from "../utils/screenshot.js";
import {
  GITHUB_BROWSER_CONTEXT_OPTIONS,
  GITHUB_SCREENSHOT_OPTIONS,
} from "../agents/generate-post/constants.js";
import { parseResult } from "../agents/find-images/nodes/validate-images.js";
import { takeScreenshotAndUpload } from "../agents/find-images/screenshot.js";

describe("GitHub utils", () => {
  it("Can fetch the files and folders of a public GitHub repo", async () => {
    const repoUrl = "https://github.com/langchain-ai/open-canvas";
    const contents = await getRepoContents(repoUrl);
    console.log(contents);
    expect(contents.length).toBeGreaterThan(1);
  });

  it("Can get the contents of a file from a public GitHub repo", async () => {
    const repoUrl = "https://github.com/langchain-ai/open-canvas";
    const contents = await getRepoContents(repoUrl);
    const packageJson = contents.find(
      (content) => content.name === "package.json" && content.type === "file",
    );
    expect(packageJson).toBeDefined();
    if (!packageJson) return;

    const packageJsonContents = await getFileContents(
      repoUrl,
      packageJson.path,
    );
    expect(packageJsonContents.content).toBeDefined();
    expect(packageJsonContents.content.length).toBeGreaterThan(10);
    expect(packageJsonContents.type).toBe("file");
  });
});

describe("Screenshot utils", () => {
  const writeScreenshotToFile = async (
    screenshotBuffer: Buffer,
    screenshotFileName: string,
  ) => {
    const rootDir = "./src/tests/data/screenshots/";
    // Create the directory if it doesn't exist
    await fs.mkdir(rootDir, { recursive: true });
    const screenshotPath = path.join(rootDir, screenshotFileName);
    await fs.writeFile(screenshotPath, screenshotBuffer);
    console.log(`Screenshot saved to ${screenshotPath}`);
  };

  const generalUrl =
    "https://cckeh.hashnode.dev/building-chatbots-with-memory-capabilities-a-comprehensive-tutorial-with-langchain-langgraph-gemini-ai-and-mongodb";
  const repoUrl = "https://github.com/langchain-ai/open-canvas";

  it("Can take a screenshot of a GitHub repo", async () => {
    const screenshotBuffer = await takeScreenshot(repoUrl);
    expect(screenshotBuffer).toBeDefined();
    // write screenshot to file
    await writeScreenshotToFile(screenshotBuffer, "github-screenshot.png");
  });

  it("Can take a screenshot of a general URL", async () => {
    const screenshotBuffer = await takeScreenshot(generalUrl);
    expect(screenshotBuffer).toBeDefined();
    // write screenshot to file
    await writeScreenshotToFile(screenshotBuffer, "general-screenshot.png");
  });

  it("Can take a screenshot of a GitHub repo and upload it to Supabase", async () => {
    const screenshotUrl = await takeScreenshotAndUpload(repoUrl);
    console.log("screenshotUrl", screenshotUrl);
    expect(screenshotUrl).toBeDefined();
    if (!screenshotUrl) return;

    const parsedUrl = new URL(screenshotUrl);
    expect(parsedUrl).toBeDefined();
  });

  it("Can take a screenshot of a general URL and upload it to Supabase", async () => {
    const screenshotUrl = await takeScreenshotAndUpload(generalUrl);
    console.log("screenshotUrl", screenshotUrl);
    expect(screenshotUrl).toBeDefined();
    if (!screenshotUrl) return;

    const parsedUrl = new URL(screenshotUrl);
    expect(parsedUrl).toBeDefined();
  });

  it("Can take a screenshot of a GitHub readme and clip only the file contents", async () => {
    const screenshot = await takeScreenshot(`${repoUrl}/blob/main/README.md`, {
      screenshotOptions: GITHUB_SCREENSHOT_OPTIONS,
      browserContextOptions: GITHUB_BROWSER_CONTEXT_OPTIONS,
    });

    expect(screenshot).toBeDefined();
    await writeScreenshotToFile(screenshot, "github-readme-screenshot.jpeg");
  });

  it("Can extract the relevant indices from a string", () => {
    // Test single number
    const singleNumberStr = `
<answer>
<analysis>
The image shows a user interface for interacting with a "Godel Agent," which is described as a self-referential AI agent. The interface includes a query input and an output area that explains the agent's key features. The image directly illustrates the core concept of the post, which is about the Godel Agent and its self-improvement capabilities. The image also aligns with the technical and informative tone of the post and report.
</analysis>
<relevant_indices>0</relevant_indices>
</answer>`;
    expect(parseResult(singleNumberStr)).toEqual([0]);

    // Test multiple comma-separated numbers
    const multipleNumbersStr = `
<answer>
<analysis>
Multiple relevant images found that align with the content.
</analysis>
<relevant_indices>0, 2, 4</relevant_indices>
</answer>`;
    expect(parseResult(multipleNumbersStr)).toEqual([0, 2, 4]);

    // Test empty result
    const emptyStr = `
<answer>
<analysis>
No relevant images found.
</analysis>
<relevant_indices></relevant_indices>
</answer>`;
    expect(parseResult(emptyStr)).toEqual([]);

    // Test with whitespace and newlines
    const messyStr = `
<answer>
<analysis>
Some images are relevant.
</analysis>
<relevant_indices>
  1,
  3,
  5
</relevant_indices>
</answer>`;
    expect(parseResult(messyStr)).toEqual([1, 3, 5]);
  });
});



================================================
FILE: src/tests/youtube.int.test.ts
================================================
import { describe, it, expect } from "@jest/globals";
import { getVideoThumbnailUrl } from "../agents/shared/nodes/youtube.utils.js";

describe("YouTube utils", () => {
  it("Can get the thumbnails of YouTube videos", async () => {
    const youTubeUrls = [
      "https://www.youtube.com/watch?v=gwE3Wv4MNLw",
      "https://www.youtube.com/watch?v=VyyJFrPlHfk",
      "https://www.youtube.com/watch?v=BGvqeRB4Jpk",
      "https://www.youtube.com/watch?v=u_Xm3vgBQ9Y",
      "https://www.youtube.com/watch?v=02IDU8eCX8o",
    ];

    for await (const url of youTubeUrls) {
      const thumbnail = await getVideoThumbnailUrl(url);
      console.log(`url & thumbnail:\nURL: ${url}\nTHUMBNAIL: ${thumbnail}`);
      expect(thumbnail).toBeDefined();
    }
  });
});




================================================
FILE: src/utils/create-dir.ts
================================================
import fs from "fs";

export function createDirIfNotExists(dir: string) {
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
  }
}



================================================
FILE: src/utils/date.ts
================================================
import { nextSaturday, setHours, setMinutes, parse, isValid } from "date-fns";
import { fromZonedTime, toZonedTime } from "date-fns-tz";
import { DateType } from "../agents/types.js";

export const PRIORITY_LEVELS = [
  // The "p" priority levels are for community posts.
  "p1",
  "p2",
  "p3",
  // The "r" priority levels are for repurposed posts.
  "r1",
  "r2",
  "r3",
];

/**
 * Converts a date string in any timezone to a UTC Date object
 * @param dateString - Date string in any timezone (e.g., "2024-01-01 12:00 PST" or "2024-01-01 12:00 America/Los_Angeles")
 * @returns Date object in UTC
 */
export function timezoneToUtc(dateString: string): Date | undefined {
  // Try to match both 3-letter codes and IANA timezone names
  const timezoneMatch = dateString.match(/ ([A-Z]{3,4}|America\/[A-Za-z_]+)$/);
  if (!timezoneMatch) {
    console.error("No timezone found in date string");
    return undefined;
  }

  const timezone = timezoneMatch[1];
  // Map common abbreviations to IANA names
  const timezoneMap: Record<string, string> = {
    // North America
    PST: "America/Los_Angeles", // Pacific Standard Time
    PDT: "America/Los_Angeles", // Pacific Daylight Time
    MST: "America/Denver", // Mountain Standard Time
    MDT: "America/Denver", // Mountain Daylight Time
    CST: "America/Chicago", // Central Standard Time
    CDT: "America/Chicago", // Central Daylight Time
    EST: "America/New_York", // Eastern Standard Time
    EDT: "America/New_York", // Eastern Daylight Time
    AKST: "America/Anchorage", // Alaska Standard Time
    AKDT: "America/Anchorage", // Alaska Daylight Time
    HST: "Pacific/Honolulu", // Hawaii Standard Time

    // Europe
    GMT: "Etc/GMT", // Greenwich Mean Time
    BST: "Europe/London", // British Summer Time
    CET: "Europe/Paris", // Central European Time
    CEST: "Europe/Paris", // Central European Summer Time
    EET: "Europe/Helsinki", // Eastern European Time
    EEST: "Europe/Helsinki", // Eastern European Summer Time

    // Asia/Pacific
    JST: "Asia/Tokyo", // Japan Standard Time
    KST: "Asia/Seoul", // Korea Standard Time
    IST: "Asia/Kolkata", // India Standard Time
    AEST: "Australia/Sydney", // Australian Eastern Standard Time
    AEDT: "Australia/Sydney", // Australian Eastern Daylight Time
    AWST: "Australia/Perth", // Australian Western Standard Time
    NZST: "Pacific/Auckland", // New Zealand Standard Time
    NZDT: "Pacific/Auckland", // New Zealand Daylight Time
  };

  const ianaTimezone = timezoneMap[timezone] || timezone;
  const withoutTz = dateString.replace(/ [A-Z]{3}$/, "");

  const parsedDate = new Date(withoutTz);
  if (isNaN(parsedDate.getTime())) {
    return undefined;
  }

  const newDate = fromZonedTime(parsedDate, ianaTimezone);
  if ((newDate as unknown as string) === "Invalid Date") {
    return undefined;
  }
  return newDate;
}

/**
 * Get a date for the next Saturday at the specified hour in Pacific Time (PST/PDT)
 * @param {number} hour - The hour to set for the next Saturday in PST/PDT (default: 12)
 * @param {number} minute - The minute to set for the next Saturday in PST/PDT (default: 0)
 * @returns {Date} The date for the next Saturday at the specified hour in PST/PDT
 */
export function getNextSaturdayDate(hour = 12, minute = 0): Date {
  const saturday = nextSaturday(new Date());
  const saturdayWithTime = setMinutes(setHours(saturday, hour), minute);
  return toZonedTime(saturdayWithTime, "America/Los_Angeles");
}

/**
 * Validates a date string in the format 'MM/dd/yyyy hh:mm a z'
 * @param dateString - The date string to validate
 * @returns {boolean} - Whether the date string is valid
 */
export function isValidDateString(dateString: string): boolean {
  try {
    // Remove timezone abbreviation if present
    const dateWithoutTz = dateString.replace(/ [A-Z]{3}$/, "");

    // Parse the date without timezone
    const parsedDate = parse(dateWithoutTz, "MM/dd/yyyy hh:mm a", new Date());
    return isValid(parsedDate);
  } catch (e) {
    console.error("Failed to parse date string:", e);
    return false;
  }
}

/**
 * Parses a date string into a DateType, handling both priority values and UTC date conversion
 * @param dateString - Input string that can be either a priority value (p1, p2, p3) or a valid date string
 * @returns Priority value (p1/p2/p3), UTC converted Date if valid date string, or undefined if invalid
 */
export function parseDateResponse(dateString: string): DateType | undefined {
  const cleanedDate = dateString.toLowerCase().trim();
  if (PRIORITY_LEVELS.find((p) => cleanedDate === p)) {
    return cleanedDate as DateType;
  }

  const isDateValid = isValidDateString(dateString);
  if (!isDateValid) {
    return undefined;
  }

  return timezoneToUtc(dateString);
}



================================================
FILE: src/utils/delay-run.ts
================================================
import { Client, Run } from "@langchain/langgraph-sdk";

interface DelayRunInputs {
  /**
   * The number of seconds to delay the run by.
   */
  seconds: number;
  /**
   * The node to resume on.
   */
  resumeNode: string;
  /**
   * The ID of the thread to resume in.
   */
  threadId: string;
  /**
   * The assistant ID to resume the run in.
   */
  assistantId: string;
  /**
   * The run ID of the current run to cancel.
   */
  runId: string;
  /**
   * The value of the state to resume the run with.
   */
  state: Record<string, any>;
  /**
   * Configurable values to pass to the run.
   */
  configurable?: Record<string, any>;
}

/**
 * Delay the execution of a run by a specified number of seconds.
 * This function will cancel the current run, and create a new run
 * for the same thread, with the specified state and configurable
 * fields to be executed after a delay.
 * @param param0 - The inputs to the function.
 * @returns The new run.
 */
export async function delayRun({
  seconds,
  resumeNode,
  threadId,
  assistantId,
  runId,
  state,
  configurable,
}: DelayRunInputs): Promise<Run> {
  const client = new Client({
    apiUrl:
      process.env.LANGGRAPH_API_URL || `http://localhost:${process.env.PORT}`,
  });

  const newRun = await client.runs.create(threadId, assistantId, {
    input: {},
    config: {
      configurable: {
        ...(configurable || {}),
      },
    },
    command: {
      update: state,
      goto: resumeNode,
    },
    afterSeconds: seconds,
  });

  await client.runs.cancel(threadId, runId);
  return newRun;
}



================================================
FILE: src/utils/firecrawl.ts
================================================
/**
 * Extracts image URLs from FireCrawl metadata by combining both regular image and OpenGraph image fields.
 * @param {any} metadata - The metadata object from FireCrawl containing potential image information
 * @param {string[]} [metadata.image] - Optional array of regular image URLs
 * @param {string} [metadata.ogImage] - Optional OpenGraph image URL
 * @returns {string[] | undefined} An array of image URLs if any images are found, undefined otherwise
 */
export function getImagesFromFireCrawlMetadata(
  metadata: any,
): string[] | undefined {
  const image = metadata.image || [];
  const ogImage = metadata.ogImage ? [metadata.ogImage] : [];
  if (image?.length || ogImage?.length) {
    return [...ogImage, ...image];
  }
  return undefined;
}



================================================
FILE: src/utils/github-repo-contents.ts
================================================
import { Octokit } from "@octokit/rest";

interface RepoContent {
  name: string;
  type: "file" | "dir";
  path: string;
  size?: number;
}

interface FileContent {
  content: string;
  type: "file";
  encoding: string;
  size: number;
  path: string;
  sha: string;
  name: string;
  url: string;
  git_url: string | null;
  html_url: string | null;
  download_url: string | null;
}

export function getOwnerRepoFromUrl(repoUrl: string): {
  owner: string;
  repo: string;
} {
  const url = new URL(repoUrl);

  if (url.hostname !== "github.com") {
    throw new Error("URL must be a GitHub repository URL");
  }

  // Remove leading slash and split path segments
  const pathSegments = url.pathname.slice(1).split("/");

  if (pathSegments.length < 2) {
    throw new Error(
      "Invalid GitHub repository URL: missing owner or repository name",
    );
  }

  const [owner, repo] = pathSegments;
  const cleanRepo = repo.replace(".git", "");

  return { owner, repo: cleanRepo };
}

/**
 * Fetches the contents of a GitHub repository's root directory
 * @param repoUrl - The full GitHub repository URL (e.g., 'https://github.com/owner/repo')
 * @returns Promise<RepoContent[]> - Array of files and directories in the repository root
 * @throws {Error} If GITHUB_TOKEN is not set in environment variables
 * @throws {Error} If the URL is invalid or not a GitHub repository
 * @throws {Error} If the API request fails
 */
export async function getRepoContents(repoUrl: string): Promise<RepoContent[]> {
  const token = process.env.GITHUB_TOKEN;
  if (!token) {
    throw new Error("GITHUB_TOKEN environment variable is required");
  }

  const octokit = new Octokit({
    auth: token,
  });

  try {
    const { owner, repo } = getOwnerRepoFromUrl(repoUrl);

    try {
      const response = await octokit.repos.getContent({
        owner,
        repo,
        path: "", // empty path for root directory
      });

      if (!Array.isArray(response.data)) {
        throw new Error("Unexpected API response format");
      }

      return response.data.map((item) => ({
        name: item.name,
        type: item.type as "file" | "dir",
        path: item.path,
        size: item.size,
      }));
    } catch (e) {
      throw new Error(
        "Failed to fetch repository contents for " + repoUrl + "\nError: " + e,
      );
    }
  } catch (error) {
    if (error instanceof Error) {
      throw new Error(
        `Failed to fetch repository contents: ${error.message}\n\nRepo URL: ${repoUrl}`,
      );
    }
    throw error;
  }
}

/**
 * Gets the contents of a specific directory in a GitHub repository
 * @param repoUrl - The full GitHub repository URL (e.g., 'https://github.com/owner/repo')
 * @param directoryPath - The path to the directory within the repository (e.g., 'src/utils')
 * @returns Promise<RepoContent[]> - Array of files and directories in the specified directory
 * @throws {Error} If GITHUB_TOKEN is not set in environment variables
 * @throws {Error} If the URL is invalid or not a GitHub repository
 * @throws {Error} If the directory path doesn't exist
 * @throws {Error} If the API request fails
 */
export async function getDirectoryContents(
  repoUrl: string,
  directoryPath: string,
): Promise<RepoContent[]> {
  const token = process.env.GITHUB_TOKEN;
  if (!token) {
    throw new Error("GITHUB_TOKEN environment variable is required");
  }

  const octokit = new Octokit({
    auth: token,
  });

  try {
    const url = new URL(repoUrl);

    if (url.hostname !== "github.com") {
      throw new Error("URL must be a GitHub repository URL");
    }

    const pathSegments = url.pathname.slice(1).split("/");

    if (pathSegments.length < 2) {
      throw new Error(
        "Invalid GitHub repository URL: missing owner or repository name",
      );
    }

    const [owner, repo] = pathSegments;
    const cleanRepo = repo.replace(".git", "");

    // Normalize directory path by removing leading and trailing slashes
    const normalizedPath = directoryPath.replace(/^\/+|\/+$/g, "");

    const response = await octokit.repos.getContent({
      owner,
      repo: cleanRepo,
      path: normalizedPath,
    });

    if (!Array.isArray(response.data)) {
      throw new Error(`Path '${normalizedPath}' does not point to a directory`);
    }

    return response.data.map((item) => ({
      name: item.name,
      type: item.type as "file" | "dir",
      path: item.path,
      size: item.size,
    }));
  } catch (error) {
    if (error instanceof Error) {
      throw new Error(`Failed to fetch directory contents: ${error.message}`);
    }
    throw error;
  }
}

/**
 * Gets the contents of a specific file in a GitHub repository
 * @param repoUrl - The full GitHub repository URL (e.g., 'https://github.com/owner/repo')
 * @param filePath - The path to the file within the repository (e.g., 'src/utils/file.ts')
 * @returns Promise<FileContent> - Object containing the file's content and metadata
 * @throws {Error} If GITHUB_TOKEN is not set in environment variables
 * @throws {Error} If the URL is invalid or not a GitHub repository
 * @throws {Error} If the file path doesn't exist or points to a directory
 * @throws {Error} If the API request fails
 */
export async function getFileContents(
  repoUrl: string,
  filePath: string,
): Promise<FileContent> {
  const token = process.env.GITHUB_TOKEN;
  if (!token) {
    throw new Error("GITHUB_TOKEN environment variable is required");
  }

  const octokit = new Octokit({
    auth: token,
  });

  try {
    const url = new URL(repoUrl);

    if (url.hostname !== "github.com") {
      throw new Error("URL must be a GitHub repository URL");
    }

    const pathSegments = url.pathname.slice(1).split("/");

    if (pathSegments.length < 2) {
      throw new Error(
        "Invalid GitHub repository URL: missing owner or repository name",
      );
    }

    const [owner, repo] = pathSegments;
    const cleanRepo = repo.replace(".git", "");

    // Normalize file path by removing leading and trailing slashes and query parameters
    const normalizedPath = filePath.split("?")[0].replace(/^\/+|\/+$/g, "");
    const response = await octokit.repos.getContent({
      owner,
      repo: cleanRepo,
      path: normalizedPath,
    });

    if (Array.isArray(response.data)) {
      throw new Error(
        `Path '${normalizedPath}' points to a directory, not a file`,
      );
    }

    if (response.data.type !== "file") {
      throw new Error(
        `Path '${normalizedPath}' is not a regular file (type: ${response.data.type})`,
      );
    }

    // GitHub returns file content as base64 encoded string
    const content = response.data.content
      ? Buffer.from(response.data.content, "base64").toString("utf-8")
      : "";

    return {
      content,
      type: response.data.type,
      encoding: response.data.encoding,
      size: response.data.size,
      path: response.data.path,
      sha: response.data.sha,
      name: response.data.name,
      url: response.data.url,
      git_url: response.data.git_url,
      html_url: response.data.html_url,
      download_url: response.data.download_url,
    };
  } catch (error) {
    if (error instanceof Error) {
      throw new Error(`Failed to fetch file contents: ${error.message}`);
    }
    throw error;
  }
}



================================================
FILE: src/utils/image-message.ts
================================================
import {
  removeQueryParams,
  getMimeTypeFromUrl,
  imageUrlToBuffer,
  BLACKLISTED_MIME_TYPES,
} from "../agents/utils.js";

export async function getImageMessageContents(
  imageChunk: string[],
  baseIndex: number,
) {
  const imageMessagesPromises = imageChunk.flatMap(
    async (fileUri, chunkIndex) => {
      const cleanedFileUri = removeQueryParams(fileUri);
      let mimeType = getMimeTypeFromUrl(fileUri);

      if (!mimeType) {
        try {
          const { contentType } = await imageUrlToBuffer(fileUri);
          if (!contentType) {
            throw new Error("Failed to fetch content type");
          }
          mimeType = contentType;
        } catch (e) {
          console.warn(
            "No mime type found, and failed to fetch content type. File URI:\n",
            fileUri,
            "\nError:\n",
            e,
          );
        }
      }
      if (
        !mimeType ||
        BLACKLISTED_MIME_TYPES.find((mt) => mimeType.startsWith(mt))
      ) {
        return [];
      }

      return [
        {
          type: "text",
          text: `The below image is index ${baseIndex + chunkIndex}`,
        },
        {
          type: "media",
          mimeType,
          fileUri: cleanedFileUri,
        },
      ];
    },
  );
  const imageMessages = (await Promise.all(imageMessagesPromises)).flat();
  return imageMessages;
}



================================================
FILE: src/utils/reflections.ts
================================================
import { Item, LangGraphRunnableConfig } from "@langchain/langgraph";

const NAMESPACE = ["reflection_rules"];
const KEY = "rules";
export const RULESET_KEY = "ruleset";
export const PROMPT_KEY = "prompt";

/**
 * Retrieves reflection rules from the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<string>} The reflection rules prompt, or an empty string if not found
 */
export async function getReflectionsPrompt(
  config: LangGraphRunnableConfig,
): Promise<string> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  const reflections = await store.get(NAMESPACE, KEY);
  return reflections?.value?.[PROMPT_KEY] || "";
}

/**
 * Stores reflection rules in the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @param {string} reflections - The reflection rules to store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<void>}
 */
export async function putReflectionsPrompt(
  config: LangGraphRunnableConfig,
  reflections: string,
): Promise<void> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, KEY, {
    [PROMPT_KEY]: reflections,
  });
}

/**
 * Retrieves reflection rules from the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<Item | undefined>} The reflection rules, or undefined if not found
 * @deprecated - use `getReflectionsPrompt` instead.
 */
export async function getReflections(
  config: LangGraphRunnableConfig,
): Promise<Item | undefined> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  const reflections = await store.get(NAMESPACE, KEY);
  return reflections || undefined;
}

/**
 * Stores reflection rules in the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @param {string[]} reflections - The reflection rules to store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<void>}
 * @deprecated - use `memory` graph instead.
 */
export async function putReflections(
  config: LangGraphRunnableConfig,
  reflections: string[],
): Promise<void> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, KEY, {
    [RULESET_KEY]: reflections,
  });
}

export const REFLECTIONS_PROMPT = `You have also been provided with a handful of reflections based on previous requests the user has made. Be sure to follow these rules when writing this new post so the user does not need to repeat their requests:
<reflections>
{reflections}
</reflections>`;

const THREAD_KEY = "thread_rules";
export const THREAD_RULESET_KEY = "ruleset";

/**
 * Retrieves thread reflection rules from the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<Item | undefined>} The thread reflection rules if they exist, undefined otherwise
 */
export async function getThreadReflections(
  config: LangGraphRunnableConfig,
): Promise<Item | undefined> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  const threadReflections = await store.get(NAMESPACE, THREAD_KEY);
  return threadReflections || undefined;
}

/**
 * Stores thread reflection rules in the store
 * @param {LangGraphRunnableConfig} config - Configuration object containing the store
 * @param {Record<string, any>} value - The thread reflection rules to store
 * @throws {Error} When no store is provided in the config
 * @returns {Promise<void>}
 */
export async function putThreadReflections(
  config: LangGraphRunnableConfig,
  threadReflections: string[],
): Promise<void> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  await store.put(NAMESPACE, THREAD_KEY, {
    [THREAD_RULESET_KEY]: threadReflections,
  });
}

export const THREAD_REFLECTIONS_PROMPT = `<reflections-context>
You have also been provided with a list of reflections generated from previous requests the user has made to change the posts in the thread.
Use these when writing or updating the thread posts to ensure the user's requests are met.
</reflections-context>

<reflection-items>
{reflections}
</reflection-items>`;



================================================
FILE: src/utils/screenshot.ts
================================================
import {
  BrowserContextOptions,
  chromium,
  PageScreenshotOptions,
  Page,
} from "playwright";

const GOTO_PAGE_TIMEOUT_ERROR = `Timeout 30000ms exceeded`;

/**
 * Attempts to navigate to a URL with retry logic for timeout errors
 * @param page - Playwright Page instance to navigate with
 * @param url - The URL to navigate to
 * @param iters - Current retry iteration count (internal use)
 * @throws {Error} If navigation fails after 3 attempts or encounters non-timeout errors
 */
async function goToPage(page: Page, url: string, iters = 0): Promise<void> {
  if (iters > 3) {
    throw new Error(`Failed to navigate to ${url} after 3 attempts`);
  }
  try {
    // Navigate and wait for the page to be fully loaded
    await page.goto(url, {
      waitUntil: "networkidle",
      timeout: 30000,
    });
  } catch (error: any) {
    if (
      typeof error === "object" &&
      error?.message &&
      typeof error?.message === "string" &&
      error?.message.includes(GOTO_PAGE_TIMEOUT_ERROR)
    ) {
      console.warn(
        `Navigation to ${url} timed out. Attempting retry ${iters + 1}/3`,
      );
      await goToPage(page, url, iters + 1);
    } else {
      throw error;
    }
  }
}

/**
 * Takes a screenshot of a webpage using Playwright
 * @param url - The URL of the webpage to screenshot
 * @param options - Configuration options for the screenshot
 * @param options.browserContextOptions - Additional options for the browser context
 * @param options.screenshotOptions - Options for the screenshot capture
 * @returns Promise resolving to a Buffer containing the JPEG screenshot data
 * @throws {Error} If screenshot capture fails for any reason
 */
export async function takeScreenshot(
  url: string,
  options?: {
    browserContextOptions?: BrowserContextOptions;
    screenshotOptions?: PageScreenshotOptions;
  },
): Promise<Buffer> {
  const browser = await chromium.launch({
    headless: true,
  });

  // Configure browser with desktop viewport and common headers to appear more like a regular user
  const context = await browser.newContext({
    viewport: { width: 1920, height: 1080 },
    userAgent:
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    extraHTTPHeaders: {
      "Accept-Language": "en-US,en;q=0.9",
      Accept:
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    },
    ...options?.browserContextOptions,
  });

  const page = await context.newPage();

  try {
    // Navigate to page and wait for it to load. This function will retry up to 3 times if it times out
    await goToPage(page, url, 0);

    // Check for various indicators that the page is fully loaded and ready for screenshot
    await page
      .waitForFunction(
        () => {
          // Basic document ready check
          if (document.readyState !== "complete") return false;

          // Look for common loading indicator patterns in class/id names
          const loadingElements = document.querySelectorAll(
            '[class*="loading"], [id*="loading"]',
          );
          if (loadingElements.length > 0) return false;

          // Ensure all images are fully loaded with valid dimensions
          const images = document.getElementsByTagName("img");
          if (images.length > 0) {
            return Array.from(images).every(
              (img) =>
                img.complete &&
                (img.naturalHeight !== 0 || img.naturalWidth !== 0),
            );
          }

          return true;
        },
        { timeout: 5000 },
      )
      .catch(() => {
        // Continue even if waiting times out
        console.warn(
          "Page might not be fully loaded, proceeding with screenshot",
        );
      });

    const screenshot = await page.screenshot({
      type: "jpeg",
      ...options?.screenshotOptions,
    });

    await context.close();
    await browser.close();
    return Buffer.from(screenshot);
  } catch (error: any) {
    await context.close();
    await browser.close();
    console.error(`Failed to take screenshot of ${url}:`, error);
    throw error;
  }
}



================================================
FILE: src/utils/supabase.ts
================================================
import { createClient } from "@supabase/supabase-js";

export function createSupabaseClient() {
  if (!process.env.SUPABASE_URL || !process.env.SUPABASE_SERVICE_ROLE_KEY) {
    const errMsg = `Missing environment variables for supabase.
SUPABASE_URL missing: ${!!process.env.SUPABASE_URL}
SUPABASE_SERVICE_ROLE_KEY missing: ${!!process.env.SUPABASE_SERVICE_ROLE_KEY}`;
    throw new Error(errMsg);
  }

  return createClient(
    process.env.SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY,
  );
}



================================================
FILE: src/utils/schedule-date/constants.ts
================================================
import { TakenScheduleDates } from "./types.js";

export const DEFAULT_TAKEN_DATES: TakenScheduleDates = {
  p1: [],
  p2: [],
  p3: [],
  r1: [],
  r2: [],
  r3: [],
};

export const ALLOWED_P1_DAY_AND_TIMES_IN_UTC = [
  // Sunday 16:00 UTC (8AM PST)
  {
    day: 0,
    hour: 16,
  },
  // Sunday 17:00 UTC (9AM PST)
  {
    day: 0,
    hour: 17,
  },
  // Sunday 18:00 UTC (10AM PST)
  {
    day: 0,
    hour: 18,
  },
  // Saturday 16:00 UTC (8AM PST)
  {
    day: 6,
    hour: 16,
  },
  // Saturday 17:00 UTC (9AM PST)
  {
    day: 6,
    hour: 17,
  },
  // Saturday 18:00 UTC (10AM PST)
  {
    day: 6,
    hour: 18,
  },
];

export const LAST_ALLOWED_P1_HOUR = 18;
export const FIRST_ALLOWED_P1_HOUR = 16;

export const ALLOWED_P2_DAY_AND_TIMES_IN_UTC = [
  // Monday 16:00 UTC (8AM PST)
  {
    day: 1,
    hour: 16,
  },
  // Monday 17:00 UTC (9AM PST)
  {
    day: 1,
    hour: 17,
  },
  // Monday 18:00 UTC (10AM PST)
  {
    day: 1,
    hour: 18,
  },
  // Friday 16:00 UTC (8AM PST)
  {
    day: 5,
    hour: 16,
  },
  // Friday 17:00 UTC (9AM PST)
  {
    day: 5,
    hour: 17,
  },
  // Friday 18:00 UTC (10AM PST)
  {
    day: 5,
    hour: 18,
  },
  // Sunday 19:00 UTC (11AM PST)
  {
    day: 0,
    hour: 19,
  },
  // Sunday 20:00 UTC (12PM PST)
  {
    day: 0,
    hour: 20,
  },
  // Sunday 21:00 UTC (1PM PST)
  {
    day: 0,
    hour: 21,
  },
  // Saturday 19:00 UTC (11AM PST)
  {
    day: 6,
    hour: 19,
  },
  // Saturday 20:00 UTC (12PM PST)
  {
    day: 6,
    hour: 20,
  },
  // Saturday 21:00 UTC (1PM PST)
  {
    day: 6,
    hour: 21,
  },
];

export const FIRST_ALLOWED_P2_HOUR_WEEKDAY = 16;
export const LAST_ALLOWED_P2_HOUR_WEEKDAY = 18;
export const FIRST_ALLOWED_P2_HOUR_WEEKEND = 19;
export const LAST_ALLOWED_P2_HOUR_WEEKEND = 21;

export const ALLOWED_P3_DAY_AND_TIMES_IN_UTC = [
  // Saturday: 21, 22, 23
  { day: 6, hour: 21 },
  { day: 6, hour: 22 },
  { day: 6, hour: 23 },
  // Sunday: 0, 1, 21, 22, 23
  { day: 0, hour: 0 },
  { day: 0, hour: 1 },
  { day: 0, hour: 21 },
  { day: 0, hour: 22 },
  { day: 0, hour: 23 },
  // Monday: 0, 1
  { day: 1, hour: 0 },
  { day: 1, hour: 1 },
];

export const ALLOWED_R1_DAY_AND_TIMES_IN_UTC = [
  // Monday 16:00 UTC (8AM PST)
  {
    day: 1,
    hour: 16,
  },
  // Monday 17:00 UTC (9AM PST)
  {
    day: 1,
    hour: 17,
  },
  // Monday 18:00 UTC (10AM PST)
  {
    day: 1,
    hour: 18,
  },

  // Tuesday 16:00 UTC (8AM PST)
  {
    day: 2,
    hour: 16,
  },
  // Tuesday 17:00 UTC (9AM PST)
  {
    day: 2,
    hour: 17,
  },
  // Tuesday 18:00 UTC (10AM PST)
  {
    day: 2,
    hour: 18,
  },

  // Wednesday 16:00 UTC (8AM PST)
  {
    day: 3,
    hour: 16,
  },
  // Wednesday 17:00 UTC (9AM PST)
  {
    day: 3,
    hour: 17,
  },
  // Wednesday 18:00 UTC (10AM PST)
  {
    day: 3,
    hour: 18,
  },

  // Thursday 16:00 UTC (8AM PST)
  {
    day: 4,
    hour: 16,
  },
  // Thursday 17:00 UTC (9AM PST)
  {
    day: 4,
    hour: 17,
  },
  // Thursday 18:00 UTC (10AM PST)
  {
    day: 4,
    hour: 18,
  },

  // Friday 16:00 UTC (8AM PST)
  {
    day: 5,
    hour: 16,
  },
  // Friday 17:00 UTC (9AM PST)
  {
    day: 5,
    hour: 17,
  },
  // Friday 18:00 UTC (10AM PST)
  {
    day: 5,
    hour: 18,
  },
];

export const ALLOWED_R2_DAY_AND_TIMES_IN_UTC = [
  // Monday 19:00 UTC (11AM PST)
  {
    day: 1,
    hour: 19,
  },
  // Monday 20:00 UTC (12PM PST)
  {
    day: 1,
    hour: 20,
  },
  // Monday 21:00 UTC (1PM PST)
  {
    day: 1,
    hour: 21,
  },

  // Tuesday 19:00 UTC (11AM PST)
  {
    day: 2,
    hour: 19,
  },
  // Tuesday 20:00 UTC (12PM PST)
  {
    day: 2,
    hour: 20,
  },
  // Tuesday 21:00 UTC (1PM PST)
  {
    day: 2,
    hour: 21,
  },

  // Wednesday 19:00 UTC (11AM PST)
  {
    day: 3,
    hour: 19,
  },
  // Wednesday 20:00 UTC (12PM PST)
  {
    day: 3,
    hour: 20,
  },
  // Wednesday 21:00 UTC (1PM PST)
  {
    day: 3,
    hour: 21,
  },

  // Thursday 19:00 UTC (11AM PST)
  {
    day: 4,
    hour: 19,
  },
  // Thursday 20:00 UTC (12PM PST)
  {
    day: 4,
    hour: 20,
  },
  // Thursday 21:00 UTC (1PM PST)
  {
    day: 4,
    hour: 21,
  },

  // Friday 19:00 UTC (11AM PST)
  {
    day: 5,
    hour: 19,
  },
  // Friday 20:00 UTC (12PM PST)
  {
    day: 5,
    hour: 20,
  },
  // Friday 21:00 UTC (1PM PST)
  {
    day: 5,
    hour: 21,
  },
];

export const ALLOWED_R3_DAY_AND_TIMES_IN_UTC = [
  // Monday 22:00 UTC (2PM PST)
  {
    day: 1,
    hour: 22,
  },
  // Monday 23:00 UTC (3PM PST)
  {
    day: 1,
    hour: 23,
  },
  // Monday 00:00 UTC (4PM PST)
  {
    day: 2,
    hour: 0,
  },

  // Tuesday 22:00 UTC (2PM PST)
  {
    day: 2,
    hour: 22,
  },
  // Tuesday 23:00 UTC (3PM PST)
  {
    day: 2,
    hour: 23,
  },
  // Tuesday 00:00 UTC (4PM PST)
  {
    day: 3,
    hour: 0,
  },

  // Wednesday 22:00 UTC (2PM PST)
  {
    day: 3,
    hour: 22,
  },
  // Wednesday 23:00 UTC (3PM PST)
  {
    day: 3,
    hour: 23,
  },
  // Wednesday 00:00 UTC (4PM PST)
  {
    day: 4,
    hour: 0,
  },

  // Thursday 22:00 UTC (2PM PST)
  {
    day: 4,
    hour: 22,
  },
  // Thursday 23:00 UTC (3PM PST)
  {
    day: 4,
    hour: 23,
  },
  // Thursday 00:00 UTC (4PM PST)
  {
    day: 5,
    hour: 0,
  },

  // Friday 22:00 UTC (2PM PST)
  {
    day: 5,
    hour: 22,
  },
  // Friday 23:00 UTC (3PM PST)
  {
    day: 5,
    hour: 23,
  },
  // Friday 00:00 UTC (4PM PST)
  {
    day: 6,
    hour: 0,
  },
];



================================================
FILE: src/utils/schedule-date/helpers.ts
================================================
import { addDays, isSaturday, isFriday, isMonday, isSunday } from "date-fns";

export function getNextSaturday(date: Date): Date {
  let isDateSaturday = false;
  while (!isDateSaturday) {
    date = addDays(date, 1);
    isDateSaturday = isSaturday(date);
  }
  return new Date(date.setUTCHours(0, 0, 0, 0));
}

export function getNextFriday(date: Date): Date {
  let isDateFriday = false;
  while (!isDateFriday) {
    date = addDays(date, 1);
    isDateFriday = isFriday(date);
  }
  return new Date(date.setUTCHours(0, 0, 0, 0));
}

export function getNextMonday(date: Date): Date {
  let isDateMonday = false;
  while (!isDateMonday) {
    date = addDays(date, 1);
    isDateMonday = isMonday(date);
  }
  return new Date(date.setUTCHours(0, 0, 0, 0));
}

export function isWeekend(date: Date): boolean {
  return isSaturday(date) || isSunday(date);
}

export function isMondayOrFriday(date: Date): boolean {
  return isMonday(date) || isFriday(date);
}



================================================
FILE: src/utils/schedule-date/index.ts
================================================
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import {
  isValid,
  addDays,
  isSunday,
  isFriday,
  isMonday,
  isSaturday,
  format,
} from "date-fns";
import {
  getNextFriday,
  getNextMonday,
  getNextSaturday,
  isMondayOrFriday,
  isWeekend,
} from "./helpers.js";
import { toZonedTime } from "date-fns-tz";
import { DateType } from "../../agents/types.js";
import { SlackClient } from "../../clients/slack/client.js";
import {
  FIRST_ALLOWED_P1_HOUR,
  ALLOWED_P2_DAY_AND_TIMES_IN_UTC,
  ALLOWED_P3_DAY_AND_TIMES_IN_UTC,
  LAST_ALLOWED_P1_HOUR,
  LAST_ALLOWED_P2_HOUR_WEEKDAY,
  FIRST_ALLOWED_P2_HOUR_WEEKDAY,
  FIRST_ALLOWED_P2_HOUR_WEEKEND,
  LAST_ALLOWED_P2_HOUR_WEEKEND,
  ALLOWED_P1_DAY_AND_TIMES_IN_UTC,
  ALLOWED_R1_DAY_AND_TIMES_IN_UTC,
  ALLOWED_R2_DAY_AND_TIMES_IN_UTC,
  ALLOWED_R3_DAY_AND_TIMES_IN_UTC,
  DEFAULT_TAKEN_DATES,
} from "./constants.js";
import { TakenScheduleDates } from "./types.js";

/**
 * Calculates a future date by adding seconds to a base date and formats it as MM/DD HH:MM AM/PM PST
 * @param afterSeconds - Number of seconds to add to the base date
 * @returns string representing the future date in format MM/DD HH:MM AM/PM PST
 */
export function getFutureDate(afterSeconds: number): string {
  const baseDate = new Date();
  const futureDate = new Date(baseDate.getTime() + afterSeconds * 1000);

  // Convert to PST
  const pstDate = toZonedTime(futureDate, "America/Los_Angeles");

  // Format the date
  return format(pstDate, "MM/dd hh:mm a").toUpperCase() + " PST";
}

export function validateAfterSeconds(afterSeconds: number): boolean {
  return afterSeconds >= 0;
}

const NAMESPACE = ["taken_schedule_dates"];
const KEY = "dates";
const TAKEN_DATES_KEY = "taken_dates";

/**
 * Searches the store for all taken schedule dates
 * @param config
 * @returns {Promise<TakenScheduleDates>} The taken schedule dates, or DEFAULT_TAKEN_DATES if no dates are taken
 */
export async function getTakenScheduleDates(
  config: LangGraphRunnableConfig,
): Promise<TakenScheduleDates> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  const takenDates = await store.get(NAMESPACE, KEY);
  if (!takenDates) {
    return DEFAULT_TAKEN_DATES;
  }
  const storedDates = takenDates.value?.[TAKEN_DATES_KEY];
  // Convert stored string dates back to Date objects
  return {
    p1: storedDates?.p1?.map((d: string) => new Date(d)) || [],
    p2: storedDates?.p2?.map((d: string) => new Date(d)) || [],
    p3: storedDates?.p3?.map((d: string) => new Date(d)) || [],
    r1: storedDates?.r1?.map((d: string) => new Date(d)) || [],
    r2: storedDates?.r2?.map((d: string) => new Date(d)) || [],
    r3: storedDates?.r3?.map((d: string) => new Date(d)) || [],
  };
}

/**
 * Updates the store with a new taken scheduled date
 * @param {TakenScheduleDates} takenDates The new taken schedule dates
 * @param {LangGraphRunnableConfig} config
 * @returns {Promise<void>}
 */
export async function putTakenScheduleDates(
  takenDates: TakenScheduleDates,
  config: LangGraphRunnableConfig,
): Promise<void> {
  const { store } = config;
  if (!store) {
    throw new Error("No store provided");
  }
  // Convert Date objects to ISO strings for storage
  const serializedDates = {
    p1: takenDates.p1.map((d) => d.toISOString()),
    p2: takenDates.p2.map((d) => d.toISOString()),
    p3: takenDates.p3.map((d) => d.toISOString()),
    r1: takenDates.r1.map((d) => d.toISOString()),
    r2: takenDates.r2.map((d) => d.toISOString()),
    r3: takenDates.r3.map((d) => d.toISOString()),
  };
  await store.put(NAMESPACE, KEY, {
    [TAKEN_DATES_KEY]: serializedDates,
  });
}

function getAfterSeconds(date: Date, baseDate: Date = new Date()): number {
  return Math.floor((date.getTime() - baseDate.getTime()) / 1000);
}

interface GetNextAvailableDateParams {
  dateToCheck: Date;
  priority: "p1" | "p2" | "p3";
  takenDates: TakenScheduleDates;
}

/**
 * Given an input date, priority level, and taken dates,
 * returns an available date on that day, or undefined if
 * no times are available that day.
 */
function getNextAvailableDate({
  dateToCheck,
  priority,
  takenDates,
}: GetNextAvailableDateParams): Date {
  const takenDatesForPriority = takenDates[priority];
  let candidate: Date;

  // -- Existing logic --
  if (!takenDatesForPriority.length) {
    const day = dateToCheck.getUTCDay();
    if (priority === "p1") {
      candidate = new Date(
        Date.UTC(
          dateToCheck.getUTCFullYear(),
          dateToCheck.getUTCMonth(),
          dateToCheck.getUTCDate(),
          FIRST_ALLOWED_P1_HOUR,
        ),
      );
    } else if (priority === "p2") {
      const allowedHour = ALLOWED_P2_DAY_AND_TIMES_IN_UTC.find(
        (d) => d.day === day,
      )?.hour;
      if (allowedHour === undefined)
        throw new Error("Unreachable code (no p2 hour found).");
      candidate = new Date(
        Date.UTC(
          dateToCheck.getUTCFullYear(),
          dateToCheck.getUTCMonth(),
          dateToCheck.getUTCDate(),
          allowedHour,
        ),
      );
    } else {
      // p3
      const allowedHour = ALLOWED_P3_DAY_AND_TIMES_IN_UTC.find(
        (d) => d.day === day,
      )?.hour;
      if (allowedHour === undefined)
        throw new Error("Unreachable code (no p3 hour found).");
      candidate = new Date(
        Date.UTC(
          dateToCheck.getUTCFullYear(),
          dateToCheck.getUTCMonth(),
          dateToCheck.getUTCDate(),
          allowedHour,
        ),
      );
    }
  } else {
    // If there's already a date for this priority, continue from the last taken date
    const lastTakenDate =
      takenDatesForPriority[takenDatesForPriority.length - 1];
    const lastHour = lastTakenDate.getUTCHours();

    if (priority === "p1") {
      if (lastHour < LAST_ALLOWED_P1_HOUR) {
        candidate = new Date(
          Date.UTC(
            lastTakenDate.getUTCFullYear(),
            lastTakenDate.getUTCMonth(),
            lastTakenDate.getUTCDate(),
            lastHour + 1,
          ),
        );
      } else {
        const nextDay = addDays(lastTakenDate, 1);
        if (isSunday(nextDay)) {
          candidate = new Date(
            Date.UTC(
              nextDay.getUTCFullYear(),
              nextDay.getUTCMonth(),
              nextDay.getUTCDate(),
              FIRST_ALLOWED_P1_HOUR,
            ),
          );
        } else {
          const nextSat = getNextSaturday(lastTakenDate);
          candidate = new Date(
            Date.UTC(
              nextSat.getUTCFullYear(),
              nextSat.getUTCMonth(),
              nextSat.getUTCDate(),
              FIRST_ALLOWED_P1_HOUR,
            ),
          );
        }
      }
    } else if (priority === "p2") {
      if (isMondayOrFriday(lastTakenDate)) {
        if (lastHour < LAST_ALLOWED_P2_HOUR_WEEKDAY) {
          candidate = new Date(
            Date.UTC(
              lastTakenDate.getUTCFullYear(),
              lastTakenDate.getUTCMonth(),
              lastTakenDate.getUTCDate(),
              lastHour + 1,
            ),
          );
        } else if (isMonday(lastTakenDate)) {
          const nextFri = getNextFriday(lastTakenDate);
          candidate = new Date(
            Date.UTC(
              nextFri.getUTCFullYear(),
              nextFri.getUTCMonth(),
              nextFri.getUTCDate(),
              FIRST_ALLOWED_P2_HOUR_WEEKDAY,
            ),
          );
        } else {
          const nextSat = getNextSaturday(lastTakenDate);
          candidate = new Date(
            Date.UTC(
              nextSat.getUTCFullYear(),
              nextSat.getUTCMonth(),
              nextSat.getUTCDate(),
              FIRST_ALLOWED_P2_HOUR_WEEKEND,
            ),
          );
        }
      } else if (isWeekend(lastTakenDate)) {
        if (lastHour < LAST_ALLOWED_P2_HOUR_WEEKEND) {
          candidate = new Date(
            Date.UTC(
              lastTakenDate.getUTCFullYear(),
              lastTakenDate.getUTCMonth(),
              lastTakenDate.getUTCDate(),
              lastHour + 1,
            ),
          );
        } else {
          const nextDay = addDays(lastTakenDate, 1);
          if (isSunday(nextDay)) {
            candidate = new Date(
              Date.UTC(
                nextDay.getUTCFullYear(),
                nextDay.getUTCMonth(),
                nextDay.getUTCDate(),
                FIRST_ALLOWED_P2_HOUR_WEEKEND,
              ),
            );
          } else {
            const nextMon = getNextMonday(lastTakenDate);
            candidate = new Date(
              Date.UTC(
                nextMon.getUTCFullYear(),
                nextMon.getUTCMonth(),
                nextMon.getUTCDate(),
                FIRST_ALLOWED_P2_HOUR_WEEKDAY,
              ),
            );
          }
        }
      } else {
        const nextFri = getNextFriday(lastTakenDate);
        candidate = new Date(
          Date.UTC(
            nextFri.getUTCFullYear(),
            nextFri.getUTCMonth(),
            nextFri.getUTCDate(),
            FIRST_ALLOWED_P2_HOUR_WEEKDAY,
          ),
        );
      }
    } else {
      // p3
      const d = lastTakenDate.getUTCDay();
      const h = lastTakenDate.getUTCHours();
      const sameDaySlots = ALLOWED_P3_DAY_AND_TIMES_IN_UTC.filter(
        (slot) => slot.day === d && slot.hour > h,
      ).sort((a, b) => a.hour - b.hour);

      if (sameDaySlots.length) {
        candidate = new Date(
          Date.UTC(
            lastTakenDate.getUTCFullYear(),
            lastTakenDate.getUTCMonth(),
            lastTakenDate.getUTCDate(),
            sameDaySlots[0].hour,
          ),
        );
      } else {
        // Move day-by-day
        let tmp = new Date(
          Date.UTC(
            lastTakenDate.getUTCFullYear(),
            lastTakenDate.getUTCMonth(),
            lastTakenDate.getUTCDate(),
          ),
        );
        tmp = addDays(tmp, 1);
        candidate = undefined as unknown as Date;

        for (let i = 0; i < 14; i += 1) {
          const dayCheck = tmp.getUTCDay();
          const validSlots = ALLOWED_P3_DAY_AND_TIMES_IN_UTC.filter(
            (slot) => slot.day === dayCheck,
          ).sort((a, b) => a.hour - b.hour);
          if (validSlots.length) {
            candidate = new Date(
              Date.UTC(
                tmp.getUTCFullYear(),
                tmp.getUTCMonth(),
                tmp.getUTCDate(),
                validSlots[0].hour,
              ),
            );
            break;
          }
          tmp = addDays(tmp, 1);
        }
        if (!candidate) {
          throw new Error("Couldn't find a valid p3 slot within 2 weeks.");
        }
      }
    }
  }

  // -- Ensure candidate is never in the past --
  if (candidate < dateToCheck) {
    // We'll shift forward day-by-day to find the next valid slot after dateToCheck
    let tmp = new Date(
      Date.UTC(
        dateToCheck.getUTCFullYear(),
        dateToCheck.getUTCMonth(),
        dateToCheck.getUTCDate(),
        dateToCheck.getUTCHours(),
      ),
    );
    for (let i = 0; i < 14; i += 1) {
      const day = tmp.getUTCDay();
      const currentHour = tmp.getUTCHours();
      const allowedSlots =
        priority === "p1"
          ? ALLOWED_P1_DAY_AND_TIMES_IN_UTC
          : priority === "p2"
            ? ALLOWED_P2_DAY_AND_TIMES_IN_UTC
            : ALLOWED_P3_DAY_AND_TIMES_IN_UTC;

      // Only allow hours >= currentHour, but if we're exactly on currentHour, minutes must be 0
      const validSlots = allowedSlots
        .filter((s) => s.day === day)
        .filter((s) => {
          // skip all slots strictly less than current hour, or equal.
          if (s.hour <= currentHour) return false;
          return true;
        })
        .sort((a, b) => a.hour - b.hour);

      if (validSlots.length) {
        candidate = new Date(
          Date.UTC(
            tmp.getUTCFullYear(),
            tmp.getUTCMonth(),
            tmp.getUTCDate(),
            validSlots[0].hour,
          ),
        );
        // Now candidate is guaranteed >= dateToCheck
        break;
      }

      // move to next day at midnight
      tmp = new Date(
        Date.UTC(tmp.getUTCFullYear(), tmp.getUTCMonth(), tmp.getUTCDate() + 1),
      );
    }
    if (candidate < dateToCheck) {
      throw new Error(
        `No valid future slot found for ${priority} within 2 weeks of ${dateToCheck}`,
      );
    }
  }

  return candidate;
}

function validateScheduleDate(date: Date, baseDate: Date): boolean {
  const afterSeconds = getAfterSeconds(date, baseDate);
  return validateAfterSeconds(afterSeconds);
}

interface FindAvailableRepurposeDatesRepurposer {
  repurposedPriority: "r1" | "r2" | "r3";
  baseDate: Date;
  numberOfDates: number;
  takenDates: TakenScheduleDates;
  /**
   * @default 1
   */
  numWeeksBetween?: number;
}

function normalizeSlots(
  slots: { day: number; hour: number }[],
): { day: number; hour: number }[] {
  // Move â€œhour=0â€ to the previous day as â€œhour=24â€
  return slots.map((slot) => {
    if (slot.hour === 0) {
      return {
        day: slot.day - 1,
        hour: 24,
      };
    }
    return slot;
  });
}

// Optional little helper to avoid confusion when setting â€œhour=24â€:
function setUTCHoursExtended(base: Date, hour: number) {
  // set to 00:00 first:
  base.setUTCHours(0, 0, 0, 0);
  // then add "hour" hours in milliseconds
  base.setTime(base.getTime() + hour * 60 * 60 * 1000);
}

export function findAvailableRepurposeDates({
  repurposedPriority,
  baseDate,
  numberOfDates,
  takenDates: allTakenDates,
  numWeeksBetween = 1,
}: FindAvailableRepurposeDatesRepurposer): Date[] {
  const results: Date[] = [];
  let weekOffset = 0;

  const takenDates = allTakenDates[repurposedPriority];

  // Pick which raw slots to use
  const rawAllowedSlots =
    repurposedPriority === "r1"
      ? ALLOWED_R1_DAY_AND_TIMES_IN_UTC
      : repurposedPriority === "r2"
        ? ALLOWED_R2_DAY_AND_TIMES_IN_UTC
        : ALLOWED_R3_DAY_AND_TIMES_IN_UTC;

  // Normalize them so day+1, hour=0 becomes day, hour=24
  const allowedSlots = normalizeSlots(rawAllowedSlots);

  // We'll allow searching up to 52 weeks (1 year) in the future to avoid infinite loops
  while (results.length < numberOfDates && weekOffset < 52) {
    // Start from Monday of the current week
    const checkDate = new Date(baseDate.getTime());
    checkDate.setUTCHours(0, 0, 0, 0);

    // Move to Monday (day 1) of the current week if we're not already there
    const currentDay = checkDate.getUTCDay();
    const daysToMonday = currentDay === 0 ? -6 : 1 - currentDay;
    checkDate.setUTCDate(
      checkDate.getUTCDate() + daysToMonday + weekOffset * 7,
    );

    let foundSlotThisWeek = false;

    // Try each day of the week (Monday-Friday)
    for (let dayOffset = 0; dayOffset < 5 && !foundSlotThisWeek; dayOffset++) {
      const candidateDay = new Date(checkDate.getTime());
      candidateDay.setUTCDate(candidateDay.getUTCDate() + dayOffset);
      const dayOfWeek = candidateDay.getUTCDay();

      // Get allowed slots for this day
      const sameDaySlots = allowedSlots
        .filter((slot) => slot.day === dayOfWeek)
        .sort((a, b) => a.hour - b.hour);

      // Try each time slot for this day
      for (const slot of sameDaySlots) {
        const candidate = new Date(candidateDay.getTime());
        setUTCHoursExtended(candidate, slot.hour);

        // Ensure it's strictly in the future
        if (candidate <= baseDate) {
          continue;
        }

        // Check if already taken
        const alreadyTaken = takenDates.some((taken) => {
          return (
            taken.getUTCFullYear() === candidate.getUTCFullYear() &&
            taken.getUTCMonth() === candidate.getUTCMonth() &&
            taken.getUTCDate() === candidate.getUTCDate() &&
            taken.getUTCHours() === candidate.getUTCHours()
          );
        });

        if (!alreadyTaken) {
          results.push(candidate);
          foundSlotThisWeek = true;
          break;
        }
      }
    }

    weekOffset += numWeeksBetween;
  }

  return results;
}

interface FindAvailableBasicDateParams {
  baseDate: Date;
  config: LangGraphRunnableConfig;
  priority: "p1" | "p2" | "p3";
  takenScheduleDates: TakenScheduleDates;
}

async function findAvailableBasicDates({
  baseDate,
  config,
  priority,
  takenScheduleDates,
}: FindAvailableBasicDateParams): Promise<Date> {
  let currentTime = baseDate;
  const currentDayUTCHours = baseDate.getUTCHours();

  if (priority === "p1") {
    // Check if the current date is a saturday/sunday
    if (isWeekend(baseDate)) {
      // Check if there are available slots for the current date
      if (currentDayUTCHours >= LAST_ALLOWED_P1_HOUR) {
        // If the current hour is 6PM (UTC) or later, advance to the next day
        currentTime = addDays(baseDate, 1);
        // Reset the hour to midnight UTC
        currentTime = new Date(currentTime.setUTCHours(0, 0, 0, 0));
        if (!isSunday(currentTime)) {
          // After adding one day, we must check if the day is Sunday. If not, we must assign
          // the next Saturday at midnight
          currentTime = getNextSaturday(currentTime);
        }
      } else {
        // This means the current time is before 6PM (UTC), and it's a weekend. We can do nothing and get the next available time.
      }
    } else {
      // If the current date is not a Saturday or Sunday, assign the next Saturday at midnight
      currentTime = getNextSaturday(currentTime);
    }
  }

  if (priority === "p2") {
    // Find the first available day
    if (isMondayOrFriday(baseDate)) {
      // Current date is a weekday, so check if the current hour is before the last allowed hour
      if (currentDayUTCHours >= LAST_ALLOWED_P2_HOUR_WEEKDAY) {
        // If the current hour is 5PM (UTC) or later, advance to the next day. To do this, check if the current day is a Friday.
        // If it's not a friday, we can assume it's a monday and we can find the next friday
        if (isFriday(baseDate)) {
          currentTime = addDays(baseDate, 1);
          currentTime = new Date(currentTime.setUTCHours(0, 0, 0, 0));
        } else {
          // base date is likely a monday
          currentTime = getNextFriday(currentTime);
        }
      } else {
        // This means the current time is before 5PM (UTC), and it's a weekday. We can do nothing and get the next available time.
      }
    } else if (isWeekend(baseDate)) {
      if (currentDayUTCHours >= LAST_ALLOWED_P2_HOUR_WEEKEND) {
        // If the current hour is 5PM (UTC) or later, advance to the next day and check if it's a Sunday. If not, get the next mondaysts
        const nextDay = addDays(baseDate, 1);
        if (isSunday(nextDay)) {
          currentTime = new Date(nextDay.setUTCHours(0, 0, 0, 0));
        } else {
          currentTime = getNextMonday(currentTime);
        }
      } else {
        // This means the current time is before 5PM (UTC), and it's a weekend. We can do nothing and get the next available time.
      }
    } else {
      // The date is not a Monday/Friday, or weekend. We can assume it's a weekday and get the next friday
      currentTime = getNextFriday(currentTime);
    }
  }

  if (priority === "p3") {
    const hour = baseDate.getUTCHours();

    if (isSaturday(baseDate)) {
      if (hour >= 23) {
        // Next day @ midnight
        let tmp = addDays(baseDate, 1);
        tmp = new Date(tmp.setUTCHours(0, 0, 0, 0));
        if (!isSunday(tmp) && !isMonday(tmp)) {
          tmp = getNextSaturday(tmp);
        }
        currentTime = tmp;
      }
      // else do nothing, weâ€™re still on Saturday < 23
    } else if (isSunday(baseDate)) {
      if (hour >= 23) {
        let tmp = addDays(baseDate, 1);
        tmp = new Date(tmp.setUTCHours(0, 0, 0, 0));
        if (!isMonday(tmp)) {
          tmp = getNextSaturday(tmp);
        }
        currentTime = tmp;
      }
    } else if (isMonday(baseDate)) {
      // Monday is valid only at hour=0 in your updated config
      if (hour >= 0) {
        // If itâ€™s already Monday 0:00 or later, you might allow that single slot,
        // but if your code sees hour=0 as "already used up," it can jump to Tuesday:
        //     let tmp = addDays(baseDate, 1);
        //     tmp = new Date(tmp.setUTCHours(0, 0, 0, 0));
        //     tmp = getNextSaturday(tmp);
        //     currentTime = tmp;
        //
        // BUT probably you do "do nothing" if hour=0,
        // else jump to next Saturday if hour>0
        if (hour > 1) {
          // Jump to next Saturday
          let tmp = addDays(baseDate, 1); // Tuesday
          tmp = new Date(tmp.setUTCHours(0, 0, 0, 0));
          tmp = getNextSaturday(tmp);
          currentTime = tmp;
        }
      }
    } else {
      // If not Sat/Sun/Mon, jump to next Saturday @ 00:00
      currentTime = getNextSaturday(baseDate);
    }
  }

  let nextAvailDate: Date | undefined;
  try {
    nextAvailDate = getNextAvailableDate({
      dateToCheck: currentTime,
      priority,
      takenDates: takenScheduleDates,
    });
    if (!nextAvailDate) {
      throw new Error("Received no available times");
    }
  } catch (e: any) {
    if (
      "message" in e &&
      e.message.includes("No valid future slot found for")
    ) {
      // Send a message to slack
      if (process.env.SLACK_CHANNEL_ID && process.env.SLACK_CHANNEL_ID) {
        const slackClient = new SlackClient();

        await slackClient.sendMessage(
          process.env.SLACK_CHANNEL_ID,
          `**FAILED TO FIND DATE TO SCHEDULE POST**

Error message:
\`\`\`
${e.message}
\`\`\`

Priority: ${priority}
Base date: ${format(baseDate, "MM/dd/yyyy hh:mm a z")}

Thread ID: ${config.configurable?.thread_id || "No thread ID found"}
Run ID: ${config.configurable?.run_id || "No run ID found"}
      `,
        );
      }
    }

    throw e;
  }

  return nextAvailDate;
}

const isRepurposedPriority = (
  priority: DateType,
): priority is "r1" | "r2" | "r3" => {
  return typeof priority === "string" && ["r1", "r2", "r3"].includes(priority);
};

const isBasicPriority = (
  priority: DateType,
): priority is "p1" | "p2" | "p3" => {
  return typeof priority === "string" && ["p1", "p2", "p3"].includes(priority);
};

type GetScheduledBasicDateArgs = {
  scheduleDate: DateType;
  config: LangGraphRunnableConfig;
  baseDate?: Date;
};

type GetScheduledRepurposeDateArgs = GetScheduledBasicDateArgs & {
  numberOfDates: number;
  numWeeksBetween: number;
};

export async function getScheduledDateSeconds(
  args: GetScheduledBasicDateArgs,
): Promise<number>;

export async function getScheduledDateSeconds(
  args: GetScheduledRepurposeDateArgs,
): Promise<number[]>;

export async function getScheduledDateSeconds(
  args: GetScheduledBasicDateArgs | GetScheduledRepurposeDateArgs,
): Promise<number | number[]> {
  const { scheduleDate, config, baseDate, numberOfDates, numWeeksBetween } = {
    baseDate: new Date(),
    numberOfDates: undefined,
    numWeeksBetween: undefined,
    ...args,
  };
  if (isValid(scheduleDate)) {
    const afterSeconds = getAfterSeconds(scheduleDate as Date, baseDate);
    if (!validateAfterSeconds(afterSeconds)) {
      throw new Error(
        `Schedule date must be in the future. Instead, received: ${scheduleDate}`,
      );
    }
    return afterSeconds;
  }

  const takenScheduleDates = await getTakenScheduleDates(config);

  if (
    isRepurposedPriority(scheduleDate) &&
    numberOfDates !== undefined &&
    numWeeksBetween !== undefined
  ) {
    const scheduleDates = findAvailableRepurposeDates({
      repurposedPriority: scheduleDate,
      baseDate,
      numberOfDates,
      takenDates: takenScheduleDates,
      numWeeksBetween,
    });

    const isValidDate = scheduleDates.every((d) =>
      validateScheduleDate(d, baseDate),
    );
    if (!isValidDate) {
      throw new Error(`FAILED TO SCHEDULE POST
  
  Priority: ${scheduleDate}
  Schedule dates: ${scheduleDates.map((d) => format(d, "MM/dd/yyyy hh:mm a z")).join(", ")}
  Base date: ${format(baseDate, "MM/dd/yyyy hh:mm a z")}`);
    }

    takenScheduleDates[scheduleDate].push(...scheduleDates);
    await putTakenScheduleDates(takenScheduleDates, config);
    return scheduleDates.map((d) => getAfterSeconds(d, baseDate));
  } else if (
    isRepurposedPriority(scheduleDate) &&
    numberOfDates === undefined
  ) {
    throw new Error(
      "Must provide numberOfDates when scheduleDate is a repurposed priority",
    );
  }

  if (isBasicPriority(scheduleDate)) {
    const nextAvailDate = await findAvailableBasicDates({
      baseDate,
      config,
      priority: scheduleDate,
      takenScheduleDates,
    });
    const isValidDate = validateScheduleDate(nextAvailDate, baseDate);
    if (!isValidDate) {
      throw new Error(`FAILED TO SCHEDULE POST
  
  Priority: ${scheduleDate}
  Schedule date: ${format(nextAvailDate, "MM/dd/yyyy hh:mm a z")}
  Base date: ${format(baseDate, "MM/dd/yyyy hh:mm a z")}`);
    }

    takenScheduleDates[scheduleDate].push(nextAvailDate);
    await putTakenScheduleDates(takenScheduleDates, config);
    return getAfterSeconds(nextAvailDate, baseDate);
  }

  throw new Error(`INVALID SCHEDULE DATE: "${scheduleDate}"
    
Must be one of: "r1", "r2", "r3", "p1", "p2", "p3", or a valid date object.
  `);
}



================================================
FILE: src/utils/schedule-date/types.ts
================================================
export type TakenScheduleDates = {
  p1: Date[];
  p2: Date[];
  p3: Date[];
  r1: Date[];
  r2: Date[];
  r3: Date[];
};



================================================
FILE: src/utils/schedule-date/tests/schedule-date.test.ts
================================================
import { jest, describe, it, expect, afterAll, afterEach } from "@jest/globals";
import { InMemoryStore } from "@langchain/langgraph";

import {
  findAvailableRepurposeDates,
  getScheduledDateSeconds,
  getTakenScheduleDates,
  putTakenScheduleDates,
} from "../index.js";
import { DEFAULT_TAKEN_DATES } from "../constants.js";
import { TakenScheduleDates } from "../types.js";

describe("Priority P1 get scheduled date", () => {
  // Define MOCK_CURRENT_DATE in UTC or as per the mocked timezone
  const MOCK_CURRENT_DATE = new Date("2025-01-03T12:00:00.000Z"); // This aligns with 'America/Los_Angeles'

  jest.useFakeTimers();
  jest.setSystemTime(MOCK_CURRENT_DATE);

  afterAll(() => {
    jest.useRealTimers();
  });

  const EXPECTED_DATE_TIMES = [
    "2025-01-04T16:00:00.000Z",
    "2025-01-04T17:00:00.000Z",
    "2025-01-04T18:00:00.000Z",
    "2025-01-05T16:00:00.000Z",
    "2025-01-05T17:00:00.000Z",
    "2025-01-05T18:00:00.000Z",

    "2025-01-11T16:00:00.000Z",
    "2025-01-11T17:00:00.000Z",
    "2025-01-11T18:00:00.000Z",
    "2025-01-12T16:00:00.000Z",
    "2025-01-12T17:00:00.000Z",
    "2025-01-12T18:00:00.000Z",

    "2025-01-18T16:00:00.000Z",
    "2025-01-18T17:00:00.000Z",
    "2025-01-18T18:00:00.000Z",
    "2025-01-19T16:00:00.000Z",
    "2025-01-19T17:00:00.000Z",
    "2025-01-19T18:00:00.000Z",

    "2025-01-25T16:00:00.000Z",
    "2025-01-25T17:00:00.000Z",
    "2025-01-25T18:00:00.000Z",
    "2025-01-26T16:00:00.000Z",
    "2025-01-26T17:00:00.000Z",
    "2025-01-26T18:00:00.000Z",

    "2025-02-01T16:00:00.000Z",
    "2025-02-01T17:00:00.000Z",
    "2025-02-01T18:00:00.000Z",
    "2025-02-02T16:00:00.000Z",
    "2025-02-02T17:00:00.000Z",
    "2025-02-02T18:00:00.000Z",

    "2025-02-08T16:00:00.000Z",
    "2025-02-08T17:00:00.000Z",
    "2025-02-08T18:00:00.000Z",
    "2025-02-09T16:00:00.000Z",
    "2025-02-09T17:00:00.000Z",
    "2025-02-09T18:00:00.000Z",

    "2025-02-15T16:00:00.000Z",
    "2025-02-15T17:00:00.000Z",
    "2025-02-15T18:00:00.000Z",
    "2025-02-16T16:00:00.000Z",
    "2025-02-16T17:00:00.000Z",
    "2025-02-16T18:00:00.000Z",

    "2025-02-22T16:00:00.000Z",
    "2025-02-22T17:00:00.000Z",
    "2025-02-22T18:00:00.000Z",
    "2025-02-23T16:00:00.000Z",
    "2025-02-23T17:00:00.000Z",
    "2025-02-23T18:00:00.000Z",
  ];

  it("can properly find and schedule dates", async () => {
    const store = new InMemoryStore();
    const config = {
      store,
    };
    // Schedule posts sequentially
    const arrayLen = Array(48).fill(0);

    for await (const _ of arrayLen) {
      await getScheduledDateSeconds({
        scheduleDate: "p1",
        config,
        baseDate: MOCK_CURRENT_DATE,
      });
    }

    const scheduledDates = await getTakenScheduleDates(config);
    expect(scheduledDates.p1.length).toBe(48);

    // Convert both arrays to ISO strings and sort them for comparison
    const normalizedScheduledDates = scheduledDates.p1.map((date) =>
      new Date(date).toISOString(),
    );
    const normalizedExpectedDates = EXPECTED_DATE_TIMES.map((date) =>
      new Date(date).toISOString(),
    );
    expect(normalizedScheduledDates.sort()).toEqual(
      normalizedExpectedDates.sort(),
    );
  });
});

describe("Priority P2 get scheduled date", () => {
  // Define MOCK_CURRENT_DATE in UTC or as per the mocked timezone
  const MOCK_CURRENT_DATE = new Date("2025-01-03T12:00:00.000Z"); // This aligns with 'America/Los_Angeles'

  jest.useFakeTimers();
  jest.setSystemTime(MOCK_CURRENT_DATE);

  afterAll(() => {
    jest.useRealTimers();
  });

  const EXPECTED_DATE_TIMES = [
    // Monday/Friday
    "2025-01-03T16:00:00.000Z",
    "2025-01-03T17:00:00.000Z",
    "2025-01-03T18:00:00.000Z",

    "2025-01-06T16:00:00.000Z",
    "2025-01-06T17:00:00.000Z",
    "2025-01-06T18:00:00.000Z",
    "2025-01-10T16:00:00.000Z",
    "2025-01-10T17:00:00.000Z",
    "2025-01-10T18:00:00.000Z",

    // Saturday/Sunday
    "2025-01-04T19:00:00.000Z",
    "2025-01-04T20:00:00.000Z",
    "2025-01-04T21:00:00.000Z",
    "2025-01-05T19:00:00.000Z",
    "2025-01-05T20:00:00.000Z",
    "2025-01-05T21:00:00.000Z",

    // Monday/Friday
    "2025-01-13T16:00:00.000Z",
    "2025-01-13T17:00:00.000Z",
    "2025-01-13T18:00:00.000Z",
    "2025-01-17T16:00:00.000Z",
    "2025-01-17T17:00:00.000Z",
    "2025-01-17T18:00:00.000Z",

    // Saturday/Sunday
    "2025-01-11T19:00:00.000Z",
    "2025-01-11T20:00:00.000Z",
    "2025-01-11T21:00:00.000Z",
    "2025-01-12T19:00:00.000Z",
    "2025-01-12T20:00:00.000Z",
    "2025-01-12T21:00:00.000Z",

    // Monday/Friday
    "2025-01-20T16:00:00.000Z",
    "2025-01-20T17:00:00.000Z",
    "2025-01-20T18:00:00.000Z",
    "2025-01-24T16:00:00.000Z",
    "2025-01-24T17:00:00.000Z",
    "2025-01-24T18:00:00.000Z",

    // Saturday/Sunday
    "2025-01-18T19:00:00.000Z",
    "2025-01-18T20:00:00.000Z",
    "2025-01-18T21:00:00.000Z",
    "2025-01-19T19:00:00.000Z",
    "2025-01-19T20:00:00.000Z",
    "2025-01-19T21:00:00.000Z",

    // Monday/Friday
    "2025-01-27T16:00:00.000Z",
    "2025-01-27T17:00:00.000Z",
    "2025-01-27T18:00:00.000Z",
    "2025-01-31T16:00:00.000Z",
    "2025-01-31T17:00:00.000Z",
    "2025-01-31T18:00:00.000Z",

    // Saturday/Sunday
    "2025-01-25T19:00:00.000Z",
    "2025-01-25T20:00:00.000Z",
    "2025-01-25T21:00:00.000Z",
    "2025-01-26T19:00:00.000Z",
    "2025-01-26T20:00:00.000Z",
    "2025-01-26T21:00:00.000Z",
  ];

  it("can properly find and schedule dates", async () => {
    const store = new InMemoryStore();
    const config = {
      store,
    };
    // Schedule posts sequentially
    const arrayLen = Array(51).fill(0);

    for await (const _ of arrayLen) {
      await getScheduledDateSeconds({
        scheduleDate: "p2",
        config,
        baseDate: MOCK_CURRENT_DATE,
      });
    }

    const scheduledDates = await getTakenScheduleDates(config);
    expect(scheduledDates.p2.length).toBe(51);

    // Convert both arrays to ISO strings and sort them for comparison
    const normalizedScheduledDates = scheduledDates.p2.map((date) =>
      new Date(date).toISOString(),
    );
    const normalizedExpectedDates = EXPECTED_DATE_TIMES.map((date) =>
      new Date(date).toISOString(),
    );
    expect(normalizedScheduledDates.sort()).toEqual(
      normalizedExpectedDates.sort(),
    );
  });
});

describe("Priority P3 get scheduled date", () => {
  // Define MOCK_CURRENT_DATE in UTC or as per the mocked timezone
  const MOCK_CURRENT_DATE = new Date("2025-01-03T12:00:00.000Z"); // This aligns with 'America/Los_Angeles'

  jest.useFakeTimers();
  jest.setSystemTime(MOCK_CURRENT_DATE);

  afterAll(() => {
    jest.useRealTimers();
  });

  const EXPECTED_DATE_TIMES = [
    // Weekend 1
    "2025-01-04T21:00:00.000Z",
    "2025-01-04T22:00:00.000Z",
    "2025-01-04T23:00:00.000Z",
    "2025-01-05T00:00:00.000Z",
    "2025-01-05T01:00:00.000Z",

    "2025-01-05T21:00:00.000Z",
    "2025-01-05T22:00:00.000Z",
    "2025-01-05T23:00:00.000Z",
    "2025-01-06T00:00:00.000Z",
    "2025-01-06T01:00:00.000Z",

    // Weekend 2
    "2025-01-11T21:00:00.000Z",
    "2025-01-11T22:00:00.000Z",
    "2025-01-11T23:00:00.000Z",
    "2025-01-12T00:00:00.000Z",
    "2025-01-12T01:00:00.000Z",

    "2025-01-12T21:00:00.000Z",
    "2025-01-12T22:00:00.000Z",
    "2025-01-12T23:00:00.000Z",
    "2025-01-13T00:00:00.000Z",
    "2025-01-13T01:00:00.000Z",

    // Weekend 3
    "2025-01-18T21:00:00.000Z",
    "2025-01-18T22:00:00.000Z",
    "2025-01-18T23:00:00.000Z",
    "2025-01-19T00:00:00.000Z",
    "2025-01-19T01:00:00.000Z",

    "2025-01-19T21:00:00.000Z",
    "2025-01-19T22:00:00.000Z",
    "2025-01-19T23:00:00.000Z",
    "2025-01-20T00:00:00.000Z",
    "2025-01-20T01:00:00.000Z",

    // Weekend 4
    "2025-01-25T21:00:00.000Z",
    "2025-01-25T22:00:00.000Z",
    "2025-01-25T23:00:00.000Z",
    "2025-01-26T00:00:00.000Z",
    "2025-01-26T01:00:00.000Z",

    "2025-01-26T21:00:00.000Z",
    "2025-01-26T22:00:00.000Z",
    "2025-01-26T23:00:00.000Z",
    "2025-01-27T00:00:00.000Z",
    "2025-01-27T01:00:00.000Z",
  ];

  it("can properly find and schedule dates", async () => {
    const store = new InMemoryStore();
    const config = {
      store,
    };
    // Schedule posts sequentially
    const arrayLen = Array(40).fill(0);

    for await (const _ of arrayLen) {
      await getScheduledDateSeconds({
        scheduleDate: "p3",
        config,
        baseDate: MOCK_CURRENT_DATE,
      });
    }

    const scheduledDates = await getTakenScheduleDates(config);
    expect(scheduledDates.p3.length).toBe(40);

    // Convert both arrays to ISO strings and sort them for comparison
    const normalizedScheduledDates = scheduledDates.p3.map((date) =>
      new Date(date).toISOString(),
    );
    const normalizedExpectedDates = EXPECTED_DATE_TIMES.map((date) =>
      new Date(date).toISOString(),
    );
    expect(normalizedScheduledDates.sort()).toEqual(
      normalizedExpectedDates.sort(),
    );
  });
});

describe("Get scheduled dates", () => {
  // Reset the timer after each test, but individual tests may set their own timers
  afterEach(() => {
    jest.useRealTimers();
  });

  it("Can schedule for under an hour from the current time", async () => {
    const defaultTakenDates: TakenScheduleDates = {
      ...DEFAULT_TAKEN_DATES,
      p1: [
        new Date("2025-01-18T16:00:00.000Z"),
        new Date("2025-01-18T17:00:00.000Z"),
        new Date("2025-01-18T18:00:00.000Z"),
        new Date("2025-01-19T16:00:00.000Z"),
        new Date("2025-01-19T17:00:00.000Z"),
        new Date("2025-01-19T18:00:00.000Z"),
      ],
      p2: [
        new Date("2025-01-17T17:00:00.000Z"),
        new Date("2025-01-17T18:00:00.000Z"),
        new Date("2025-01-18T19:00:00.000Z"),
        new Date("2025-01-18T20:00:00.000Z"),
        new Date("2025-01-18T21:00:00.000Z"),
        new Date("2025-01-19T19:00:00.000Z"),
        new Date("2025-01-19T20:00:00.000Z"),
        new Date("2025-01-19T21:00:00.000Z"),
        new Date("2025-01-20T16:00:00.000Z"),
      ],
      p3: [
        new Date("2025-01-18T21:00:00.000Z"),
        new Date("2025-01-18T22:00:00.000Z"),
      ],
    };
    const store = new InMemoryStore();
    const config = {
      store,
    };
    await putTakenScheduleDates(defaultTakenDates, config);

    // This is 8:04 AM PST (16:04 UTC)
    const mockCurrentDate = new Date("2025-01-25T16:04:00.000Z");
    jest.useFakeTimers();
    jest.setSystemTime(mockCurrentDate);

    const scheduledDate = await getScheduledDateSeconds({
      scheduleDate: "p1",
      config,
      baseDate: mockCurrentDate,
    });
    expect(scheduledDate).toBeDefined();
    // It should be 9AM, so check it's more than 3300 sec (55 min) and less than 3600 sec (1 hour)
    // If this is true, then it means the post was likely scheduled for 9AM.
    expect(scheduledDate).toBeGreaterThan(3300);
    expect(scheduledDate).toBeLessThan(3600);
  });
});

describe.only("Priority R1 get scheduled date", () => {
  it("returns exact number of requested future dates with one per week when none are taken", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 3;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      // Week 1: Monday 16:00 UTC
      [
        new Date("2025-01-20T15:55:00.000Z"),
        new Date("2025-01-20T16:05:00.000Z"),
      ],
      // Week 2: Monday 16:00 UTC
      [
        new Date("2025-01-27T15:55:00.000Z"),
        new Date("2025-01-27T16:05:00.000Z"),
      ],
      // Week 3: Monday 16:00 UTC
      [
        new Date("2025-02-03T15:55:00.000Z"),
        new Date("2025-02-03T16:05:00.000Z"),
      ],
    ];

    // Verify each date is in the expected range
    result.forEach((date, i) => {
      expect(date.getTime()).toBeGreaterThan(
        expectedDateRanges[i][0].getTime(),
      );
      expect(date.getTime()).toBeLessThan(expectedDateRanges[i][1].getTime());
    });

    // Verify dates are at least 7 days apart
    for (let i = 1; i < result.length; i++) {
      const daysDiff =
        (result[i].getTime() - result[i - 1].getTime()) / (1000 * 60 * 60 * 24);
      expect(daysDiff).toBeGreaterThanOrEqual(7);
    }
  });

  it("skips taken dates and finds next available slot in the week", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r1: [
        // Monday slots are taken
        new Date("2025-01-20T16:00:00.000Z"),
        new Date("2025-01-20T17:00:00.000Z"),
        new Date("2025-01-20T18:00:00.000Z"),

        // Next week's Monday is also taken
        new Date("2025-01-27T16:00:00.000Z"),
        new Date("2025-01-27T17:00:00.000Z"),
        new Date("2025-01-27T18:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      // Week 1: Should get Tuesday since Monday is taken
      [
        new Date("2025-01-21T15:55:00.000Z"),
        new Date("2025-01-21T16:05:00.000Z"),
      ],
      // Week 2: Should get Tuesday since Monday is taken
      [
        new Date("2025-01-28T15:55:00.000Z"),
        new Date("2025-01-28T16:05:00.000Z"),
      ],
      // Week 3: Should get Monday (first available)
      [
        new Date("2025-02-03T15:55:00.000Z"),
        new Date("2025-02-03T16:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result).toHaveLength(numDates);

    // Verify each date is in the expected range
    result.forEach((date, i) => {
      expect(date.getTime()).toBeGreaterThan(
        expectedDateRanges[i][0].getTime(),
      );
      expect(date.getTime()).toBeLessThan(expectedDateRanges[i][1].getTime());
    });
  });

  it("skips entire days if there are already taken dates", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r1: [
        new Date("2025-01-20T16:00:00.000Z"),
        new Date("2025-01-20T17:00:00.000Z"),
        new Date("2025-01-20T18:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-21T15:55:00.000Z"),
        new Date("2025-01-21T16:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T15:55:00.000Z"),
        new Date("2025-01-27T16:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );
  });

  it("only returns allowed day/hour combos", () => {
    const baseDate = new Date(Date.UTC(2025, 0, 6, 0));
    const numDates = 10;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });

    // Allowed days are Monday(1)-Friday(5) and hours 16-19
    result.forEach((date) => {
      const day = date.getUTCDay();
      const hour = date.getUTCHours();
      expect(day).toBeGreaterThanOrEqual(1);
      expect(day).toBeLessThanOrEqual(5);
      expect(hour).toBe(16); // Since there are no taken slots, each slot should be 16:00
    });
  });

  it("does not return dates in the past", () => {
    // baseDate is mid-week in the afternoon, some allowed slots have passed
    const baseDate = new Date(Date.UTC(2025, 0, 8, 18));
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const numDates = 5;
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Every returned date should be strictly after baseDate
    result.forEach((d) => {
      expect(d.getTime()).toBeGreaterThan(baseDate.getTime());
    });
  });

  it("ignores weekends entirely", () => {
    // baseDate is set on Saturday
    const baseDate = new Date(Date.UTC(2025, 0, 11, 10));
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Should push to Monday
    expect(result[0].getUTCDay()).toBe(1);
  });

  it("can find dates when the base date is after all taken dates", () => {
    const baseDate = new Date("2025-01-22T15:00:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r1: [
        new Date("2025-01-20T16:00:00.000Z"),
        new Date("2025-01-20T17:00:00.000Z"),
        new Date("2025-01-20T18:00:00.000Z"),

        new Date("2025-01-21T16:00:00.000Z"),
        new Date("2025-01-21T17:00:00.000Z"),
        new Date("2025-01-21T18:00:00.000Z"),

        new Date("2025-01-22T16:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-22T16:55:00.000Z"),
        new Date("2025-01-22T17:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T15:55:00.000Z"),
        new Date("2025-01-27T16:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T15:55:00.000Z"),
        new Date("2025-02-03T16:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("can find dates when the base date time is after allowed times", () => {
    // Base date is 5 min past the last allowed hour. it should return dates starting on the next day
    const baseDate = new Date("2025-01-22T18:05:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r1: [
        new Date("2025-01-20T16:00:00.000Z"),
        new Date("2025-01-20T17:00:00.000Z"),
        new Date("2025-01-20T18:00:00.000Z"),

        new Date("2025-01-21T16:00:00.000Z"),
        new Date("2025-01-21T17:00:00.000Z"),
        new Date("2025-01-21T18:00:00.000Z"),

        new Date("2025-01-22T16:00:00.000Z"),
        new Date("2025-01-22T17:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-23T15:55:00.000Z"),
        new Date("2025-01-23T16:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T15:55:00.000Z"),
        new Date("2025-01-27T16:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T15:55:00.000Z"),
        new Date("2025-02-03T16:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("works when a custom week offset is specified", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r1",
      baseDate,
      numberOfDates: numDates,
      takenDates,
      numWeeksBetween: 2,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      // Week 1: Monday 16:00 UTC
      [
        new Date("2025-01-20T15:55:00.000Z"),
        new Date("2025-01-20T16:05:00.000Z"),
      ],
      // Week 2: Monday 16:00 UTC
      // SKIP
      // Week 3: Monday 16:00 UTC
      [
        new Date("2025-02-03T15:55:00.000Z"),
        new Date("2025-02-03T16:05:00.000Z"),
      ],
    ];

    // Verify each date is in the expected range
    result.forEach((date, i) => {
      expect(date.getTime()).toBeGreaterThan(
        expectedDateRanges[i][0].getTime(),
      );
      expect(date.getTime()).toBeLessThan(expectedDateRanges[i][1].getTime());
    });

    // Verify dates are at least 7 days apart
    for (let i = 1; i < result.length; i++) {
      const daysDiff =
        (result[i].getTime() - result[i - 1].getTime()) / (1000 * 60 * 60 * 24);
      expect(daysDiff).toBeGreaterThanOrEqual(7);
    }
  });
});

describe.only("Priority R2 get scheduled date", () => {
  it("returns exact number of requested future dates when none are taken", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 3;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());
    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      // After 15:55 and before 16:05 (should be 16:00 but dont want to check for exact seconds.)
      [
        new Date("2025-01-20T18:55:00.000Z"),
        new Date("2025-01-20T19:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T18:55:00.000Z"),
        new Date("2025-01-27T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T18:55:00.000Z"),
        new Date("2025-02-03T19:05:00.000Z"),
      ],
    ];

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("skips already taken hours", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 5;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r2: [
        new Date("2025-01-20T19:00:00.000Z"),
        new Date("2025-01-20T20:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-20T20:55:00.000Z"),
        new Date("2025-01-20T21:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T18:55:00.000Z"),
        new Date("2025-01-27T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T18:55:00.000Z"),
        new Date("2025-02-03T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-10T18:55:00.000Z"),
        new Date("2025-02-10T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-17T18:55:00.000Z"),
        new Date("2025-02-17T19:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );

    expect(result[3].getTime()).toBeGreaterThan(
      expectedDateRanges[3][0].getTime(),
    );
    expect(result[3].getTime()).toBeLessThan(
      expectedDateRanges[3][1].getTime(),
    );

    expect(result[4].getTime()).toBeGreaterThan(
      expectedDateRanges[4][0].getTime(),
    );
    expect(result[4].getTime()).toBeLessThan(
      expectedDateRanges[4][1].getTime(),
    );
  });

  it("skips entire days if there are already taken dates", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r2: [
        new Date("2025-01-20T19:00:00.000Z"),
        new Date("2025-01-20T20:00:00.000Z"),
        new Date("2025-01-20T21:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-21T18:55:00.000Z"),
        new Date("2025-01-21T19:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T18:55:00.000Z"),
        new Date("2025-01-27T19:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );
  });

  it("only returns allowed day/hour combos", () => {
    const baseDate = new Date(Date.UTC(2025, 0, 6, 0));
    const numDates = 10;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });

    // Allowed days are Monday(1)-Friday(5) and hours 16-19
    result.forEach((date) => {
      const day = date.getUTCDay();
      const hour = date.getUTCHours();
      expect(day).toBeGreaterThanOrEqual(1);
      expect(day).toBeLessThanOrEqual(5);
      expect(hour).toBe(19); // Since there are no taken slots, each slot should be 16:00
    });
  });

  it("does not return dates in the past", () => {
    // baseDate is mid-week in the afternoon, some allowed slots have passed
    const baseDate = new Date(Date.UTC(2025, 0, 8, 18));
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const numDates = 5;
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Every returned date should be strictly after baseDate
    result.forEach((d) => {
      expect(d.getTime()).toBeGreaterThan(baseDate.getTime());
    });
  });

  it("ignores weekends entirely", () => {
    // baseDate is set on Saturday
    const baseDate = new Date(Date.UTC(2025, 0, 11, 10));
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Should push to Monday
    expect(result[0].getUTCDay()).toBe(1);
  });

  it("can find dates when the base date is after all taken dates", () => {
    const baseDate = new Date("2025-01-22T15:00:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r2: [
        new Date("2025-01-20T19:00:00.000Z"),
        new Date("2025-01-20T20:00:00.000Z"),
        new Date("2025-01-20T21:00:00.000Z"),

        new Date("2025-01-21T19:00:00.000Z"),
        new Date("2025-01-21T20:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-22T18:55:00.000Z"),
        new Date("2025-01-22T19:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T18:55:00.000Z"),
        new Date("2025-01-27T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T18:55:00.000Z"),
        new Date("2025-02-03T19:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("can find dates when the base date time is after allowed times", () => {
    // Base date is 5 min past the last allowed hour. it should return dates starting on the next day
    const baseDate = new Date("2025-01-22T21:05:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r2: [
        new Date("2025-01-20T19:00:00.000Z"),
        new Date("2025-01-20T20:00:00.000Z"),
        new Date("2025-01-20T21:00:00.000Z"),

        new Date("2025-01-21T19:00:00.000Z"),
        new Date("2025-01-21T20:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-23T18:55:00.000Z"),
        new Date("2025-01-23T19:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T18:55:00.000Z"),
        new Date("2025-01-27T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T18:55:00.000Z"),
        new Date("2025-02-03T19:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("works when a custom week offset is specified", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r2",
      baseDate,
      numberOfDates: numDates,
      takenDates,
      numWeeksBetween: 2,
    }).sort((a, b) => a.getTime() - b.getTime());
    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      // After 15:55 and before 16:05 (should be 16:00 but dont want to check for exact seconds.)
      [
        new Date("2025-01-20T18:55:00.000Z"),
        new Date("2025-01-20T19:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T18:55:00.000Z"),
        new Date("2025-02-03T19:05:00.000Z"),
      ],
    ];

    // Verify each date is in the expected range
    result.forEach((date, i) => {
      expect(date.getTime()).toBeGreaterThan(
        expectedDateRanges[i][0].getTime(),
      );
      expect(date.getTime()).toBeLessThan(expectedDateRanges[i][1].getTime());
    });
  });
});

describe.only("Priority R3 get scheduled date", () => {
  it("returns exact number of requested future dates when none are taken", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 3;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());
    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      // After 15:55 and before 16:05 (should be 16:00 but dont want to check for exact seconds.)
      [
        new Date("2025-01-20T21:55:00.000Z"),
        new Date("2025-01-20T22:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T21:55:00.000Z"),
        new Date("2025-01-27T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T21:55:00.000Z"),
        new Date("2025-02-03T22:05:00.000Z"),
      ],
    ];

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("skips already taken hours", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 5;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r3: [
        new Date("2025-01-20T22:00:00.000Z"),
        new Date("2025-01-20T23:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-20T23:55:00.000Z"),
        new Date("2025-01-21T00:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T21:55:00.000Z"),
        new Date("2025-01-27T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T21:55:00.000Z"),
        new Date("2025-02-03T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-10T21:55:00.000Z"),
        new Date("2025-02-10T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-17T21:55:00.000Z"),
        new Date("2025-02-17T22:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );

    expect(result[3].getTime()).toBeGreaterThan(
      expectedDateRanges[3][0].getTime(),
    );
    expect(result[3].getTime()).toBeLessThan(
      expectedDateRanges[3][1].getTime(),
    );

    expect(result[4].getTime()).toBeGreaterThan(
      expectedDateRanges[4][0].getTime(),
    );
    expect(result[4].getTime()).toBeLessThan(
      expectedDateRanges[4][1].getTime(),
    );
  });

  it("skips entire days if there are already taken dates", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r3: [
        new Date("2025-01-20T22:00:00.000Z"),
        new Date("2025-01-20T23:00:00.000Z"),
        new Date("2025-01-21T00:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-21T21:55:00.000Z"),
        new Date("2025-01-21T22:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T21:55:00.000Z"),
        new Date("2025-01-27T22:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );
  });

  it("only returns allowed day/hour combos", () => {
    const baseDate = new Date(Date.UTC(2025, 0, 6, 0));
    const numDates = 10;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });

    // Allowed days are Monday(1)-Friday(5) and hours 22-00
    result.forEach((date) => {
      const day = date.getUTCDay();
      const hour = date.getUTCHours();
      expect(day).toBeGreaterThanOrEqual(1);
      expect(day).toBeLessThanOrEqual(5);
      expect(hour).toBe(22); // Since there are no taken slots, each slot should be 22:00
    });
  });

  it("does not return dates in the past", () => {
    // baseDate is mid-week in the afternoon, some allowed slots have passed
    const baseDate = new Date(Date.UTC(2025, 0, 8, 18));
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const numDates = 5;
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Every returned date should be strictly after baseDate
    result.forEach((d) => {
      expect(d.getTime()).toBeGreaterThan(baseDate.getTime());
    });
  });

  it("ignores weekends entirely", () => {
    // baseDate is set on Saturday
    const baseDate = new Date(Date.UTC(2025, 0, 11, 10));
    const numDates = 2;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
    };
    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    });
    // Should push to Monday
    expect(result[0].getUTCDay()).toBe(1);
  });

  it("can find dates when the base date is after all taken dates", () => {
    const baseDate = new Date("2025-01-22T15:00:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r3: [
        new Date("2025-01-20T22:00:00.000Z"),
        new Date("2025-01-20T23:00:00.000Z"),
        new Date("2025-01-21T00:00:00.000Z"),

        new Date("2025-01-21T22:00:00.000Z"),
        new Date("2025-01-21T23:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-22T21:55:00.000Z"),
        new Date("2025-01-22T22:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T21:55:00.000Z"),
        new Date("2025-01-27T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T21:55:00.000Z"),
        new Date("2025-02-03T22:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("can find dates when the base date time is after allowed times", () => {
    // Base date is 5 min past the last allowed hour. it should return dates starting on the next day (same dat since UTC)
    const baseDate = new Date("2025-01-22T00:05:00.000Z");
    const numDates = 3;
    const takenDates = {
      ...DEFAULT_TAKEN_DATES,
      r3: [
        new Date("2025-01-20T22:00:00.000Z"),
        new Date("2025-01-20T23:00:00.000Z"),
        new Date("2025-01-21T00:00:00.000Z"),

        new Date("2025-01-21T22:00:00.000Z"),
        new Date("2025-01-21T23:00:00.000Z"),
      ],
    };

    const expectedDateRanges = [
      [
        new Date("2025-01-22T21:55:00.000Z"),
        new Date("2025-01-22T22:05:00.000Z"),
      ],
      [
        new Date("2025-01-27T21:55:00.000Z"),
        new Date("2025-01-27T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T21:55:00.000Z"),
        new Date("2025-02-03T22:05:00.000Z"),
      ],
    ];

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
    }).sort((a, b) => a.getTime() - b.getTime());

    expect(result[0].getTime()).toBeGreaterThan(
      expectedDateRanges[0][0].getTime(),
    );
    expect(result[0].getTime()).toBeLessThan(
      expectedDateRanges[0][1].getTime(),
    );

    expect(result[1].getTime()).toBeGreaterThan(
      expectedDateRanges[1][0].getTime(),
    );
    expect(result[1].getTime()).toBeLessThan(
      expectedDateRanges[1][1].getTime(),
    );

    expect(result[2].getTime()).toBeGreaterThan(
      expectedDateRanges[2][0].getTime(),
    );
    expect(result[2].getTime()).toBeLessThan(
      expectedDateRanges[2][1].getTime(),
    );
  });

  it("works when a custom week offset is specified", () => {
    // baseDate is Monday at 15:00 UTC, just before the first allowed slot
    const baseDate = new Date("2025-01-20T15:00:00.000Z");
    const numDates = 2;
    const takenDates = DEFAULT_TAKEN_DATES;

    const result = findAvailableRepurposeDates({
      repurposedPriority: "r3",
      baseDate,
      numberOfDates: numDates,
      takenDates,
      numWeeksBetween: 2,
    }).sort((a, b) => a.getTime() - b.getTime());
    expect(result).toHaveLength(numDates);

    const expectedDateRanges = [
      [
        new Date("2025-01-20T21:55:00.000Z"),
        new Date("2025-01-20T22:05:00.000Z"),
      ],
      [
        new Date("2025-02-03T21:55:00.000Z"),
        new Date("2025-02-03T22:05:00.000Z"),
      ],
    ];

    // Verify each date is in the expected range
    result.forEach((date, i) => {
      expect(date.getTime()).toBeGreaterThan(
        expectedDateRanges[i][0].getTime(),
      );
      expect(date.getTime()).toBeLessThan(expectedDateRanges[i][1].getTime());
    });
  });
});




================================================
FILE: .github/workflows/ci.yml
================================================
# Run formatting on all PRs

name: CI

on:
  push:
    branches: ["main"]
  pull_request:
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
#
# There's no point in testing an outdated version of the code. GitHub only allows
# a limited number of job runners to be active at the same time, so it's better to cancel
# pointless jobs early so that more useful jobs can run sooner.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  format:
    name: Check formatting
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x
          cache: "yarn"
      - name: Install dependencies
        run: yarn install --immutable --mode=skip-build
      - name: Check formatting
        run: yarn format:check

  lint:
    name: Check linting
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Use Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: 18.x
          cache: "yarn"
      - name: Install dependencies
        run: yarn install --immutable --mode=skip-build
      - name: Check linting
        run: yarn run lint:all

  readme-spelling:
    name: Check README spelling
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: README.md

  check-spelling:
    name: Check code spelling
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: src



================================================
FILE: .github/workflows/integration-tests.yml
================================================
# This workflow will run integration tests for the current project once per day

name: Integration Tests

on:
  schedule:
    - cron: "37 14 * * *" # Run at 7:37 AM Pacific Time (14:37 UTC) every day
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another scheduled run starts while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  integration-tests:
    name: Integration Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        node-version: [18.x, 20.x]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: "yarn"

      - name: Install dependencies
        run: yarn install --immutable

      - name: Build project
        run: yarn build

      - name: Run integration tests
        run: yarn test:int



================================================
FILE: .github/workflows/unit-tests.yml
================================================
# This workflow will run unit tests for the current project

name: Unit Tests

on:
  push:
    branches: ["main"]
  pull_request:
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        node-version: [18.x, 20.x]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v3
        with:
          node-version: ${{ matrix.node-version }}
          cache: "yarn"

      - name: Install dependencies
        run: yarn install --immutable

      - name: Build project
        run: yarn build

      - name: Run tests
        run: yarn test


