Directory structure:
â””â”€â”€ langchain-ai-open_deep_research/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ langgraph.json
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ arxiv.md
    â”‚   â”œâ”€â”€ inference-market-gpt45.md
    â”‚   â”œâ”€â”€ inference-market.md
    â”‚   â””â”€â”€ pubmed.md
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ open_deep_research/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ configuration.py
    â”‚       â”œâ”€â”€ graph.ipynb
    â”‚       â”œâ”€â”€ graph.py
    â”‚       â”œâ”€â”€ multi_agent.ipynb
    â”‚       â”œâ”€â”€ multi_agent.py
    â”‚       â”œâ”€â”€ prompts.py
    â”‚       â”œâ”€â”€ state.py
    â”‚       â””â”€â”€ utils.py
    â””â”€â”€ tests/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ conftest.py
        â”œâ”€â”€ run_test.py
        â””â”€â”€ test_report_quality.py

================================================
FILE: README.md
================================================
# Open Deep Research

Open Deep Research is an experimental, fully open-source research assistant that automates deep research and produces comprehensive reports on any topic. It features two implementations - a [workflow](https://langchain-ai.github.io/langgraph/tutorials/workflows/) and a multi-agent architecture - each with distinct advantages. You can customize the entire research and writing process with specific models, prompts, report structure, and search tools.

#### Workflow

![open-deep-research-overview](https://github.com/user-attachments/assets/a171660d-b735-4587-ab2f-cd771f773756)

####  Multi-agent

![multi-agent-researcher](https://github.com/user-attachments/assets/3c734c3c-57aa-4bc0-85dd-74e2ec2c0880)

### ðŸš€ Quickstart

Clone the repository:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
```

Then edit the `.env` file to customize the environment variables (for model selection, search tools, and other configuration settings):
```bash
cp .env.example .env
```

Launch the assistant with the LangGraph server locally, which will open in your browser:

#### Mac

```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
```

#### Windows / Linux

```powershell
# Install dependencies 
pip install -e .
pip install -U "langgraph-cli[inmem]" 

# Start the LangGraph server
langgraph dev
```

Use this to open the Studio UI:
```
- ðŸš€ API: http://127.0.0.1:2024
- ðŸŽ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ðŸ“š API Docs: http://127.0.0.1:2024/docs
```

#### Multi-agent

(1) Chat with the agent about your topic of interest, and it will initiate report generation:

<img width="1326" alt="input" src="https://github.com/user-attachments/assets/dc8f59dd-14b3-4a62-ac18-d2f99c8bbe83" />

(2) The report is produced as markdown.

#### Workflow

(1) Provide a `Topic`:

<img width="1326" alt="input" src="https://github.com/user-attachments/assets/de264b1b-8ea5-4090-8e72-e1ef1230262f" />

(2) This will generate a report plan and present it to the user for review.

(3) We can pass a string (`"..."`) with feedback to regenerate the plan based on the feedback.

<img width="1326" alt="feedback" src="https://github.com/user-attachments/assets/c308e888-4642-4c74-bc78-76576a2da919" />

(4) Or, we can just pass `true` to the JSON input box in Studio accept the plan.

<img width="1480" alt="accept" src="https://github.com/user-attachments/assets/ddeeb33b-fdce-494f-af8b-bd2acc1cef06" />

(5) Once accepted, the report sections will be generated.

<img width="1326" alt="report_gen" src="https://github.com/user-attachments/assets/74ff01cc-e7ed-47b8-bd0c-4ef615253c46" />

The report is produced as markdown.

<img width="1326" alt="report" src="https://github.com/user-attachments/assets/92d9f7b7-3aea-4025-be99-7fb0d4b47289" />

### Search Tools

Available search tools:

* [Tavily API](https://tavily.com/) - General web search
* [Perplexity API](https://www.perplexity.ai/hub/blog/introducing-the-sonar-pro-api) - General web search
* [Exa API](https://exa.ai/) - Powerful neural search for web content
* [ArXiv](https://arxiv.org/) - Academic papers in physics, mathematics, computer science, and more
* [PubMed](https://pubmed.ncbi.nlm.nih.gov/) - Biomedical literature from MEDLINE, life science journals, and online books
* [Linkup API](https://www.linkup.so/) - General web search
* [DuckDuckGo API](https://duckduckgo.com/) - General web search
* [Google Search API/Scrapper](https://google.com/) - Create custom search engine [here](https://programmablesearchengine.google.com/controlpanel/all) and get API key [here](https://developers.google.com/custom-search/v1/introduction)

Open Deep Research is compatible with many different LLMs: 

* You can select any model that is integrated [with the `init_chat_model()` API](https://python.langchain.com/docs/how_to/chat_models_universal_init/)
* See full list of supported integrations [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html)

### Using the package

```bash
pip install open-deep-research
```

See [src/open_deep_research/graph.ipynb](src/open_deep_research/graph.ipynb) and [src/open_deep_research/multi_agent.ipynb](src/open_deep_research/multi_agent.ipynb) for example usage in a Jupyter notebook:

## Open Deep Research Implementations

Open Deep Research features two distinct implementation approaches, each with its own strengths:

## 1. Graph-based Workflow Implementation (`src/open_deep_research/graph.py`)

The graph-based implementation follows a structured plan-and-execute workflow:

- **Planning Phase**: Uses a planner model to analyze the topic and generate a structured report plan
- **Human-in-the-Loop**: Allows for human feedback and approval of the report plan before proceeding
- **Sequential Research Process**: Creates sections one by one with reflection between search iterations
- **Section-Specific Research**: Each section has dedicated search queries and content retrieval
- **Supports Multiple Search Tools**: Works with all search providers (Tavily, Perplexity, Exa, ArXiv, PubMed, Linkup, etc.)

This implementation provides a more interactive experience with greater control over the report structure, making it ideal for situations where report quality and accuracy are critical.

You can customize the research assistant workflow through several parameters:

- `report_structure`: Define a custom structure for your report (defaults to a standard research report format)
- `number_of_queries`: Number of search queries to generate per section (default: 2)
- `max_search_depth`: Maximum number of reflection and search iterations (default: 2)
- `planner_provider`: Model provider for planning phase (default: "anthropic", but can be any provider from supported integrations with `init_chat_model` as listed [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html))
- `planner_model`: Specific model for planning (default: "claude-3-7-sonnet-latest")
- `planner_model_kwargs`: Additional parameter for planner_model
- `writer_provider`: Model provider for writing phase (default: "anthropic", but can be any provider from supported integrations with `init_chat_model` as listed [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html))
- `writer_model`: Model for writing the report (default: "claude-3-5-sonnet-latest")
- `writer_model_kwargs`: Additional parameter for writer_model
- `search_api`: API to use for web searches (default: "tavily", options include "perplexity", "exa", "arxiv", "pubmed", "linkup")

## 2. Multi-Agent Implementation (`src/open_deep_research/multi_agent.py`)

The multi-agent implementation uses a supervisor-researcher architecture:

- **Supervisor Agent**: Manages the overall research process, plans sections, and assembles the final report
- **Researcher Agents**: Multiple independent agents work in parallel, each responsible for researching and writing a specific section
- **Parallel Processing**: All sections are researched simultaneously, significantly reducing report generation time
- **Specialized Tool Design**: Each agent has access to specific tools for its role (search for researchers, section planning for supervisors)
- **Currently Limited to Tavily Search**: The multi-agent implementation currently only works with Tavily for search, though the framework is designed to support additional search tools in the future

This implementation focuses on efficiency and parallelization, making it ideal for faster report generation with less direct user involvement.

## Search API Configuration

Not all search APIs support additional configuration parameters. Here are the ones that do:

- **Exa**: `max_characters`, `num_results`, `include_domains`, `exclude_domains`, `subpages`
  - Note: `include_domains` and `exclude_domains` cannot be used together
  - Particularly useful when you need to narrow your research to specific trusted sources, ensure information accuracy, or when your research requires using specified domains (e.g., academic journals, government sites)
  - Provides AI-generated summaries tailored to your specific query, making it easier to extract relevant information from search results
- **ArXiv**: `load_max_docs`, `get_full_documents`, `load_all_available_meta`
- **PubMed**: `top_k_results`, `email`, `api_key`, `doc_content_chars_max`
- **Linkup**: `depth`

Example with Exa configuration:
```python
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "exa",
                           "search_api_config": {
                               "num_results": 5,
                               "include_domains": ["nature.com", "sciencedirect.com"]
                           },
                           # Other configuration...
                           }}
```

## Model Considerations

(1) You can use models supported with [the `init_chat_model()` API](https://python.langchain.com/docs/how_to/chat_models_universal_init/). See full list of supported integrations [here](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html).

(2) ***The workflow planner and writer models need to support structured outputs***: Check whether structured outputs are supported by the model you are using [here](https://python.langchain.com/docs/integrations/chat/).

(3) ***The agent models need to support tool calling:*** Ensure tool calling is well supoorted; tests have been done with Claude 3.7, o3, o3-mini, and gpt4.1. See [here](https://smith.langchain.com/public/adc5d60c-97ee-4aa0-8b2c-c776fb0d7bd6/d).

(4) With Groq, there are token per minute (TPM) limits if you are on the `on_demand` service tier:
- The `on_demand` service tier has a limit of `6000 TPM`
- You will want a [paid plan](https://github.com/cline/cline/issues/47#issuecomment-2640992272) for section writing with Groq models

(5) `deepseek-R1` [is not strong at function calling](https://api-docs.deepseek.com/guides/reasoning_model), which the assistant uses to generate structured outputs for report sections and report section grading. See example traces [here](https://smith.langchain.com/public/07d53997-4a6d-4ea8-9a1f-064a85cd6072/r).  
- Consider providers that are strong at function calling such as OpenAI, Anthropic, and certain OSS models like Groq's `llama-3.3-70b-versatile`.
- If you see the following error, it is likely due to the model not being able to produce structured outputs (see [trace](https://smith.langchain.com/public/8a6da065-3b8b-4a92-8df7-5468da336cbe/r)):
```
groq.APIError: Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.
```

(6) Follow [here[(https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408) to use with OpenRouter.

(7) For working with local models via Ollama, see [here](https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318).

## Testing Report Quality

To compare the quality of reports generated by both implementations:

```bash
# Test with default Anthropic models
python tests/run_test.py --all

# Test with OpenAI o3 models
python tests/run_test.py --all \
  --supervisor-model "openai:o3" \
  --researcher-model "openai:o3" \
  --planner-provider "openai" \
  --planner-model "o3" \
  --writer-provider "openai" \
  --writer-model "o3" \
  --eval-model "openai:o3" \
  --search-api "tavily"
```

The test results will be logged to LangSmith, allowing you to compare the quality of reports generated by each implementation with different model configurations.

## UX

### Local deployment

Follow the [quickstart](#-quickstart) to start LangGraph server locally.

### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 



================================================
FILE: langgraph.json
================================================
{
    "dockerfile_lines": [],
    "graphs": {
      "open_deep_research": "./src/open_deep_research/graph.py:graph",
      "open_deep_research_multi_agent": "./src/open_deep_research/multi_agent.py:graph"
    },
    "python_version": "3.11",
    "env": "./.env",
    "dependencies": [
      "."
    ]
  }


================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 LangChain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
FILE: pyproject.toml
================================================
[project]
name = "open_deep_research"
version = "0.0.15"
description = "Planning, research, and report generation."
authors = [
    { name = "Lance Martin" }
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.10"
dependencies = [
    "langgraph>=0.2.55",
    "langchain-community>=0.3.9",
    "langchain-openai>=0.3.7",
    "langchain-anthropic>=0.3.9",
    "openai>=1.61.0",
    "tavily-python>=0.5.0",
    "langchain-groq>=0.2.4",
    "arxiv>=2.1.3",
    "pymupdf>=1.25.3",
    "xmltodict>=0.14.2",
    "linkup-sdk>=0.2.3",
    "duckduckgo-search>=3.0.0",
    "exa-py>=1.8.8",
    "requests>=2.32.3",
    "beautifulsoup4==4.13.3",
    "langchain-deepseek>=0.1.2",
    "python-dotenv==1.0.1",
    "langgraph_supervisor",
    "langchain_tavily",
    "pytest",
    "httpx>=0.24.0",
    "markdownify>=0.11.6"
]

[project.optional-dependencies]
dev = ["mypy>=1.11.1", "ruff>=0.6.1"]

[build-system]
requires = ["setuptools>=73.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["open_deep_research"]

[tool.setuptools.package-dir]
"open_deep_research" = "src/open_deep_research"

[tool.setuptools.package-data]
"*" = ["py.typed"]

[tool.ruff]
lint.select = [
    "E",    # pycodestyle
    "F",    # pyflakes
    "I",    # isort
    "D",    # pydocstyle
    "D401", # First line should be in imperative mood
    "T201",
    "UP",
]
lint.ignore = [
    "UP006",
    "UP007",
    "UP035",
    "D417",
    "E501",
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["D", "UP"]

[tool.ruff.lint.pydocstyle]
convention = "google"



================================================
FILE: .env.example
================================================
OPENAI_API_KEY=sk-xxx
ANTHROPIC_API_KEY=sk-xxx
TAVILY_API_KEY=xxx
GROQ_API_KEY=xxx
PERPLEXITY_API_KEY=xxx
LINKUP_API_KEY=xxx



================================================
FILE: examples/arxiv.md
================================================
# Obesity Among Young Adults in the United States: A Growing Public Health Challenge

The obesity epidemic among young adults in the United States represents a complex public health crisis shaped by interconnected social, economic, and environmental factors. Recent research reveals that over one-third of US adults suffer from obesity, with rates disproportionately affecting disadvantaged communities. This health challenge extends beyond individual choices, as built environment characteristics and socioeconomic conditions explain up to 90% of obesity prevalence variation across American cities. Understanding these systemic influences is crucial for developing effective interventions that address both individual and community-level factors contributing to obesity among young adults.

## Obesity Prevalence and Trends in US Young Adults

**Over one-third of US adults suffer from obesity, with the condition showing strong correlations to socioeconomic and environmental factors that disproportionately affect disadvantaged communities.** National data reveals systematic variations in obesity rates that map closely to neighborhood characteristics and built environment features.

Advanced analysis using satellite imagery and machine learning has demonstrated that built environment characteristics explain 72-90% of obesity prevalence variation at the census tract level across major US cities. These correlations are particularly pronounced in disadvantaged neighborhoods where multiple social determinants of health intersect.

Key factors associated with higher adult obesity rates include:
- Lower median household income
- Limited health insurance coverage
- Higher concentration of rental housing
- Reduced access to physical activity resources
- Higher poverty rates

A comprehensive study in Shelby County, Tennessee exemplifies these patterns, showing significantly higher obesity prevalence in areas with multiple socioeconomic challenges. The findings suggest that addressing structural and environmental factors may be as crucial as individual interventions for reducing obesity rates.

### Sources
- Association Between Neighborhood Factors and Adult Obesity in Shelby County, Tennessee (2022): http://arxiv.org/abs/2208.05335v1
- Using Deep Learning to Examine the Association between the Built Environment and Neighborhood Adult Obesity Prevalence (2017): http://arxiv.org/abs/1711.00885v1
- Progress of the anti-obesity of Berberine (2025): http://arxiv.org/abs/2501.02282v1

## Socioeconomic Determinants of Obesity in Young Adults

**Social and economic disparities create stark differences in obesity prevalence among young adults, with disadvantaged neighborhoods showing up to 90% higher rates compared to affluent areas.** Research from Shelby County, Tennessee demonstrates how multiple socioeconomic factors intersect to influence obesity risk through both direct and indirect pathways.

Key social determinants shaping obesity outcomes include:
* Median household income - Affects access to healthy food options
* Insurance status - Determines preventive care availability
* Housing conditions - Influences exposure to obesity-promoting environments
* Education level - Impacts health literacy and dietary choices
* Geographic location - Correlates with neighborhood resources

Advanced geospatial analysis reveals that built environment characteristics explain 72-90% of obesity variation across cities. In Shelby County, census tracts with higher percentages of uninsured residents, home renters, and individuals living below the poverty level demonstrated significantly elevated obesity rates.

These findings emphasize the need for obesity interventions that address systemic inequalities rather than focusing solely on individual behavior modification. Public health initiatives must consider how social determinants create barriers to healthy weight maintenance.

### Sources
- Association Between Neighborhood Factors and Adult Obesity in Shelby County, Tennessee: http://arxiv.org/abs/2208.05335v1
- Using Deep Learning to Examine the Association between the Built Environment and Neighborhood Adult Obesity Prevalence: http://arxiv.org/abs/1711.00885v1

## Built Environment's Impact on Obesity

**The physical design of urban spaces significantly influences obesity rates, with walkability and food accessibility emerging as critical factors that can increase obesity risk by up to 42% in underserved areas.** Research demonstrates that neighborhood characteristics create complex ecosystems affecting dietary health and physical activity patterns.

The built environment shapes obesity risk through three primary mechanisms: food accessibility, physical activity opportunities, and socioeconomic factors. Studies reveal that areas with limited walkability and higher concentrations of fast-food establishments, particularly through online food delivery platforms, create "cyber food swamps" that contribute to unhealthy dietary choices. A 10% increase in accessible fast-food options raises the probability of unhealthy food orders by 22%.

Key built environment factors affecting obesity include:
* Walking infrastructure and neighborhood walkability
* Distance to healthy food retailers versus fast food
* Availability of recreational facilities
* Transportation access
* Socioeconomic status of the area

Recent research in tertiary education campuses demonstrates that improving walkability can increase positive walking experiences by 9.75%, suggesting that targeted modifications to the built environment could help reduce obesity rates.

### Sources
- Using Tableau and Google Map API for Understanding the Impact of Walkability on Dublin City: http://arxiv.org/abs/2310.07563v1
- Exploring the Causal Relationship between Walkability and Affective Walking Experience: http://arxiv.org/abs/2311.06262v1
- Cyber Food Swamps: Investigating the Impacts of Online-to-Offline Food Delivery Platforms: http://arxiv.org/abs/2409.16601v2
- The association between neighborhood obesogenic factors and prostate cancer risk and mortality: http://arxiv.org/abs/2405.18456v1

## Machine Learning Applications in Obesity Analysis

**Advanced machine learning and deep learning techniques are revolutionizing obesity research by uncovering complex patterns in environmental, behavioral, and socioeconomic factors, with prediction accuracies reaching up to 88% for adolescent obesity risk.**

Recent studies using deep learning analysis of satellite imagery have demonstrated that built environment features can explain 72-90% of obesity prevalence variation across U.S. cities. This breakthrough enables automated assessment of neighborhood characteristics that influence obesity rates at the census tract level.

Machine learning models have identified key social determinants of health strongly correlated with adult obesity, including:
* Median household income
* Housing status (rental vs. ownership)
* Insurance coverage
* Race and ethnicity demographics
* Age distribution
* Marital status

Novel applications include DeepHealthNet, which achieves 88.4% accuracy in adolescent obesity prediction by analyzing physical activity patterns and health metrics. Similarly, recurrent neural networks analyzing longitudinal patient records and wearable device data have achieved 77-86% accuracy in predicting obesity status improvements.

These insights are particularly valuable for public health decision-making, enabling targeted interventions in disadvantaged neighborhoods where obesity prevalence is significantly higher.

### Sources
- Using Deep Learning to Examine the Built Environment and Neighborhood Adult Obesity: http://arxiv.org/abs/1711.00885v1
- DeepHealthNet: Adolescent Obesity Prediction System: http://arxiv.org/abs/2308.14657v2
- Association Between Neighborhood Factors and Adult Obesity in Shelby County, Tennessee: http://arxiv.org/abs/2208.05335v1
- Recurrent Neural Networks based Obesity Status Prediction: http://arxiv.org/abs/1809.07828v1

## Current Interventions and Policy Recommendations

**Current obesity interventions targeting young adults must shift from individual-focused approaches to addressing systemic neighborhood-level factors that drive health disparities.** Research demonstrates that built environment characteristics explain up to 90% of obesity prevalence variation across cities, highlighting the critical role of structural determinants.

Recent geospatial analyses have identified key social determinants that shape obesity rates in disadvantaged communities, including housing stability, food access, and neighborhood infrastructure. The Shelby County, Tennessee case study reveals significant associations between obesity prevalence and multiple socioeconomic factors, particularly in areas with lower median household incomes and higher percentages of uninsured residents.

To develop more effective interventions, policymakers should prioritize:
* Implementing zoning policies that promote physical activity
* Improving access to healthy food options in underserved areas
* Addressing housing stability through rental assistance programs
* Expanding health insurance coverage in high-risk communities
* Investing in neighborhood infrastructure improvements

These evidence-based policy measures represent a crucial shift toward addressing the root causes of obesity through coordinated community-level interventions rather than focusing solely on individual behavior change.

### Sources
- Association Between Neighborhood Factors and Adult Obesity in Shelby County, Tennessee: http://arxiv.org/abs/2208.05335v1
- Using Deep Learning to Examine the Built Environment and Neighborhood Adult Obesity Prevalence: http://arxiv.org/abs/1711.00885v1
- Structured psychosocial stress and the US obesity epidemic: http://arxiv.org/abs/q-bio/0312011v1

# Obesity in Young Adults: A Complex Public Health Challenge

The rising prevalence of obesity among young adults in the United States represents a critical public health challenge shaped by interconnected social, economic, and environmental factors. Recent research reveals that over one-third of US adults suffer from obesity, with rates disproportionately affecting disadvantaged communities. Advanced analysis demonstrates that neighborhood characteristics and built environment features explain up to 90% of obesity prevalence variation across major cities, highlighting how systemic inequalities create barriers to maintaining healthy weight.

## Key Findings and Future Directions

The evidence demonstrates that obesity in young adults stems from complex interactions between built environment, socioeconomic factors, and healthcare access. Machine learning analyses have revolutionized our understanding of these relationships, achieving prediction accuracies up to 88% for obesity risk. The research points to critical areas requiring immediate intervention:

* Built Environment Modifications
  - Improve neighborhood walkability
  - Increase access to recreational facilities
  - Address food desert challenges
  - Regulate "cyber food swamps"

* Policy Interventions
  - Expand health insurance coverage
  - Implement supportive housing policies
  - Develop targeted community programs
  - Enhance public transportation access

Success in reducing obesity rates will require coordinated efforts that address these systemic factors rather than focusing solely on individual behavior change. Future initiatives must prioritize evidence-based structural interventions that promote health equity across all communities.


================================================
FILE: examples/inference-market-gpt45.md
================================================
# Introduction

The AI inference market is rapidly expanding, driven by growing demand for real-time data processing and advancements in specialized hardware and cloud-based solutions. This report examines three innovative companiesâ€”Fireworks AI, Together.ai, and Groqâ€”that are shaping the competitive landscape. Fireworks AI offers flexible, multimodal inference solutions; Together.ai emphasizes optimized performance for open-source models; and Groq delivers unmatched speed through custom hardware. By analyzing their technologies, market positioning, and performance metrics, this report provides insights into how these key players are influencing the future of AI inference.

## Market Overview of AI Inference

**The global AI inference server market is experiencing rapid growth, projected to expand from USD 38.4 billion in 2023 to USD 166.7 billion by 2031, at a CAGR of 18%.** This growth is driven by increasing demand for real-time data processing, advancements in AI technologies, and widespread adoption of cloud-based and edge computing solutions.

North America currently dominates the market, accounting for approximately 38% of global revenue, due to its advanced technological infrastructure, significant R&D investments, and presence of major industry players such as NVIDIA, Intel, and Dell. Asia-Pacific is expected to exhibit the highest growth rate, driven by rapid digital transformation initiatives and government support for AI adoption, particularly in China, India, and Japan.

Key factors influencing market growth include:

- Rising adoption of AI-driven applications in healthcare, finance, automotive, and retail sectors.
- Increased deployment of specialized hardware (GPUs, TPUs, FPGAs) optimized for AI workloads.
- Growing preference for cloud-based deployment models due to scalability and cost-effectiveness.

However, high initial implementation costs, complexity of integration, and data privacy concerns remain significant challenges.

### Sources

- AI Inference Server Market Size, Scope, Growth, and Forecast : https://www.verifiedmarketresearch.com/product/ai-inference-server-market/
- AI Server Market Size & Share, Growth Forecasts Report 2032 : https://www.gminsights.com/industry-analysis/ai-server-market
- AI Inference Server Market Forecast To 2032 : https://www.businessresearchinsights.com/market-reports/ai-inference-server-market-118293

## Deep Dive: Fireworks AI

**Fireworks AI provides a flexible inference platform optimized for deploying and fine-tuning large language models (LLMs), emphasizing ease of use, scalability, and performance customization.**

The platform supports two primary deployment modes: serverless inference and dedicated deployments. Serverless inference allows quick experimentation with popular pre-deployed models like Llama 3.1 405B, billed per token without guaranteed SLAs. Dedicated deployments offer private, GPU-based infrastructure with performance guarantees, supporting both base models and efficient Low-Rank Adaptation (LoRA) addons.

Fireworks AI's Document Inlining feature notably extends text-based models into multimodal capabilities, enabling visual reasoning tasks by seamlessly integrating image and PDF content. Performance optimization techniques include quantization, batching, and caching, tailored to specific use cases such as chatbots and coding assistants requiring low latency.

Competitively, Fireworks AI positions itself against providers like OpenAI and Cohere, with a recent Series B funding round of $52M, total funding of $77M, and estimated annual recurring revenue (ARR) around $6M.

- Founded: 2022
- Headquarters: Redwood City, CA
- Employees: ~60
- Key Investors: Sequoia Capital, NVIDIA, AMD Ventures

### Sources
- Overview - Fireworks AI Docs : https://docs.fireworks.ai/models/overview  
- Performance optimization - Fireworks AI Docs : https://docs.fireworks.ai/faq/deployment/performance/optimization  
- DeepSeek R1 Just Got Eyes with Fireworks AI Document Inlining : https://fireworks.ai/blog/deepseek-r1-got-eyes  
- Fireworks AI 2025 Company Profile: Valuation, Funding & Investors : https://pitchbook.com/profiles/company/561272-14  
- Fireworks AI: Contact Details, Revenue, Funding, Employees and Company Profile : https://siliconvalleyjournals.com/company/fireworks-ai/  
- Fireworks AI - Overview, News & Similar companies - ZoomInfo : https://www.zoominfo.com/c/fireworks-ai-inc/5000025791  
- Fireworks AI Stock Price, Funding, Valuation, Revenue & Financial : https://www.cbinsights.com/company/fireworks-ai/financials

## Deep Dive: Together.ai

**Together.ai differentiates itself in the AI inference market through its comprehensive cloud platform, optimized for rapid inference, extensive model selection, and flexible GPU infrastructure.**

Together.ai provides a robust cloud-based solution for training, fine-tuning, and deploying generative AI models, emphasizing high-performance inference capabilities. Its inference engine leverages proprietary technologies such as FlashAttention-3 and speculative decoding, achieving inference speeds up to four times faster than competitors. The platform supports over 100 open-source models, including popular large language models (LLMs) like Llama-2 and RedPajama, enabling developers to quickly experiment and deploy tailored AI solutions.

Together.ai's flexible GPU clusters, featuring NVIDIA H100 and H200 GPUs interconnected via high-speed Infiniband networks, facilitate scalable distributed training and inference workloads. This infrastructure positions Together.ai competitively against GPU cloud providers like CoreWeave and Lambda Labs, particularly for startups and enterprises requiring variable compute resources.

Financially, Together.ai has demonstrated rapid growth, reaching an estimated $130M ARR in 2024, driven by increasing demand for generative AI applications and developer-friendly tooling.

### Sources
- Together AI: Reviews, Features, Pricing, Guides, and Alternatives : https://aipure.ai/products/together-ai
- Together AI revenue, valuation & growth rate | Sacra : https://sacra.com/c/together-ai/
- AI Solutions with Together.ai: Inference, Fine-Tuning & Models : https://pwraitools.com/generative-ai-tools/ai-solutions-with-together-ai-inference-fine-tuning-and-models/

## Deep Dive: Groq

**Groq's vertically integrated Tensor Streaming Processor (TSP) architecture delivers unmatched inference performance and energy efficiency, significantly outperforming traditional GPUs.**

Groq's TSP chip achieves inference speeds of 500-700 tokens per second on large language models, representing a 5-10x improvement over Nvidia's latest GPUs. Independent benchmarks confirm Groq's LPU (Language Processing Unit) reaches 276 tokens per second on Meta's Llama 3.3 70B model, maintaining consistent performance across varying context lengths without typical latency trade-offs.

Groq's unique hardware-software co-design eliminates external memory dependencies, embedding memory directly on-chip. This approach reduces data movement, resulting in up to 10x greater energy efficiency compared to GPUs. GroqCloud, the company's cloud inference platform, supports popular open-source models and has attracted over 360,000 developers.

Financially, Groq has raised $640 million in a Series D round at a $2.8 billion valuation, reflecting strong market confidence. Groq plans to deploy over 108,000 LPUs by early 2025, positioning itself as a leading provider of low-latency AI inference infrastructure.

### Sources
- Groq revenue, valuation & funding | Sacra : https://sacra.com/c/groq/
- Groq Raises $640M To Meet Soaring Demand for Fast AI Inference : https://groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/
- New AI Inference Speed Benchmark for Llama 3.3 70B, Powered by Groq : https://groq.com/new-ai-inference-speed-benchmark-for-llama-3-3-70b-powered-by-groq/
- Groq Inference Performance, Quality, & Cost Savings : https://groq.com/inference/
- GroqThoughts PowerPaper 2024 : https://groq.com/wp-content/uploads/2024/07/GroqThoughts_PowerPaper_2024.pdf

## Comparative Analysis

**Fireworks AI, Together.ai, and Groq each offer distinct strengths in AI inference, targeting different market segments and performance needs.**

Fireworks AI emphasizes speed and scalability through its proprietary FireAttention inference engine, delivering multi-modal capabilities (text, image, audio) with low latency. It prioritizes data privacy, maintaining HIPAA and SOC2 compliance, and offers flexible deployment options including serverless and on-demand models.

Together.ai differentiates itself by providing optimized inference for over 200 open-source large language models (LLMs). It achieves sub-100ms latency through automated infrastructure optimizations such as token caching, load balancing, and model quantization. Its cost-effective approach makes it attractive for developers requiring extensive model variety and scalability.

Groq specializes in hardware-accelerated inference, leveraging its custom Tensor Streaming Processor (TSP) chip architecture. GroqCloud provides ultra-low latency inference performance (500-700 tokens/second), significantly outperforming traditional GPUs. Groq targets latency-sensitive enterprise applications, including conversational AI and autonomous systems, with both cloud and on-premises deployment options.

| Feature             | Fireworks AI                 | Together.ai                  | Groq                          |
|---------------------|------------------------------|------------------------------|-------------------------------|
| Technology          | Proprietary inference engine | Optimized open-source models | Custom hardware (TSP chips)   |
| Market Positioning  | Multi-modal, privacy-focused | Cost-effective, scalable     | Ultra-low latency enterprise  |
| Revenue Estimates   | Not publicly available       | Not publicly available       | $3.4M (2023)                  |
| Performance Metrics | Low latency, multi-modal     | Sub-100ms latency            | 500-700 tokens/sec inference  |

### Sources
- Fireworks AI vs GroqCloud Platform Comparison 2025 | PeerSpot : https://www.peerspot.com/products/comparisons/fireworks-ai_vs_groqcloud-platform
- Fireworks AI vs Together Inference Comparison 2025 | PeerSpot : https://www.peerspot.com/products/comparisons/fireworks-ai_vs_together-inference
- Top 10 AI Inference Platforms in 2025 - DEV Community : https://dev.to/lina_lam_9ee459f98b67e9d5/top-10-ai-inference-platforms-in-2025-56kd
- Groq revenue, valuation & funding | Sacra : https://sacra.com/c/groq/

## Conclusion and Synthesis

The AI inference market is rapidly expanding, projected to reach $166.7 billion by 2031, driven by demand for real-time processing and specialized hardware. Fireworks AI, Together.ai, and Groq each offer distinct competitive advantages:

| Feature            | Fireworks AI                      | Together.ai                      | Groq                             |
|--------------------|-----------------------------------|----------------------------------|----------------------------------|
| Core Strength      | Multi-modal, privacy-focused      | Extensive open-source support    | Custom hardware, ultra-low latency |
| Technology         | Proprietary inference engine      | Optimized GPU infrastructure     | Tensor Streaming Processor (TSP) |
| Revenue Estimates  | ~$6M ARR                          | ~$130M ARR                       | ~$3.4M ARR                       |
| Performance        | Low latency, flexible deployment  | Sub-100ms latency                | 500-700 tokens/sec inference     |

Next steps include monitoring Groq's hardware adoption, evaluating Together.ai's scalability for diverse models, and assessing Fireworks AI's multimodal capabilities for specialized enterprise applications.


================================================
FILE: examples/inference-market.md
================================================
# The AI Inference Market: Analyzing Emerging Leaders

The AI inference market is experiencing unprecedented growth, projected to reach $133.2 billion by 2034, as specialized providers challenge traditional semiconductor dominance. While established chip manufacturers control over 80% of the market, new entrants like Fireworks, Together.ai, and Groq are reshaping the competitive landscape through innovative approaches to inference optimization and pricing.

This analysis examines how these emerging players are disrupting the market through differentiated technologies, aggressive pricing strategies, and superior performance metrics, particularly in the rapidly expanding cloud-based inference segment that now represents 55% of total market share. Their success highlights a fundamental shift in how AI computation is being delivered and monetized.

## AI Inference Market Overview

**The global AI inference market is experiencing unprecedented growth, projected to reach $133.2 billion by 2034, with a transformative shift occurring in market dynamics as new specialized providers challenge traditional semiconductor dominance.**

While established chip manufacturers (NVIDIA, AMD, Intel) control 80-82% of the market, emerging players are gaining traction through differentiated approaches. The market expansion is particularly evident in cloud-based deployments, which now represent 55% of total market share.

Key factors driving market evolution include:
* Increasing demand for real-time processing capabilities
* Shift toward token-based pricing models
* Rising adoption of specialized AI hardware
* Growth in open-source model deployment
* Integration of edge computing solutions

North America maintains market leadership with 38% global share, generating $9.34 billion in revenue (2024). This dominance stems from robust digital infrastructure and concentrated presence of technology companies, particularly in the United States where revenue reaches $8.6 billion.

The market shows sustained growth potential, supported by ongoing infrastructure investments and technological innovation, particularly in cloud-based deployments where North America maintains clear leadership.

### Sources
- AI Inference Server Market Forecast : https://www.einpresswire.com/article/779610673/ai-inference-server-market-supports-new-technology-with-usd-133-2-billion-by-2034-regional-growth-at-usd-9-34-billion
- SemiAnalysis Market Report : https://semianalysis.com/2024/02/21/groq-inference-tokenomics-speed-but/
- Markets and Markets AI Inference Report : https://www.marketsandmarkets.com/Market-Reports/ai-inference-market-189921964.html

## Fireworks.ai Profile

**Fireworks.ai has emerged as a significant AI inference provider by focusing on performance optimization, reaching a $552M valuation in 2024 with an estimated $44M in annual revenue.** Their platform serves over 25 billion tokens daily to more than 23,000 developers through a tiered pricing structure that scales with usage.

The company's technical differentiation comes from custom optimizations like FireAttention, which demonstrates superior performance metrics compared to competitors. Benchmark tests show up to 5.6x higher throughput and 12.2x lower latency versus vLLM for Mixtral 8x7B models in fp8 format.

Their pricing model combines usage-based tiers with flexible deployment options:
* Basic tier: $50/month spending limit
* Growth tier: $500/month spending limit
* Scale tier: $5,000/month spending limit
* Enterprise tier: Custom limits with dedicated support
* On-demand GPU deployments: $2.90-$9.99 per hour

Notable enterprise customers including DoorDash, Quora, and Upwork validate their approach. Since founding in 2022, Fireworks has secured $77M in funding from investors like Benchmark and Sequoia Capital.

### Sources
- Fireworks AI Valued at $552M: https://www.pymnts.com/news/investment-tracker/2024/fireworks-ai-valued-552-million-dollars-after-new-funding-round/
- FireAttention v3 Performance Metrics: https://fireworks.ai/blog/fireattention-v3
- AWS Case Study: https://aws.amazon.com/solutions/case-studies/fireworks-ai-case-study/

## Together.ai Profile

**Together.ai has established itself as a major AI inference provider by combining competitive pricing with superior technical performance, reaching a $3.3B valuation in early 2024.** Their platform supports over 200 open-source models and serves both individual developers and enterprise customers through a tiered pricing structure.

The company's technical advantage stems from their integrated inference stack, which delivers up to 400 tokens per second on Llama models. This performance translates to significant cost savings, with their 70B parameter models priced at $0.88 per million tokensâ€”substantially below market rates.

Their pricing strategy segments customers into three tiers:
- Build: Pay-as-you-go with $1 free credit for developers
- Scale: Reserved GPU instances for production workloads
- Enterprise: Private deployments with custom optimization

Notable enterprise adoption includes Salesforce, Zoom, and The Washington Post, validating their platform's capabilities. Together.ai's recent $305M Series B funding demonstrates strong market confidence in their approach to democratizing AI infrastructure.

### Sources
- Together.ai Series B Announcement: https://www.together.ai/blog/together-ai-announcing-305m-series-b
- Together.ai Pricing Strategy: https://canvasbusinessmodel.com/blogs/marketing-strategy/together-ai-marketing-strategy
- Salesforce Ventures Investment: https://salesforceventures.com/perspectives/welcome-together-ai/

## Groq Profile

**Groq's Language Processing Unit (LPU) represents a radical departure from traditional GPU architectures, delivering superior inference performance at significantly lower costs.** Their proprietary tensor-streaming processor achieves 241 tokens per second for Llama 2 Chat (70B), more than double competing solutions, while maintaining exceptional energy efficiency at 1-3 joules per token.

The company's aggressive pricing strategy undercuts competitors, offering Mixtral 8x7B inference at $0.24 per million tokens compared to Fireworks' $0.50. This pricing advantage stems from lower manufacturing costs ($6,000 per 14nm wafer vs. $16,000 for NVIDIA's 5nm H100) and architectural efficiencies.

Key competitive advantages:
- Superior inference speed: Up to 18x faster than cloud competitors
- Cost efficiency: $20,000 per LPU vs $25,000+ for NVIDIA H100
- Energy optimization: 80 TB/s bandwidth with 750 TOPS at INT8

Recently valued at $2.8 billion after raising $640M, Groq has gained significant traction with over 360,000 developers on GroqCloud. While 2023 revenue was modest at $3.4M, planned deployment of 108,000 LPUs by Q1 2025 positions them for substantial growth in the expanding inference market.

### Sources
- Groq Report Analysis: https://notice-reports.s3.amazonaws.com/Groq%20Report%202024.12.23_17.58.23.pdf
- SemiAnalysis Pricing Study: https://semianalysis.com/2024/02/21/groq-inference-tokenomics-speed-but/
- Groq Funding Announcement: https://www.prnewswire.com/news-releases/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference-302214097.html

## Comparative Performance Analysis

**Recent benchmarks reveal Groq as the current performance leader in LLM inference, with Together.ai and Fireworks competing for second position across key metrics.** Independent testing from ArtificialAnalysis.ai shows significant variations in core performance indicators:

| Provider | TTFT (seconds) | Tokens/Second | Cost (per 1M tokens) |
|----------|---------------|---------------|---------------------|
| Groq | 0.22 | 241 | $0.27 |
| Together | 0.50 | 117 | $0.88 |
| Fireworks | 0.40 | 98 | $0.90 |

Performance advantages can vary significantly based on specific workloads and model sizes. Together.ai's Inference Engine 2.0 demonstrates strong performance with smaller models, while Fireworks maintains consistent performance across their model range.

A notable limitation emerges with larger inputs - Groq shows a 560% increase in TTFT when processing 10K versus 1K input tokens. This suggests optimal use cases may differ between providers despite headline performance metrics.

The competitive landscape remains dynamic, with providers regularly releasing optimization updates that can significantly impact these metrics.

### Sources
- ArtificialAnalysis.ai LLM Benchmark: https://wandb.ai/capecape/benchmark_llama_70b/reports/Is-the-new-Cerebras-API-the-fastest-LLM-service-provider
- Comparative Analysis of AI API Providers: https://friendli.ai/blog/comparative-analysis-ai-api-provider
- Together Inference Engine Analysis: https://www.together.ai/blog/together-inference-engine-v1

## Conclusion and Market Outlook

The AI inference market is rapidly evolving with specialized providers challenging traditional semiconductor dominance. Our analysis reveals distinct competitive advantages among emerging leaders:

| Provider | Key Strength | Performance | Pricing | Market Position |
|----------|--------------|-------------|----------|-----------------|
| Groq | Custom LPU Architecture | 241 tokens/sec | $0.24/M tokens | $2.8B valuation, disruptive hardware |
| Together.ai | Model Variety | 117 tokens/sec | $0.88/M tokens | $3.3B valuation, broad adoption |
| Fireworks | Optimization Tech | 98 tokens/sec | $0.90/M tokens | $552M valuation, developer focus |

Looking ahead, Groq's superior performance metrics and aggressive pricing position them to capture significant market share, particularly in high-throughput applications. Together.ai's extensive model support and enterprise relationships suggest continued growth in the mid-market segment, while Fireworks' optimization technology provides a strong foundation for specialized use cases. As the market expands toward $133.2B by 2034, these providers are well-positioned to challenge NVIDIA's dominance through differentiated approaches to inference delivery.


================================================
FILE: examples/pubmed.md
================================================
# Diabetic Nephropathy Treatment: Current Approaches and Future Directions

Diabetic nephropathy has emerged as the leading cause of end-stage renal disease worldwide, affecting approximately 40% of diabetes patients. The condition's progressive nature and complex pathophysiology demand early intervention through comprehensive treatment strategies. Recent advances in therapeutic options, from SGLT2 inhibitors to non-steroidal mineralocorticoid receptor antagonists, have transformed the management landscape. This report examines current treatment protocols, emerging therapies, and diagnostic approaches, with particular emphasis on the growing importance of personalized medicine and integrated care models in improving patient outcomes.

## Key Treatment Advances and Future Directions

Modern diabetic nephropathy management has evolved into a sophisticated, multi-faceted approach that combines established treatments with innovative therapies. The emergence of the four-pillar treatment strategy, incorporating RAS blockers, SGLT2 inhibitors, GLP-1 receptor agonists, and finerenone, represents a significant advancement in care standards. Technological progress in diagnostic tools, particularly multiparametric MRI and novel biomarkers, enables earlier intervention and more precise monitoring of disease progression.

Key developments driving treatment evolution:
* Integration of multiple therapeutic agents for enhanced outcomes
* Adoption of personalized medicine approaches using proteomics
* Implementation of comprehensive care models showing cost-effective results
* Advanced imaging techniques enabling non-invasive monitoring
* Emergence of novel biomarkers for earlier detection

The future of diabetic nephropathy treatment lies in closing the evidence-to-practice gap and expanding access to these advanced therapeutic options.

## Prevalence and Mechanisms of Diabetic Nephropathy

**Diabetic nephropathy has become the leading cause of end-stage renal disease worldwide, affecting approximately 40% of diabetes patients and contributing to 38% of renal disease cases in regions like the Philippines.**

The pathogenesis involves complex interactions between metabolic and hemodynamic factors. Hyperglycemia triggers increased production of advanced glycation end-products (AGEs) and activates inflammatory pathways, while concurrent hypertension amplifies kidney damage through elevated glomerular pressure. The condition typically develops over 10-15 years as these mechanisms progressively damage the kidney's filtering system.

Key risk factors that accelerate nephropathy progression include:
* Poorly controlled blood glucose (HbA1c >7%)
* Sustained hypertension (>130/80 mmHg)
* Genetic variants in ACE and APOL1 genes
* Obesity and smoking
* Limited access to regular screening

Recent guidelines from KDIGO emphasize the importance of early detection and holistic care through multidisciplinary teams. The initial presentation typically involves microalbuminuria, which can progress to overt proteinuria and declining glomerular filtration rate without intervention. Research shows that aggressive early treatment can delay or prevent progression, particularly when addressing both glycemic control and blood pressure management.

### Sources
- Diabetic Nephropathy: StatPearls : https://pubmed.ncbi.nlm.nih.gov/30480939/
- Current status of diabetes mellitus care in the Philippines : https://pubmed.ncbi.nlm.nih.gov/38382166/
- Lifestyle Modifications in Delaying CKD Progression : https://pubmed.ncbi.nlm.nih.gov/36874334/

## Biomarkers for Early Detection of Diabetic Nephropathy

**The landscape of diabetic nephropathy detection is rapidly evolving beyond traditional microalbuminuria testing, as emerging biomarkers offer more precise and earlier disease identification.** While microalbuminuria remains the clinical standard, its limited predictive power has driven research into more sophisticated detection methods.

Recent studies have identified several promising biomarker categories that can detect kidney damage before albumin changes become apparent. These include markers of specific nephron damage sites, oxidative stress indicators, and inflammatory signals. A comprehensive 2024 review highlighted five key biomarker categories:

- Glomerular damage markers
- Tubular damage indicators
- Oxidative stress biomarkers
- Inflammatory biomarkers
- Novel molecular markers (miRNAs, proteomics, metabolomics)

A significant advancement comes from combining multiple biomarker types. For example, integrating serum creatinine with cystatin C measurements has demonstrated superior accuracy in detecting early kidney dysfunction, particularly when using newer race-free prediction equations. This multi-marker approach reflects the complex pathophysiology of diabetic kidney disease and enables more personalized intervention strategies.

### Sources
- Insights into the Novel Biomarkers Expressed in Diabetic Nephropathy (2024): https://pubmed.ncbi.nlm.nih.gov/39415582/
- Diagnostic challenges of diabetic kidney disease (2023): https://pubmed.ncbi.nlm.nih.gov/37545693/
- Urinary biomarkers for early diabetic nephropathy (2014): https://pubmed.ncbi.nlm.nih.gov/25060761/

## Treatment Protocols for Diabetic Nephropathy

**Modern diabetic nephropathy management requires a comprehensive approach combining established treatments with emerging therapeutic options to effectively slow disease progression and protect kidney function.** The foundation remains strict glycemic control (HbA1c <7%) and blood pressure management (<130/80 mmHg in patients with albuminuria).

Renin-angiotensin system (RAS) blockers, particularly ACE inhibitors and ARBs, continue as first-line treatments for their dual action on blood pressure and nephroprotection. Recent evidence supports combination therapy with newer agents for enhanced outcomes.

Key therapeutic advances include:
* SGLT2 inhibitors (dapagliflozin, empagliflozin) - reduce disease progression by promoting urinary potassium excretion and normalizing plasma potassium levels
* Non-steroidal mineralocorticoid receptor antagonists (finerenone) - decrease albuminuria and cardiovascular complications
* Lifestyle modifications - Mediterranean diet adherence and regular exercise show significant benefits
* Antioxidant interventions - target oxidative stress mechanisms

The SONAR trial demonstrated that atrasentan, an endothelin receptor antagonist, significantly decreased renal events in diabetic kidney disease patients. Regular monitoring of kidney function, albuminuria, and electrolyte levels remains essential for optimizing treatment outcomes.

### Sources
- What Not to Overlook in the Management of Patients with Type 2 Diabetes Mellitus: https://pubmed.ncbi.nlm.nih.gov/39062970/
- Lifestyle Modifications and Nutritional and Therapeutic Interventions: https://pubmed.ncbi.nlm.nih.gov/36874334/
- Diabetic Kidney Disease: https://pubmed.ncbi.nlm.nih.gov/25905328/
- Impaired distal renal potassium handling in diabetic mice: https://pubmed.ncbi.nlm.nih.gov/38779755/

## Recent Advances in Diabetic Nephropathy Treatment

**The emergence of a four-pillar treatment approach represents a paradigm shift in diabetic nephropathy management, moving beyond the traditional reliance on RAS blockade alone to include multiple complementary therapeutic agents.** This comprehensive strategy has demonstrated superior cardiorenal protection compared to single-agent approaches.

The four essential pillars of modern treatment include:

* RAS blockers (ACE inhibitors/ARBs) as foundational therapy
* SGLT2 inhibitors for reducing kidney disease progression
* GLP-1 receptor agonists for glycemic control and renoprotection
* Finerenone, a non-steroidal mineralocorticoid receptor antagonist, for additional protection

Recent clinical trials suggest that combining these therapies may provide additive benefits, though ongoing studies are still evaluating optimal combinations. The PRIORITY study exemplifies the movement toward personalized medicine, using urinary proteomics to predict treatment response and guide therapy selection.

Implementation challenges persist, with many eligible patients not receiving recommended combinations. Healthcare systems are addressing this through specialized clinics and electronic health record-based decision support tools to narrow the evidence-to-practice gap.

### Sources
- Finerenone: Do We Really Need an Additional Therapy in Type 2 Diabetes Mellitus and Kidney Disease?: https://pubmed.ncbi.nlm.nih.gov/39862018/
- Slowing the Progression of Chronic Kidney Disease in Patients with Type 2 Diabetes Using Four Pillars of Therapy: https://pubmed.ncbi.nlm.nih.gov/39259460/
- Updated evidence on cardiovascular and renal effects of GLP-1 receptor agonists: https://pubmed.ncbi.nlm.nih.gov/39548500/

## Noninvasive MRI Techniques for Diabetic Nephropathy Assessment

**Multiparametric MRI represents a breakthrough in noninvasive renal assessment, enabling detailed evaluation of kidney structure and function without radiation or contrast agents.** This technology combines multiple specialized imaging sequences to provide comprehensive insights into kidney health.

The diffusion-weighted imaging (DWI) sequence measures water molecule movement, offering early detection of interstitial fibrosis and predictive value for renal function deterioration in diabetic nephropathy. Blood oxygen level-dependent (BOLD) MRI assesses tissue oxygenation by detecting deoxyhemoglobin levels, proving particularly valuable for monitoring chronic kidney disease progression.

Key MRI sequences and their clinical applications:
- T1/T2 Relaxometry: Evaluates tissue water content and fibrosis; corticomedullary changes correlate with filtration rate
- DWI: Measures microstructural changes and fibrosis development
- BOLD: Monitors tissue oxygenation and predicts functional decline
- Arterial Spin Labeling: Assesses renal hemodynamics without contrast

While these techniques show promise for early disease detection and monitoring, further clinical trials are needed before widespread implementation. The technology's potential for personalized treatment decisions and virtual biopsy capabilities represents a significant advance in diabetic nephropathy management.

### Sources
- Multiparametric MRI: can we assess renal function differently? (2024): https://pubmed.ncbi.nlm.nih.gov/40008350/
- Noninvasive Assessment of Diabetic Kidney Disease With MRI: Hype or Hope? (2023): https://pubmed.ncbi.nlm.nih.gov/37675919/

## Integrated Care and Systemic Challenges in Diabetic Nephropathy Management

**Quality improvement collaboratives in integrated diabetes care settings can significantly improve patient outcomes while remaining cost-effective, with studies showing increased life expectancy of nearly one year for male patients and 0.76 years for female patients.** The success of such integrated approaches demonstrates the critical importance of coordinated care between specialists in managing diabetic nephropathy.

However, implementing effective integrated care faces several systemic barriers that must be addressed:

* Limited specialist availability in rural regions
* Poor communication between healthcare providers
* Insurance coverage restrictions
* Lack of standardized protocols
* Delayed specialist referrals

A notable example comes from a Netherlands study of integrated diabetes care across 37 general practices and 13 outpatient clinics. Their collaborative care model reduced cardiovascular event risk (hazard ratio: 0.83 for men, 0.98 for women) and cardiovascular mortality (hazard ratio: 0.78 for men, 0.88 for women). The program cost approximately â‚¬22 per patient initially, with lifetime costs increasing by â‚¬860 for men and â‚¬645 for women â€“ proving highly cost-effective at under â‚¬2,000 per quality-adjusted life year.

### Sources
- Cost-effectiveness of a quality improvement collaborative focusing on patients with diabetes: https://pubmed.ncbi.nlm.nih.gov/20808258/

# Diabetic Nephropathy Treatment: Current Approaches and Future Directions

Diabetic nephropathy has emerged as the leading cause of end-stage renal disease globally, affecting 40% of diabetes patients and demanding increasingly sophisticated treatment approaches. The evolution of treatment strategies from single-agent protocols to comprehensive four-pillar approaches, combined with advances in early detection and monitoring, has transformed the management landscape. This report examines current best practices, emerging therapies, and the critical role of integrated care in improving patient outcomes.

## Key Findings and Treatment Framework

Modern diabetic nephropathy management has evolved into a multi-faceted approach requiring careful coordination of therapeutic strategies. The evidence supports a structured treatment framework that combines established protocols with emerging innovations.

* Foundation Treatments
  - Glycemic control (HbA1c <7%)
  - Blood pressure management (<130/80 mmHg)
  - RAS blockers (ACE inhibitors/ARBs)
  - Lifestyle modifications

* Emerging Therapeutic Advances
  - SGLT2 inhibitors for disease progression
  - Non-steroidal mineralocorticoid receptor antagonists
  - GLP-1 receptor agonists
  - Multiparametric MRI for monitoring

The path forward requires addressing implementation challenges through integrated care models while leveraging new diagnostic tools and biomarkers for earlier intervention. Success depends on bridging the evidence-to-practice gap through specialized clinics and improved coordination among healthcare providers.


================================================
FILE: src/open_deep_research/__init__.py
================================================
"""Planning, research, and report generation."""

__version__ = "0.0.15"


================================================
FILE: src/open_deep_research/configuration.py
================================================
import os
from enum import Enum
from dataclasses import dataclass, fields
from typing import Any, Optional, Dict 

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables import RunnableConfig
from dataclasses import dataclass

DEFAULT_REPORT_STRUCTURE = """Use this structure to create a report on the user-provided topic:

1. Introduction (no research needed)
   - Brief overview of the topic area

2. Main Body Sections:
   - Each section should focus on a sub-topic of the user-provided topic
   
3. Conclusion
   - Aim for 1 structural element (either a list of table) that distills the main body sections 
   - Provide a concise summary of the report"""

class SearchAPI(Enum):
    PERPLEXITY = "perplexity"
    TAVILY = "tavily"
    EXA = "exa"
    ARXIV = "arxiv"
    PUBMED = "pubmed"
    LINKUP = "linkup"
    DUCKDUCKGO = "duckduckgo"
    GOOGLESEARCH = "googlesearch"

@dataclass(kw_only=True)
class Configuration:
    """The configurable fields for the chatbot."""
    # Common configuration
    report_structure: str = DEFAULT_REPORT_STRUCTURE # Defaults to the default report structure
    search_api: SearchAPI = SearchAPI.TAVILY # Default to TAVILY
    search_api_config: Optional[Dict[str, Any]] = None
    
    # Graph-specific configuration
    number_of_queries: int = 2 # Number of search queries to generate per iteration
    max_search_depth: int = 2 # Maximum number of reflection + search iterations
    planner_provider: str = "anthropic"  # Defaults to Anthropic as provider
    planner_model: str = "claude-3-7-sonnet-latest" # Defaults to claude-3-7-sonnet-latest
    planner_model_kwargs: Optional[Dict[str, Any]] = None # kwargs for planner_model
    writer_provider: str = "anthropic" # Defaults to Anthropic as provider
    writer_model: str = "claude-3-5-sonnet-latest" # Defaults to claude-3-5-sonnet-latest
    writer_model_kwargs: Optional[Dict[str, Any]] = None # kwargs for writer_model
    search_api: SearchAPI = SearchAPI.TAVILY # Default to TAVILY
    search_api_config: Optional[Dict[str, Any]] = None 
    
    # Multi-agent specific configuration
    supervisor_model: str = "openai:gpt-4.1" # Model for supervisor agent in multi-agent setup
    researcher_model: str = "openai:gpt-4.1" # Model for research agents in multi-agent setup 

    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> "Configuration":
        """Create a Configuration instance from a RunnableConfig."""
        configurable = (
            config["configurable"] if config and "configurable" in config else {}
        )
        values: dict[str, Any] = {
            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))
            for f in fields(cls)
            if f.init
        }
        return cls(**{k: v for k, v in values.items() if v})



================================================
FILE: src/open_deep_research/graph.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Research Workflow

This notebook demonstrates the research [workflow](https://langchain-ai.github.io/langgraph/tutorials/workflows/) that creates comprehensive reports through a series of focused steps. The system:

1. Uses a **graph workflow** with specialized nodes for each report creation stage
2. Enables user **feedback and approval** at critical planning points 
3. Produces a well-structured report with introduction, researched body sections, and conclusion

## From repo 
"""

%cd ..
%load_ext autoreload
%autoreload 2
# Output:
#   /Users/rlm/Desktop/Code/open_deep_research/src

#   /Users/rlm/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.

#     self.shell.db['dhist'] = compress_dhist(dhist)[-100:]


"""
## From package 
"""

! pip install -U -q open-deep-research
# Output:
#   

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m23.2.1[0m[39;49m -> [0m[32;49m25.0.1[0m

#   [1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m


"""
# Compile the Graph-Based Research Workflow

The next step is to compile the LangGraph workflow that orchestrates the report creation process. This defines the sequence of operations and decision points in the research pipeline.
"""

# Import required modules and initialize the builder from open_deep_research
import uuid 
import os, getpass
import open_deep_research   
print(open_deep_research.__version__) 
from IPython.display import Image, display, Markdown
from langgraph.types import Command
from langgraph.checkpoint.memory import MemorySaver
from open_deep_research.graph import builder

# Create a memory-based checkpointer and compile the graph
# This enables state persistence and tracking throughout the workflow execution

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# Visualize the graph structure
# This shows the nodes and edges in the research workflow

display(Image(graph.get_graph(xray=1).draw_mermaid_png()))

# Helper function to set environment variables for API keys
# This ensures all necessary credentials are available for various services

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

# Set the API keys used for any model or search tool selections below, such as:
_set_env("OPENAI_API_KEY")
_set_env("ANTHROPIC_API_KEY")
_set_env("TAVILY_API_KEY")
_set_env("GROQ_API_KEY")
_set_env("PERPLEXITY_API_KEY")

# Define report structure template and configure the research workflow
# This sets parameters for models, search tools, and report organization

REPORT_STRUCTURE = """Use this structure to create a report on the user-provided topic:

1. Introduction (no research needed)
   - Brief overview of the topic area

2. Main Body Sections:
   - Each section should focus on a sub-topic of the user-provided topic
   
3. Conclusion
   - Aim for 1 structural element (either a list of table) that distills the main body sections 
   - Provide a concise summary of the report"""

# Configuration option 1: Claude 3.7 Sonnet for planning with perplexity search
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "perplexity",
                           "planner_provider": "anthropic",
                           "planner_model": "claude-3-7-sonnet-latest",
                           # "planner_model_kwargs": {"temperature":0.8}, # if set custom parameters
                           "writer_provider": "anthropic",
                           "writer_model": "claude-3-5-sonnet-latest",
                           # "writer_model_kwargs": {"temperature":0.8}, # if set custom parameters
                           "max_search_depth": 2,
                           "report_structure": REPORT_STRUCTURE,
                           }}

# Configuration option 2: DeepSeek-R1-Distill-Llama-70B for planning and llama-3.3-70b-versatile for writing
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "tavily",
                           "planner_provider": "groq",
                           "planner_model": "deepseek-r1-distill-llama-70b",
                           "writer_provider": "groq",
                           "writer_model": "llama-3.3-70b-versatile",
                           "report_structure": REPORT_STRUCTURE,
                           "max_search_depth": 1,}
                           }

# Configuration option 3: Use OpenAI o3 for both planning and writing (selected option)
thread = {"configurable": {"thread_id": str(uuid.uuid4()),
                           "search_api": "tavily",
                           "planner_provider": "openai",
                           "planner_model": "o3",
                           "writer_provider": "openai",
                           "writer_model": "o3",
                           "max_search_depth": 2,
                           "report_structure": REPORT_STRUCTURE,
                           }}

# Define research topic about Model Context Protocol
topic = "Overview of Model Context Protocol (MCP), an Anthropicâ€‘backed open standard for integrating external context and tools with LLMs. Give an architectural overview for developers, tell me about interesting MCP servers, and compare to google Agent2Agent (A2A) protocol."

# Run the graph workflow until first interruption (waiting for user feedback)
async for event in graph.astream({"topic":topic,}, thread, stream_mode="updates"):
    if '__interrupt__' in event:
        interrupt_value = event['__interrupt__'][0].value
        display(Markdown(interrupt_value))

"""
# User Feedback Phase

* This allows for providing directed feedback on the initial report plan
* The user can review the proposed report structure and provide specific guidance
* The system will incorporate this feedback into the final report plan
"""

# Submit feedback on the report plan
# The system will continue execution with the updated requirements

# Provide specific feedback to focus and refine the report structure
async for event in graph.astream(Command(resume="Looks great! Just do one section related to Agent2Agent (A2A) protocol, introducing it and comparing to MCP."), thread, stream_mode="updates"):
    if '__interrupt__' in event:
        interrupt_value = event['__interrupt__'][0].value
        display(Markdown(interrupt_value))

"""
# Final Approval Phase
* After incorporating feedback, approve the plan to start content generation
"""

# Approve the final plan and execute the report generation
# This triggers the research and writing phases for all sections

# The system will now:
# 1. Research each section topic
# 2. Generate content with citations
# 3. Create introduction and conclusion
# 4. Compile the final report

async for event in graph.astream(Command(resume=True), thread, stream_mode="updates"):
    print(event)
    print("\n")

# Display the final generated report
# Retrieve the completed report from the graph's state and format it for display

final_state = graph.get_state(thread)
report = final_state.values.get('final_report')
Markdown(report)
# Output:
#   <IPython.core.display.Markdown object>

"""
Trace: 

> Note: uses 80k tokens 

https://smith.langchain.com/public/31eca7c9-beae-42a3-bef4-5bce9488d7be/r
"""



================================================
FILE: src/open_deep_research/graph.py
================================================
from typing import Literal

from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig

from langgraph.constants import Send
from langgraph.graph import START, END, StateGraph
from langgraph.types import interrupt, Command

from open_deep_research.state import (
    ReportStateInput,
    ReportStateOutput,
    Sections,
    ReportState,
    SectionState,
    SectionOutputState,
    Queries,
    Feedback
)

from open_deep_research.prompts import (
    report_planner_query_writer_instructions,
    report_planner_instructions,
    query_writer_instructions, 
    section_writer_instructions,
    final_section_writer_instructions,
    section_grader_instructions,
    section_writer_inputs
)

from open_deep_research.configuration import Configuration
from open_deep_research.utils import (
    format_sections, 
    get_config_value, 
    get_search_params, 
    select_and_execute_search
)

## Nodes -- 

async def generate_report_plan(state: ReportState, config: RunnableConfig):
    """Generate the initial report plan with sections.
    
    This node:
    1. Gets configuration for the report structure and search parameters
    2. Generates search queries to gather context for planning
    3. Performs web searches using those queries
    4. Uses an LLM to generate a structured plan with sections
    
    Args:
        state: Current graph state containing the report topic
        config: Configuration for models, search APIs, etc.
        
    Returns:
        Dict containing the generated sections
    """

    # Inputs
    topic = state["topic"]
    feedback = state.get("feedback_on_report_plan", None)

    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    report_structure = configurable.report_structure
    number_of_queries = configurable.number_of_queries
    search_api = get_config_value(configurable.search_api)
    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty
    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters

    # Convert JSON object to string if necessary
    if isinstance(report_structure, dict):
        report_structure = str(report_structure)

    # Set writer model (model used for query writing)
    writer_provider = get_config_value(configurable.writer_provider)
    writer_model_name = get_config_value(configurable.writer_model)
    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})
    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) 
    structured_llm = writer_model.with_structured_output(Queries)

    # Format system instructions
    system_instructions_query = report_planner_query_writer_instructions.format(topic=topic, report_organization=report_structure, number_of_queries=number_of_queries)

    # Generate queries  
    results = await structured_llm.ainvoke([SystemMessage(content=system_instructions_query),
                                     HumanMessage(content="Generate search queries that will help with planning the sections of the report.")])

    # Web search
    query_list = [query.search_query for query in results.queries]

    # Search the web with parameters
    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)

    # Format system instructions
    system_instructions_sections = report_planner_instructions.format(topic=topic, report_organization=report_structure, context=source_str, feedback=feedback)

    # Set the planner
    planner_provider = get_config_value(configurable.planner_provider)
    planner_model = get_config_value(configurable.planner_model)
    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})

    # Report planner instructions
    planner_message = """Generate the sections of the report. Your response must include a 'sections' field containing a list of sections. 
                        Each section must have: name, description, plan, research, and content fields."""

    # Run the planner
    if planner_model == "claude-3-7-sonnet-latest":
        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model
        planner_llm = init_chat_model(model=planner_model, 
                                      model_provider=planner_provider, 
                                      max_tokens=20_000, 
                                      thinking={"type": "enabled", "budget_tokens": 16_000})

    else:
        # With other models, thinking tokens are not specifically allocated
        planner_llm = init_chat_model(model=planner_model, 
                                      model_provider=planner_provider,
                                      model_kwargs=planner_model_kwargs)
    
    # Generate the report sections
    structured_llm = planner_llm.with_structured_output(Sections)
    report_sections = await structured_llm.ainvoke([SystemMessage(content=system_instructions_sections),
                                             HumanMessage(content=planner_message)])

    # Get sections
    sections = report_sections.sections

    return {"sections": sections}

def human_feedback(state: ReportState, config: RunnableConfig) -> Command[Literal["generate_report_plan","build_section_with_web_research"]]:
    """Get human feedback on the report plan and route to next steps.
    
    This node:
    1. Formats the current report plan for human review
    2. Gets feedback via an interrupt
    3. Routes to either:
       - Section writing if plan is approved
       - Plan regeneration if feedback is provided
    
    Args:
        state: Current graph state with sections to review
        config: Configuration for the workflow
        
    Returns:
        Command to either regenerate plan or start section writing
    """

    # Get sections
    topic = state["topic"]
    sections = state['sections']
    sections_str = "\n\n".join(
        f"Section: {section.name}\n"
        f"Description: {section.description}\n"
        f"Research needed: {'Yes' if section.research else 'No'}\n"
        for section in sections
    )

    # Get feedback on the report plan from interrupt
    interrupt_message = f"""Please provide feedback on the following report plan. 
                        \n\n{sections_str}\n
                        \nDoes the report plan meet your needs?\nPass 'true' to approve the report plan.\nOr, provide feedback to regenerate the report plan:"""
    
    feedback = interrupt(interrupt_message)

    # If the user approves the report plan, kick off section writing
    if isinstance(feedback, bool) and feedback is True:
        # Treat this as approve and kick off section writing
        return Command(goto=[
            Send("build_section_with_web_research", {"topic": topic, "section": s, "search_iterations": 0}) 
            for s in sections 
            if s.research
        ])
    
    # If the user provides feedback, regenerate the report plan 
    elif isinstance(feedback, str):
        # Treat this as feedback
        return Command(goto="generate_report_plan", 
                       update={"feedback_on_report_plan": feedback})
    else:
        raise TypeError(f"Interrupt value of type {type(feedback)} is not supported.")
    
async def generate_queries(state: SectionState, config: RunnableConfig):
    """Generate search queries for researching a specific section.
    
    This node uses an LLM to generate targeted search queries based on the 
    section topic and description.
    
    Args:
        state: Current state containing section details
        config: Configuration including number of queries to generate
        
    Returns:
        Dict containing the generated search queries
    """

    # Get state 
    topic = state["topic"]
    section = state["section"]

    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    number_of_queries = configurable.number_of_queries

    # Generate queries 
    writer_provider = get_config_value(configurable.writer_provider)
    writer_model_name = get_config_value(configurable.writer_model)
    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})
    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) 
    structured_llm = writer_model.with_structured_output(Queries)

    # Format system instructions
    system_instructions = query_writer_instructions.format(topic=topic, 
                                                           section_topic=section.description, 
                                                           number_of_queries=number_of_queries)

    # Generate queries  
    queries = await structured_llm.ainvoke([SystemMessage(content=system_instructions),
                                     HumanMessage(content="Generate search queries on the provided topic.")])

    return {"search_queries": queries.queries}

async def search_web(state: SectionState, config: RunnableConfig):
    """Execute web searches for the section queries.
    
    This node:
    1. Takes the generated queries
    2. Executes searches using configured search API
    3. Formats results into usable context
    
    Args:
        state: Current state with search queries
        config: Search API configuration
        
    Returns:
        Dict with search results and updated iteration count
    """

    # Get state
    search_queries = state["search_queries"]

    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    search_api = get_config_value(configurable.search_api)
    search_api_config = configurable.search_api_config or {}  # Get the config dict, default to empty
    params_to_pass = get_search_params(search_api, search_api_config)  # Filter parameters

    # Web search
    query_list = [query.search_query for query in search_queries]

    # Search the web with parameters
    source_str = await select_and_execute_search(search_api, query_list, params_to_pass)

    return {"source_str": source_str, "search_iterations": state["search_iterations"] + 1}

async def write_section(state: SectionState, config: RunnableConfig) -> Command[Literal[END, "search_web"]]:
    """Write a section of the report and evaluate if more research is needed.
    
    This node:
    1. Writes section content using search results
    2. Evaluates the quality of the section
    3. Either:
       - Completes the section if quality passes
       - Triggers more research if quality fails
    
    Args:
        state: Current state with search results and section info
        config: Configuration for writing and evaluation
        
    Returns:
        Command to either complete section or do more research
    """

    # Get state 
    topic = state["topic"]
    section = state["section"]
    source_str = state["source_str"]

    # Get configuration
    configurable = Configuration.from_runnable_config(config)

    # Format system instructions
    section_writer_inputs_formatted = section_writer_inputs.format(topic=topic, 
                                                             section_name=section.name, 
                                                             section_topic=section.description, 
                                                             context=source_str, 
                                                             section_content=section.content)

    # Generate section  
    writer_provider = get_config_value(configurable.writer_provider)
    writer_model_name = get_config_value(configurable.writer_model)
    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})
    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) 

    section_content = await writer_model.ainvoke([SystemMessage(content=section_writer_instructions),
                                           HumanMessage(content=section_writer_inputs_formatted)])
    
    # Write content to the section object  
    section.content = section_content.content

    # Grade prompt 
    section_grader_message = ("Grade the report and consider follow-up questions for missing information. "
                              "If the grade is 'pass', return empty strings for all follow-up queries. "
                              "If the grade is 'fail', provide specific search queries to gather missing information.")
    
    section_grader_instructions_formatted = section_grader_instructions.format(topic=topic, 
                                                                               section_topic=section.description,
                                                                               section=section.content, 
                                                                               number_of_follow_up_queries=configurable.number_of_queries)

    # Use planner model for reflection
    planner_provider = get_config_value(configurable.planner_provider)
    planner_model = get_config_value(configurable.planner_model)
    planner_model_kwargs = get_config_value(configurable.planner_model_kwargs or {})

    if planner_model == "claude-3-7-sonnet-latest":
        # Allocate a thinking budget for claude-3-7-sonnet-latest as the planner model
        reflection_model = init_chat_model(model=planner_model, 
                                           model_provider=planner_provider, 
                                           max_tokens=20_000, 
                                           thinking={"type": "enabled", "budget_tokens": 16_000}).with_structured_output(Feedback)
    else:
        reflection_model = init_chat_model(model=planner_model, 
                                           model_provider=planner_provider, model_kwargs=planner_model_kwargs).with_structured_output(Feedback)
    # Generate feedback
    feedback = await reflection_model.ainvoke([SystemMessage(content=section_grader_instructions_formatted),
                                        HumanMessage(content=section_grader_message)])

    # If the section is passing or the max search depth is reached, publish the section to completed sections 
    if feedback.grade == "pass" or state["search_iterations"] >= configurable.max_search_depth:
        # Publish the section to completed sections 
        return  Command(
        update={"completed_sections": [section]},
        goto=END
    )

    # Update the existing section with new content and update search queries
    else:
        return  Command(
        update={"search_queries": feedback.follow_up_queries, "section": section},
        goto="search_web"
        )
    
async def write_final_sections(state: SectionState, config: RunnableConfig):
    """Write sections that don't require research using completed sections as context.
    
    This node handles sections like conclusions or summaries that build on
    the researched sections rather than requiring direct research.
    
    Args:
        state: Current state with completed sections as context
        config: Configuration for the writing model
        
    Returns:
        Dict containing the newly written section
    """

    # Get configuration
    configurable = Configuration.from_runnable_config(config)

    # Get state 
    topic = state["topic"]
    section = state["section"]
    completed_report_sections = state["report_sections_from_research"]
    
    # Format system instructions
    system_instructions = final_section_writer_instructions.format(topic=topic, section_name=section.name, section_topic=section.description, context=completed_report_sections)

    # Generate section  
    writer_provider = get_config_value(configurable.writer_provider)
    writer_model_name = get_config_value(configurable.writer_model)
    writer_model_kwargs = get_config_value(configurable.writer_model_kwargs or {})
    writer_model = init_chat_model(model=writer_model_name, model_provider=writer_provider, model_kwargs=writer_model_kwargs) 
    
    section_content = await writer_model.ainvoke([SystemMessage(content=system_instructions),
                                           HumanMessage(content="Generate a report section based on the provided sources.")])
    
    # Write content to section 
    section.content = section_content.content

    # Write the updated section to completed sections
    return {"completed_sections": [section]}

def gather_completed_sections(state: ReportState):
    """Format completed sections as context for writing final sections.
    
    This node takes all completed research sections and formats them into
    a single context string for writing summary sections.
    
    Args:
        state: Current state with completed sections
        
    Returns:
        Dict with formatted sections as context
    """

    # List of completed sections
    completed_sections = state["completed_sections"]

    # Format completed section to str to use as context for final sections
    completed_report_sections = format_sections(completed_sections)

    return {"report_sections_from_research": completed_report_sections}

def compile_final_report(state: ReportState):
    """Compile all sections into the final report.
    
    This node:
    1. Gets all completed sections
    2. Orders them according to original plan
    3. Combines them into the final report
    
    Args:
        state: Current state with all completed sections
        
    Returns:
        Dict containing the complete report
    """

    # Get sections
    sections = state["sections"]
    completed_sections = {s.name: s.content for s in state["completed_sections"]}

    # Update sections with completed content while maintaining original order
    for section in sections:
        section.content = completed_sections[section.name]

    # Compile final report
    all_sections = "\n\n".join([s.content for s in sections])

    return {"final_report": all_sections}

def initiate_final_section_writing(state: ReportState):
    """Create parallel tasks for writing non-research sections.
    
    This edge function identifies sections that don't need research and
    creates parallel writing tasks for each one.
    
    Args:
        state: Current state with all sections and research context
        
    Returns:
        List of Send commands for parallel section writing
    """

    # Kick off section writing in parallel via Send() API for any sections that do not require research
    return [
        Send("write_final_sections", {"topic": state["topic"], "section": s, "report_sections_from_research": state["report_sections_from_research"]}) 
        for s in state["sections"] 
        if not s.research
    ]

# Report section sub-graph -- 

# Add nodes 
section_builder = StateGraph(SectionState, output=SectionOutputState)
section_builder.add_node("generate_queries", generate_queries)
section_builder.add_node("search_web", search_web)
section_builder.add_node("write_section", write_section)

# Add edges
section_builder.add_edge(START, "generate_queries")
section_builder.add_edge("generate_queries", "search_web")
section_builder.add_edge("search_web", "write_section")

# Outer graph for initial report plan compiling results from each section -- 

# Add nodes
builder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=Configuration)
builder.add_node("generate_report_plan", generate_report_plan)
builder.add_node("human_feedback", human_feedback)
builder.add_node("build_section_with_web_research", section_builder.compile())
builder.add_node("gather_completed_sections", gather_completed_sections)
builder.add_node("write_final_sections", write_final_sections)
builder.add_node("compile_final_report", compile_final_report)

# Add edges
builder.add_edge(START, "generate_report_plan")
builder.add_edge("generate_report_plan", "human_feedback")
builder.add_edge("build_section_with_web_research", "gather_completed_sections")
builder.add_conditional_edges("gather_completed_sections", initiate_final_section_writing, ["write_final_sections"])
builder.add_edge("write_final_sections", "compile_final_report")
builder.add_edge("compile_final_report", END)

graph = builder.compile()



================================================
FILE: src/open_deep_research/multi_agent.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Multi-Agent Researcher

This notebook demonstrates the multi-agent research approach, which uses a supervisor-researcher collaborative pattern to create comprehensive reports. The system consists of:

1. A **Supervisor Agent** that plans the overall report structure and coordinates work
2. Multiple **Research Agents** that investigate specific topics in parallel
3. A workflow that produces a structured report with introduction, body sections, and conclusion

## From repo 
"""

%cd ..
%load_ext autoreload
%autoreload 2
# Output:
#   /Users/rlm/Desktop/Code/open_deep_research/src

#   /Users/rlm/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.

#     self.shell.db['dhist'] = compress_dhist(dhist)[-100:]


! pip install -U -q open-deep-research

"""
# Compile the multi-agent graph

Next, we'll compile the LangGraph workflow for the multi-agent research approach. This step creates the orchestration layer that manages communication between the supervisor and research agents.
"""

import uuid 
import os, getpass
import open_deep_research   
print(open_deep_research.__version__) 
from IPython.display import Image, display, Markdown
from langgraph.checkpoint.memory import MemorySaver
from open_deep_research.multi_agent import supervisor_builder
# Output:
#   0.0.14


# Create a MemorySaver for checkpointing the agent's state
# This enables tracking and debugging of the multi-agent interaction
checkpointer = MemorySaver()
agent = supervisor_builder.compile(name="research_team", checkpointer=checkpointer)

# Visualize the graph structure
# This shows how supervisor and research agents are connected in the workflow
display(Image(agent.get_graph(xray=1).draw_mermaid_png(max_retries=5)))

# Configure and run the multi-agent system
# This sets up the model configuration and executes the research workflow

# Configure models and search API for both supervisor and researcher roles
config = {
    "thread_id": str(uuid.uuid4()),
    "search_api": "tavily",
    "supervisor_model": "openai:o3",
    "researcher_model": "openai:o3",
    }

# Set up thread configuration with the specified parameters
thread_config = {"configurable": config}

# Define the research topic as a user message
msg = [{"role": "user", "content": "What is model context protocol?"}]

# Run the multi-agent workflow with the specified configuration
response = await agent.ainvoke({"messages": msg}, config=thread_config)

messages = agent.get_state(thread_config).values['messages']
messages[-1].pretty_print()
# Output:
#   ==================================[1m Ai Message [0m==================================

#   

#   Hereâ€™s what Iâ€™ve gathered so far:

#   

#   â€¢ Model Context ProtocolÂ (MCP) is an open, clientâ€‘server standard created to let largeâ€‘languageâ€‘model assistants (Claude, Azure OpenAI, Copilot Studio, etc.) securely â€œplug intoâ€ external data sources and tools.  

#   â€¢ It works like a â€œUSBâ€‘C for AIâ€Â â€” an LLM (the â€œhostâ€) connects through an MCP client to one or more lightweight â€œMCP servers.â€ Each server exposes specific capabilities (APIs, files, databases, SaaS apps, etc.) in a uniform JSON schema, so the model can fetch context or invoke actions.  

#   â€¢ The spec is public and already ships with SDKs (Python, TypeScript, Kotlin). Typical transports are STDIO, Serverâ€‘Sent Events, and WebSocket.  

#   â€¢ Early adopters include Anthropicâ€™s Claude Desktop, Microsoft Azure OpenAI, and Copilot Studio. Thereâ€™s a growing catalogue of preâ€‘built servers for Slack, GitHub, Postgres, Google Drive, etc.

#   

#   Before I outline the report, Iâ€™d like to be sure Iâ€™m targeting the right depth and angle for you.

#   

#   1. What audience should the explanation assume? (e.g., nonâ€‘technical execs, software architects, handsâ€‘on developers)  

#   2. Which aspects interest you most?  

#      a. Highâ€‘level concept & benefits  

#      b. Detailed architecture, data flow, and transport layers  

#      c. SDK usage & code snippets  

#      d. Security / compliance considerations  

#      e. Realâ€‘world adoption stories and roadmap  

#   3. Do you need comparisons with alternative approaches (e.g., â€œfunction calling,â€ LangChain tool interfaces, RAG pipelines without MCP)?  

#   4. Any length or format constraints (slide deck notes, whiteâ€‘paper style, quick FAQ)?  

#   5. Timeline sensitivity: should we cover only the openâ€‘sourced spec (NovÂ 2024) or include 2025 Microsoft integrations as well?

#   

#   Let me know, and Iâ€™ll tailor the research plan accordingly.


msg =  [{"role": "user", "content": "Focus on Anthropicâ€‘backed open standard for integrating external context and tools with LLMs, give an architectural overview for developers, tell me about interesting MCP servers, compare to google Agent2Agent (A2A) protocol. write the report and dont ask any follow up questions"}]
response = await agent.ainvoke({"messages": msg}, config=thread_config)
for m in agent.get_state(thread_config).values['messages']:
    m.pretty_print()
# Output:
#   ================================[1m Human Message [0m=================================

#   

#   What is model context protocol?

#   ==================================[1m Ai Message [0m==================================

#   Tool Calls:

#     tavily_search (call_UAovh3IIwOUUGLXUu56A6VhD)

#    Call ID: call_UAovh3IIwOUUGLXUu56A6VhD

#     Args:

#       queries: ['"model context protocol" MCP']

#   =================================[1m Tool Message [0m=================================

#   Name: tavily_search

#   

#   Search results: 

#   

#   

#   

#   --- SOURCE 1: Unleashing the Power of Model Context Protocol (MCP): A Game-Changer in ... ---

#   URL: https://techcommunity.microsoft.com/blog/educatordeveloperblog/unleashing-the-power-of-model-context-protocol-mcp-a-game-changer-in-ai-integrat/4397564

#   

#   SUMMARY:

#   What is Model Context Protocol (MCP)? MCP is a protocol designed to enable AI models, such as Azure OpenAI models, to interact seamlessly with external tools and services. Think of MCP as a universal USB-C connector for AI, allowing language models to fetch information, interact with APIs, and execute tasks beyond their built-in knowledge. Key

#   

#   FULL CONTENT:

#   Published Time: 3/27/2025, 7:57:10 AM

#   Unleashing the Power of Model Context Protocol (MCP): A Game-Changer in AI Integration | Microsoft Community Hub

#   Skip to content

#   Tech CommunityCommunity Hubs

#   Products

#   Topics

#   BlogsEvents

#   Microsoft Learn

#   Lounge

#   More

#   RegisterSign In

#   

#   

#   Microsoft Community Hub

#   

#   

#   CommunitiesTopics

#   

#   

#   Education Sector

#   

#   

#   Educator Developer Blog

#   

#   

#   Report

#   Connect with experts and redefine whatâ€™s possible at work â€“ join us at the Microsoft 365 Community Conference May 6-8. Learn more >

#   Educator Developer Blog

#   Blog Post

#   Educator Developer Blog

#   4 MIN READ

#   Unleashing the Power of Model Context Protocol (MCP): A Game-Changer in AI Integration

#   

#   Sharda_Kaur

#   Brass Contributor

#   Mar 27, 2025

#   Artificial Intelligence is evolving rapidly, and one of the most pressing challenges is enabling AI models to interact effectively with external tools, data sources, and APIs. The Model Context Protocol (MCP) solves this problem by acting as a bridge between AI models and external services, creating a standardized communication framework that enhances tool integration, accessibility, and AI reasoning capabilities.

#   What is Model Context Protocol (MCP)?

#   MCP is a protocol designed to enable AI models, such as Azure OpenAI models, to interact seamlessly with external tools and services. Think of MCP as a universal USB-C connector for AI, allowing language models to fetch information, interact with APIs, and execute tasks beyond their built-in knowledge.

#   

#   Key Features of MCP

#   

#   Standardized Communication â€“ MCP provides a structured way for AI models to interact with various tools.

#   Tool Access & Expansion â€“ AI assistants can now utilize external tools for real-time insights.

#   Secure & Scalable â€“ Enables safe and scalable integration with enterprise applications.

#   Multi-Modal Integration â€“ Supports STDIO, SSE (Server-Sent Events), and WebSocket communication methods.

#   

#   MCP Architecture & How It Works

#   MCP follows a client-server architecture that allows AI models to interact with external tools efficiently. Hereâ€™s how it works:

#   Components of MCP

#   

#   MCP Host â€“ The AI model (e.g., Azure OpenAI GPT) requesting data or actions.

#   MCP Client â€“ An intermediary service that forwards the AI model's requests to MCP servers.

#   MCP Server â€“ Lightweight applications that expose specific capabilities (APIs, databases, files, etc.).

#   Data Sources â€“ Various backend systems, including local storage, cloud databases, and external APIs.

#   

#   Data Flow in MCP

#   

#   The AI model sends a request (e.g., "fetch user profile data").

#   The MCP client forwards the request to the appropriate MCP server.

#   The MCP server retrieves the required data from a database or API.

#   The response is sent back to the AI model via the MCP client.

#   

#   Integrating MCP with Azure OpenAI Services

#   Microsoft has integrated MCP with Azure OpenAI Services, allowing GPT models to interact with external services and fetch live data. This means AI models are no longer limited to static knowledge but can access real-time information.

#   Benefits of Azure OpenAI Services + MCP Integration

#   âœ” Real-time Data Fetching â€“ AI assistants can retrieve fresh information from APIs, databases, and internal systems.

#   âœ” Contextual AI Responses â€“ Enhances AI responses by providing accurate, up-to-date information.

#   âœ” Enterprise-Ready â€“ Secure and scalable for business applications, including finance, healthcare, and retail.

#   Hands-On Tools for MCP Implementation

#   To implement MCP effectively, Microsoft provides two powerful tools: Semantic Workbench and AI Gateway.

#   Microsoft Semantic Workbench

#   A development environment for prototyping AI-powered assistants and integrating MCP-based functionalities.

#   Features:

#   

#   Build and test multi-agent AI assistants.

#   Configure settings and interactions between AI models and external tools.

#   Supports GitHub Codespaces for cloud-based development.

#   

#   Explore Semantic Workbench

#   Workbench interface examples

#   

#   Microsoft AI Gateway

#   A plug-and-play interface that allows developers to experiment with MCP using Azure API Management.

#   Features:

#   

#   Credential Manager â€“ Securely handle API credentials.

#   Live Experimentation â€“ Test AI model interactions with external tools.

#   Pre-built Labs â€“ Hands-on learning for developers.

#   

#   Explore AI Gateway

#   Setting Up MCP with Azure OpenAI Services

#   Step 1: Create a Virtual Environment

#   First, create a virtual environment using Python:

#   python

#   python -m venv .venv

#   Activate the environment:

#   # Windows

#   python

#   venv\Scripts\activate

#   # MacOS/Linux

#   python

#   source .venv/bin/activate

#   Step 2: Install Required Libraries

#   Create a requirements.txt file and add the following dependencies:

#   ```python

#   langchain-mcp-adapters

#   langgraph

#   langchain-openai

#   ```

#   Then, install the required libraries:

#   python

#   pip install -r requirements.txt

#   Step 3: Set Up OpenAI API Key

#   Ensure you have your OpenAI API key set up:

#   # Windows

#   python

#   setx OPENAI_API_KEY "<your_api_key>

#   # MacOS/Linux

#   python

#   export OPENAI_API_KEY=<your_api_key>

#   Building an MCP Server

#   This server performs basic mathematical operations like addition and multiplication.

#   Create the Server File

#   First, create a new Python file:

#   python

#   touch math_server.py

#   Then, implement the server:

#   python

#   from mcp.server.fastmcp import FastMCP

#   ```python

#   Initialize the server

#   mcp = FastMCP("Math")

#   MCP.tool()

#   def add(a: int, b: int) -> int:

#   return a + b

#   

#   MCP.tool()

#   def multiply(a: int, b: int) -> int:

#   return a * b

#   

#   if name == "main":

#   mcp.run(transport="stdio")

#   

#   ```

#   Your MCP server is now ready to run.

#   Building an MCP Client

#   This client connects to the MCP server and interacts with it.

#   Create the Client File

#   First, create a new file:

#   python

#   touch client.py

#   Then, implement the client:

#   ```python

#   import asyncio

#   from mcp import ClientSession, StdioServerParameters

#   from langchain_openai import ChatOpenAI

#   from mcp.client.stdio import stdio_client

#   Define server parameters

#   server_params = StdioServerParameters(

#   command="python",

#   

#   args=["math_server.py"],

#   

#   )

#   Define the model

#   model = ChatOpenAI(model="gpt-4o")

#   async def run_agent():

#   async with stdio_client(server_params) as (read, write):

#   

#       async with ClientSession(read, write) as session:

#   

#           await session.initialize()

#   

#           tools = await load_mcp_tools(session)

#   

#           agent = create_react_agent(model, tools)

#   

#           agent_response = await agent.ainvoke({"messages": "what's (4 + 6) x 14?"})

#   

#           return agent_response["messages"][3].content

#   

#   if name == "main":

#   result = asyncio.run(run_agent())

#   

#   print(result)

#   

#   ```

#   Your client is now set up and ready to interact with the MCP server.

#   Running the MCP Server and Client

#   Step 1: Start the MCP Server

#   Open a terminal and run:

#   python

#   python math_server.py

#   This starts the MCP server, making it available for client connections.

#   Step 2: Run the MCP Client

#   In another terminal, run:

#   python

#   python client.py

#   Expected Output

#   140

#   This means the AI agent correctly computed (4 + 6) x 14 using both the MCP server and GPT-4o.

#   Conclusion

#   Integrating MCP with Azure OpenAI Services enables AI applications to securely interact with external tools, enhancing functionality beyond text-based responses. With standardized communication and improved AI capabilities, developers can build smarter and more interactive AI-powered solutions. By following this guide, you can set up an MCP server and client, unlocking the full potential of AI with structured external interactions.

#   Next Steps:

#   

#   Explore more MCP tools and integrations.

#   Extend your MCP setup to work with additional APIs.

#   Deploy your solution in a cloud environment for broader accessibility.

#   

#   For further details, visit the GitHub repository for MCP integration examples and best practices.

#   

#   MCP GitHub Repository

#   MCP Documentation

#   Semantic Workbench

#   AI Gateway

#   MCP Video Walkthrough

#   MCP Blog

#   MCP Github End to End Demo

#   

#   Updated Mar 27, 2025

#   Version 1.0

#   azure

#   Azure AI Agents

#   Azure AI Model

#   mcp

#   openai

#   Semantic Kernal

#   Semantic Search

#   Like

#   2

#   Comment

#   

#   Sharda_Kaur

#   Brass Contributor

#   Joined December 16, 2023

#   Send Message

#   View Profile

#   

#   Educator Developer Blog

#   Follow this blog board to get notified when there's new activity

#   Share

#   

#   

#   

#   

#   

#   

#   

#   

#   What's new

#   

#   Surface Pro 9

#   Surface Laptop 5

#   Surface Studio 2+

#   Surface Laptop Go 2

#   Surface Laptop Studio

#   Surface Duo 2

#   Microsoft 365

#   Windows 11 apps

#   

#   Microsoft Store

#   

#   Account profile

#   Download Center

#   Microsoft Store support

#   Returns

#   Order tracking

#   Virtual workshops and training

#   Microsoft Store Promise

#   Flexible Payments

#   

#   Education

#   

#   Microsoft in education

#   Devices for education

#   Microsoft Teams for Education

#   Microsoft 365 Education

#   Education consultation appointment

#   Educator training and development

#   Deals for students and parents

#   Azure for students

#   

#   Business

#   

#   Microsoft Cloud

#   Microsoft Security

#   Dynamics 365

#   Microsoft 365

#   Microsoft Power Platform

#   Microsoft Teams

#   Microsoft Industry

#   Small Business

#   

#   Developer & IT

#   

#   Azure

#   Developer Center

#   Documentation

#   Microsoft Learn

#   Microsoft Tech Community

#   Azure Marketplace

#   AppSource

#   Visual Studio

#   

#   Company

#   

#   Careers

#   About Microsoft

#   Company news

#   Privacy at Microsoft

#   Investors

#   Diversity and inclusion

#   Accessibility

#   Sustainability

#   

#   Your Privacy Choices

#   

#   Sitemap

#   Contact Microsoft

#   Privacy

#   Manage cookies

#   Terms of use

#   Trademarks

#   Safety & eco

#   About our ads

#   Â© Microsoft 2024

#   

#   "}},"componentScriptGroups({\"componentId\":\"custom.widget.Social_Sharing\"})":{"__typename":"ComponentScriptGroups","scriptGroups":{"__typename":"ComponentScriptGroupsDefinition","afterInteractive":{"__typename":"PageScriptGroupDefinition","group":"AFTER_INTERACTIVE","scriptIds":[]},"lazyOnLoad":{"__typename":"PageScriptGroupDefinition","group":"LAZY_ON_LOAD","scriptIds":[]}},"componentScripts":[]},"component({\"componentId\":\"custom.widget.MicrosoftFooter\"})":{"__typename":"Component","render({\"context\":{\"component\":{\"entities\":[],\"props\":{}},\"page\":{\"entities\":[\"board:EducatorDeveloperBlog\",\"message:4397564\"],\"name\":\"BlogMessagePage\",\"props\":{},\"url\":\"https://techcommunity.microsoft.com/blog/educatordeveloperblog/unleashing-the-power-of-model-context-protocol-mcp-a-game-changer-in-ai-integrat/4397564\"}}})":{"__typename":"ComponentRenderResult","html":"

#   What's new

#   

#   Surface Pro 9

#   Surface Laptop 5

#   Surface Studio 2+

#   Surface Laptop Go 2

#   Surface Laptop Studio

#   Surface Duo 2

#   Microsoft 365

#   Windows 11 apps

#   

#   Microsoft Store

#   

#   Account profile

#   Download Center

#   Microsoft Store support

#   Returns

#   Order tracking

#   Virtual workshops and training

#   Microsoft Store Promise

#   Flexible Payments

#   

#   Education

#   

#   Microsoft in education

#   Devices for education

#   Microsoft Teams for Education

#   Microsoft 365 Education

#   Education consultation appointment

#   Educator training and development

#   Deals for students and parents

#   Azure for students

#   

#   Business

#   

#   Microsoft Cloud

#   Microsoft Security

#   Dynamics 365

#   Microsoft 365

#   Microsoft Power Platform

#   Microsoft Teams

#   Microsoft Industry

#   Small Business

#   

#   Developer & IT

#   

#   Azure

#   Developer Center

#   Documentation

#   Microsoft Learn

#   Microsoft Tech Community

#   Azure Marketplace

#   AppSource

#   Visual Studio

#   

#   Company

#   

#   Careers

#   About Microsoft

#   Company news

#   Privacy at Microsoft

#   Investors

#   Diversity and inclusion

#   Accessibility

#   Sustainability

#   

#   Your Privacy Choices

#   

#   Sitemap

#   Contact Microsoft

#   Privacy

#   Manage cookies

#   Terms of use

#   Trademarks

#   Safety & eco

#   About our ads

#   Â© Microsoft 2024

#   

#   "}},"componentScriptGroups({\"componentId\":\"custom.widget.MicrosoftFooter\"})":{"__typename":"ComponentScriptGroups","scriptGroups":{"__typename":"ComponentScriptGroupsDefinition","afterInteractive":{"__typename":"PageScriptGroupDefinition","group":"AFTER_INTERACTIVE","scriptIds":[]},"lazyOnLoad":{"__typename":"PageScriptGroupDefinition","group":"LAZY_ON_LOAD","scriptIds":[]}},"componentScripts":[]},"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/community/NavbarDropdownToggle\"]})":[{"__ref":"CachedAsset:text:en_US-components/community/NavbarDropdownToggle-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/common/QueryHandler\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/common/QueryHandler-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageCoverImage\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageCoverImage-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/nodes/NodeTitle\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/nodes/NodeTitle-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageTimeToRead\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageTimeToRead-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageSubject\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageSubject-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/users/UserLink\"]})":[{"__ref":"CachedAsset:text:en_US-components/users/UserLink-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/users/UserRank\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/users/UserRank-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageTime\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageTime-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageBody\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageBody-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageCustomFields\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageCustomFields-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageRevision\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageRevision-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageReplyButton\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageReplyButton-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/messages/MessageAuthorBio\"]})":[{"__ref":"CachedAsset:text:en_US-components/messages/MessageAuthorBio-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/users/UserAvatar\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/users/UserAvatar-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/ranks/UserRankLabel\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/ranks/UserRankLabel-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/users/UserRegistrationDate\"]})":[{"__ref":"CachedAsset:text:en_US-components/users/UserRegistrationDate-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/nodes/NodeAvatar\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/nodes/NodeAvatar-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/nodes/NodeDescription\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/nodes/NodeDescription-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"components/tags/TagView/TagViewChip\"]})":[{"__ref":"CachedAsset:text:en_US-components/tags/TagView/TagViewChip-1743151752454"}],"cachedText({\"lastModified\":\"1743151752454\",\"locale\":\"en-US\",\"namespaces\":[\"shared/client/components/nodes/NodeIcon\"]})":[{"__ref":"CachedAsset:text:en_US-shared/client/components/nodes/NodeIcon-1743151752454"}]},"CachedAsset:pages-1743057517558":{"__typename":"CachedAsset","id":"pages-1743057517558","value":[{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogViewAllPostsPage","type":"BLOG","urlPath":"/category/:categoryId/blog/:boardId/all-posts/(/:after|/:before)?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CasePortalPage","type":"CASE_PORTAL","urlPath":"/caseportal","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CreateGroupHubPage","type":"GROUP_HUB","urlPath":"/groups/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CaseViewPage","type":"CASE_DETAILS","urlPath":"/case/:caseId/:caseNumber","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"InboxPage","type":"COMMUNITY","urlPath":"/inbox","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"HelpFAQPage","type":"COMMUNITY","urlPath":"/help","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaMessagePage","type":"IDEA_POST","urlPath":"/idea/:boardId/:messageSubject/:messageId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaViewAllIdeasPage","type":"IDEA","urlPath":"/category/:categoryId/ideas/:boardId/all-ideas/(/:after|/:before)?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"LoginPage","type":"USER","urlPath":"/signin","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogPostPage","type":"BLOG","urlPath":"/category/:categoryId/blogs/:boardId/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"UserBlogPermissions.Page","type":"COMMUNITY","urlPath":"/c/user-blog-permissions/page","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ThemeEditorPage","type":"COMMUNITY","urlPath":"/designer/themes","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TkbViewAllArticlesPage","type":"TKB","urlPath":"/category/:categoryId/kb/:boardId/all-articles/(/:after|/:before)?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1730819800000,"localOverride":null,"page":{"id":"AllEvents","type":"CUSTOM","urlPath":"/Events","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"OccasionEditPage","type":"EVENT","urlPath":"/event/:boardId/:messageSubject/:messageId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"OAuthAuthorizationAllowPage","type":"USER","urlPath":"/auth/authorize/allow","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"PageEditorPage","type":"COMMUNITY","urlPath":"/designer/pages","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"PostPage","type":"COMMUNITY","urlPath":"/category/:categoryId/:boardId/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForumBoardPage","type":"FORUM","urlPath":"/category/:categoryId/discussions/:boardId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TkbBoardPage","type":"TKB","urlPath":"/category/:categoryId/kb/:boardId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"EventPostPage","type":"EVENT","urlPath":"/category/:categoryId/events/:boardId/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"UserBadgesPage","type":"COMMUNITY","urlPath":"/users/:login/:userId/badges","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"GroupHubMembershipAction","type":"GROUP_HUB","urlPath":"/membership/join/:nodeId/:membershipType","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"MaintenancePage","type":"COMMUNITY","urlPath":"/maintenance","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaReplyPage","type":"IDEA_REPLY","urlPath":"/idea/:boardId/:messageSubject/:messageId/comments/:replyId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"UserSettingsPage","type":"USER","urlPath":"/mysettings/:userSettingsTab","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"GroupHubsPage","type":"GROUP_HUB","urlPath":"/groups","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForumPostPage","type":"FORUM","urlPath":"/category/:categoryId/discussions/:boardId/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"OccasionRsvpActionPage","type":"OCCASION","urlPath":"/event/:boardId/:messageSubject/:messageId/rsvp/:responseType","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"VerifyUserEmailPage","type":"USER","urlPath":"/verifyemail/:userId/:verifyEmailToken","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"AllOccasionsPage","type":"OCCASION","urlPath":"/category/:categoryId/events/:boardId/all-events/(/:after|/:before)?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"EventBoardPage","type":"EVENT","urlPath":"/category/:categoryId/events/:boardId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TkbReplyPage","type":"TKB_REPLY","urlPath":"/kb/:boardId/:messageSubject/:messageId/comments/:replyId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaBoardPage","type":"IDEA","urlPath":"/category/:categoryId/ideas/:boardId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CommunityGuideLinesPage","type":"COMMUNITY","urlPath":"/communityguidelines","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CaseCreatePage","type":"SALESFORCE_CASE_CREATION","urlPath":"/caseportal/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TkbEditPage","type":"TKB","urlPath":"/kb/:boardId/:messageSubject/:messageId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForgotPasswordPage","type":"USER","urlPath":"/forgotpassword","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaEditPage","type":"IDEA","urlPath":"/idea/:boardId/:messageSubject/:messageId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TagPage","type":"COMMUNITY","urlPath":"/tag/:tagName","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogBoardPage","type":"BLOG","urlPath":"/category/:categoryId/blog/:boardId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"OccasionMessagePage","type":"OCCASION_TOPIC","urlPath":"/event/:boardId/:messageSubject/:messageId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ManageContentPage","type":"COMMUNITY","urlPath":"/managecontent","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ClosedMembershipNodeNonMembersPage","type":"GROUP_HUB","urlPath":"/closedgroup/:groupHubId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"CommunityPage","type":"COMMUNITY","urlPath":"/","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForumMessagePage","type":"FORUM_TOPIC","urlPath":"/discussions/:boardId/:messageSubject/:messageId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"IdeaPostPage","type":"IDEA","urlPath":"/category/:categoryId/ideas/:boardId/create","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1730819800000,"localOverride":null,"page":{"id":"CommunityHub.Page","type":"CUSTOM","urlPath":"/Directory","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogMessagePage","type":"BLOG_ARTICLE","urlPath":"/blog/:boardId/:messageSubject/:messageId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"RegistrationPage","type":"USER","urlPath":"/register","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"EditGroupHubPage","type":"GROUP_HUB","urlPath":"/group/:groupHubId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForumEditPage","type":"FORUM","urlPath":"/discussions/:boardId/:messageSubject/:messageId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ResetPasswordPage","type":"USER","urlPath":"/resetpassword/:userId/:resetPasswordToken","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1730819800000,"localOverride":null,"page":{"id":"AllBlogs.Page","type":"CUSTOM","urlPath":"/blogs","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"TkbMessagePage","type":"TKB_ARTICLE","urlPath":"/kb/:boardId/:messageSubject/:messageId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogEditPage","type":"BLOG","urlPath":"/blog/:boardId/:messageSubject/:messageId/edit","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ManageUsersPage","type":"USER","urlPath":"/users/manage/:tab?/:manageUsersTab?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ForumReplyPage","type":"FORUM_REPLY","urlPath":"/discussions/:boardId/:messageSubject/:messageId/replies/:replyId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"PrivacyPolicyPage","type":"COMMUNITY","urlPath":"/privacypolicy","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"NotificationPage","type":"COMMUNITY","urlPath":"/notifications","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"UserPage","type":"USER","urlPath":"/users/:login/:userId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"OccasionReplyPage","type":"OCCASION_REPLY","urlPath":"/event/:boardId/:messageSubject/:messageId/comments/:replyId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"ManageMembersPage","type":"GROUP_HUB","urlPath":"/group/:groupHubId/manage/:tab?","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"SearchResultsPage","type":"COMMUNITY","urlPath":"/search","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"BlogReplyPage","type":"BLOG_REPLY","urlPath":"/blog/:boardId/:messageSubject/:messageId/replies/:replyId","__typename":"PageDescriptor"},"__typename":"PageResource"},{"lastUpdatedTime":1743057517558,"localOverride":null,"page":{"id":"GroupHubPage","type":"GROUP_HUB","urlPath":"/group/:groupHubId","__typename":"PageDescriptor"},"__typename":"PageR

#   

#   --------------------------------------------------------------------------------

#   

#   

#   --- SOURCE 2: Introducing the Model Context Protocol \ Anthropic ---

#   URL: https://www.anthropic.com/news/model-context-protocol

#   

#   SUMMARY:

#   Today, we're open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. Early adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platformsâ€”enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts. All Claude.ai plans support connecting MCP servers to the Claude Desktop app.

#   

#   FULL CONTENT:

#   Introducing the Model Context Protocol \ Anthropic

#   Skip to main contentSkip to footer

#   

#   Claude

#   API

#   Research

#   Commitments

#   Learn

#   News

#   Try Claude

#   Announcements

#   Introducing the Model Context Protocol

#   Nov 25, 2024â—3 min read

#   

#   Today, we're open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.

#   As AI assistants gain mainstream adoption, the industry has invested heavily in model capabilities, achieving rapid advances in reasoning and quality. Yet even the most sophisticated models are constrained by their isolation from dataâ€”trapped behind information silos and legacy systems. Every new data source requires its own custom implementation, making truly connected systems difficult to scale.

#   MCP addresses this challenge. It provides a universal, open standard for connecting AI systems with data sources, replacing fragmented integrations with a single protocol. The result is a simpler, more reliable way to give AI systems access to the data they need.

#   Model Context Protocol

#   The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.

#   Today, we're introducing three major components of the Model Context Protocol for developers:

#   

#   The Model Context Protocol specification and SDKs

#   Local MCP server support in the Claude Desktop apps

#   An open-source repository of MCP servers

#   

#   Claude 3.5 Sonnet is adept at quickly building MCP server implementations, making it easy for organizations and individuals to rapidly connect their most important datasets with a range of AI-powered tools. To help developers start exploring, weâ€™re sharing pre-built MCP servers for popular enterprise systems like Google Drive, Slack, GitHub, Git, Postgres, and Puppeteer.

#   Early adopters like Block and Apollo have integrated MCP into their systems, while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platformsâ€”enabling AI agents to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with fewer attempts.

#   "At Block, open source is more than a development modelâ€”itâ€™s the foundation of our work and a commitment to creating technology that drives meaningful change and serves as a public good for all,â€ said Dhanji R. Prasanna, Chief Technology Officer at Block. â€œOpen technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build agentic systems, which remove the burden of the mechanical so people can focus on the creative.â€

#   Instead of maintaining separate connectors for each data source, developers can now build against a standard protocol. As the ecosystem matures, AI systems will maintain context as they move between different tools and datasets, replacing today's fragmented integrations with a more sustainable architecture.

#   Getting started

#   Developers can start building and testing MCP connectors today. All Claude.ai plans support connecting MCP servers to the Claude Desktop app.

#   Claude for Work customers can begin testing MCP servers locally, connecting Claude to internal systems and datasets. We'll soon provide developer toolkits for deploying remote production MCP servers that can serve your entire Claude for Work organization.

#   To start building:

#   

#   Install pre-built MCP servers through the Claude Desktop app

#   Follow our quickstart guide to build your first MCP server

#   Contribute to our open-source repositories of connectors and implementations

#   

#   An open community

#   Weâ€™re committed to building MCP as a collaborative, open-source project and ecosystem, and weâ€™re eager to hear your feedback. Whether youâ€™re an AI tool developer, an enterprise looking to leverage existing data, or an early adopter exploring the frontier, we invite you to build the future of context-aware AI together.

#   

#   

#   Product

#   

#   Claude overview

#   Claude team plan

#   Claude enterprise plan

#   Download Claude apps

#   Claude.ai pricing plans

#   Claude.ai login

#   

#   API Platform

#   

#   API overview

#   Developer docs

#   Pricing

#   Console login

#   

#   Research

#   

#   Research overview

#   Economic Index

#   

#   Claude models

#   

#   Claude 3.7 Sonnet

#   Claude 3.5 Haiku

#   Claude 3 Opus

#   

#   Commitments

#   

#   Transparency

#   Responsible scaling policy

#   Security and compliance

#   

#   Solutions

#   

#   Coding

#   

#   Learning resources

#   

#   News

#   Customer stories

#   Engineering at Anthropic

#   

#   Company

#   

#   About us

#   Careers

#   

#   Help and security

#   

#   Status

#   Availability

#   Support center

#   

#   Terms and policies

#   Privacy choices*   Privacy policy

#   *   Responsible disclosure policy

#   *   Terms of service - consumer

#   *   Terms of service - commercial

#   *   Usage policy

#   Â© 2025 Anthropic PBC

#   

#   

#   --------------------------------------------------------------------------------

#   

#   

#   --- SOURCE 3: Model Context Protocol (MCP) - Anthropic ---

#   URL: https://docs.anthropic.com/en/docs/agents-and-tools/mcp

#   

#   SUMMARY:

#   Model Context Protocol (MCP) - Anthropic Anthropic home page Go to claude.ai Go to claude.ai Model Context Protocol (MCP) Support Intro to Claude Learn about Claude Use cases Build with Claude Develop test cases Tool use (function calling) Claude Code Model Context Protocol (MCP) Using the Evaluation Tool Admin API Claude 3 model card Claude 3.7 system card Model Context Protocol (MCP) MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools. Computer use (beta)Google Sheets add-on

#   

#   FULL CONTENT:

#   Model Context Protocol (MCP) - Anthropic

#   Anthropic home page

#   English

#   Search...

#   Ctrl K

#   

#   Research

#   News

#   Go to claude.ai

#   Go to claude.ai

#   

#   Search...

#   Navigation

#   Agents and tools

#   Model Context Protocol (MCP)

#   WelcomeUser GuidesAPI ReferencePrompt LibraryRelease Notes

#   

#   Developer Console

#   Developer Discord

#   Support

#   

#   Get started

#   

#   Overview

#   Initial setup

#   Intro to Claude

#   

#   Learn about Claude

#   

#   

#   Use cases

#   

#   

#   Models & pricing

#   

#   

#   Security and compliance

#   

#   

#   Build with Claude

#   

#   Define success criteria

#   Develop test cases

#   Context windows

#   Vision

#   

#   Prompt engineering

#   

#   

#   Extended thinking

#   

#   Multilingual support

#   

#   Tool use (function calling)

#   

#   

#   Prompt caching

#   

#   PDF support

#   Citations

#   Token counting

#   Batch processing

#   Embeddings

#   

#   Agents and tools

#   

#   

#   Claude Code

#   

#   

#   Computer use (beta)

#   

#   Model Context Protocol (MCP)

#   Google Sheets add-on

#   

#   Test and evaluate

#   

#   

#   Strengthen guardrails

#   

#   

#   Using the Evaluation Tool

#   

#   

#   Administration

#   

#   Admin API

#   

#   Resources

#   

#   Glossary

#   Model deprecations

#   System status

#   Claude 3 model card

#   Claude 3.7 system card

#   Anthropic Cookbook

#   Anthropic Courses

#   API features

#   

#   Legal center

#   

#   Anthropic Privacy Policy

#   

#   Agents and tools

#   Model Context Protocol (MCP)

#   MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

#   MCP Documentation ----------------- Learn more about the protocol, how to build servers and clients, and discover those made by others.MCP in Claude Desktop --------------------- Learn how to set up MCP in Claude for Desktop, such as letting Claude read and write files to your computerâ€™s file system.

#   Was this page helpful?

#   YesNo

#   Computer use (beta)Google Sheets add-on

#   xlinkedin

#   

#   --------------------------------------------------------------------------------

#   

#   

#   --- SOURCE 4: Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified ... ---

#   URL: https://www.microsoft.com/en-us/microsoft-copilot/blog/copilot-studio/introducing-model-context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents/

#   

#   SUMMARY:

#   Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents | Microsoft Copilot Blog Microsoft All Microsoft Microsoft Copilot Studio Blog Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents Thatâ€™s why weâ€™re thrilled to announce the first release of Model Context Protocol (MCP) support in Microsoft Copilot Studio. Thatâ€™s why weâ€™re thrilled to announce the first release of Model Context Protocol (MCP) support in Microsoft Copilot Studio. Learn more about these new capabilities here: Extend your agent with Model Context Protocol (preview) â€“ Microsoft Copilot Studio | Microsoft Learn. Microsoft Copilot Microsoft 365 Copilot

#   

#   FULL CONTENT:

#   Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents | Microsoft Copilot Blog

#   Skip to main content

#    Microsoft

#   Copilot

#   Copilot

#   Copilot

#   

#   Home

#   

#   Get Started

#   

#   Download the Copilot app

#   Try free version of Copilot

#   For business

#   

#   

#   

#   Products

#   

#   Copilot Labs

#   Copilot in Edge

#   Copilot in Windows

#   Copilot Pro

#   

#   

#   

#   Resources

#   

#   Do more with Copilot

#   AI art prompting guide

#   Copilot blog

#   AI blog

#   AI

#   Learn

#   Build

#   

#   

#   

#   For organizations

#   

#   

#   More

#   

#   

#   All Microsoft

#   

#   

#   Global

#       ------

#   

#   Microsoft 365

#   Teams

#   Copilot

#   Windows

#   Surface

#   Xbox

#   Deals

#   Small Business

#   Support

#   Software Software

#   

#   

#   Windows Apps

#   AI

#   Outlook

#   OneDrive

#   Microsoft Teams

#   OneNote

#   Microsoft Edge

#   Skype

#   PCs & Devices PCs & Devices

#   

#   

#   Computers

#   Shop Xbox

#   Accessories

#   VR & mixed reality

#   Certified Refurbished

#   Trade-in for cash

#   Entertainment Entertainment

#   

#   

#   Xbox Game Pass Ultimate

#   PC Game Pass

#   Xbox games

#   PC and Windows games

#   Movies & TV

#   Business Business

#   

#   

#   Microsoft Cloud

#   Microsoft Security

#   Dynamics 365

#   Microsoft 365 for business

#   Microsoft Power Platform

#   Windows 365

#   Microsoft Industry

#   Small Business

#   Developer & IT Developer & IT

#   

#   

#   Azure

#   Microsoft Developer

#   Microsoft Learn

#   Explore ISV Success

#   Microsoft Tech Community

#   Azure Marketplace

#   AppSource

#   Visual Studio

#   Other Other

#   

#   

#   Microsoft Rewards

#   Free downloads & security

#   Education

#   Gift cards

#   Licensing

#   Unlocked stories

#   View Sitemap

#   

#   

#   

#   

#   

#   

#   

#   Search Search Microsoft.com

#   

#   No results

#   

#   Cancel

#   Light Dark

#   

#   Home

#   Microsoft Copilot Studio Blog

#   Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents

#   

#   Search for:  Submit search

#   

#   Published Mar 19, 2025

#   3 min read

#   

#   Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents

#   By Zankar Desai

#   Share

#   

#   

#   

#   

#   

#   

#   

#   Category

#   

#   Announcements

#   Copilot Studio

#   Extensibility

#   

#   more

#   At Microsoft, we believe in creating tools that empower you to work smarter and more efficiently. Thatâ€™s why weâ€™re thrilled to announce the first release of Model Context Protocol (MCP) support in Microsoft Copilot Studio. With MCP, you can easily add AI apps and agents into Copilot Studio with just a few clicks. Whatâ€™s new:

#   At Microsoft, we believe in creating tools that empower you to work smarter and more efficiently. Thatâ€™s why weâ€™re thrilled to announce the first release of Model Context Protocol (MCP) support in Microsoft Copilot Studio. With MCP, you can easily add AI apps and agents into Copilot Studio with just a few clicks.

#   Whatâ€™s new: Model Context Protocol integration

#   Model Context Protocol (MCP) enables makers to connect to existing knowledge servers and APIs directly from Copilot Studio. When connecting to an MCP server, actions and knowledge are automatically added to the agent and updated as functionality evolves. This simplifies the process of building agents and reduces time spent maintaining the agents.

#   MCP servers are made available to Copilot Studio using connector infrastructure. This means they can employ enterprise security and governance controls such as Virtual Network integration, Data Loss Prevention controls, multiple authentication methodsâ€”all of which are available in this releaseâ€”while supporting real-time data access for AI-powered agents.

#   MCP enables our customers to:

#   

#   Easily connect to data sources: Whether you have a custom internal API or external data providers, the MCP protocol enables smooth and reliable integration into Copilot Studio.

#   Access the marketplace of existing servers: In addition to custom connectors and integrations, users can now tap into a growing library of pre-built, MCP-enabled connectors available in the marketplace. This capability gives you more ways to connect with other tools and makes using them faster and easier.

#   Flexible action capabilities: MCP servers can dynamically provide tools and data to agents. This enables greater flexibility while reducing maintenance and integration costs.

#   

#   To get started, access your agent in Copilot Studio, select â€˜Add an action,â€™ and search for your MCP server!

#   This offering additionally includes Software Development Kit (SDK) support, enabling further customization and flexibility for your integrations. To create your own Model Context Protocol server, the process can be broken down into three key steps:

#   

#   Create the server: The first step in integrating Copilot Studio with the MCP is to create a server via one of the SDKs that will serve as the foundation for handling your data, models, and interactions. You can tailor the server to your specific needs, such as enabling it to handle custom model types and data formats or to support specific workflows.Â  Â 

#   Publish through a connector: Once the server is in place, the next step involves creating a custom connector that links your Copilot Studio environment to the model or data source.

#   Consume the data via Copilot Studio: Finally, once the server is set up and the connector is defined, you can begin consuming the data and interacting with the models via Copilot Studio.

#   

#   By following these three steps, you create a streamlined, adaptable integration with Copilot Studio that not only connects systems but also enhances your ability to maintain and scale that integration according to your needs.

#   We support Server-Sent Events (SSE) as the transport mechanism; this feature is currently in environments in preview regions and will be available across all environments shortly.

#   Learn more about these new capabilities here: Extend your agent with Model Context Protocol (preview) â€“ Microsoft Copilot Studio | Microsoft Learn.

#   Whatâ€™s next?

#   Weâ€™re excited about the potential of Model Context Protocol and its ability to transform the way users interact with Copilot Studio. But this is just the beginning. Our team is actively working on additional features and improvements to further enhance the integration experience. Stay tuned for more updates, as we plan to introduce even more ways to connect your data and tools effortlessly into Copilot Studio.

#   We look forward to your feedback and learning more on how this new capability enhances your experience and helps you unlock the full power of Copilot Studio.

#   

#   Extend your agents with Model Context Protocol in Copilot Studio

#   Learn more

#   

#   Zankar Desai

#   See more articles from this author >

#   Related Posts

#   

#   

#   

#   

#   Copilot Studio

#   Mar 11

#   7 min read

#   

#   How to deploy transformational enterprise-wide agents: Microsoft as Customer Zero

#   

#   

#   

#   

#   Copilot Studio

#   Mar 3

#   8 min read

#   

#   Whatâ€™s new in Copilot Studio: February 2025

#   

#   

#   

#   

#   Copilot Studio

#   Feb 4

#   7 min read

#   

#   Whatâ€™s new in Copilot Studio: January 2025

#   

#   

#   Try Microsoft 365 Copilot Chat

#   The power of AI wherever you go. Available on desktop and mobile devices.

#   Try Copilot Chat

#   Learn more

#   

#   Connect with us on social

#   

#   X

#   LinkedIn

#   Instagram

#   TikTok

#   

#   What's new

#   

#   Surface Pro

#   Surface Laptop

#   Surface Laptop Studio 2

#   Surface Laptop Go 3

#   Microsoft Copilot

#   AI in Windows

#   Explore Microsoft products

#   Windows 11 apps

#   

#   Microsoft Store

#   

#   Account profile

#   Download Center

#   Microsoft Store support

#   Returns

#   Order tracking

#   Certified Refurbished

#   Microsoft Store Promise

#   Flexible Payments

#   

#   Education

#   

#   Microsoft in education

#   Devices for education

#   Microsoft Teams for Education

#   Microsoft 365 Education

#   How to buy for your school

#   Educator training and development

#   Deals for students and parents

#   Azure for students

#   

#   Business

#   

#   Microsoft Cloud

#   Microsoft Security

#   Dynamics 365

#   Microsoft 365

#   Microsoft Power Platform

#   Microsoft Teams

#   Microsoft 365 Copilot

#   Small Business

#   

#   Developer & IT

#   

#   Azure

#   Microsoft Developer

#   Microsoft Learn

#   Explore ISV Success

#   Microsoft Tech Community

#   Azure Marketplace

#   AppSource

#   Visual Studio

#   

#   Company

#   

#   Careers

#   About Microsoft

#   Company news

#   Privacy at Microsoft

#   Investors

#   Diversity and inclusion

#   Accessibility

#   Sustainability

#   

#   English (United States) Your Privacy ChoicesConsumer Health Privacy

#   

#   Sitemap

#   Contact Microsoft

#   Privacy

#   Manage cookies

#   Terms of use

#   Trademarks

#   Safety & eco

#   Recycling

#   About our ads

#   

#   Â© Microsoft 2025

#   Notifications

#   

#   

#   

#   Microsoft is conducting an online survey to understand your opinions about the Microsoft Copilot website. If you choose to participate, the online survey will be presented to you when you leave the website.  

#   Would you like to participate?

#   Privacy Statement

#   

#   --------------------------------------------------------------------------------

#   

#   

#   --- SOURCE 5: Introduction - Model Context Protocol ---

#   URL: https://modelcontextprotocol.io/introduction

#   

#   SUMMARY:

#   MCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol Building MCP with LLMs ---------------------- Learn how to use LLMs like Claude to speed up your MCP developmentDebugging Guide --------------- Learn how to effectively debug MCP servers and integrationsMCP Inspector ------------- Test and inspect your MCP servers with our interactive debugging tool Core architecture ----------------- Understand how MCP connects clients, servers, and LLMsResources --------- Expose data and content from your servers to LLMsPrompts ------- Create reusable prompt templates and workflowsTools ----- Enable LLMs to perform actions through your serverSampling -------- Let your servers request completions from LLMsTransports ---------- Learn about MCPâ€™s communication mechanism

#   

#   FULL CONTENT:

#   Introduction - Model Context Protocol

#   Model Context Protocol home page

#   Search...

#   

#   GitHub

#   GitHub

#   

#   Search...

#   Navigation

#   Get Started

#   Introduction

#   Model Context Protocol home page

#   

#   Documentation

#   Python SDK

#   TypeScript SDK

#   Kotlin SDK

#   Specification

#   

#   Get Started

#   

#   Introduction

#   

#   Quickstart

#   

#   

#   Example Servers

#   

#   Example Clients

#   

#   Tutorials

#   

#   Building MCP with LLMs

#   Debugging

#   Inspector

#   

#   Concepts

#   

#   Core architecture

#   Resources

#   Prompts

#   Tools

#   Sampling

#   Roots

#   Transports

#   

#   Development

#   

#   What's New

#   Roadmap

#   Contributing

#   

#   Get Started

#   Introduction

#   Get started with the Model Context Protocol (MCP)

#   Kotlin SDK released! Check out what else is new.

#   MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

#   â€‹

#   Why MCP?

#   MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides:

#   

#   A growing list of pre-built integrations that your LLM can directly plug into

#   The flexibility to switch between LLM providers and vendors

#   Best practices for securing your data within your infrastructure

#   

#   

#   â€‹

#   General architecture

#   At its core, MCP follows a client-server architecture where a host application can connect to multiple servers:

#   

#   MCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP

#   MCP Clients: Protocol clients that maintain 1:1 connections with servers

#   MCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol

#   Local Data Sources: Your computerâ€™s files, databases, and services that MCP servers can securely access

#   Remote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to

#   

#   â€‹

#   Get started

#   Choose the path that best fits your needs:

#   

#   â€‹

#   Quick Starts

#   For Server Developers --------------------- Get started building your own server to use in Claude for Desktop and other clientsFor Client Developers --------------------- Get started building your own client that can integrate with all MCP serversFor Claude Desktop Users ------------------------ Get started using pre-built servers in Claude for Desktop

#   

#   â€‹

#   Examples

#   Example Servers --------------- Check out our gallery of official MCP servers and implementationsExample Clients --------------- View the list of clients that support MCP integrations

#   â€‹

#   Tutorials

#   Building MCP with LLMs ---------------------- Learn how to use LLMs like Claude to speed up your MCP developmentDebugging Guide --------------- Learn how to effectively debug MCP servers and integrationsMCP Inspector ------------- Test and inspect your MCP servers with our interactive debugging tool

#   â€‹

#   Explore MCP

#   Dive deeper into MCPâ€™s core concepts and capabilities:

#   Core architecture ----------------- Understand how MCP connects clients, servers, and LLMsResources --------- Expose data and content from your servers to LLMsPrompts ------- Create reusable prompt templates and workflowsTools ----- Enable LLMs to perform actions through your serverSampling -------- Let your servers request completions from LLMsTransports ---------- Learn about MCPâ€™s communication mechanism

#   â€‹

#   Contributing

#   Want to contribute? Check out our Contributing Guide to learn how you can help improve MCP.

#   â€‹

#   Support and Feedback

#   Hereâ€™s how to get help or provide feedback:

#   

#   For bug reports and feature requests related to the MCP specification, SDKs, or documentation (open source), please create a GitHub issue

#   For discussions or Q&A about the MCP specification, use the specification discussions

#   For discussions or Q&A about other MCP open source components, use the organization discussions

#   For bug reports, feature requests, and questions related to Claude.app and claude.aiâ€™s MCP integration, please email mcp-support@anthropic.com

#   

#   Was this page helpful?

#   YesNo

#   For Server Developers

#   github

#   On this page

#   

#   Why MCP?

#   General architecture

#   Get started

#   Quick Starts

#   Examples

#   Tutorials

#   Explore MCP

#   Contributing

#   Support and Feedback

#   

#   

#   --------------------------------------------------------------------------------

#   

#   ==================================[1m Ai Message [0m==================================

#   

#   Hereâ€™s what Iâ€™ve gathered so far:

#   

#   â€¢ Model Context ProtocolÂ (MCP) is an open, clientâ€‘server standard created to let largeâ€‘languageâ€‘model assistants (Claude, Azure OpenAI, Copilot Studio, etc.) securely â€œplug intoâ€ external data sources and tools.  

#   â€¢ It works like a â€œUSBâ€‘C for AIâ€Â â€” an LLM (the â€œhostâ€) connects through an MCP client to one or more lightweight â€œMCP servers.â€ Each server exposes specific capabilities (APIs, files, databases, SaaS apps, etc.) in a uniform JSON schema, so the model can fetch context or invoke actions.  

#   â€¢ The spec is public and already ships with SDKs (Python, TypeScript, Kotlin). Typical transports are STDIO, Serverâ€‘Sent Events, and WebSocket.  

#   â€¢ Early adopters include Anthropicâ€™s Claude Desktop, Microsoft Azure OpenAI, and Copilot Studio. Thereâ€™s a growing catalogue of preâ€‘built servers for Slack, GitHub, Postgres, Google Drive, etc.

#   

#   Before I outline the report, Iâ€™d like to be sure Iâ€™m targeting the right depth and angle for you.

#   

#   1. What audience should the explanation assume? (e.g., nonâ€‘technical execs, software architects, handsâ€‘on developers)  

#   2. Which aspects interest you most?  

#      a. Highâ€‘level concept & benefits  

#      b. Detailed architecture, data flow, and transport layers  

#      c. SDK usage & code snippets  

#      d. Security / compliance considerations  

#      e. Realâ€‘world adoption stories and roadmap  

#   3. Do you need comparisons with alternative approaches (e.g., â€œfunction calling,â€ LangChain tool interfaces, RAG pipelines without MCP)?  

#   4. Any length or format constraints (slide deck notes, whiteâ€‘paper style, quick FAQ)?  

#   5. Timeline sensitivity: should we cover only the openâ€‘sourced spec (NovÂ 2024) or include 2025 Microsoft integrations as well?

#   

#   Let me know, and Iâ€™ll tailor the research plan accordingly.

#   ================================[1m Human Message [0m=================================

#   

#   Focus on Anthropicâ€‘backed open standard for integrating external context and tools with LLMs, give an architectural overview for developers, tell me about interesting MCP servers, compare to google Agent2Agent (A2A) protocol. write the report and dont ask any follow up questions

#   ==================================[1m Ai Message [0m==================================

#   Tool Calls:

#     Sections (call_5nYqN27e3UUruWoKTsSf8OkA)

#    Call ID: call_5nYqN27e3UUruWoKTsSf8OkA

#     Args:

#       sections: ['Model Context Protocol Overview: Chronicle of the open standardâ€™s origins under Anthropic, objectives in connecting LLMs to external data/tools, version timeline, current governance, and licensing.', 'MCP Architecture: Detailed breakdown of the hostâ€“clientâ€“server model, message schema, transports (STDIO, SSE, WebSocket), session lifecycle, and how LLMs consume tools and resources.', 'Security & Compliance Layers: Authentication flows, sandboxing, permissions, data residency, and secure transport considerations for production MCP deployments.', 'Ecosystem of MCP Servers: Survey of notable official and community servers (GitHub, Slack, Google Drive, Postgres, Puppeteer, Filesystem, Math, etc.), how each exposes resources, and usage patterns.', 'Case Studies of Early Adopters: Examination of integrations by Claude Desktop, Block, Apollo, Microsoft Copilot Studio, highlighting implementation approaches and lessons.', 'Comparative Analysis â€“ MCP vs Google Agent2Agent (A2A): Featureâ€‘byâ€‘feature comparison covering protocol goals, architecture, schema, security, ecosystem maturity, and developer experience.']

#   =================================[1m Tool Message [0m=================================

#   Name: Sections

#   

#   sections=['Model Context Protocol Overview: Chronicle of the open standardâ€™s origins under Anthropic, objectives in connecting LLMs to external data/tools, version timeline, current governance, and licensing.', 'MCP Architecture: Detailed breakdown of the hostâ€“clientâ€“server model, message schema, transports (STDIO, SSE, WebSocket), session lifecycle, and how LLMs consume tools and resources.', 'Security & Compliance Layers: Authentication flows, sandboxing, permissions, data residency, and secure transport considerations for production MCP deployments.', 'Ecosystem of MCP Servers: Survey of notable official and community servers (GitHub, Slack, Google Drive, Postgres, Puppeteer, Filesystem, Math, etc.), how each exposes resources, and usage patterns.', 'Case Studies of Early Adopters: Examination of integrations by Claude Desktop, Block, Apollo, Microsoft Copilot Studio, highlighting implementation approaches and lessons.', 'Comparative Analysis â€“ MCP vs Google Agent2Agent (A2A): Featureâ€‘byâ€‘feature comparison covering protocol goals, architecture, schema, security, ecosystem maturity, and developer experience.']

#   ==================================[1m Ai Message [0m==================================

#   Tool Calls:

#     Introduction (call_cJXa72KA1z4igwTziofV54uh)

#    Call ID: call_cJXa72KA1z4igwTziofV54uh

#     Args:

#       name: introduction

#       content: # Model Context Protocol (MCP): An Architectural Guide for Developers

#   

#   Largeâ€‘languageâ€‘model applications are only as powerful as the context and tools they can reach. Anthropicâ€™s openâ€‘sourced Model Context Protocol (MCP) emerged in lateÂ 2024 to provide a universal, vendorâ€‘agnostic interface that lets LLM â€œhostsâ€ discover, authenticate and interact with external data repositories, APIs and executable tools through lightweight servers. 

#   

#   This report offers a developerâ€‘centric walkthrough of MCPâ€™s design, security model and growing ecosystem. It also benchmarks MCP against Googleâ€™s newer Agent2Agent (A2A) protocol to help architects choose the right abstraction layer for contextâ€‘aware or multiâ€‘agent systems.Â 

#   

#   The ensuing sections trace MCPâ€™s evolution, dissect its JSONâ€‘RPCâ€‘based architecture, highlight productionâ€‘grade security patterns, catalogue notable servers, and examine field deployments inside companies such as Block, Apollo and Microsoft. We close with a comparative analysis that positions MCP within the broader landscape of agent interoperability standards.

#   =================================[1m Tool Message [0m=================================

#   Name: Introduction

#   

#   name='introduction' content='# Model Context Protocol (MCP): An Architectural Guide for Developers\n\nLargeâ€‘languageâ€‘model applications are only as powerful as the context and tools they can reach. Anthropicâ€™s openâ€‘sourced Model Context Protocol (MCP) emerged in late\xa02024 to provide a universal, vendorâ€‘agnostic interface that lets LLM â€œhostsâ€ discover, authenticate and interact with external data repositories, APIs and executable tools through lightweight servers. \n\nThis report offers a developerâ€‘centric walkthrough of MCPâ€™s design, security model and growing ecosystem. It also benchmarks MCP against Googleâ€™s newer Agent2Agent (A2A) protocol to help architects choose the right abstraction layer for contextâ€‘aware or multiâ€‘agent systems.\xa0\n\nThe ensuing sections trace MCPâ€™s evolution, dissect its JSONâ€‘RPCâ€‘based architecture, highlight productionâ€‘grade security patterns, catalogue notable servers, and examine field deployments inside companies such as Block, Apollo and Microsoft. We close with a comparative analysis that positions MCP within the broader landscape of agent interoperability standards.'

#   ================================[1m Human Message [0m=================================

#   

#   Introduction written. Now write a conclusion section.

#   ==================================[1m Ai Message [0m==================================

#   Tool Calls:

#     Conclusion (call_g8McTOEf8hnD4hao7CygCWwd)

#    Call ID: call_g8McTOEf8hnD4hao7CygCWwd

#     Args:

#       name: conclusion

#       content: ## Conclusion

#   

#   Model Context Protocol delivers a pragmatic, languageâ€‘modelâ€“first answer to the longâ€‘standing problem of unifying data and tool access. Its strengths stem from a simple JSONâ€‘schema, transport flexibility, and an ecosystem that can be expanded or replaced without retraining models. Early adopters report faster agent development cycles, reduced connector maintenance, and cleaner security boundaries compared with bespoke integrations.

#   

#   Compared with Googleâ€™s A2A, MCP is:

#   * earlier to openâ€‘source release and already shipping in production tools (Claude Desktop, Copilot Studio)

#   * narrower in scope (context & tool invocation vs. full multiâ€‘agent orchestration) yet easier to embed inside a singleâ€‘agent product

#   * backed by a growing catalogue of lightweight servers that developers can remix or extend.

#   

#   Choosing a protocol ultimately hinges on system goals: if you need dropâ€‘in connectors for content repositories and APIs today, MCP provides the quickest path. If youâ€™re building a researchâ€‘grade swarm of specialized agents, A2Aâ€™s peerâ€‘toâ€‘peer semantics may appeal, though the standard is still maturing.

#   

#   ### Quick sideâ€‘byâ€‘side snapshot

#   

#   | Dimension | MCP (Anthropic) | A2A (Google) |

#   |---|---|---|

#   | Initial release | NovÂ 2024 (openâ€‘sourced) | FebÂ 2025 (alpha docs) |

#   | Core focus | External context + tool calls for an LLM host | Messaging & coordination among multiple agents |

#   | Transport options | STDIO, SSE, WebSocket | GRPCâ€‘stream, WebSocket |

#   | Message format | JSONâ€‘RPCâ€‘like with typed resources & tools | Protobuf envelopes (AgentRequest/Response) |

#   | Security primitives | OAuth2, local sandbox, fineâ€‘grained scopes | Mutualâ€‘TLS + JWT claims, ACL per agent |

#   | Ecosystem status | 40+ OSS servers, SDKs (Py/TS/Kotlin) | Few reference agents, limited connectors |

#   

#   Across the board, MCP represents a lean, productionâ€‘ready layer for enriching LLMs with live data and actionable tools, while maintaining the portability and safety developers expect from an open standard.

#   =================================[1m Tool Message [0m=================================

#   Name: Conclusion

#   

#   name='conclusion' content='## Conclusion\n\nModel Context Protocol delivers a pragmatic, languageâ€‘modelâ€“first answer to the longâ€‘standing problem of unifying data and tool access. Its strengths stem from a simple JSONâ€‘schema, transport flexibility, and an ecosystem that can be expanded or replaced without retraining models. Early adopters report faster agent development cycles, reduced connector maintenance, and cleaner security boundaries compared with bespoke integrations.\n\nCompared with Googleâ€™s A2A, MCP is:\n* earlier to openâ€‘source release and already shipping in production tools (Claude Desktop, Copilot Studio)\n* narrower in scope (context & tool invocation vs. full multiâ€‘agent orchestration) yet easier to embed inside a singleâ€‘agent product\n* backed by a growing catalogue of lightweight servers that developers can remix or extend.\n\nChoosing a protocol ultimately hinges on system goals: if you need dropâ€‘in connectors for content repositories and APIs today, MCP provides the quickest path. If youâ€™re building a researchâ€‘grade swarm of specialized agents, A2Aâ€™s peerâ€‘toâ€‘peer semantics may appeal, though the standard is still maturing.\n\n### Quick sideâ€‘byâ€‘side snapshot\n\n| Dimension | MCP (Anthropic) | A2A (Google) |\n|---|---|---|\n| Initial release | Nov\xa02024 (openâ€‘sourced) | Feb\xa02025 (alpha docs) |\n| Core focus | External context + tool calls for an LLM host | Messaging & coordination among multiple agents |\n| Transport options | STDIO, SSE, WebSocket | GRPCâ€‘stream, WebSocket |\n| Message format | JSONâ€‘RPCâ€‘like with typed resources & tools | Protobuf envelopes (AgentRequest/Response) |\n| Security primitives | OAuth2, local sandbox, fineâ€‘grained scopes | Mutualâ€‘TLS + JWT claims, ACL per agent |\n| Ecosystem status | 40+ OSS servers, SDKs (Py/TS/Kotlin) | Few reference agents, limited connectors |\n\nAcross the board, MCP represents a lean, productionâ€‘ready layer for enriching LLMs with live data and actionable tools, while maintaining the portability and safety developers expect from an open standard.'

#   ================================[1m Human Message [0m=================================

#   

#   Report is now complete with introduction, body sections, and conclusion.

#   ==================================[1m Ai Message [0m==================================

#   

#   The report now contains:

#   

#   â€¢ An H1â€‘level Introduction that sets context and goals  

#   â€¢ Six fully scoped body sections for developers (listed in â€œSectionsâ€)  

#   â€¢ An H2â€‘level Conclusion that summarizes key insights and provides a compact MCPâ€‘vsâ€‘A2A comparison table

#   

#   No additional steps are required.

#   /Users/rlm/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py:504: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.

#     return cls(**tup[2])


from IPython.display import Markdown
Markdown(agent.get_state(thread_config).values['final_report'])
# Output:
#   <IPython.core.display.Markdown object>

"""
Trace: 

> Note: uses 456k tokens 

https://smith.langchain.com/public/f1581fa5-dfc9-445c-a8f4-3518a05cd139/r
"""



================================================
FILE: src/open_deep_research/multi_agent.py
================================================
from typing import List, Annotated, TypedDict, operator, Literal
from pydantic import BaseModel, Field

from langchain.chat_models import init_chat_model
from langchain_core.tools import tool
from langchain_core.runnables import RunnableConfig
from langgraph.graph import MessagesState

from langgraph.types import Command, Send
from langgraph.graph import START, END, StateGraph

from open_deep_research.configuration import Configuration
from open_deep_research.utils import get_config_value, tavily_search, duckduckgo_search
from open_deep_research.prompts import SUPERVISOR_INSTRUCTIONS, RESEARCH_INSTRUCTIONS

## Tools factory - will be initialized based on configuration
def get_search_tool(config: RunnableConfig):
    """Get the appropriate search tool based on configuration"""
    configurable = Configuration.from_runnable_config(config)
    search_api = get_config_value(configurable.search_api)

    # TODO: Configure other search functions as tools
    if search_api.lower() == "tavily":
        # Use Tavily search tool
        return tavily_search
    elif search_api.lower() == "duckduckgo":
        # Use the DuckDuckGo search tool
        return duckduckgo_search
    else:
        # Raise NotImplementedError for search APIs other than Tavily
        raise NotImplementedError(
            f"The search API '{search_api}' is not yet supported in the multi-agent implementation. "
            f"Currently, only Tavily is supported. Please use the graph-based implementation in "
            f"src/open_deep_research/graph.py for other search APIs, or set search_api to 'tavily'."
        )

@tool
class Section(BaseModel):
    name: str = Field(
        description="Name for this section of the report.",
    )
    description: str = Field(
        description="Research scope for this section of the report.",
    )
    content: str = Field(
        description="The content of the section."
    )

@tool
class Sections(BaseModel):
    sections: List[str] = Field(
        description="Sections of the report.",
    )

@tool
class Introduction(BaseModel):
    name: str = Field(
        description="Name for the report.",
    )
    content: str = Field(
        description="The content of the introduction, giving an overview of the report."
    )

@tool
class Conclusion(BaseModel):
    name: str = Field(
        description="Name for the conclusion of the report.",
    )
    content: str = Field(
        description="The content of the conclusion, summarizing the report."
    )

## State
class ReportStateOutput(TypedDict):
    final_report: str # Final report

class ReportState(MessagesState):
    sections: list[str] # List of report sections 
    completed_sections: Annotated[list, operator.add] # Send() API key
    final_report: str # Final report

class SectionState(MessagesState):
    section: str # Report section  
    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API

class SectionOutputState(TypedDict):
    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API

# Tool lists will be built dynamically based on configuration
def get_supervisor_tools(config: RunnableConfig):
    """Get supervisor tools based on configuration"""
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Sections, Introduction, Conclusion]
    return tool_list, {tool.name: tool for tool in tool_list}

def get_research_tools(config: RunnableConfig):
    """Get research tools based on configuration"""
    search_tool = get_search_tool(config)
    tool_list = [search_tool, Section]
    return tool_list, {tool.name: tool for tool in tool_list}

async def supervisor(state: ReportState, config: RunnableConfig):
    """LLM decides whether to call a tool or not"""

    # Messages
    messages = state["messages"]

    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    supervisor_model = get_config_value(configurable.supervisor_model)
    
    # Initialize the model
    llm = init_chat_model(model=supervisor_model)
    
    # If sections have been completed, but we don't yet have the final report, then we need to initiate writing the introduction and conclusion
    if state.get("completed_sections") and not state.get("final_report"):
        research_complete_message = {"role": "user", "content": "Research is complete. Now write the introduction and conclusion for the report. Here are the completed main body sections: \n\n" + "\n\n".join([s.content for s in state["completed_sections"]])}
        messages = messages + [research_complete_message]

    # Get tools based on configuration
    supervisor_tool_list, _ = get_supervisor_tools(config)
    
    # Invoke
    return {
        "messages": [
            await llm.bind_tools(supervisor_tool_list).ainvoke(
                [
                    {"role": "system",
                     "content": SUPERVISOR_INSTRUCTIONS,
                    }
                ]
                + messages
            )
        ]
    }

async def supervisor_tools(state: ReportState, config: RunnableConfig)  -> Command[Literal["supervisor", "research_team", "__end__"]]:
    """Performs the tool call and sends to the research agent"""

    result = []
    sections_list = []
    intro_content = None
    conclusion_content = None

    # Get tools based on configuration
    _, supervisor_tools_by_name = get_supervisor_tools(config)
    
    # First process all tool calls to ensure we respond to each one (required for OpenAI)
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool
        tool = supervisor_tools_by_name[tool_call["name"]]
        # Perform the tool call - use ainvoke for async tools
        if hasattr(tool, 'ainvoke'):
            observation = await tool.ainvoke(tool_call["args"])
        else:
            observation = tool.invoke(tool_call["args"])

        # Append to messages 
        result.append({"role": "tool", 
                       "content": observation, 
                       "name": tool_call["name"], 
                       "tool_call_id": tool_call["id"]})
        
        # Store special tool results for processing after all tools have been called
        if tool_call["name"] == "Sections":
            sections_list = observation.sections
        elif tool_call["name"] == "Introduction":
            # Format introduction with proper H1 heading if not already formatted
            if not observation.content.startswith("# "):
                intro_content = f"# {observation.name}\n\n{observation.content}"
            else:
                intro_content = observation.content
        elif tool_call["name"] == "Conclusion":
            # Format conclusion with proper H2 heading if not already formatted
            if not observation.content.startswith("## "):
                conclusion_content = f"## {observation.name}\n\n{observation.content}"
            else:
                conclusion_content = observation.content
    
    # After processing all tool calls, decide what to do next
    if sections_list:
        # Send the sections to the research agents
        return Command(goto=[Send("research_team", {"section": s}) for s in sections_list], update={"messages": result})
    elif intro_content:
        # Store introduction while waiting for conclusion
        # Append to messages to guide the LLM to write conclusion next
        result.append({"role": "user", "content": "Introduction written. Now write a conclusion section."})
        return Command(goto="supervisor", update={"final_report": intro_content, "messages": result})
    elif conclusion_content:
        # Get all sections and combine in proper order: Introduction, Body Sections, Conclusion
        intro = state.get("final_report", "")
        body_sections = "\n\n".join([s.content for s in state["completed_sections"]])
        
        # Assemble final report in correct order
        complete_report = f"{intro}\n\n{body_sections}\n\n{conclusion_content}"
        
        # Append to messages to indicate completion
        result.append({"role": "user", "content": "Report is now complete with introduction, body sections, and conclusion."})
        return Command(goto="supervisor", update={"final_report": complete_report, "messages": result})
    else:
        # Default case (for search tools, etc.)
        return Command(goto="supervisor", update={"messages": result})

async def supervisor_should_continue(state: ReportState) -> Literal["supervisor_tools", END]:
    """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

    messages = state["messages"]
    last_message = messages[-1]

    # If the LLM makes a tool call, then perform an action
    if last_message.tool_calls:
        return "supervisor_tools"
    
    # Else end because the supervisor asked a question or is finished
    else:
        return END

async def research_agent(state: SectionState, config: RunnableConfig):
    """LLM decides whether to call a tool or not"""
    
    # Get configuration
    configurable = Configuration.from_runnable_config(config)
    researcher_model = get_config_value(configurable.researcher_model)
    
    # Initialize the model
    llm = init_chat_model(model=researcher_model)

    # Get tools based on configuration
    research_tool_list, _ = get_research_tools(config)
    
    return {
        "messages": [
            # Enforce tool calling to either perform more search or call the Section tool to write the section
            await llm.bind_tools(research_tool_list).ainvoke(
                [
                    {"role": "system",
                     "content": RESEARCH_INSTRUCTIONS.format(section_description=state["section"])
                    }
                ]
                + state["messages"]
            )
        ]
    }

async def research_agent_tools(state: SectionState, config: RunnableConfig):
    """Performs the tool call and route to supervisor or continue the research loop"""

    result = []
    completed_section = None
    
    # Get tools based on configuration
    _, research_tools_by_name = get_research_tools(config)
    
    # Process all tool calls first (required for OpenAI)
    for tool_call in state["messages"][-1].tool_calls:
        # Get the tool
        tool = research_tools_by_name[tool_call["name"]]
        # Perform the tool call - use ainvoke for async tools
        if hasattr(tool, 'ainvoke'):
            observation = await tool.ainvoke(tool_call["args"])
        else:
            observation = tool.invoke(tool_call["args"])
        # Append to messages 
        result.append({"role": "tool", 
                       "content": observation, 
                       "name": tool_call["name"], 
                       "tool_call_id": tool_call["id"]})
        
        # Store the section observation if a Section tool was called
        if tool_call["name"] == "Section":
            completed_section = observation
    
    # After processing all tools, decide what to do next
    if completed_section:
        # Write the completed section to state and return to the supervisor
        return {"messages": result, "completed_sections": [completed_section]}
    else:
        # Continue the research loop for search tools, etc.
        return {"messages": result}

async def research_agent_should_continue(state: SectionState) -> Literal["research_agent_tools", END]:
    """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

    messages = state["messages"]
    last_message = messages[-1]

    # If the LLM makes a tool call, then perform an action
    if last_message.tool_calls:
        return "research_agent_tools"

    else:
        return END
    
"""Build the multi-agent workflow"""

# Research agent workflow
research_builder = StateGraph(SectionState, output=SectionOutputState, config_schema=Configuration)
research_builder.add_node("research_agent", research_agent)
research_builder.add_node("research_agent_tools", research_agent_tools)
research_builder.add_edge(START, "research_agent") 
research_builder.add_conditional_edges(
    "research_agent",
    research_agent_should_continue,
    {
        # Name returned by should_continue : Name of next node to visit
        "research_agent_tools": "research_agent_tools",
        END: END,
    },
)
research_builder.add_edge("research_agent_tools", "research_agent")

# Supervisor workflow
supervisor_builder = StateGraph(ReportState, input=MessagesState, output=ReportStateOutput, config_schema=Configuration)
supervisor_builder.add_node("supervisor", supervisor)
supervisor_builder.add_node("supervisor_tools", supervisor_tools)
supervisor_builder.add_node("research_team", research_builder.compile())

# Flow of the supervisor agent
supervisor_builder.add_edge(START, "supervisor")
supervisor_builder.add_conditional_edges(
    "supervisor",
    supervisor_should_continue,
    {
        # Name returned by should_continue : Name of next node to visit
        "supervisor_tools": "supervisor_tools",
        END: END,
    },
)
supervisor_builder.add_edge("research_team", "supervisor")

graph = supervisor_builder.compile()


================================================
FILE: src/open_deep_research/prompts.py
================================================
report_planner_query_writer_instructions="""You are performing research for a report. 

<Report topic>
{topic}
</Report topic>

<Report organization>
{report_organization}
</Report organization>

<Task>
Your goal is to generate {number_of_queries} web search queries that will help gather information for planning the report sections. 

The queries should:

1. Be related to the Report topic
2. Help satisfy the requirements specified in the report organization

Make the queries specific enough to find high-quality, relevant sources while covering the breadth needed for the report structure.
</Task>

<Format>
Call the Queries tool 
</Format>
"""

report_planner_instructions="""I want a plan for a report that is concise and focused.

<Report topic>
The topic of the report is:
{topic}
</Report topic>

<Report organization>
The report should follow this organization: 
{report_organization}
</Report organization>

<Context>
Here is context to use to plan the sections of the report: 
{context}
</Context>

<Task>
Generate a list of sections for the report. Your plan should be tight and focused with NO overlapping sections or unnecessary filler. 

For example, a good report structure might look like:
1/ intro
2/ overview of topic A
3/ overview of topic B
4/ comparison between A and B
5/ conclusion

Each section should have the fields:

- Name - Name for this section of the report.
- Description - Brief overview of the main topics covered in this section.
- Research - Whether to perform web research for this section of the report. IMPORTANT: Main body sections (not intro/conclusion) MUST have Research=True. A report must have AT LEAST 2-3 sections with Research=True to be useful.
- Content - The content of the section, which you will leave blank for now.

Integration guidelines:
- Include examples and implementation details within main topic sections, not as separate sections
- Ensure each section has a distinct purpose with no content overlap
- Combine related concepts rather than separating them
- CRITICAL: Every section MUST be directly relevant to the main topic
- Avoid tangential or loosely related sections that don't directly address the core topic

Before submitting, review your structure to ensure it has no redundant sections and follows a logical flow.
</Task>

<Feedback>
Here is feedback on the report structure from review (if any):
{feedback}
</Feedback>

<Format>
Call the Sections tool 
</Format>
"""

query_writer_instructions="""You are an expert technical writer crafting targeted web search queries that will gather comprehensive information for writing a technical report section.

<Report topic>
{topic}
</Report topic>

<Section topic>
{section_topic}
</Section topic>

<Task>
Your goal is to generate {number_of_queries} search queries that will help gather comprehensive information above the section topic. 

The queries should:

1. Be related to the topic 
2. Examine different aspects of the topic

Make the queries specific enough to find high-quality, relevant sources.
</Task>

<Format>
Call the Queries tool 
</Format>
"""

section_writer_instructions = """Write one section of a research report.

<Task>
1. Review the report topic, section name, and section topic carefully.
2. If present, review any existing section content. 
3. Then, look at the provided Source material.
4. Decide the sources that you will use it to write a report section.
5. Write the report section and list your sources. 
</Task>

<Writing Guidelines>
- If existing section content is not populated, write from scratch
- If existing section content is populated, synthesize it with the source material
- Strict 150-200 word limit
- Use simple, clear language
- Use short paragraphs (2-3 sentences max)
- Use ## for section title (Markdown format)
</Writing Guidelines>

<Citation Rules>
- Assign each unique URL a single citation number in your text
- End with ### Sources that lists each source with corresponding numbers
- IMPORTANT: Number sources sequentially without gaps (1,2,3,4...) in the final list regardless of which sources you choose
- Example format:
  [1] Source Title: URL
  [2] Source Title: URL
</Citation Rules>

<Final Check>
1. Verify that EVERY claim is grounded in the provided Source material
2. Confirm each URL appears ONLY ONCE in the Source list
3. Verify that sources are numbered sequentially (1,2,3...) without any gaps
</Final Check>
"""

section_writer_inputs=""" 
<Report topic>
{topic}
</Report topic>

<Section name>
{section_name}
</Section name>

<Section topic>
{section_topic}
</Section topic>

<Existing section content (if populated)>
{section_content}
</Existing section content>

<Source material>
{context}
</Source material>
"""

section_grader_instructions = """Review a report section relative to the specified topic:

<Report topic>
{topic}
</Report topic>

<section topic>
{section_topic}
</section topic>

<section content>
{section}
</section content>

<task>
Evaluate whether the section content adequately addresses the section topic.

If the section content does not adequately address the section topic, generate {number_of_follow_up_queries} follow-up search queries to gather missing information.
</task>

<format>
Call the Feedback tool and output with the following schema:

grade: Literal["pass","fail"] = Field(
    description="Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail')."
)
follow_up_queries: List[SearchQuery] = Field(
    description="List of follow-up search queries.",
)
</format>
"""

final_section_writer_instructions="""You are an expert technical writer crafting a section that synthesizes information from the rest of the report.

<Report topic>
{topic}
</Report topic>

<Section name>
{section_name}
</Section name>

<Section topic> 
{section_topic}
</Section topic>

<Available report content>
{context}
</Available report content>

<Task>
1. Section-Specific Approach:

For Introduction:
- Use # for report title (Markdown format)
- 50-100 word limit
- Write in simple and clear language
- Focus on the core motivation for the report in 1-2 paragraphs
- Use a clear narrative arc to introduce the report
- Include NO structural elements (no lists or tables)
- No sources section needed

For Conclusion/Summary:
- Use ## for section title (Markdown format)
- 100-150 word limit
- For comparative reports:
    * Must include a focused comparison table using Markdown table syntax
    * Table should distill insights from the report
    * Keep table entries clear and concise
- For non-comparative reports: 
    * Only use ONE structural element IF it helps distill the points made in the report:
    * Either a focused table comparing items present in the report (using Markdown table syntax)
    * Or a short list using proper Markdown list syntax:
      - Use `*` or `-` for unordered lists
      - Use `1.` for ordered lists
      - Ensure proper indentation and spacing
- End with specific next steps or implications
- No sources section needed

3. Writing Approach:
- Use concrete details over general statements
- Make every word count
- Focus on your single most important point
</Task>

<Quality Checks>
- For introduction: 50-100 word limit, # for report title, no structural elements, no sources section
- For conclusion: 100-150 word limit, ## for section title, only ONE structural element at most, no sources section
- Markdown format
- Do not include word count or any preamble in your response
</Quality Checks>"""


## Supervisor
SUPERVISOR_INSTRUCTIONS = """
You are scoping research for a report based on a user-provided topic.

### Your responsibilities:

1. **Gather Background Information**  
   Based upon the user's topic, use the `enhanced_tavily_search` to collect relevant information about the topic. 
   - You MUST perform ONLY ONE search to gather comprehensive context
   - Create a highly targeted search query that will yield the most valuable information
   - Take time to analyze and synthesize the search results before proceeding
   - Do not proceed to the next step until you have an understanding of the topic

2. **Clarify the Topic**  
   After your initial research, engage with the user to clarify any questions that arose.
   - Ask specific follow-up questions based on what you learned from your searches
   - Do not proceed until you fully understand the topic, goals, constraints, and any preferences
   - Synthesize what you've learned so far before asking questions
   - You MUST engage in at least one clarification exchange with the user before proceeding

3. **Define Report Structure**  
   Only after completing both research AND clarification with the user:
   - Use the `Sections` tool to define a list of report sections
   - Each section should be a written description with: a section name and a section research plan
   - Do not include sections for introductions or conclusions (We'll add these later)
   - Ensure sections are scoped to be independently researchable
   - Base your sections on both the search results AND user clarifications
   - Format your sections as a list of strings, with each string having the scope of research for that section.

4. **Assemble the Final Report**  
   When all sections are returned:
   - IMPORTANT: First check your previous messages to see what you've already completed
   - If you haven't created an introduction yet, use the `Introduction` tool to generate one
     - Set content to include report title with a single # (H1 level) at the beginning
     - Example: "# [Report Title]\n\n[Introduction content...]"
   - After the introduction, use the `Conclusion` tool to summarize key insights
     - Set content to include conclusion title with ## (H2 level) at the beginning
     - Example: "## Conclusion\n\n[Conclusion content...]"
     - Only use ONE structural element IF it helps distill the points made in the report:
     - Either a focused table comparing items present in the report (using Markdown table syntax)
     - Or a short list using proper Markdown list syntax:
      - Use `*` or `-` for unordered lists
      - Use `1.` for ordered lists
      - Ensure proper indentation and spacing
   - Do not call the same tool twice - check your message history

### Additional Notes:
- You are a reasoning model. Think through problems step-by-step before acting.
- IMPORTANT: Do not rush to create the report structure. Gather information thoroughly first.
- Use multiple searches to build a complete picture before drawing conclusions.
- Maintain a clear, informative, and professional tone throughout."""

RESEARCH_INSTRUCTIONS = """
You are a researcher responsible for completing a specific section of a report.

### Your goals:

1. **Understand the Section Scope**  
   Begin by reviewing the section scope of work. This defines your research focus. Use it as your objective.

<Section Description>
{section_description}
</Section Description>

2. **Strategic Research Process**  
   Follow this precise research strategy:

   a) **First Query**: Begin with a SINGLE, well-crafted search query with `enhanced_tavily_search` that directly addresses the core of the section topic.
      - Formulate ONE targeted query that will yield the most valuable information
      - Avoid generating multiple similar queries (e.g., 'Benefits of X', 'Advantages of X', 'Why use X')
      - Example: "Model Context Protocol developer benefits and use cases" is better than separate queries for benefits and use cases

   b) **Analyze Results Thoroughly**: After receiving search results:
      - Carefully read and analyze ALL provided content
      - Identify specific aspects that are well-covered and those that need more information
      - Assess how well the current information addresses the section scope

   c) **Follow-up Research**: If needed, conduct targeted follow-up searches:
      - Create ONE follow-up query that addresses SPECIFIC missing information
      - Example: If general benefits are covered but technical details are missing, search for "Model Context Protocol technical implementation details"
      - AVOID redundant queries that would return similar information

   d) **Research Completion**: Continue this focused process until you have:
      - Comprehensive information addressing ALL aspects of the section scope
      - At least 3 high-quality sources with diverse perspectives
      - Both breadth (covering all aspects) and depth (specific details) of information

3. **Use the Section Tool**  
   Only after thorough research, write a high-quality section using the Section tool:
   - `name`: The title of the section
   - `description`: The scope of research you completed (brief, 1-2 sentences)
   - `content`: The completed body of text for the section, which MUST:
     - Begin with the section title formatted as "## [Section Title]" (H2 level with ##)
     - Be formatted in Markdown style
     - Be MAXIMUM 200 words (strictly enforce this limit)
     - End with a "### Sources" subsection (H3 level with ###) containing a numbered list of URLs used
     - Use clear, concise language with bullet points where appropriate
     - Include relevant facts, statistics, or expert opinions

Example format for content:
```
## [Section Title]

[Body text in markdown format, maximum 200 words...]

### Sources
1. [URL 1]
2. [URL 2]
3. [URL 3]
```

---

### Research Decision Framework

Before each search query or when writing the section, think through:

1. **What information do I already have?**
   - Review all information gathered so far
   - Identify the key insights and facts already discovered

2. **What information is still missing?**
   - Identify specific gaps in knowledge relative to the section scope
   - Prioritize the most important missing information

3. **What is the most effective next action?**
   - Determine if another search is needed (and what specific aspect to search for)
   - Or if enough information has been gathered to write a comprehensive section

---

### Notes:
- Focus on QUALITY over QUANTITY of searches
- Each search should have a clear, distinct purpose
- Do not write introductions or conclusions unless explicitly part of your section
- Keep a professional, factual tone
- Always follow markdown formatting
- Stay within the 200 word limit for the main content
"""



================================================
FILE: src/open_deep_research/state.py
================================================
from typing import Annotated, List, TypedDict, Literal
from pydantic import BaseModel, Field
import operator

class Section(BaseModel):
    name: str = Field(
        description="Name for this section of the report.",
    )
    description: str = Field(
        description="Brief overview of the main topics and concepts to be covered in this section.",
    )
    research: bool = Field(
        description="Whether to perform web research for this section of the report."
    )
    content: str = Field(
        description="The content of the section."
    )   

class Sections(BaseModel):
    sections: List[Section] = Field(
        description="Sections of the report.",
    )

class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Query for web search.")

class Queries(BaseModel):
    queries: List[SearchQuery] = Field(
        description="List of search queries.",
    )

class Feedback(BaseModel):
    grade: Literal["pass","fail"] = Field(
        description="Evaluation result indicating whether the response meets requirements ('pass') or needs revision ('fail')."
    )
    follow_up_queries: List[SearchQuery] = Field(
        description="List of follow-up search queries.",
    )

class ReportStateInput(TypedDict):
    topic: str # Report topic
    
class ReportStateOutput(TypedDict):
    final_report: str # Final report

class ReportState(TypedDict):
    topic: str # Report topic    
    feedback_on_report_plan: str # Feedback on the report plan
    sections: list[Section] # List of report sections 
    completed_sections: Annotated[list, operator.add] # Send() API key
    report_sections_from_research: str # String of any completed sections from research to write final sections
    final_report: str # Final report

class SectionState(TypedDict):
    topic: str # Report topic
    section: Section # Report section  
    search_iterations: int # Number of search iterations done
    search_queries: list[SearchQuery] # List of search queries
    source_str: str # String of formatted source content from web search
    report_sections_from_research: str # String of any completed sections from research to write final sections
    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API

class SectionOutputState(TypedDict):
    completed_sections: list[Section] # Final key we duplicate in outer state for Send() API



================================================
FILE: src/open_deep_research/utils.py
================================================
import os
import asyncio
import requests
import random 
import concurrent
import aiohttp
import httpx
import time
from typing import List, Optional, Dict, Any, Union
from urllib.parse import unquote

from exa_py import Exa
from linkup import LinkupClient
from tavily import AsyncTavilyClient
from duckduckgo_search import DDGS 
from bs4 import BeautifulSoup
from markdownify import markdownify

from langchain_community.retrievers import ArxivRetriever
from langchain_community.utilities.pubmed import PubMedAPIWrapper
from langchain_core.tools import tool

from langsmith import traceable

from open_deep_research.state import Section
    
def get_config_value(value):
    """
    Helper function to handle string, dict, and enum cases of configuration values
    """
    if isinstance(value, str):
        return value
    elif isinstance(value, dict):
        return value
    else:
        return value.value

def get_search_params(search_api: str, search_api_config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Filters the search_api_config dictionary to include only parameters accepted by the specified search API.

    Args:
        search_api (str): The search API identifier (e.g., "exa", "tavily").
        search_api_config (Optional[Dict[str, Any]]): The configuration dictionary for the search API.

    Returns:
        Dict[str, Any]: A dictionary of parameters to pass to the search function.
    """
    # Define accepted parameters for each search API
    SEARCH_API_PARAMS = {
        "exa": ["max_characters", "num_results", "include_domains", "exclude_domains", "subpages"],
        "tavily": ["max_results", "topic"],
        "perplexity": [],  # Perplexity accepts no additional parameters
        "arxiv": ["load_max_docs", "get_full_documents", "load_all_available_meta"],
        "pubmed": ["top_k_results", "email", "api_key", "doc_content_chars_max"],
        "linkup": ["depth"],
    }

    # Get the list of accepted parameters for the given search API
    accepted_params = SEARCH_API_PARAMS.get(search_api, [])

    # If no config provided, return an empty dict
    if not search_api_config:
        return {}

    # Filter the config to only include accepted parameters
    return {k: v for k, v in search_api_config.items() if k in accepted_params}

def deduplicate_and_format_sources(search_response, max_tokens_per_source=5000, include_raw_content=True):
    """
    Takes a list of search responses and formats them into a readable string.
    Limits the raw_content to approximately max_tokens_per_source tokens.
 
    Args:
        search_responses: List of search response dicts, each containing:
            - query: str
            - results: List of dicts with fields:
                - title: str
                - url: str
                - content: str
                - score: float
                - raw_content: str|None
        max_tokens_per_source: int
        include_raw_content: bool
            
    Returns:
        str: Formatted string with deduplicated sources
    """
     # Collect all results
    sources_list = []
    for response in search_response:
        sources_list.extend(response['results'])
    
    # Deduplicate by URL
    unique_sources = {source['url']: source for source in sources_list}

    # Format output
    formatted_text = "Content from sources:\n"
    for i, source in enumerate(unique_sources.values(), 1):
        formatted_text += f"{'='*80}\n"  # Clear section separator
        formatted_text += f"Source: {source['title']}\n"
        formatted_text += f"{'-'*80}\n"  # Subsection separator
        formatted_text += f"URL: {source['url']}\n===\n"
        formatted_text += f"Most relevant content from source: {source['content']}\n===\n"
        if include_raw_content:
            # Using rough estimate of 4 characters per token
            char_limit = max_tokens_per_source * 4
            # Handle None raw_content
            raw_content = source.get('raw_content', '')
            if raw_content is None:
                raw_content = ''
                print(f"Warning: No raw_content found for source {source['url']}")
            if len(raw_content) > char_limit:
                raw_content = raw_content[:char_limit] + "... [truncated]"
            formatted_text += f"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\n\n"
        formatted_text += f"{'='*80}\n\n" # End section separator
                
    return formatted_text.strip()

def format_sections(sections: list[Section]) -> str:
    """ Format a list of sections into a string """
    formatted_str = ""
    for idx, section in enumerate(sections, 1):
        formatted_str += f"""
{'='*60}
Section {idx}: {section.name}
{'='*60}
Description:
{section.description}
Requires Research: 
{section.research}

Content:
{section.content if section.content else '[Not yet written]'}

"""
    return formatted_str

@traceable
async def tavily_search_async(search_queries, max_results: int = 5, topic: str = "general", include_raw_content: bool = True):
    """
    Performs concurrent web searches with the Tavily API

    Args:
        search_queries (List[str]): List of search queries to process

    Returns:
            List[dict]: List of search responses from Tavily API:
                {
                    'query': str,
                    'follow_up_questions': None,      
                    'answer': None,
                    'images': list,
                    'results': [                     # List of search results
                        {
                            'title': str,            # Title of the webpage
                            'url': str,              # URL of the result
                            'content': str,          # Summary/snippet of content
                            'score': float,          # Relevance score
                            'raw_content': str|None  # Full page content if available
                        },
                        ...
                    ]
                }
    """
    tavily_async_client = AsyncTavilyClient()
    search_tasks = []
    for query in search_queries:
            search_tasks.append(
                tavily_async_client.search(
                    query,
                    max_results=max_results,
                    include_raw_content=include_raw_content,
                    topic=topic
                )
            )

    # Execute all searches concurrently
    search_docs = await asyncio.gather(*search_tasks)
    return search_docs

@traceable
def perplexity_search(search_queries):
    """Search the web using the Perplexity API.
    
    Args:
        search_queries (List[SearchQuery]): List of search queries to process
  
    Returns:
        List[dict]: List of search responses from Perplexity API, one per query. Each response has format:
            {
                'query': str,                    # The original search query
                'follow_up_questions': None,      
                'answer': None,
                'images': list,
                'results': [                     # List of search results
                    {
                        'title': str,            # Title of the search result
                        'url': str,              # URL of the result
                        'content': str,          # Summary/snippet of content
                        'score': float,          # Relevance score
                        'raw_content': str|None  # Full content or None for secondary citations
                    },
                    ...
                ]
            }
    """

    headers = {
        "accept": "application/json",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('PERPLEXITY_API_KEY')}"
    }
    
    search_docs = []
    for query in search_queries:

        payload = {
            "model": "sonar-pro",
            "messages": [
                {
                    "role": "system",
                    "content": "Search the web and provide factual information with sources."
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
        }
        
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=payload
        )
        response.raise_for_status()  # Raise exception for bad status codes
        
        # Parse the response
        data = response.json()
        content = data["choices"][0]["message"]["content"]
        citations = data.get("citations", ["https://perplexity.ai"])
        
        # Create results list for this query
        results = []
        
        # First citation gets the full content
        results.append({
            "title": f"Perplexity Search, Source 1",
            "url": citations[0],
            "content": content,
            "raw_content": content,
            "score": 1.0  # Adding score to match Tavily format
        })
        
        # Add additional citations without duplicating content
        for i, citation in enumerate(citations[1:], start=2):
            results.append({
                "title": f"Perplexity Search, Source {i}",
                "url": citation,
                "content": "See primary source for full content",
                "raw_content": None,
                "score": 0.5  # Lower score for secondary sources
            })
        
        # Format response to match Tavily structure
        search_docs.append({
            "query": query,
            "follow_up_questions": None,
            "answer": None,
            "images": [],
            "results": results
        })
    
    return search_docs

@traceable
async def exa_search(search_queries, max_characters: Optional[int] = None, num_results=5, 
                     include_domains: Optional[List[str]] = None, 
                     exclude_domains: Optional[List[str]] = None,
                     subpages: Optional[int] = None):
    """Search the web using the Exa API.
    
    Args:
        search_queries (List[SearchQuery]): List of search queries to process
        max_characters (int, optional): Maximum number of characters to retrieve for each result's raw content.
                                       If None, the text parameter will be set to True instead of an object.
        num_results (int): Number of search results per query. Defaults to 5.
        include_domains (List[str], optional): List of domains to include in search results. 
            When specified, only results from these domains will be returned.
        exclude_domains (List[str], optional): List of domains to exclude from search results.
            Cannot be used together with include_domains.
        subpages (int, optional): Number of subpages to retrieve per result. If None, subpages are not retrieved.
        
    Returns:
        List[dict]: List of search responses from Exa API, one per query. Each response has format:
            {
                'query': str,                    # The original search query
                'follow_up_questions': None,      
                'answer': None,
                'images': list,
                'results': [                     # List of search results
                    {
                        'title': str,            # Title of the search result
                        'url': str,              # URL of the result
                        'content': str,          # Summary/snippet of content
                        'score': float,          # Relevance score
                        'raw_content': str|None  # Full content or None for secondary citations
                    },
                    ...
                ]
            }
    """
    # Check that include_domains and exclude_domains are not both specified
    if include_domains and exclude_domains:
        raise ValueError("Cannot specify both include_domains and exclude_domains")
    
    # Initialize Exa client (API key should be configured in your .env file)
    exa = Exa(api_key = f"{os.getenv('EXA_API_KEY')}")
    
    # Define the function to process a single query
    async def process_query(query):
        # Use run_in_executor to make the synchronous exa call in a non-blocking way
        loop = asyncio.get_event_loop()
        
        # Define the function for the executor with all parameters
        def exa_search_fn():
            # Build parameters dictionary
            kwargs = {
                # Set text to True if max_characters is None, otherwise use an object with max_characters
                "text": True if max_characters is None else {"max_characters": max_characters},
                "summary": True,  # This is an amazing feature by EXA. It provides an AI generated summary of the content based on the query
                "num_results": num_results
            }
            
            # Add optional parameters only if they are provided
            if subpages is not None:
                kwargs["subpages"] = subpages
                
            if include_domains:
                kwargs["include_domains"] = include_domains
            elif exclude_domains:
                kwargs["exclude_domains"] = exclude_domains
                
            return exa.search_and_contents(query, **kwargs)
        
        response = await loop.run_in_executor(None, exa_search_fn)
        
        # Format the response to match the expected output structure
        formatted_results = []
        seen_urls = set()  # Track URLs to avoid duplicates
        
        # Helper function to safely get value regardless of if item is dict or object
        def get_value(item, key, default=None):
            if isinstance(item, dict):
                return item.get(key, default)
            else:
                return getattr(item, key, default) if hasattr(item, key) else default
        
        # Access the results from the SearchResponse object
        results_list = get_value(response, 'results', [])
        
        # First process all main results
        for result in results_list:
            # Get the score with a default of 0.0 if it's None or not present
            score = get_value(result, 'score', 0.0)
            
            # Combine summary and text for content if both are available
            text_content = get_value(result, 'text', '')
            summary_content = get_value(result, 'summary', '')
            
            content = text_content
            if summary_content:
                if content:
                    content = f"{summary_content}\n\n{content}"
                else:
                    content = summary_content
            
            title = get_value(result, 'title', '')
            url = get_value(result, 'url', '')
            
            # Skip if we've seen this URL before (removes duplicate entries)
            if url in seen_urls:
                continue
                
            seen_urls.add(url)
            
            # Main result entry
            result_entry = {
                "title": title,
                "url": url,
                "content": content,
                "score": score,
                "raw_content": text_content
            }
            
            # Add the main result to the formatted results
            formatted_results.append(result_entry)
        
        # Now process subpages only if the subpages parameter was provided
        if subpages is not None:
            for result in results_list:
                subpages_list = get_value(result, 'subpages', [])
                for subpage in subpages_list:
                    # Get subpage score
                    subpage_score = get_value(subpage, 'score', 0.0)
                    
                    # Combine summary and text for subpage content
                    subpage_text = get_value(subpage, 'text', '')
                    subpage_summary = get_value(subpage, 'summary', '')
                    
                    subpage_content = subpage_text
                    if subpage_summary:
                        if subpage_content:
                            subpage_content = f"{subpage_summary}\n\n{subpage_content}"
                        else:
                            subpage_content = subpage_summary
                    
                    subpage_url = get_value(subpage, 'url', '')
                    
                    # Skip if we've seen this URL before
                    if subpage_url in seen_urls:
                        continue
                        
                    seen_urls.add(subpage_url)
                    
                    formatted_results.append({
                        "title": get_value(subpage, 'title', ''),
                        "url": subpage_url,
                        "content": subpage_content,
                        "score": subpage_score,
                        "raw_content": subpage_text
                    })
        
        # Collect images if available (only from main results to avoid duplication)
        images = []
        for result in results_list:
            image = get_value(result, 'image')
            if image and image not in images:  # Avoid duplicate images
                images.append(image)
                
        return {
            "query": query,
            "follow_up_questions": None,
            "answer": None,
            "images": images,
            "results": formatted_results
        }
    
    # Process all queries sequentially with delay to respect rate limit
    search_docs = []
    for i, query in enumerate(search_queries):
        try:
            # Add delay between requests (0.25s = 4 requests per second, well within the 5/s limit)
            if i > 0:  # Don't delay the first request
                await asyncio.sleep(0.25)
            
            result = await process_query(query)
            search_docs.append(result)
        except Exception as e:
            # Handle exceptions gracefully
            print(f"Error processing query '{query}': {str(e)}")
            # Add a placeholder result for failed queries to maintain index alignment
            search_docs.append({
                "query": query,
                "follow_up_questions": None,
                "answer": None,
                "images": [],
                "results": [],
                "error": str(e)
            })
            
            # Add additional delay if we hit a rate limit error
            if "429" in str(e):
                print("Rate limit exceeded. Adding additional delay...")
                await asyncio.sleep(1.0)  # Add a longer delay if we hit a rate limit
    
    return search_docs

@traceable
async def arxiv_search_async(search_queries, load_max_docs=5, get_full_documents=True, load_all_available_meta=True):
    """
    Performs concurrent searches on arXiv using the ArxivRetriever.

    Args:
        search_queries (List[str]): List of search queries or article IDs
        load_max_docs (int, optional): Maximum number of documents to return per query. Default is 5.
        get_full_documents (bool, optional): Whether to fetch full text of documents. Default is True.
        load_all_available_meta (bool, optional): Whether to load all available metadata. Default is True.

    Returns:
        List[dict]: List of search responses from arXiv, one per query. Each response has format:
            {
                'query': str,                    # The original search query
                'follow_up_questions': None,      
                'answer': None,
                'images': [],
                'results': [                     # List of search results
                    {
                        'title': str,            # Title of the paper
                        'url': str,              # URL (Entry ID) of the paper
                        'content': str,          # Formatted summary with metadata
                        'score': float,          # Relevance score (approximated)
                        'raw_content': str|None  # Full paper content if available
                    },
                    ...
                ]
            }
    """
    
    async def process_single_query(query):
        try:
            # Create retriever for each query
            retriever = ArxivRetriever(
                load_max_docs=load_max_docs,
                get_full_documents=get_full_documents,
                load_all_available_meta=load_all_available_meta
            )
            
            # Run the synchronous retriever in a thread pool
            loop = asyncio.get_event_loop()
            docs = await loop.run_in_executor(None, lambda: retriever.invoke(query))
            
            results = []
            # Assign decreasing scores based on the order
            base_score = 1.0
            score_decrement = 1.0 / (len(docs) + 1) if docs else 0
            
            for i, doc in enumerate(docs):
                # Extract metadata
                metadata = doc.metadata
                
                # Use entry_id as the URL (this is the actual arxiv link)
                url = metadata.get('entry_id', '')
                
                # Format content with all useful metadata
                content_parts = []

                # Primary information
                if 'Summary' in metadata:
                    content_parts.append(f"Summary: {metadata['Summary']}")

                if 'Authors' in metadata:
                    content_parts.append(f"Authors: {metadata['Authors']}")

                # Add publication information
                published = metadata.get('Published')
                published_str = published.isoformat() if hasattr(published, 'isoformat') else str(published) if published else ''
                if published_str:
                    content_parts.append(f"Published: {published_str}")

                # Add additional metadata if available
                if 'primary_category' in metadata:
                    content_parts.append(f"Primary Category: {metadata['primary_category']}")

                if 'categories' in metadata and metadata['categories']:
                    content_parts.append(f"Categories: {', '.join(metadata['categories'])}")

                if 'comment' in metadata and metadata['comment']:
                    content_parts.append(f"Comment: {metadata['comment']}")

                if 'journal_ref' in metadata and metadata['journal_ref']:
                    content_parts.append(f"Journal Reference: {metadata['journal_ref']}")

                if 'doi' in metadata and metadata['doi']:
                    content_parts.append(f"DOI: {metadata['doi']}")

                # Get PDF link if available in the links
                pdf_link = ""
                if 'links' in metadata and metadata['links']:
                    for link in metadata['links']:
                        if 'pdf' in link:
                            pdf_link = link
                            content_parts.append(f"PDF: {pdf_link}")
                            break

                # Join all content parts with newlines 
                content = "\n".join(content_parts)
                
                result = {
                    'title': metadata.get('Title', ''),
                    'url': url,  # Using entry_id as the URL
                    'content': content,
                    'score': base_score - (i * score_decrement),
                    'raw_content': doc.page_content if get_full_documents else None
                }
                results.append(result)
                
            return {
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': results
            }
        except Exception as e:
            # Handle exceptions gracefully
            print(f"Error processing arXiv query '{query}': {str(e)}")
            return {
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': [],
                'error': str(e)
            }
    
    # Process queries sequentially with delay to respect arXiv rate limit (1 request per 3 seconds)
    search_docs = []
    for i, query in enumerate(search_queries):
        try:
            # Add delay between requests (3 seconds per ArXiv's rate limit)
            if i > 0:  # Don't delay the first request
                await asyncio.sleep(3.0)
            
            result = await process_single_query(query)
            search_docs.append(result)
        except Exception as e:
            # Handle exceptions gracefully
            print(f"Error processing arXiv query '{query}': {str(e)}")
            search_docs.append({
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': [],
                'error': str(e)
            })
            
            # Add additional delay if we hit a rate limit error
            if "429" in str(e) or "Too Many Requests" in str(e):
                print("ArXiv rate limit exceeded. Adding additional delay...")
                await asyncio.sleep(5.0)  # Add a longer delay if we hit a rate limit
    
    return search_docs

@traceable
async def pubmed_search_async(search_queries, top_k_results=5, email=None, api_key=None, doc_content_chars_max=4000):
    """
    Performs concurrent searches on PubMed using the PubMedAPIWrapper.

    Args:
        search_queries (List[str]): List of search queries
        top_k_results (int, optional): Maximum number of documents to return per query. Default is 5.
        email (str, optional): Email address for PubMed API. Required by NCBI.
        api_key (str, optional): API key for PubMed API for higher rate limits.
        doc_content_chars_max (int, optional): Maximum characters for document content. Default is 4000.

    Returns:
        List[dict]: List of search responses from PubMed, one per query. Each response has format:
            {
                'query': str,                    # The original search query
                'follow_up_questions': None,      
                'answer': None,
                'images': [],
                'results': [                     # List of search results
                    {
                        'title': str,            # Title of the paper
                        'url': str,              # URL to the paper on PubMed
                        'content': str,          # Formatted summary with metadata
                        'score': float,          # Relevance score (approximated)
                        'raw_content': str       # Full abstract content
                    },
                    ...
                ]
            }
    """
    
    async def process_single_query(query):
        try:
            # print(f"Processing PubMed query: '{query}'")
            
            # Create PubMed wrapper for the query
            wrapper = PubMedAPIWrapper(
                top_k_results=top_k_results,
                doc_content_chars_max=doc_content_chars_max,
                email=email if email else "your_email@example.com",
                api_key=api_key if api_key else ""
            )
            
            # Run the synchronous wrapper in a thread pool
            loop = asyncio.get_event_loop()
            
            # Use wrapper.lazy_load instead of load to get better visibility
            docs = await loop.run_in_executor(None, lambda: list(wrapper.lazy_load(query)))
            
            print(f"Query '{query}' returned {len(docs)} results")
            
            results = []
            # Assign decreasing scores based on the order
            base_score = 1.0
            score_decrement = 1.0 / (len(docs) + 1) if docs else 0
            
            for i, doc in enumerate(docs):
                # Format content with metadata
                content_parts = []
                
                if doc.get('Published'):
                    content_parts.append(f"Published: {doc['Published']}")
                
                if doc.get('Copyright Information'):
                    content_parts.append(f"Copyright Information: {doc['Copyright Information']}")
                
                if doc.get('Summary'):
                    content_parts.append(f"Summary: {doc['Summary']}")
                
                # Generate PubMed URL from the article UID
                uid = doc.get('uid', '')
                url = f"https://pubmed.ncbi.nlm.nih.gov/{uid}/" if uid else ""
                
                # Join all content parts with newlines
                content = "\n".join(content_parts)
                
                result = {
                    'title': doc.get('Title', ''),
                    'url': url,
                    'content': content,
                    'score': base_score - (i * score_decrement),
                    'raw_content': doc.get('Summary', '')
                }
                results.append(result)
            
            return {
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': results
            }
        except Exception as e:
            # Handle exceptions with more detailed information
            error_msg = f"Error processing PubMed query '{query}': {str(e)}"
            print(error_msg)
            import traceback
            print(traceback.format_exc())  # Print full traceback for debugging
            
            return {
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': [],
                'error': str(e)
            }
    
    # Process all queries with a reasonable delay between them
    search_docs = []
    
    # Start with a small delay that increases if we encounter rate limiting
    delay = 1.0  # Start with a more conservative delay
    
    for i, query in enumerate(search_queries):
        try:
            # Add delay between requests
            if i > 0:  # Don't delay the first request
                # print(f"Waiting {delay} seconds before next query...")
                await asyncio.sleep(delay)
            
            result = await process_single_query(query)
            search_docs.append(result)
            
            # If query was successful with results, we can slightly reduce delay (but not below minimum)
            if result.get('results') and len(result['results']) > 0:
                delay = max(0.5, delay * 0.9)  # Don't go below 0.5 seconds
            
        except Exception as e:
            # Handle exceptions gracefully
            error_msg = f"Error in main loop processing PubMed query '{query}': {str(e)}"
            print(error_msg)
            
            search_docs.append({
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': [],
                'error': str(e)
            })
            
            # If we hit an exception, increase delay for next query
            delay = min(5.0, delay * 1.5)  # Don't exceed 5 seconds
    
    return search_docs

@traceable
async def linkup_search(search_queries, depth: Optional[str] = "standard"):
    """
    Performs concurrent web searches using the Linkup API.

    Args:
        search_queries (List[SearchQuery]): List of search queries to process
        depth (str, optional): "standard" (default)  or "deep". More details here https://docs.linkup.so/pages/documentation/get-started/concepts

    Returns:
        List[dict]: List of search responses from Linkup API, one per query. Each response has format:
            {
                'results': [            # List of search results
                    {
                        'title': str,   # Title of the search result
                        'url': str,     # URL of the result
                        'content': str, # Summary/snippet of content
                    },
                    ...
                ]
            }
    """
    client = LinkupClient()
    search_tasks = []
    for query in search_queries:
        search_tasks.append(
                client.async_search(
                    query,
                    depth,
                    output_type="searchResults",
                )
            )

    search_results = []
    for response in await asyncio.gather(*search_tasks):
        search_results.append(
            {
                "results": [
                    {"title": result.name, "url": result.url, "content": result.content}
                    for result in response.results
                ],
            }
        )

    return search_results

@traceable
async def google_search_async(search_queries: Union[str, List[str]], max_results: int = 5, include_raw_content: bool = True):
    """
    Performs concurrent web searches using Google.
    Uses Google Custom Search API if environment variables are set, otherwise falls back to web scraping.

    Args:
        search_queries (List[str]): List of search queries to process
        max_results (int): Maximum number of results to return per query
        include_raw_content (bool): Whether to fetch full page content

    Returns:
        List[dict]: List of search responses from Google, one per query
    """


    # Check for API credentials from environment variables
    api_key = os.environ.get("GOOGLE_API_KEY")
    cx = os.environ.get("GOOGLE_CX")
    use_api = bool(api_key and cx)
    
    # Handle case where search_queries is a single string
    if isinstance(search_queries, str):
        search_queries = [search_queries]
    
    # Define user agent generator
    def get_useragent():
        """Generates a random user agent string."""
        lynx_version = f"Lynx/{random.randint(2, 3)}.{random.randint(8, 9)}.{random.randint(0, 2)}"
        libwww_version = f"libwww-FM/{random.randint(2, 3)}.{random.randint(13, 15)}"
        ssl_mm_version = f"SSL-MM/{random.randint(1, 2)}.{random.randint(3, 5)}"
        openssl_version = f"OpenSSL/{random.randint(1, 3)}.{random.randint(0, 4)}.{random.randint(0, 9)}"
        return f"{lynx_version} {libwww_version} {ssl_mm_version} {openssl_version}"
    
    # Create executor for running synchronous operations
    executor = None if use_api else concurrent.futures.ThreadPoolExecutor(max_workers=5)
    
    # Use a semaphore to limit concurrent requests
    semaphore = asyncio.Semaphore(5 if use_api else 2)
    
    async def search_single_query(query):
        async with semaphore:
            try:
                results = []
                
                # API-based search
                if use_api:
                    # The API returns up to 10 results per request
                    for start_index in range(1, max_results + 1, 10):
                        # Calculate how many results to request in this batch
                        num = min(10, max_results - (start_index - 1))
                        
                        # Make request to Google Custom Search API
                        params = {
                            'q': query,
                            'key': api_key,
                            'cx': cx,
                            'start': start_index,
                            'num': num
                        }
                        print(f"Requesting {num} results for '{query}' from Google API...")

                        async with aiohttp.ClientSession() as session:
                            async with session.get('https://www.googleapis.com/customsearch/v1', params=params) as response:
                                if response.status != 200:
                                    error_text = await response.text()
                                    print(f"API error: {response.status}, {error_text}")
                                    break
                                    
                                data = await response.json()
                                
                                # Process search results
                                for item in data.get('items', []):
                                    result = {
                                        "title": item.get('title', ''),
                                        "url": item.get('link', ''),
                                        "content": item.get('snippet', ''),
                                        "score": None,
                                        "raw_content": item.get('snippet', '')
                                    }
                                    results.append(result)
                        
                        # Respect API quota with a small delay
                        await asyncio.sleep(0.2)
                        
                        # If we didn't get a full page of results, no need to request more
                        if not data.get('items') or len(data.get('items', [])) < num:
                            break
                
                # Web scraping based search
                else:
                    # Add delay between requests
                    await asyncio.sleep(0.5 + random.random() * 1.5)
                    print(f"Scraping Google for '{query}'...")

                    # Define scraping function
                    def google_search(query, max_results):
                        try:
                            lang = "en"
                            safe = "active"
                            start = 0
                            fetched_results = 0
                            fetched_links = set()
                            search_results = []
                            
                            while fetched_results < max_results:
                                # Send request to Google
                                resp = requests.get(
                                    url="https://www.google.com/search",
                                    headers={
                                        "User-Agent": get_useragent(),
                                        "Accept": "*/*"
                                    },
                                    params={
                                        "q": query,
                                        "num": max_results + 2,
                                        "hl": lang,
                                        "start": start,
                                        "safe": safe,
                                    },
                                    cookies = {
                                        'CONSENT': 'PENDING+987',  # Bypasses the consent page
                                        'SOCS': 'CAESHAgBEhIaAB',
                                    }
                                )
                                resp.raise_for_status()
                                
                                # Parse results
                                soup = BeautifulSoup(resp.text, "html.parser")
                                result_block = soup.find_all("div", class_="ezO2md")
                                new_results = 0
                                
                                for result in result_block:
                                    link_tag = result.find("a", href=True)
                                    title_tag = link_tag.find("span", class_="CVA68e") if link_tag else None
                                    description_tag = result.find("span", class_="FrIlee")
                                    
                                    if link_tag and title_tag and description_tag:
                                        link = unquote(link_tag["href"].split("&")[0].replace("/url?q=", ""))
                                        
                                        if link in fetched_links:
                                            continue
                                        
                                        fetched_links.add(link)
                                        title = title_tag.text
                                        description = description_tag.text
                                        
                                        # Store result in the same format as the API results
                                        search_results.append({
                                            "title": title,
                                            "url": link,
                                            "content": description,
                                            "score": None,
                                            "raw_content": description
                                        })
                                        
                                        fetched_results += 1
                                        new_results += 1
                                        
                                        if fetched_results >= max_results:
                                            break
                                
                                if new_results == 0:
                                    break
                                    
                                start += 10
                                time.sleep(1)  # Delay between pages
                            
                            return search_results
                                
                        except Exception as e:
                            print(f"Error in Google search for '{query}': {str(e)}")
                            return []
                    
                    # Execute search in thread pool
                    loop = asyncio.get_running_loop()
                    search_results = await loop.run_in_executor(
                        executor, 
                        lambda: google_search(query, max_results)
                    )
                    
                    # Process the results
                    results = search_results
                
                # If requested, fetch full page content asynchronously (for both API and web scraping)
                if include_raw_content and results:
                    content_semaphore = asyncio.Semaphore(3)
                    
                    async with aiohttp.ClientSession() as session:
                        fetch_tasks = []
                        
                        async def fetch_full_content(result):
                            async with content_semaphore:
                                url = result['url']
                                headers = {
                                    'User-Agent': get_useragent(),
                                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                                }
                                
                                try:
                                    await asyncio.sleep(0.2 + random.random() * 0.6)
                                    async with session.get(url, headers=headers, timeout=10) as response:
                                        if response.status == 200:
                                            # Check content type to handle binary files
                                            content_type = response.headers.get('Content-Type', '').lower()
                                            
                                            # Handle PDFs and other binary files
                                            if 'application/pdf' in content_type or 'application/octet-stream' in content_type:
                                                # For PDFs, indicate that content is binary and not parsed
                                                result['raw_content'] = f"[Binary content: {content_type}. Content extraction not supported for this file type.]"
                                            else:
                                                try:
                                                    # Try to decode as UTF-8 with replacements for non-UTF8 characters
                                                    html = await response.text(errors='replace')
                                                    soup = BeautifulSoup(html, 'html.parser')
                                                    result['raw_content'] = soup.get_text()
                                                except UnicodeDecodeError as ude:
                                                    # Fallback if we still have decoding issues
                                                    result['raw_content'] = f"[Could not decode content: {str(ude)}]"
                                except Exception as e:
                                    print(f"Warning: Failed to fetch content for {url}: {str(e)}")
                                    result['raw_content'] = f"[Error fetching content: {str(e)}]"
                                return result
                        
                        for result in results:
                            fetch_tasks.append(fetch_full_content(result))
                        
                        updated_results = await asyncio.gather(*fetch_tasks)
                        results = updated_results
                        print(f"Fetched full content for {len(results)} results")
                
                return {
                    "query": query,
                    "follow_up_questions": None,
                    "answer": None,
                    "images": [],
                    "results": results
                }
            except Exception as e:
                print(f"Error in Google search for query '{query}': {str(e)}")
                return {
                    "query": query,
                    "follow_up_questions": None,
                    "answer": None,
                    "images": [],
                    "results": []
                }
    
    try:
        # Create tasks for all search queries
        search_tasks = [search_single_query(query) for query in search_queries]
        
        # Execute all searches concurrently
        search_results = await asyncio.gather(*search_tasks)
        
        return search_results
    finally:
        # Only shut down executor if it was created
        if executor:
            executor.shutdown(wait=False)

async def scrape_pages(titles: List[str], urls: List[str]) -> str:
    """
    Scrapes content from a list of URLs and formats it into a readable markdown document.
    
    This function:
    1. Takes a list of page titles and URLs
    2. Makes asynchronous HTTP requests to each URL
    3. Converts HTML content to markdown
    4. Formats all content with clear source attribution
    
    Args:
        titles (List[str]): A list of page titles corresponding to each URL
        urls (List[str]): A list of URLs to scrape content from
        
    Returns:
        str: A formatted string containing the full content of each page in markdown format,
             with clear section dividers and source attribution
    """
    
    # Create an async HTTP client
    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as client:
        pages = []
        
        # Fetch each URL and convert to markdown
        for url in urls:
            try:
                # Fetch the content
                response = await client.get(url)
                response.raise_for_status()
                
                # Convert HTML to markdown if successful
                if response.status_code == 200:
                    # Handle different content types
                    content_type = response.headers.get('Content-Type', '')
                    if 'text/html' in content_type:
                        # Convert HTML to markdown
                        markdown_content = markdownify(response.text)
                        pages.append(markdown_content)
                    else:
                        # For non-HTML content, just mention the content type
                        pages.append(f"Content type: {content_type} (not converted to markdown)")
                else:
                    pages.append(f"Error: Received status code {response.status_code}")
        
            except Exception as e:
                # Handle any exceptions during fetch
                pages.append(f"Error fetching URL: {str(e)}")
        
        # Create formatted output 
        formatted_output = f"Search results: \n\n"
        
        for i, (title, url, page) in enumerate(zip(titles, urls, pages)):
            formatted_output += f"\n\n--- SOURCE {i+1}: {title} ---\n"
            formatted_output += f"URL: {url}\n\n"
            formatted_output += f"FULL CONTENT:\n {page}"
            formatted_output += "\n\n" + "-" * 80 + "\n"
        
    return  formatted_output

@tool
async def duckduckgo_search(search_queries: List[str]):
    """Perform searches using DuckDuckGo with retry logic to handle rate limits
    
    Args:
        search_queries (List[str]): List of search queries to process
        
    Returns:
        List[dict]: List of search results
    """
    
    async def process_single_query(query):
        # Execute synchronous search in the event loop's thread pool
        loop = asyncio.get_event_loop()
        
        def perform_search():
            max_retries = 3
            retry_count = 0
            backoff_factor = 2.0
            last_exception = None
            
            while retry_count <= max_retries:
                try:
                    results = []
                    with DDGS() as ddgs:
                        # Change query slightly and add delay between retries
                        if retry_count > 0:
                            # Random delay with exponential backoff
                            delay = backoff_factor ** retry_count + random.random()
                            print(f"Retry {retry_count}/{max_retries} for query '{query}' after {delay:.2f}s delay")
                            time.sleep(delay)
                            
                            # Add a random element to the query to bypass caching/rate limits
                            modifiers = ['about', 'info', 'guide', 'overview', 'details', 'explained']
                            modified_query = f"{query} {random.choice(modifiers)}"
                        else:
                            modified_query = query
                        
                        # Execute search
                        ddg_results = list(ddgs.text(modified_query, max_results=5))
                        
                        # Format results
                        for i, result in enumerate(ddg_results):
                            results.append({
                                'title': result.get('title', ''),
                                'url': result.get('href', ''),
                                'content': result.get('body', ''),
                                'score': 1.0 - (i * 0.1),  # Simple scoring mechanism
                                'raw_content': result.get('body', '')
                            })
                        
                        # Return successful results
                        return {
                            'query': query,
                            'follow_up_questions': None,
                            'answer': None,
                            'images': [],
                            'results': results
                        }
                except Exception as e:
                    # Store the exception and retry
                    last_exception = e
                    retry_count += 1
                    print(f"DuckDuckGo search error: {str(e)}. Retrying {retry_count}/{max_retries}")
                    
                    # If not a rate limit error, don't retry
                    if "Ratelimit" not in str(e) and retry_count >= 1:
                        print(f"Non-rate limit error, stopping retries: {str(e)}")
                        break
            
            # If we reach here, all retries failed
            print(f"All retries failed for query '{query}': {str(last_exception)}")
            # Return empty results but with query info preserved
            return {
                'query': query,
                'follow_up_questions': None,
                'answer': None,
                'images': [],
                'results': [],
                'error': str(last_exception)
            }
            
        return await loop.run_in_executor(None, perform_search)

    # Process queries with delay between them to reduce rate limiting
    search_docs = []
    urls = []
    titles = []
    for i, query in enumerate(search_queries):
        # Add delay between queries (except first one)
        if i > 0:
            delay = 2.0 + random.random() * 2.0  # Random delay 2-4 seconds
            await asyncio.sleep(delay)
        
        # Process the query
        result = await process_single_query(query)
        search_docs.append(result)
        
        # Safely extract URLs and titles from results, handling empty result cases
        if result['results'] and len(result['results']) > 0:
            for res in result['results']:
                if 'url' in res and 'title' in res:
                    urls.append(res['url'])
                    titles.append(res['title'])
    
    # If we got any valid URLs, scrape the pages
    if urls:
        return await scrape_pages(titles, urls)
    else:
        # Return a formatted error message if no valid URLs were found
        return "No valid search results found. Please try different search queries or use a different search API."

@tool
async def tavily_search(queries: List[str], max_results: int = 5, topic: str = "general") -> str:
    """
    Fetches results from Tavily search API.
    
    Args:
        queries (List[str]): List of search queries
        
    Returns:
        str: A formatted string of search results
    """
    # Use tavily_search_async with include_raw_content=True to get content directly
    search_results = await tavily_search_async(
        queries,
        max_results=max_results,
        topic=topic,
        include_raw_content=True
    )

    # Format the search results directly using the raw_content already provided
    formatted_output = f"Search results: \n\n"
    
    # Deduplicate results by URL
    unique_results = {}
    for response in search_results:
        for result in response['results']:
            url = result['url']
            if url not in unique_results:
                unique_results[url] = result
    
    # Format the unique results
    for i, (url, result) in enumerate(unique_results.items()):
        formatted_output += f"\n\n--- SOURCE {i+1}: {result['title']} ---\n"
        formatted_output += f"URL: {url}\n\n"
        formatted_output += f"SUMMARY:\n{result['content']}\n\n"
        if result.get('raw_content'):
            formatted_output += f"FULL CONTENT:\n{result['raw_content'][:30000]}"  # Limit content size
        formatted_output += "\n\n" + "-" * 80 + "\n"
    
    if unique_results:
        return formatted_output
    else:
        return "No valid search results found. Please try different search queries or use a different search API."

async def select_and_execute_search(search_api: str, query_list: list[str], params_to_pass: dict) -> str:
    """Select and execute the appropriate search API.
    
    Args:
        search_api: Name of the search API to use
        query_list: List of search queries to execute
        params_to_pass: Parameters to pass to the search API
        
    Returns:
        Formatted string containing search results
        
    Raises:
        ValueError: If an unsupported search API is specified
    """
    if search_api == "tavily":
        # Tavily search tool used with both workflow and agent 
        return await tavily_search.ainvoke({'queries': query_list}, **params_to_pass)
    elif search_api == "duckduckgo":
        # DuckDuckGo search tool used with both workflow and agent 
        return await duckduckgo_search.ainvoke({'search_queries': query_list})
    elif search_api == "perplexity":
        search_results = perplexity_search(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    elif search_api == "exa":
        search_results = await exa_search(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    elif search_api == "arxiv":
        search_results = await arxiv_search_async(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    elif search_api == "pubmed":
        search_results = await pubmed_search_async(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    elif search_api == "linkup":
        search_results = await linkup_search(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    elif search_api == "googlesearch":
        search_results = await google_search_async(query_list, **params_to_pass)
        return deduplicate_and_format_sources(search_results, max_tokens_per_source=4000)
    else:
        raise ValueError(f"Unsupported search API: {search_api}")



================================================
FILE: tests/__init__.py
================================================
# Test package initialization


================================================
FILE: tests/conftest.py
================================================
"""
Pytest configuration for open_deep_research tests.
"""

import pytest

def pytest_addoption(parser):
    """Add command-line options to pytest."""
    parser.addoption("--research-agent", action="store", help="Agent type: multi_agent or graph")
    parser.addoption("--search-api", action="store", help="Search API to use")
    parser.addoption("--eval-model", action="store", help="Model for evaluation")
    parser.addoption("--supervisor-model", action="store", help="Model for supervisor agent")
    parser.addoption("--researcher-model", action="store", help="Model for researcher agent")
    parser.addoption("--planner-provider", action="store", help="Provider for planner model")
    parser.addoption("--planner-model", action="store", help="Model for planning")
    parser.addoption("--writer-provider", action="store", help="Provider for writer model")
    parser.addoption("--writer-model", action="store", help="Model for writing")
    parser.addoption("--max-search-depth", action="store", help="Maximum search depth")


================================================
FILE: tests/run_test.py
================================================
#!/usr/bin/env python
import os
import subprocess
import sys
import argparse

''' 
Example w/ o3 -- 
python tests/run_test.py --all \
    --supervisor-model "openai:o3" \
    --researcher-model "openai:o3" \
    --planner-provider "openai" \
    --planner-model "o3" \
    --writer-provider "openai" \
    --writer-model "o3" \
    --eval-model "openai:o3" \
    --search-api "tavily"

Example w/ gpt-4.1 -- 
python tests/run_test.py --all \
    --supervisor-model "openai:gpt-4.1" \
    --researcher-model "openai:gpt-4.1" \
    --planner-provider "openai" \
    --planner-model "gpt-4.1" \
    --writer-provider "openai" \
    --writer-model "gpt-4.1" \
    --eval-model "openai:o3" \
    --search-api "tavily"
'''

def main():
    # LangSmith project name for report quality testing
    langsmith_project = "Open Deep Research: Report Quality Testing"

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run report quality tests for different agents")
    parser.add_argument("--rich-output", action="store_true", help="Show rich output in terminal")
    parser.add_argument("--experiment-name", help="Name for the LangSmith experiment")
    parser.add_argument("--agent", choices=["multi_agent", "graph"], help="Run tests for a specific agent")
    parser.add_argument("--all", action="store_true", help="Run tests for all agents")
    
    # Model configuration options
    parser.add_argument("--supervisor-model", help="Model for supervisor agent (e.g., 'anthropic:claude-3-7-sonnet-latest')")
    parser.add_argument("--researcher-model", help="Model for researcher agent (e.g., 'anthropic:claude-3-5-sonnet-latest')")
    parser.add_argument("--planner-provider", help="Provider for planner model (e.g., 'anthropic')")
    parser.add_argument("--planner-model", help="Model for planner in graph-based agent (e.g., 'claude-3-7-sonnet-latest')")
    parser.add_argument("--writer-provider", help="Provider for writer model (e.g., 'anthropic')")
    parser.add_argument("--writer-model", help="Model for writer in graph-based agent (e.g., 'claude-3-5-sonnet-latest')")
    parser.add_argument("--eval-model", help="Model for evaluating report quality (default: openai:gpt-4-turbo)")
    
    # Search API configuration
    parser.add_argument("--search-api", choices=["tavily", "duckduckgo"], 
                        help="Search API to use for content retrieval")
    
    args = parser.parse_args()
    
    # Base pytest options
    base_pytest_options = ["-v", "--disable-warnings", "--langsmith-output"]
    if args.rich_output:
        base_pytest_options.append("--rich-output")
    
    # Define available agents
    agents = ["multi_agent", "graph"]
    
    # Determine which agents to test
    if args.agent:
        if args.agent in agents:
            agents_to_test = [args.agent]
        else:
            print(f"Error: Unknown agent '{args.agent}'")
            print(f"Available agents: {', '.join(agents)}")
            return 1
    elif args.all:
        agents_to_test = agents
    else:
        # Default to testing all agents
        agents_to_test = agents
    
    # Run tests for each agent
    for agent in agents_to_test:
        print(f"\nRunning tests for {agent}...")
        
        # Set up LangSmith environment for this agent
        os.environ["LANGSMITH_PROJECT"] = langsmith_project
        os.environ["LANGSMITH_TEST_SUITE"] = langsmith_project
        
        # Ensure tracing is enabled
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        
        # Create a fresh copy of the pytest options for this run
        pytest_options = base_pytest_options.copy()
        
        # We're now using direct pytest command line arguments instead of environment variables
        # No need to set environment variables for test parameters
        
        # Test file path
        test_file = "tests/test_report_quality.py"
                    
        # Set up experiment name
        experiment_name = args.experiment_name if args.experiment_name else f"Report Quality Test | Agent: {agent}"
        print(f"   Project: {langsmith_project}")
        print(f"   Experiment: {experiment_name}")
        
        os.environ["LANGSMITH_EXPERIMENT"] = experiment_name
        
        print(f"\nâ„¹ï¸ Test results for {agent} are being logged to LangSmith")
        
        # Run the test with direct pytest arguments instead of environment variables
        cmd = ["python", "-m", "pytest", test_file] + pytest_options + [
            f"--research-agent={agent}"
        ]
        
        # Add model configurations if provided
        if args.supervisor_model:
            cmd.append(f"--supervisor-model={args.supervisor_model}")
        if args.researcher_model:
            cmd.append(f"--researcher-model={args.researcher_model}")
        if args.planner_provider:
            cmd.append(f"--planner-provider={args.planner_provider}")
        if args.planner_model:
            cmd.append(f"--planner-model={args.planner_model}")
        if args.writer_provider:
            cmd.append(f"--writer-provider={args.writer_provider}")
        if args.writer_model:
            cmd.append(f"--writer-model={args.writer_model}")
        if args.eval_model:
            cmd.append(f"--eval-model={args.eval_model}")
        if args.search_api:
            cmd.append(f"--search-api={args.search_api}")
            
        print(f"Running command: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # Print test output with highlighting for section relevance analysis
        stdout = result.stdout
        
        # Highlight section relevance information with colors if supported
        if "POTENTIAL IRRELEVANT SECTIONS DETECTED" in stdout:
            # Extract and highlight the section relevance analysis
            import re
            section_analysis = re.search(r'âš ï¸ POTENTIAL IRRELEVANT SECTIONS DETECTED:.*?(?=\n\n|\Z)', 
                                         stdout, re.DOTALL)
            if section_analysis:
                analysis_text = section_analysis.group(0)
                # Use ANSI color codes for highlighting (red for irrelevant sections)
                highlighted_analysis = f"\033[1;31m{analysis_text}\033[0m"
                stdout = stdout.replace(analysis_text, highlighted_analysis)
        
        print(stdout)
        
        if result.stderr:
            print("Errors/warnings:")
            print(result.stderr)
            
        # Print a summary of section relevance issues
        if "POTENTIAL IRRELEVANT SECTIONS DETECTED" in result.stdout:
            print("\n\033[1;33m==== SECTION RELEVANCE ISSUES DETECTED ====\033[0m")
            print("Some sections may not be relevant to the main topic.")
            print("Review the detailed analysis in the test output above.")
            print("Consider updating the prompts to improve section relevance.")
                
if __name__ == "__main__":
    sys.exit(main() or 0)


================================================
FILE: tests/test_report_quality.py
================================================
#!/usr/bin/env python

import os
import uuid
import pytest
import asyncio
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model
from langsmith import testing as t

from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command

# Import the report generation agents
from open_deep_research.graph import builder
from open_deep_research.multi_agent import supervisor_builder

class CriteriaGrade(BaseModel):
    """Score the response against specific criteria."""
    grade: bool = Field(description="Does the response meet the provided criteria?")
    justification: str = Field(description="The justification for the grade and score, including specific examples from the response.")

# Function to create evaluation LLM at test time
def get_evaluation_llm(eval_model=None):
    """Create and return an evaluation LLM.
    
    Args:
        eval_model: Model identifier to use for evaluation
                    Format: "provider:model_name" (e.g., "openai:gpt-4-turbo")
                    If None, it will use environment variable or default
    
    Returns:
        Structured LLM for generating evaluation grades
    """
    # Use provided model, then environment variable, then default
    model_to_use = eval_model or os.environ.get("EVAL_MODEL", "openai:gpt-4-turbo")
    
    criteria_eval_llm = init_chat_model(model_to_use)
    return criteria_eval_llm.with_structured_output(CriteriaGrade)

RESPONSE_CRITERIA_SYSTEM_PROMPT = """
You are evaluating the quality of a research report. Please assess the report against the following criteria, being especially strict about section relevance.

1. Topic Relevance (Overall): Does the report directly address the user's input topic thoroughly?

2. Section Relevance (Critical): CAREFULLY assess each individual section for relevance to the main topic:
   - Identify each section by its ## header
   - For each section, determine if it is directly relevant to the primary topic
   - Flag any sections that seem tangential, off-topic, or only loosely connected to the main topic
   - A high-quality report should have NO irrelevant sections

3. Structure and Flow: Do the sections flow logically from one to the next, creating a cohesive narrative?

4. Introduction Quality: Does the introduction effectively provide context and set up the scope of the report?

5. Conclusion Quality: Does the conclusion meaningfully summarize key findings and insights from the report?

6. Structural Elements: Does the report use structural elements (e.g., tables, lists) to effectively convey information?

7. Section Headers: Are section headers properly formatted with Markdown (# for title, ## for sections, ### for subsections)?

8. Citations: Does the report properly cite sources in each main body section?

9. Overall Quality: Is the report well-researched, accurate, and professionally written?

Evaluation Instructions:
- Be STRICT about section relevance - ALL sections must clearly connect to the primary topic
- A report with even ONE irrelevant section should be considered flawed
- You must individually mention each section by name and assess its relevance
- Provide specific examples from the report to justify your evaluation for each criterion
- The report fails if any sections are irrelevant to the main topic, regardless of other qualities
""" 

# Define fixtures for test configuration
@pytest.fixture
def research_agent(request):
    """Get the research agent type from command line or environment variable."""
    return request.config.getoption("--research-agent") or os.environ.get("RESEARCH_AGENT", "multi_agent")

@pytest.fixture
def search_api(request):
    """Get the search API from command line or environment variable."""
    return request.config.getoption("--search-api") or os.environ.get("SEARCH_API", "tavily")

@pytest.fixture
def eval_model(request):
    """Get the evaluation model from command line or environment variable."""
    return request.config.getoption("--eval-model") or os.environ.get("EVAL_MODEL", "openai:gpt-4-turbo")

@pytest.fixture
def models(request, research_agent):
    """Get model configurations based on agent type."""
    if research_agent == "multi_agent":
        return {
            "supervisor_model": (
                request.config.getoption("--supervisor-model") or 
                os.environ.get("SUPERVISOR_MODEL", "anthropic:claude-3-7-sonnet-latest")
            ),
            "researcher_model": (
                request.config.getoption("--researcher-model") or 
                os.environ.get("RESEARCHER_MODEL", "anthropic:claude-3-5-sonnet-latest")
            ),
        }
    else:  # graph agent
        return {
            "planner_provider": (
                request.config.getoption("--planner-provider") or 
                os.environ.get("PLANNER_PROVIDER", "anthropic")
            ),
            "planner_model": (
                request.config.getoption("--planner-model") or 
                os.environ.get("PLANNER_MODEL", "claude-3-7-sonnet-latest")
            ),
            "writer_provider": (
                request.config.getoption("--writer-provider") or 
                os.environ.get("WRITER_PROVIDER", "anthropic")
            ),
            "writer_model": (
                request.config.getoption("--writer-model") or 
                os.environ.get("WRITER_MODEL", "claude-3-5-sonnet-latest")
            ),
            "max_search_depth": int(
                request.config.getoption("--max-search-depth") or 
                os.environ.get("MAX_SEARCH_DEPTH", "2")
            ),
        }

# Note: Command line options are defined in conftest.py
# These fixtures still work with options defined there

@pytest.mark.langsmith
def test_response_criteria_evaluation(research_agent, search_api, models, eval_model):
    """Test if a report meets the specified quality criteria."""
    print(f"Testing {research_agent} report generation with {search_api} search...")
    print(f"Models: {models}")
    print(f"Eval model: {eval_model}")
    
    # Log inputs to LangSmith
    t.log_inputs({
        "agent_type": research_agent, 
        "search_api": search_api,
        "models": models,
        "eval_model": eval_model,
        "test": "report_quality_evaluation",
        "description": f"Testing report quality for {research_agent} with {search_api}"
    })
 
    # Run the appropriate agent based on the parameter
    if research_agent == "multi_agent":

        # Initial messages
        initial_msg = [{"role": "user", "content": "What is model context protocol?"}]
        followup_msg = [{"role": "user", "content": "high-level overview of MCP, tell me about interesting specific MCP servers, developer audience, just focus on MCP. generate the report now and don't ask any more follow-up questions."}]

        # Checkpointer for the multi-agent approach
        checkpointer = MemorySaver()
        graph = supervisor_builder.compile(checkpointer=checkpointer)

        # Create configuration with the provided parameters
        config = {
            "thread_id": str(uuid.uuid4()),
            "search_api": search_api,
            "supervisor_model": models.get("supervisor_model"),
            "researcher_model": models.get("researcher_model"),
        }
        
        thread_config = {"configurable": config}

        # Run the workflow with asyncio
        asyncio.run(graph.ainvoke({"messages": initial_msg}, config=thread_config))
        asyncio.run(graph.ainvoke({"messages": followup_msg}, config=thread_config))
        
        # Get the final state once both invocations are complete
        final_state = graph.get_state(thread_config)
        print(f"Final state values: {final_state.values}")
        report = final_state.values.get('final_report', "No report generated")
        print(f"Report length: {len(report)}")

    elif research_agent == "graph":
        
        # Topic query 
        topic_query = "What is model context protocol? high-level overview of MCP, tell me about interesting specific MCP servers, developer audience, just focus on MCP"
   
        # Checkpointer for the graph approach
        checkpointer = MemorySaver()
        graph = builder.compile(checkpointer=checkpointer)
        
        # Configuration for the graph agent with provided parameters
        thread = {"configurable": {
            "thread_id": str(uuid.uuid4()),
            "search_api": search_api,
            "planner_provider": models.get("planner_provider", "anthropic"),
            "planner_model": models.get("planner_model", "claude-3-7-sonnet-latest"),
            "writer_provider": models.get("writer_provider", "anthropic"),
            "writer_model": models.get("writer_model", "claude-3-5-sonnet-latest"),
            "max_search_depth": models.get("max_search_depth", 2),
        }}
        
        async def run_graph_agent(thread):    
            # Run the graph until the interruption
            async for event in graph.astream({"topic":topic_query}, thread, stream_mode="updates"):
                if '__interrupt__' in event:
                    interrupt_value = event['__interrupt__'][0].value

            # Pass True to approve the report plan 
            async for event in graph.astream(Command(resume=True), thread, stream_mode="updates"):
                print(event)
                print("\n")
            
            final_state = graph.get_state(thread)
            report = final_state.values.get('final_report', "No report generated")
            return report
    
        report = asyncio.run(run_graph_agent(thread))

    # Get evaluation LLM using the specified model
    criteria_eval_structured_llm = get_evaluation_llm(eval_model)
    
    # Evaluate the report against our quality criteria
    eval_result = criteria_eval_structured_llm.invoke([
        {"role": "system", "content": RESPONSE_CRITERIA_SYSTEM_PROMPT},
        {"role": "user", "content": f"""\n\n Report: \n\n{report}\n\nEvaluate whether the report meets the criteria and provide detailed justification for your evaluation."""}
    ])

    # Extract section headers for analysis
    import re
    section_headers = re.findall(r'##\s+([^\n]+)', report)
    
    # Print the evaluation results
    print(f"Evaluation result: {'PASSED' if eval_result.grade else 'FAILED'}")
    print(f"Report contains {len(section_headers)} sections: {', '.join(section_headers)}")
    print(f"Justification: {eval_result.justification}")
    
    # Log outputs to LangSmith
    t.log_outputs({
        "report": report,
        "evaluation_result": eval_result.grade,
        "justification": eval_result.justification,
        "report_length": len(report),
        "section_count": len(section_headers),
        "section_headers": section_headers,
    })
    
    # Test passes if the evaluation criteria are met
    assert eval_result.grade

